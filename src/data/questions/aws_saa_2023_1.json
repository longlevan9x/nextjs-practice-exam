[
  {
    "id": 1,
    "question": "A company operates a dynamic web application on Amazon EC2 instances spread across several Availability Zones. The EC2 instances are located in private subnets. A solutions architect launches an internet-facing Network Load Balancer (NLB) and adds the EC2 instances to the target group. However, the instances are not receiving internet traffic.\n\nHow should the solutions architect modify the architecture to rectify this problem?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Replace the NLB with an Application Load Balancer (ALB). Set up a NAT gateway in a public subnet to facilitate internet traffic.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Transfer the EC2 instances to public subnets. Modify the security group rules for the EC2 instances to allow outbound traffic to 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create public subnets in each Availability Zone. Associate the public subnets with the NLB. Modify the route tables for the public subnets to include a route to the private subnets.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Update the route tables of the subnets where EC2 instances are situated to route 0.0.0.0/0 traffic via the internet gateway. Adjust the security group rules of the EC2 instances to allow outbound traffic to 0.0.0.0/0.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nCreate public subnets in each Availability Zone. Associate the public subnets with the NLB. Modify the route tables for the public subnets to include a route to the private subnets.\n\nNetwork Load Balancer (NLB) operates at the fourth layer of the Open Systems Interconnection (OSI) model. It is designed to handle millions of requests per second while maintaining high throughput at ultra-low latency, with no effort on your part.\n\nFor the internet traffic to reach the instances in the private subnets, you should associate the NLB with public subnets and then modify the route tables to include a route from the public subnet to the private subnets where the instances are located.\n\nNLB is capable of routing requests to targets, like Amazon EC2 instances, located within Amazon Virtual Private Cloud (VPC) and are specified by IP address, irrespective of the Availability Zone. The public subnet is internet-facing whereas the private subnet does not have a direct route to the internet. By associating the NLB with the public subnets and creating a route from the public subnets to the private subnets, traffic can reach the EC2 instances in the private subnets.\n\n\n\n\n\n\n\nIncorrect Options:\n\nReplace the NLB with an Application Load Balancer (ALB). Set up a NAT gateway in a public subnet to facilitate internet traffic.\n\nAn Application Load Balancer (ALB) does allow access from the internet to private subnets, but replacing the NLB with an ALB may not be necessary. Additionally, a NAT gateway allows instances in a private subnet to access the internet but not the other way around.\n\n\n\n\nTransfer the EC2 instances to public subnets. Modify the security group rules for the EC2 instances to allow outbound traffic to 0.0.0.0/0.\n\nMoving EC2 instances to a public subnet might expose them to additional risks. It is a best practice to keep instances that process data in private subnets for increased security. Allowing all outbound traffic does not solve the problem of allowing inbound internet traffic.\n\n\n\n\nUpdate the route tables of the subnets where EC2 instances are situated to route 0.0.0.0/0 traffic via the internet gateway. Adjust the security group rules of the EC2 instances to allow outbound traffic to 0.0.0.0/0.\n\nUpdating the route table to allow internet traffic into private subnets would not make the instances accessible because the Network Load Balancer (NLB) does not perform NAT. Furthermore, allowing all outbound traffic does not solve the problem of allowing inbound internet traffic.\n\n\n\n\n\n\n\nReferences:\n\nhttps://repost.aws/knowledge-center/public-load-balancer-private-ec2",
    "correctAnswerExplanations": [
      {
        "answer": "Create public subnets in each Availability Zone. Associate the public subnets with the NLB. Modify the route tables for the public subnets to include a route to the private subnets.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Network Load Balancer (NLB) operates at the fourth layer of the Open Systems Interconnection (OSI) model. It is designed to handle millions of requests per second while maintaining high throughput at ultra-low latency, with no effort on your part."
      },
      {
        "answer": "",
        "explanation": "For the internet traffic to reach the instances in the private subnets, you should associate the NLB with public subnets and then modify the route tables to include a route from the public subnet to the private subnets where the instances are located."
      },
      {
        "answer": "",
        "explanation": "NLB is capable of routing requests to targets, like Amazon EC2 instances, located within Amazon Virtual Private Cloud (VPC) and are specified by IP address, irrespective of the Availability Zone. The public subnet is internet-facing whereas the private subnet does not have a direct route to the internet. By associating the NLB with the public subnets and creating a route from the public subnets to the private subnets, traffic can reach the EC2 instances in the private subnets."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Replace the NLB with an Application Load Balancer (ALB). Set up a NAT gateway in a public subnet to facilitate internet traffic.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An Application Load Balancer (ALB) does allow access from the internet to private subnets, but replacing the NLB with an ALB may not be necessary. Additionally, a NAT gateway allows instances in a private subnet to access the internet but not the other way around."
      },
      {
        "answer": "Transfer the EC2 instances to public subnets. Modify the security group rules for the EC2 instances to allow outbound traffic to 0.0.0.0/0.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Moving EC2 instances to a public subnet might expose them to additional risks. It is a best practice to keep instances that process data in private subnets for increased security. Allowing all outbound traffic does not solve the problem of allowing inbound internet traffic."
      },
      {
        "answer": "Update the route tables of the subnets where EC2 instances are situated to route 0.0.0.0/0 traffic via the internet gateway. Adjust the security group rules of the EC2 instances to allow outbound traffic to 0.0.0.0/0.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Updating the route table to allow internet traffic into private subnets would not make the instances accessible because the Network Load Balancer (NLB) does not perform NAT. Furthermore, allowing all outbound traffic does not solve the problem of allowing inbound internet traffic."
      }
    ],
    "references": [
      "https://repost.aws/knowledge-center/public-load-balancer-private-ec2"
    ]
  },
  {
    "id": 2,
    "question": "A multinational company stores confidential information across multiple Amazon S3 buckets. They require a solution to audit and control the configurations of all these buckets while ensuring strict compliance by tracking any configuration changes. Moreover, the system should automatically identify any buckets that might potentially be publicly accessible to prevent data leaks.\n\nAs a Solution Architect, which of the following strategies would you recommend to meet these requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS IAM to generate a credential report.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS CloudTrail and review the event history.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS Config rules to govern configurations.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use AWS Trusted Advisor for comprehensive analysis.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nUse AWS Config rules to govern configurations.\n\nAWS Config enables you to assess, audit, and evaluate the configurations of your AWS resources. It continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines.\n\nAWS Config allows you to create Config rules, which represent your ideal configuration settings. These rules can help you to identify any buckets that are potentially publicly accessible. Once a rule is created, AWS Config runs an evaluation for the resources specified in the rule. If a resource violates a rule, AWS Config flags the resource and the details about the current and desired configurations can be reviewed in the AWS Config console.\n\nFor Example, When a new EC2 volume is created, AWS Config has the capability to assess an EC2 volume's compliance with encryption rules upon creation. In the event that the volume is not encrypted, both the volume itself and the rule are marked as noncompliant by AWS Config. Moreover, AWS Config is able to verify if all your resources meet account-wide criteria. For example, it can ensure that the total number of EC2 volumes in an account remains within a specified range or determine if AWS CloudTrail is being used for logging purposes.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS CloudTrail and review the event history.\n\nAWS CloudTrail provides event history of your AWS account activity, it is focused on API activity rather than resource configuration. Therefore, it won't fulfill the need of auditing S3 bucket configurations and tracking compliance.\n\n\n\n\nUse AWS IAM to generate a credential report.\n\nAWS IAM credential report provides information about the users in the AWS account, including the access key and password information. However, it does not provide insights into S3 bucket configurations, thus wouldn't help you track bucket configurations and compliance.\n\n\n\n\nUse AWS Trusted Advisor for comprehensive analysis.\n\nAWS Trusted Advisor provides real-time guidance to help provision your resources following AWS best practices, but it does not have the capability to continuously monitor configurations and evaluate them against specific rules like AWS Config does.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/config\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Config rules to govern configurations.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Config enables you to assess, audit, and evaluate the configurations of your AWS resources. It continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines."
      },
      {
        "answer": "",
        "explanation": "AWS Config allows you to create Config rules, which represent your ideal configuration settings. These rules can help you to identify any buckets that are potentially publicly accessible. Once a rule is created, AWS Config runs an evaluation for the resources specified in the rule. If a resource violates a rule, AWS Config flags the resource and the details about the current and desired configurations can be reviewed in the AWS Config console."
      },
      {
        "answer": "",
        "explanation": "For Example, When a new EC2 volume is created, AWS Config has the capability to assess an EC2 volume's compliance with encryption rules upon creation. In the event that the volume is not encrypted, both the volume itself and the rule are marked as noncompliant by AWS Config. Moreover, AWS Config is able to verify if all your resources meet account-wide criteria. For example, it can ensure that the total number of EC2 volumes in an account remains within a specified range or determine if AWS CloudTrail is being used for logging purposes."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use AWS CloudTrail and review the event history.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS CloudTrail provides event history of your AWS account activity, it is focused on API activity rather than resource configuration. Therefore, it won't fulfill the need of auditing S3 bucket configurations and tracking compliance."
      },
      {
        "answer": "Use AWS IAM to generate a credential report.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS IAM credential report provides information about the users in the AWS account, including the access key and password information. However, it does not provide insights into S3 bucket configurations, thus wouldn't help you track bucket configurations and compliance."
      },
      {
        "answer": "Use AWS Trusted Advisor for comprehensive analysis.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Trusted Advisor provides real-time guidance to help provision your resources following AWS best practices, but it does not have the capability to continuously monitor configurations and evaluate them against specific rules like AWS Config does."
      }
    ],
    "references": [
      "https://aws.amazon.com/config",
      "https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html"
    ]
  },
  {
    "id": 3,
    "question": "An organization has just rolled out its retail website to customers worldwide. The platform runs on multiple Amazon EC2 instances, which are placed behind an Elastic Load Balancer and are set up within an Auto Scaling group across multiple Availability Zones.\n\nThe organization intends to deliver various versions to customers based on the devices they use to browse the website.\n\nWhich pair of steps should a solutions architect take to fulfill these requirements? (Select TWO.)",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure a Lambda@Edge function to send specific objects to consumers based on the User-Agent header.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Configure AWS Global Accelerator. Route requests to a Network Load Balancer (NLB). Configure the NLB to set host-oriented routing to different EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure AWS Global Accelerator. Route requests to a Network Load Balancer (NLB). Configure the NLB to set path-oriented routing to different EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure a host header within a Network Load Balancer to direct traffic to different instances.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Configure Amazon CloudFront to hold multiple versions in its cache.",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nConfigure Amazon CloudFront to hold multiple content versions in its cache.\n\nAmazon CloudFront is a content delivery network (CDN), which is designed to deliver data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, and within a developer-friendly environment. You can configure CloudFront to cache multiple versions of content, allowing different versions of your application to be served to users depending on their device type or other criteria.\n\n\n\n\nConfigure a Lambda@Edge function to send specific objects to consumers based on the User-Agent header.\n\nLambda@Edge is a feature of Amazon CloudFront that allows developers to execute custom code closer to users of the application, which improves performance and reduces latency. It can be used to inspect the User-Agent HTTP header, identify the device type, and modify the HTTP responses to serve different versions of content based on the type of device. This provides a serverless solution to dynamically adjust the content served to users based on their device type.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure a host header within a Network Load Balancer to direct traffic to different instances.\n\nNetwork Load Balancer (NLB) is not capable of inspecting the host header or any other application-level attributes to route traffic. NLB operates at the Transport Layer (Layer 4) of the OSI model and makes routing decisions based on IP protocol data, such as source IP address, source port, destination IP address, and destination port.\n\n\n\n\nConfigure AWS Global Accelerator. Route requests to a Network Load Balancer (NLB). Configure the NLB to set host-oriented routing to different EC2 instances.\n\nAWS Global Accelerator is used to improve the availability and performance of the applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions. However, the Network Load Balancer (NLB) in the configuration cannot make decisions based on the host header or application-level data to route traffic to different EC2 instances.\n\n\n\n\nConfigure AWS Global Accelerator. Route requests to a Network Load Balancer (NLB). Configure the NLB to set path-oriented routing to different EC2 instances.\n\nNetwork Load Balancer (NLB) does not support path-based routing. Path-based routing is a feature of Application Load Balancer (ALB) that routes requests to different target groups based on the URL path in the request.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/lambda/edge\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon CloudFront to hold multiple content versions in its cache.</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront is a content delivery network (CDN), which is designed to deliver data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, and within a developer-friendly environment. You can configure CloudFront to cache multiple versions of content, allowing different versions of your application to be served to users depending on their device type or other criteria."
      },
      {
        "answer": "Configure a Lambda@Edge function to send specific objects to consumers based on the User-Agent header.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Lambda@Edge is a feature of Amazon CloudFront that allows developers to execute custom code closer to users of the application, which improves performance and reduces latency. It can be used to inspect the User-Agent HTTP header, identify the device type, and modify the HTTP responses to serve different versions of content based on the type of device. This provides a serverless solution to dynamically adjust the content served to users based on their device type."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Configure a host header within a Network Load Balancer to direct traffic to different instances.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Network Load Balancer (NLB) is not capable of inspecting the host header or any other application-level attributes to route traffic. NLB operates at the Transport Layer (Layer 4) of the OSI model and makes routing decisions based on IP protocol data, such as source IP address, source port, destination IP address, and destination port."
      },
      {
        "answer": "Configure AWS Global Accelerator. Route requests to a Network Load Balancer (NLB). Configure the NLB to set host-oriented routing to different EC2 instances.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Global Accelerator is used to improve the availability and performance of the applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions. However, the Network Load Balancer (NLB) in the configuration cannot make decisions based on the host header or application-level data to route traffic to different EC2 instances."
      },
      {
        "answer": "Configure AWS Global Accelerator. Route requests to a Network Load Balancer (NLB). Configure the NLB to set path-oriented routing to different EC2 instances.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Network Load Balancer (NLB) does not support path-based routing. Path-based routing is a feature of Application Load Balancer (ALB) that routes requests to different target groups based on the URL path in the request."
      }
    ],
    "references": [
      "https://aws.amazon.com/lambda/edge",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html"
    ]
  },
  {
    "id": 4,
    "question": "A media company requires to maintain its AWS CloudFront access logs for a span of 5 years. The company is using AWS Organizations across multiple AWS accounts, managed from a master account. The CloudFront logs are sent to an S3 bucket, which has S3 Versioning enabled. To automatically delete current objects after 5 years, an S3 Lifecycle policy has been created.\n\nIn the sixth year of using the S3 bucket, despite the consistent rate of new CloudFront logs being sent to the S3 bucket, the company notices an ongoing increase in the number of objects in the bucket.\n\nWhat is the most cost-effective solution to eliminate objects that exceed the 5-year mark?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure the master account as the owner of all objects sent to the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an AWS Lambda function to locate and remove objects in Amazon S3 older than 5 years.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure the organization’s unified CloudFront access logs to expire objects post 5 years.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure the S3 Lifecycle policy to delete both previous and current versions of the objects.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nConfigure the S3 Lifecycle policy to delete both previous and current versions of the objects.\n\nAn S3 Lifecycle policy is a set of rules that define actions AWS S3 should apply to a group of objects. These actions can be transition actions (like moving objects between different storage classes) or expiration actions (defining when objects expire). The ongoing increase in the number of objects, despite a consistent rate of incoming logs, suggests that only the current versions of the objects are being deleted by the existing Lifecycle policy. This means the previous versions, which are maintained due to S3 Versioning, are accumulating in the bucket. Configuring the S3 Lifecycle policy to delete both previous and current versions of the objects would effectively and cost-efficiently eliminate all objects that exceed the 5-year mark, thus controlling the overall object count and associated costs.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure the organization’s unified CloudFront access logs to expire objects post 5 years.\n\nCloudFront access logs themselves do not have an expiration feature. They are stored in an S3 bucket and expiration is controlled using S3 Lifecycle policies.\n\n\n\n\nCreate an AWS Lambda function to locate and remove objects in Amazon S3 older than 5 years.\n\nThis could work, it's not as cost-effective as using an S3 Lifecycle policy. Lifecycle policies are built into S3 and do not incur extra costs, unlike Lambda invocations which would lead to additional charges.\n\n\n\n\nConfigure the master account as the owner of all objects sent to the S3 bucket.\n\nThis wouldn't help in reducing the number of objects in the bucket or control their lifetime. Ownership of objects doesn't affect their expiration or deletion.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure the S3 Lifecycle policy to delete both previous and current versions of the objects.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An S3 Lifecycle policy is a set of rules that define actions AWS S3 should apply to a group of objects. These actions can be transition actions (like moving objects between different storage classes) or expiration actions (defining when objects expire). The ongoing increase in the number of objects, despite a consistent rate of incoming logs, suggests that only the current versions of the objects are being deleted by the existing Lifecycle policy. This means the previous versions, which are maintained due to S3 Versioning, are accumulating in the bucket. Configuring the S3 Lifecycle policy to delete both previous and current versions of the objects would effectively and cost-efficiently eliminate all objects that exceed the 5-year mark, thus controlling the overall object count and associated costs."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Configure the organization’s unified CloudFront access logs to expire objects post 5 years.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "CloudFront access logs themselves do not have an expiration feature. They are stored in an S3 bucket and expiration is controlled using S3 Lifecycle policies."
      },
      {
        "answer": "Create an AWS Lambda function to locate and remove objects in Amazon S3 older than 5 years.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This could work, it's not as cost-effective as using an S3 Lifecycle policy. Lifecycle policies are built into S3 and do not incur extra costs, unlike Lambda invocations which would lead to additional charges."
      },
      {
        "answer": "Configure the master account as the owner of all objects sent to the S3 bucket.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This wouldn't help in reducing the number of objects in the bucket or control their lifetime. Ownership of objects doesn't affect their expiration or deletion."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html"
    ]
  },
  {
    "id": 5,
    "question": "A software company is developing a cloud-based project management tool. The application will use multiple Amazon EC2 instances with Amazon EBS volumes, connected to an Application Load Balancer (ALB), and an Amazon RDS for PostgreSQL database. The company has a strict policy for all data to be encrypted both in transit and at rest.\n\nWhich solution will fulfill these requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Certificate Manager (ACM) certificates to the ALB for encrypting data in transit, and use AWS Key Management Service (AWS KMS) to encrypt EBS volumes and RDS database storage at rest.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use the AWS root account to access the AWS Management Console, upload the company's encryption certificates, and from the root account, select the option to enable encryption for all data at rest and in transit for the account.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use VeraCrypt to encrypt all data at rest, import the company's TLS certificate keys into AWS Key Management Service (AWS KMS), and attach the KMS keys to the ALB for encrypting data in transit.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Key Management Service (AWS KMS) for encrypting EBS volumes and RDS database storage at rest, and attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nUse AWS Key Management Service (AWS KMS) for encrypting EBS volumes and RDS database storage at rest, and attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.\n\nAWS Key Management Service (KMS) can be used for encryption at rest. KMS offers a highly available key storage, management, and auditing solution for encrypting data within your own applications and controlling access to this data across the AWS services. By using AWS KMS, EBS volumes and RDS database storage can be encrypted, hence ensuring the data at rest is secure.\n\nAWS Certificate Manager (ACM) can handle the complexity of creating, storing, and renewing public and private SSL/TLS X.509 certificates that can be used for secure network communications and to establish the identity of websites over the Internet. By attaching an ACM certificate to the Application Load Balancer (ALB), all data in transit between the ALB and the clients will be encrypted. The ALB automatically handles the decryption of requests and encryption of responses, which can simplify the application design.\n\nUsing the combination of AWS KMS for encryption at rest and ACM for encryption in transit, ensures that the company's policy for all data to be encrypted both in transit and at rest is satisfied, while taking advantage of managed, scalable, and highly available services.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS Certificate Manager (ACM) certificates to the ALB for encrypting data in transit, and use AWS Key Management Service (AWS KMS) to encrypt EBS volumes and RDS database storage at rest.\n\nIt's true that ACM and KMS are used for encryption, in this option their use cases are swapped, which is incorrect.\n\n\n\n\nUse the AWS root account to access the AWS Management Console, upload the company's encryption certificates, and from the root account, select the option to enable encryption for all data at rest and in transit for the account.\n\nUsing the root account for such purposes is a security risk and against best practices. Besides, AWS doesn't provide a one-click option to enable encryption for all data at rest and in transit for an entire account.\n\n\n\n\nUse VeraCrypt to encrypt all data at rest, import the company's TLS certificate keys into AWS Key Management Service (AWS KMS), and attach the KMS keys to the ALB for encrypting data in transit.\n\nVeraCrypt is a third-party tool and is unnecessary because AWS provides its own managed services for encryption. Importing TLS certificate keys into AWS KMS and attaching them to the ALB is not a standard or recommended AWS practice.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/overview.html\n\nhttps://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html\n\nhttps://repost.aws/knowledge-center/associate-acm-certificate-alb-nlb",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Key Management Service (AWS KMS) for encrypting EBS volumes and RDS database storage at rest, and attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Key Management Service (KMS) can be used for encryption at rest. KMS offers a highly available key storage, management, and auditing solution for encrypting data within your own applications and controlling access to this data across the AWS services. By using AWS KMS, EBS volumes and RDS database storage can be encrypted, hence ensuring the data at rest is secure."
      },
      {
        "answer": "",
        "explanation": "AWS Certificate Manager (ACM) can handle the complexity of creating, storing, and renewing public and private SSL/TLS X.509 certificates that can be used for secure network communications and to establish the identity of websites over the Internet. By attaching an ACM certificate to the Application Load Balancer (ALB), all data in transit between the ALB and the clients will be encrypted. The ALB automatically handles the decryption of requests and encryption of responses, which can simplify the application design."
      },
      {
        "answer": "",
        "explanation": "Using the combination of AWS KMS for encryption at rest and ACM for encryption in transit, ensures that the company's policy for all data to be encrypted both in transit and at rest is satisfied, while taking advantage of managed, scalable, and highly available services."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use AWS Certificate Manager (ACM) certificates to the ALB for encrypting data in transit, and use AWS Key Management Service (AWS KMS) to encrypt EBS volumes and RDS database storage at rest.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "It's true that ACM and KMS are used for encryption, in this option their use cases are swapped, which is incorrect."
      },
      {
        "answer": "Use the AWS root account to access the AWS Management Console, upload the company's encryption certificates, and from the root account, select the option to enable encryption for all data at rest and in transit for the account.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Using the root account for such purposes is a security risk and against best practices. Besides, AWS doesn't provide a one-click option to enable encryption for all data at rest and in transit for an entire account."
      },
      {
        "answer": "Use VeraCrypt to encrypt all data at rest, import the company's TLS certificate keys into AWS Key Management Service (AWS KMS), and attach the KMS keys to the ALB for encrypting data in transit.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "VeraCrypt is a third-party tool and is unnecessary because AWS provides its own managed services for encryption. Importing TLS certificate keys into AWS KMS and attaching them to the ALB is not a standard or recommended AWS practice."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/overview.html",
      "https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html",
      "https://repost.aws/knowledge-center/associate-acm-certificate-alb-nlb"
    ]
  },
  {
    "id": 6,
    "question": "A company operates an application that uploads hundreds of .xml files into an Amazon S3 bucket every hour. The files are around 1 GB in size. Each time a file is uploaded, the company needs to convert the file into an Apache Parquet format and place the output file into a different S3 bucket.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application deposits the .xml files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Apache Parquet format, and place the output files into an S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Apache Spark job to read the .xml files, convert the files into Apache Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an AWS Lambda function to download the .xml files, convert the files into Apache Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an AWS Glue extract, transform, and load (ETL) job to convert the .xml files into Apache Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to trigger the ETL job.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nCreate an AWS Glue extract, transform, and load (ETL) job to convert the .xml files into Apache Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to trigger the ETL job.\n\nAWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for users to prepare and load their data for analytics. AWS Glue can read .xml files, perform transformations on them, and then output the data in a different format, such as Apache Parquet. Given the large size and quantity of the files, an ETL job is more efficient and suitable than directly handling the files via a Lambda function.\n\nThis solution minimizes operational overhead as AWS Glue is a serverless service, requiring no infrastructure to set up or manage. Additionally, the ETL jobs can handle large volumes of data and the associated compute resources scale automatically to accommodate the workload. The Lambda function, while a serverless compute service itself, is lightweight in this case as it is merely triggering the Glue job in response to the S3 event, not performing the heavy lifting of transforming large .xml files.\n\nTherefore, using AWS Glue ETL jobs for transformation along with AWS Lambda for triggering the ETL job on each S3 PUT event would meet the requirements with the least operational overhead.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an AWS Lambda function to download the .xml files, convert the files into Apache Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.\n\nThis option is not suitable because Lambda functions have a maximum execution duration per request of 15 minutes and a deployment package size limit of 250 MB. Given that the .xml files are around 1 GB in size, processing such large files within a Lambda function is not feasible.\n\n\n\n\nCreate an Apache Spark job to read the .xml files, convert the files into Apache Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.\n\nWhile an Apache Spark job can handle the data transformation, managing a Spark cluster requires a significant operational overhead and is not considered serverless. This option doesn't meet the requirement to minimize operational overhead.\n\n\n\n\nCreate an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application deposits the .xml files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Apache Parquet format, and place the output files into an S3 bucket.\n\nThis option introduces unnecessary complexity and potential delay. AWS Glue crawlers, Athena queries, and scheduled Lambda functions add overhead and delay in processing files as they are uploaded. This solution does not meet the requirement of processing the .xml files as soon as they are uploaded.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-data-to-apache-parquet.html\n\nhttps://aws.amazon.com/blogs/database/how-to-extract-transform-and-load-data-for-analytic-processing-using-aws-glue-part-2",
    "correctAnswerExplanations": [
      {
        "answer": "Create an AWS Glue extract, transform, and load (ETL) job to convert the .xml files into Apache Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to trigger the ETL job.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for users to prepare and load their data for analytics. AWS Glue can read .xml files, perform transformations on them, and then output the data in a different format, such as Apache Parquet. Given the large size and quantity of the files, an ETL job is more efficient and suitable than directly handling the files via a Lambda function."
      },
      {
        "answer": "",
        "explanation": "This solution minimizes operational overhead as AWS Glue is a serverless service, requiring no infrastructure to set up or manage. Additionally, the ETL jobs can handle large volumes of data and the associated compute resources scale automatically to accommodate the workload. The Lambda function, while a serverless compute service itself, is lightweight in this case as it is merely triggering the Glue job in response to the S3 event, not performing the heavy lifting of transforming large .xml files."
      },
      {
        "answer": "",
        "explanation": "Therefore, using AWS Glue ETL jobs for transformation along with AWS Lambda for triggering the ETL job on each S3 PUT event would meet the requirements with the least operational overhead."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create an AWS Lambda function to download the .xml files, convert the files into Apache Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option is not suitable because Lambda functions have a maximum execution duration per request of 15 minutes and a deployment package size limit of 250 MB. Given that the .xml files are around 1 GB in size, processing such large files within a Lambda function is not feasible."
      },
      {
        "answer": "Create an Apache Spark job to read the .xml files, convert the files into Apache Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "While an Apache Spark job can handle the data transformation, managing a Spark cluster requires a significant operational overhead and is not considered serverless. This option doesn't meet the requirement to minimize operational overhead."
      },
      {
        "answer": "Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application deposits the .xml files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Apache Parquet format, and place the output files into an S3 bucket.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option introduces unnecessary complexity and potential delay. AWS Glue crawlers, Athena queries, and scheduled Lambda functions add overhead and delay in processing files as they are uploaded. This solution does not meet the requirement of processing the .xml files as soon as they are uploaded."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-data-to-apache-parquet.html",
      "https://aws.amazon.com/blogs/database/how-to-extract-transform-and-load-data-for-analytic-processing-using-aws-glue-part-2"
    ]
  },
  {
    "id": 7,
    "question": "A corporation is developing a system that will report Amazon RDS instance scaling events for all the applications within an AWS account. The corporation wants to use a serverless approach to deposit the RDS instance scaling status data in Amazon S3. The data in Amazon S3 will be then used to provide near-real-time updates on a dashboard. The solution must not impact the speed of RDS instance provisioning.\n\nHow should the corporation transfer the data to Amazon S3 to satisfy these demands?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use an Amazon CloudWatch metric stream to send the RDS instance scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Launch an Amazon EMR cluster to collect the RDS instance scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use a bootstrap script during the launch of an RDS instance to install Amazon Kinesis Agent. Configure Kinesis Agent to gather the RDS instance scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Set up the Lambda function to deliver the RDS instance scaling status data directly to Amazon S3.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nUse an Amazon CloudWatch metric stream to send the RDS instance scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.\n\nA CloudWatch metric stream is a feature that you can use to continuously stream CloudWatch metrics to a destination in near-real-time. Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data into Amazon S3, among other AWS services.\n\nUsing a CloudWatch metric stream to send RDS instance scaling status data to Amazon Kinesis Data Firehose and then storing the data in Amazon S3 would be an effective solution. It would allow for near-real-time updates for the dashboard, while also being a serverless approach which would not impact the speed of RDS instance provisioning. This is because the metric stream and Kinesis Data Firehose operate independently of the RDS instances themselves, only receiving and processing the metric data that RDS sends to CloudWatch.\n\n\n\n\n\n\n\nIncorrect Options:\n\nLaunch an Amazon EMR cluster to collect the RDS instance scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.\n\nLaunching an EMR cluster for collecting RDS instance scaling status data is an overkill for the requirement. EMR is used for big data processing and analysis, not for simple data collection tasks. Also, this approach isn't serverless and may have an impact on the speed of RDS instance provisioning.\n\n\n\n\nCreate an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Set up the Lambda function to deliver the RDS instance scaling status data directly to Amazon S3.\n\nThis option would involve polling, which is not as efficient or real-time as streaming the metrics data. It could also result in higher costs due to constant Lambda invocations and may not provide near-real-time updates required for the dashboard.\n\n\n\n\nUse a bootstrap script during the launch of an RDS instance to install Amazon Kinesis Agent. Configure Kinesis Agent to gather the RDS instance scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.\n\nAmazon RDS does not provide the ability to run a bootstrap script or install a Kinesis Agent on the underlying server hosting the RDS instance, making this option unfeasible.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use an Amazon CloudWatch metric stream to send the RDS instance scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A CloudWatch metric stream is a feature that you can use to continuously stream CloudWatch metrics to a destination in near-real-time. Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data into Amazon S3, among other AWS services."
      },
      {
        "answer": "",
        "explanation": "Using a CloudWatch metric stream to send RDS instance scaling status data to Amazon Kinesis Data Firehose and then storing the data in Amazon S3 would be an effective solution. It would allow for near-real-time updates for the dashboard, while also being a serverless approach which would not impact the speed of RDS instance provisioning. This is because the metric stream and Kinesis Data Firehose operate independently of the RDS instances themselves, only receiving and processing the metric data that RDS sends to CloudWatch."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Launch an Amazon EMR cluster to collect the RDS instance scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Launching an EMR cluster for collecting RDS instance scaling status data is an overkill for the requirement. EMR is used for big data processing and analysis, not for simple data collection tasks. Also, this approach isn't serverless and may have an impact on the speed of RDS instance provisioning."
      },
      {
        "answer": "Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Set up the Lambda function to deliver the RDS instance scaling status data directly to Amazon S3.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option would involve polling, which is not as efficient or real-time as streaming the metrics data. It could also result in higher costs due to constant Lambda invocations and may not provide near-real-time updates required for the dashboard."
      },
      {
        "answer": "Use a bootstrap script during the launch of an RDS instance to install Amazon Kinesis Agent. Configure Kinesis Agent to gather the RDS instance scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon RDS does not provide the ability to run a bootstrap script or install a Kinesis Agent on the underlying server hosting the RDS instance, making this option unfeasible."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html"
    ]
  },
  {
    "id": 8,
    "question": "An e-commerce company uses several RDS databases for its platform. During a recent security audit, one of the RDS databases was identified as unencrypted.\n\nAs a Solution Architect, what steps should you take to encrypt the RDS database to fulfill the security requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Enable encryption for the RDS instances using either the AWS Console or CLI.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Enable Multi-AZ deployment for the database, enable encryption for the standby instances, close the primary database to trigger the standby database, and then disable Multi-AZ.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an encrypted read replica of the existing database, move it to a separate database, and close the original database.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a snapshot of the current DB instance, create an encrypted replica of that snapshot, and then restore a DB instance from the encrypted snapshot.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate a snapshot of the current DB instance, create an encrypted replica of that snapshot, and then restore a DB instance from the encrypted snapshot.\n\nAmazon RDS does not support enabling encryption for an existing DB instance directly. However, you can create a snapshot of your existing database, which is an exact copy of the database at the time the snapshot was taken. Then you can create a copy of this snapshot but this time, you will have the option to enable encryption. The process is simple, you select the snapshot you created, copy it, and then choose an encryption key while creating the copy.\n\nOnce you have an encrypted snapshot, you can use it to restore a new encrypted DB instance. In our case, you are replacing your unencrypted database with an encrypted version of it. This is the method AWS recommends for encrypting an existing RDS database. It's an effective way to fulfill the company's security requirements without having to start from scratch. However, it's important to plan downtime since the process of creating snapshots and restoring from it will make the database temporarily unavailable.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an encrypted read replica of the existing database, move it to a separate database, and close the original database.\n\nAWS does not support creating an encrypted read replica of an unencrypted database. The encryption status of the source database and the read replica must match.\n\n\n\n\nEnable Multi-AZ deployment for the database, enable encryption for the standby instances, close the primary database to trigger the standby database, and then disable Multi-AZ.\n\nMulti-AZ deployments in AWS do not support enabling or disabling encryption for standby instances separately from the primary instance. The primary and standby instances must have the same encryption settings.\n\n\n\n\nEnable encryption for the RDS instances using either the AWS Console or CLI.\n\nAWS does not allow enabling encryption for existing unencrypted RDS instances directly from AWS Console or CLI.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a snapshot of the current DB instance, create an encrypted replica of that snapshot, and then restore a DB instance from the encrypted snapshot.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon RDS does not support enabling encryption for an existing DB instance directly. However, you can create a snapshot of your existing database, which is an exact copy of the database at the time the snapshot was taken. Then you can create a copy of this snapshot but this time, you will have the option to enable encryption. The process is simple, you select the snapshot you created, copy it, and then choose an encryption key while creating the copy."
      },
      {
        "answer": "",
        "explanation": "Once you have an encrypted snapshot, you can use it to restore a new encrypted DB instance. In our case, you are replacing your unencrypted database with an encrypted version of it. This is the method AWS recommends for encrypting an existing RDS database. It's an effective way to fulfill the company's security requirements without having to start from scratch. However, it's important to plan downtime since the process of creating snapshots and restoring from it will make the database temporarily unavailable."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create an encrypted read replica of the existing database, move it to a separate database, and close the original database.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS does not support creating an encrypted read replica of an unencrypted database. The encryption status of the source database and the read replica must match."
      },
      {
        "answer": "Enable Multi-AZ deployment for the database, enable encryption for the standby instances, close the primary database to trigger the standby database, and then disable Multi-AZ.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Multi-AZ deployments in AWS do not support enabling or disabling encryption for standby instances separately from the primary instance. The primary and standby instances must have the same encryption settings."
      },
      {
        "answer": "Enable encryption for the RDS instances using either the AWS Console or CLI.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS does not allow enabling encryption for existing unencrypted RDS instances directly from AWS Console or CLI."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html"
    ]
  },
  {
    "id": 9,
    "question": "A company's auditing department demands that all application logs be collected in CloudWatch Logs. These logs will be frequently accessed for the initial 60 days and then accessed sporadically.\n\nWhat steps should a solutions architect take to meet these requirements when setting up the logs?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-Infrequent Access (S3 Standard-IA) after 60 days.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket and enable S3 Intelligent-Tiering.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 60 days.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Kinesis Data Firehose as the target. Set the Kinesis Data Firehose stream to retain the logs for 60 days.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nUse Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-Infrequent Access (S3 Standard-IA) after 60 days.\n\nAmazon S3 is a scalable object storage service that allows you to store and retrieve any amount of data from anywhere. This makes it a perfect choice for storing application logs. When combined with the Lifecycle policy feature of S3, you can automatically transition your data to other storage classes when it meets certain criteria.\n\nSince the logs will be frequently accessed for the initial 60 days and then accessed sporadically, you can initially store the logs in the S3 Standard storage class, which offers high durability, availability, and performance object storage for frequently accessed data. After 60 days, you can use the S3 Lifecycle policy to automatically move the data to the S3 Standard-IA (Infrequent Access) storage class, which is cost-effective for data that is accessed less frequently but requires rapid access when needed.\n\nThis solution is cost-effective and aligns well with the usage pattern of the logs. It also automates the process of moving the data between storage classes, reducing the operational overhead.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon Kinesis Data Firehose as the target. Set the Kinesis Data Firehose stream to retain the logs for 60 days.\n\nAmazon Kinesis Data Firehose is used to load streams of data to data stores and analytics tools. However, it does not support data retention for 60 days. It is designed to capture, transform, and load streaming data into other AWS services, not for long-term storage.\n\n\n\n\nUse AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket, and enable S3 Intelligent-Tiering.\n\nAWS CloudTrail provides event history of your AWS account activity, it is not designed for application logs storage. It can be used with Amazon S3 for storage, this doesn't specifically cater to the storage and access pattern described in the scenario.\n\n\n\n\nUse Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 60 days.\n\nCloudWatch Logs does allow you to set an expiration period for log events, after which the events are automatically deleted, it doesn't handle the transition to a lower-cost storage class for infrequent access after the initial 60 days as the scenario requires.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-Infrequent Access (S3 Standard-IA) after 60 days.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 is a scalable object storage service that allows you to store and retrieve any amount of data from anywhere. This makes it a perfect choice for storing application logs. When combined with the Lifecycle policy feature of S3, you can automatically transition your data to other storage classes when it meets certain criteria."
      },
      {
        "answer": "",
        "explanation": "Since the logs will be frequently accessed for the initial 60 days and then accessed sporadically, you can initially store the logs in the S3 Standard storage class, which offers high durability, availability, and performance object storage for frequently accessed data. After 60 days, you can use the S3 Lifecycle policy to automatically move the data to the S3 Standard-IA (Infrequent Access) storage class, which is cost-effective for data that is accessed less frequently but requires rapid access when needed."
      },
      {
        "answer": "",
        "explanation": "This solution is cost-effective and aligns well with the usage pattern of the logs. It also automates the process of moving the data between storage classes, reducing the operational overhead."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Amazon Kinesis Data Firehose as the target. Set the Kinesis Data Firehose stream to retain the logs for 60 days.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is used to load streams of data to data stores and analytics tools. However, it does not support data retention for 60 days. It is designed to capture, transform, and load streaming data into other AWS services, not for long-term storage."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket, and enable S3 Intelligent-Tiering.</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS CloudTrail provides event history of your AWS account activity, it is not designed for application logs storage. It can be used with Amazon S3 for storage, this doesn't specifically cater to the storage and access pattern described in the scenario."
      },
      {
        "answer": "Use Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 60 days.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "CloudWatch Logs does allow you to set an expiration period for log events, after which the events are automatically deleted, it doesn't handle the transition to a lower-cost storage class for infrequent access after the initial 60 days as the scenario requires."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html"
    ]
  },
  {
    "id": 10,
    "question": "A company runs a web application across multiple Amazon EC2 instances, with traffic routed via Amazon Route 53. Occasionally, users experience a failure when trying to access the application. Upon investigation, it was found that the DNS queries occasionally return the IP addresses of unhealthy instances, resulting in these access failures.\n\nWhat can a solutions architect do to eliminate these access failures?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a Route 53 failover routing policy record for each of the EC2 instances and associate a health check with each record.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a Route 53 simple routing policy record for each of the EC2 instances and associate a health check with each record.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon CloudFront distribution with the EC2 instances as its origin servers, and associate a health check with the EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances, and route the traffic from Route 53 to the ALB.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nCreate an Application Load Balancer (ALB) with a health check in front of the EC2 instances, and route the traffic from Route 53 to the ALB.\n\nApplication Load Balancer (ALB) is a fully managed service that automatically distributes incoming application traffic across multiple Amazon EC2 instances. It also conducts health checks on these instances and only routes traffic to healthy ones. Therefore, if you create an ALB with a health check in front of the EC2 instances and configure Amazon Route 53 to route traffic to the ALB, it ensures that users are routed only to healthy instances, eliminating the access failures.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a Route 53 simple routing policy record for each of the EC2 instances and associate a health check with each record.\n\nAmazon Route 53 health checks can monitor the health of EC2 instances, a simple routing policy doesn't support active health checks, meaning it would still return the IP addresses of all instances, regardless of their health status.\n\n\n\n\nCreate a Route 53 failover routing policy record for each of the EC2 instances and associate a health check with each record.\n\nA failover routing policy can route traffic based on the health of your resources, it's designed for active-passive failover scenarios, rather than for distributing traffic across multiple, equally-qualified resources like an ALB can.\n\n\n\n\nCreate an Amazon CloudFront distribution with the EC2 instances as its origin servers, and associate a health check with the EC2 instances.\n\nCloudFront is a CDN service and it's not designed to perform health checks on origin servers. CloudFront doesn't exclude unhealthy EC2 instances from its returned results.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\n\nhttps://aws.amazon.com/elasticloadbalancing/features/#Details_for_Amazon_Elastic_Load_Balancing_Products",
    "correctAnswerExplanations": [
      {
        "answer": "Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances, and route the traffic from Route 53 to the ALB.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Application Load Balancer (ALB) is a fully managed service that automatically distributes incoming application traffic across multiple Amazon EC2 instances. It also conducts health checks on these instances and only routes traffic to healthy ones. Therefore, if you create an ALB with a health check in front of the EC2 instances and configure Amazon Route 53 to route traffic to the ALB, it ensures that users are routed only to healthy instances, eliminating the access failures."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a Route 53 simple routing policy record for each of the EC2 instances and associate a health check with each record.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Route 53 health checks can monitor the health of EC2 instances, a simple routing policy doesn't support active health checks, meaning it would still return the IP addresses of all instances, regardless of their health status."
      },
      {
        "answer": "Create a Route 53 failover routing policy record for each of the EC2 instances and associate a health check with each record.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A failover routing policy can route traffic based on the health of your resources, it's designed for active-passive failover scenarios, rather than for distributing traffic across multiple, equally-qualified resources like an ALB can."
      },
      {
        "answer": "Create an Amazon CloudFront distribution with the EC2 instances as its origin servers, and associate a health check with the EC2 instances.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "CloudFront is a CDN service and it's not designed to perform health checks on origin servers. CloudFront doesn't exclude unhealthy EC2 instances from its returned results."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://aws.amazon.com/elasticloadbalancing/features/#Details_for_Amazon_Elastic_Load_Balancing_Products"
    ]
  },
  {
    "id": 11,
    "question": "A tech company has 500 TB of archived data stored in network attached storage (NAS) in its onsite data center. This archived data is rarely accessed but must be retained for 10 years due to compliance requirements. The company decides to transfer this archived data from its data center to AWS and complete the migration within 3 weeks. The company's public internet connection has 500 Mbps of bandwidth allocated for data transfer.\n\nWhat should a solutions architect recommend for migrating and storing this data at the LOWEST cost?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS DataSync to transfer the data and install a DataSync agent onsite. Use the DataSync task to transfer files from the on-site SAN storage to Amazon S3 Glacier.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up an IPSec VPN connection between the data center and Amazon VPC. Use the AWS CLI to migrate the data from on-premises to Amazon S3 Glacier.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Order AWS Snowball Edge devices to move the data. Use a lifecycle policy to move the files to Amazon S3 Glacier Deep Archive.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nOrder AWS Snowball Edge devices to move the data. Use a lifecycle policy to move the files to Amazon S3 Glacier Deep Archive.\n\nAWS Snowball Edge devices are physical appliances that AWS ships to your location, and they allow you to transfer large amounts of data from your local data center to the AWS cloud. The devices have high storage capacity, which means you can move a huge volume of data, such as the 500 TB in your scenario, without relying on your internet connection. This can be incredibly beneficial when dealing with limited bandwidth or data transfer over the internet that could take a considerable amount of time.\n\nOnce the data is transferred to the Snowball device, it is returned to AWS, where the data is uploaded into the specified S3 bucket. After the data is safely stored in S3, you can use a lifecycle policy to move the files to Amazon S3 Glacier Deep Archive. This storage class is designed for long-term archiving of data that is accessed very infrequently, making it perfect for your needs. It is also the cheapest storage class offered by AWS, helping keep costs low for long-term data retention.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up an IPSec VPN connection between the data center and Amazon VPC. Use the AWS CLI to migrate the data from on-premises to Amazon S3 Glacier.\n\nThis option may not be feasible given the data volume and the limited bandwidth. The time to complete such a migration would likely exceed the specified 3 weeks.\n\n\n\n\nProvision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.\n\nThis is technically possible, the cost of provisioning a Direct Connect link may not be the most cost-effective solution, given the less frequent access to this archived data.\n\n\n\n\nUse AWS DataSync to transfer the data and install a DataSync agent onsite. Use the DataSync task to transfer files from the on-site SAN storage to Amazon S3 Glacier.\n\nAWS DataSync can be used for data transfer, it doesn't support direct transfers to Amazon S3 Glacier. You would first need to transfer to S3 and then transition to Glacier, which would be an additional step and cost.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/snowball\n\nhttps://aws.amazon.com/s3/storage-classes",
    "correctAnswerExplanations": [
      {
        "answer": "Order AWS Snowball Edge devices to move the data. Use a lifecycle policy to move the files to Amazon S3 Glacier Deep Archive.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Snowball Edge devices are physical appliances that AWS ships to your location, and they allow you to transfer large amounts of data from your local data center to the AWS cloud. The devices have high storage capacity, which means you can move a huge volume of data, such as the 500 TB in your scenario, without relying on your internet connection. This can be incredibly beneficial when dealing with limited bandwidth or data transfer over the internet that could take a considerable amount of time."
      },
      {
        "answer": "",
        "explanation": "Once the data is transferred to the Snowball device, it is returned to AWS, where the data is uploaded into the specified S3 bucket. After the data is safely stored in S3, you can use a lifecycle policy to move the files to Amazon S3 Glacier Deep Archive. This storage class is designed for long-term archiving of data that is accessed very infrequently, making it perfect for your needs. It is also the cheapest storage class offered by AWS, helping keep costs low for long-term data retention."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up an IPSec VPN connection between the data center and Amazon VPC. Use the AWS CLI to migrate the data from on-premises to Amazon S3 Glacier.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option may not be feasible given the data volume and the limited bandwidth. The time to complete such a migration would likely exceed the specified 3 weeks."
      },
      {
        "answer": "Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This is technically possible, the cost of provisioning a Direct Connect link may not be the most cost-effective solution, given the less frequent access to this archived data."
      },
      {
        "answer": "Use AWS DataSync to transfer the data and install a DataSync agent onsite. Use the DataSync task to transfer files from the on-site SAN storage to Amazon S3 Glacier.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS DataSync can be used for data transfer, it doesn't support direct transfers to Amazon S3 Glacier. You would first need to transfer to S3 and then transition to Glacier, which would be an additional step and cost."
      }
    ],
    "references": [
      "https://aws.amazon.com/snowball",
      "https://aws.amazon.com/s3/storage-classes"
    ]
  },
  {
    "id": 12,
    "question": "A biotechnology company generates large volumes of data related to a groundbreaking study. The company wishes to make this data available with the least possible latency to various hospitals nationwide for their on-premises, file-based systems. The data files reside in an Amazon S3 bucket with read-only permissions assigned to each hospital.\n\nWhat should a solutions architect propose to meet these requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Attach an Amazon Elastic File System (Amazon EFS) file system to each hospital's on-premises servers.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy an AWS Storage Gateway volume gateway as a virtual machine (VM) at each hospital's on-premises location.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Transfer the files to each hospital's on-premises systems using AWS DataSync for processing.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) at each hospital's on-premises server.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nDeploy an AWS Storage Gateway file gateway as a virtual machine (VM) at each hospital's on-premises server.\n\nAWS Storage Gateway is a hybrid cloud storage service that enables your on-premises applications to seamlessly use AWS cloud storage. It offers file-based, volume-based, and tape-based storage solutions. The file gateway type provides a seamless way to connect to the low-cost, scalable file storage in Amazon S3.\n\nFile Gateway is a type of AWS Storage Gateway for connecting on-premises software applications with cloud-based storage. It provides low-latency access to data through transparent local caching. When your applications request file data, the file gateway first checks the cache. If the data is not cached, the file gateway retrieves the data from your S3 bucket.\n\nEach hospital needs to have low latency access to large volumes of data residing in an S3 bucket. The data is required for on-premises, file-based systems. Deploying an AWS Storage Gateway file gateway as a virtual machine (VM) at each hospital's on-premises location will allow them to interact with the data as if it was locally available, thus reducing latency.\n\n\n\n\n\n\n\nIncorrect Options:\n\nTransfer the files to each hospital's on-premises systems using AWS DataSync for processing.\n\nAWS DataSync is a data transfer service that simplifies, automates, and accelerates moving and replicating data between on-premises storage systems and AWS storage services. It doesn't support constant, low-latency access to the data in S3.\n\n\n\n\nImplement an AWS Storage Gateway volume gateway as a virtual machine (VM) at each hospital's on-premises location.\n\nThe AWS Storage Gateway volume gateway provides block storage to your applications using the iSCSI protocol. It could technically be used to provide access to the data, but it doesn't fit well with the requirement for file-based access on-premises.\n\n\n\n\nAttach an Amazon Elastic File System (Amazon EFS) file system to each hospital's on-premises servers.\n\nAmazon EFS provides a scalable and elastic file system for Linux-based workloads for use with AWS Cloud services. However, it cannot be directly connected to an on-premises server as it's a cloud-based service.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/filegateway/latest/files3/what-is-file-s3.html",
    "correctAnswerExplanations": [
      {
        "answer": "Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) at each hospital's on-premises server.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Storage Gateway is a hybrid cloud storage service that enables your on-premises applications to seamlessly use AWS cloud storage. It offers file-based, volume-based, and tape-based storage solutions. The file gateway type provides a seamless way to connect to the low-cost, scalable file storage in Amazon S3."
      },
      {
        "answer": "",
        "explanation": "File Gateway is a type of AWS Storage Gateway for connecting on-premises software applications with cloud-based storage. It provides low-latency access to data through transparent local caching. When your applications request file data, the file gateway first checks the cache. If the data is not cached, the file gateway retrieves the data from your S3 bucket."
      },
      {
        "answer": "",
        "explanation": "Each hospital needs to have low latency access to large volumes of data residing in an S3 bucket. The data is required for on-premises, file-based systems. Deploying an AWS Storage Gateway file gateway as a virtual machine (VM) at each hospital's on-premises location will allow them to interact with the data as if it was locally available, thus reducing latency."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Transfer the files to each hospital's on-premises systems using AWS DataSync for processing.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS DataSync is a data transfer service that simplifies, automates, and accelerates moving and replicating data between on-premises storage systems and AWS storage services. It doesn't support constant, low-latency access to the data in S3."
      },
      {
        "answer": "",
        "explanation": "<strong>Implement an AWS Storage Gateway volume gateway as a virtual machine (VM) at each hospital's on-premises location.</strong>"
      },
      {
        "answer": "",
        "explanation": "The AWS Storage Gateway volume gateway provides block storage to your applications using the iSCSI protocol. It could technically be used to provide access to the data, but it doesn't fit well with the requirement for file-based access on-premises."
      },
      {
        "answer": "Attach an Amazon Elastic File System (Amazon EFS) file system to each hospital's on-premises servers.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon EFS provides a scalable and elastic file system for Linux-based workloads for use with AWS Cloud services. However, it cannot be directly connected to an on-premises server as it's a cloud-based service."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/filegateway/latest/files3/what-is-file-s3.html"
    ]
  },
  {
    "id": 13,
    "question": "A multimedia organization is deploying a collaborative editing platform in the AWS Cloud. They require the capability to use NFS clients for accessing project data. The solution must be fully managed.\n\nWhich solution should meet these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon Elastic File System (EFS) file system. Mount the file system on the application server.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an Amazon EC2 Linux instance. Install and adjust an NFS server on the instance. Connect the application server to the NFS share.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon FSx for Lustre file system. Connect the file system to the originating server. Link the application server to the file system.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an AWS Storage Gateway file gateway. Create a file share that uses the necessary client protocol. Connect the application server to the file share.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nCreate an Amazon Elastic File System (EFS) file system. Mount the file system on the application server.\n\nAmazon Elastic File System (EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It's designed to provide massively parallel shared access to thousands of Amazon EC2 instances, enabling applications to achieve high levels of aggregate throughput and IOPS with consistent low latencies. As EFS uses the NFS protocol, it can be used by NFS clients, satisfying the organization's requirement for a fully managed solution with NFS client capability.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an Amazon FSx for Lustre file system. Connect the file system to the originating server. Link the application server to the file system.\n\nAmazon FSx for Lustre is a fully managed file system that is optimized for compute-intensive workloads. It doesn’t support the NFS protocol. Rather, it's designed for applications that require fast storage - where you want to work on larger files and require high throughput, like machine learning, high performance computing (HPC), video processing, and financial modeling.\n\n\n\n\nCreate an AWS Storage Gateway file gateway. Create a file share that uses the necessary client protocol. Connect the application server to the file share.\n\nAWS Storage Gateway's file interface, or file gateway, offers you a seamless way to connect to the cloud in order to store application data files and backup images as durable objects on Amazon S3 cloud storage. However, it primarily uses SMB protocol, not NFS.\n\n\n\n\nCreate an Amazon EC2 Linux instance. Install and adjust an NFS server on the instance. Connect the application server to the NFS share.\n\nThis is not a fully managed solution. While this option would technically allow the company to use NFS clients to access data, it would involve manual setup and maintenance of the NFS server on an Amazon EC2 instance. It's a more hands-on approach and would require ongoing administration.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/efs/latest/ug/whatisefs.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create an Amazon Elastic File System (EFS) file system. Mount the file system on the application server.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic File System (EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It's designed to provide massively parallel shared access to thousands of Amazon EC2 instances, enabling applications to achieve high levels of aggregate throughput and IOPS with consistent low latencies. As EFS uses the NFS protocol, it can be used by NFS clients, satisfying the organization's requirement for a fully managed solution with NFS client capability."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create an Amazon FSx for Lustre file system. Connect the file system to the originating server. Link the application server to the file system.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon FSx for Lustre is a fully managed file system that is optimized for compute-intensive workloads. It doesn’t support the NFS protocol. Rather, it's designed for applications that require fast storage - where you want to work on larger files and require high throughput, like machine learning, high performance computing (HPC), video processing, and financial modeling."
      },
      {
        "answer": "Create an AWS Storage Gateway file gateway. Create a file share that uses the necessary client protocol. Connect the application server to the file share.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Storage Gateway's file interface, or file gateway, offers you a seamless way to connect to the cloud in order to store application data files and backup images as durable objects on Amazon S3 cloud storage. However, it primarily uses SMB protocol, not NFS."
      },
      {
        "answer": "Create an Amazon EC2 Linux instance. Install and adjust an NFS server on the instance. Connect the application server to the NFS share.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This is not a fully managed solution. While this option would technically allow the company to use NFS clients to access data, it would involve manual setup and maintenance of the NFS server on an Amazon EC2 instance. It's a more hands-on approach and would require ongoing administration."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html"
    ]
  },
  {
    "id": 14,
    "question": "A company operates a popular mobile app with one million users. The company needs to process user data in near-real time and secure the data via encryption. The data must be stored in a centralized repository in Apache Parquet format for additional processing tasks.\n\nWhich solution will meet these needs while minimizing operational overhead?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up an Amazon Kinesis data stream to save the data in Amazon S3. Use an Amazon Kinesis Data Analytics application to process the data. Invoke an AWS Lambda function to forward the data to the Kinesis Data Analytics application.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use an Amazon Kinesis Data Analytics application to process the data.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Set up an Amazon Kinesis data stream to store the data in Amazon S3. Use an Amazon EMR cluster to process the data. Invoke an AWS Lambda function to forward the data to the EMR cluster.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon Kinesis Data Firehose delivery stream to save the data in Amazon S3. Use an Amazon EMR cluster to process the data.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use an Amazon Kinesis Data Analytics application to process the data.\n\nAmazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon S3, Amazon Redshift, and more. In our case, setting up a Kinesis Data Firehose delivery stream allows data to be ingested in real-time from the app and stored in Amazon S3, with automatic scaling to handle the volume of data from one million users. This setup provides the centralized repository needed. Kinesis Data Firehose also supports automatic data transformation and compression. It can convert incoming data to Apache Parquet format before storing it in Amazon S3, which meets the company's requirement of storing data in Parquet format.\n\nAmazon Kinesis Data Analytics is the easiest way to analyze streaming data in real time, extract actionable insights, and respond to your business and customer needs promptly. It can process and analyze the data in near real-time, directly from the Kinesis Data Firehose, without any need to manually invoke AWS Lambda for data forwarding.\n\nBoth Kinesis Data Firehose and Kinesis Data Analytics are fully managed services, which minimize operational overhead. Also, data stored in Amazon S3 is secure and can be encrypted using AWS Key Management Service (KMS) keys, satisfying the encryption requirement.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up an Amazon Kinesis data stream to save the data in Amazon S3. Use an Amazon Kinesis Data Analytics application to process the data. Invoke an AWS Lambda function to forward the data to the Kinesis Data Analytics application.\n\nThis approach is more complex and has additional operational overhead due to managing the Kinesis Data Stream and the AWS Lambda function. Kinesis Data Firehose can automatically deliver the data to S3, which simplifies the architecture and reduces management overhead.\n\n\n\n\nSet up an Amazon Kinesis data stream to store the data in Amazon S3. Use an Amazon EMR cluster to process the data. Invoke an AWS Lambda function to forward the data to the EMR cluster.\n\nAmazon EMR is typically used for big data processing, not real-time data processing. It also requires more operational overhead in managing the EMR cluster, which contradicts the requirement of minimizing operational overhead.\n\n\n\n\nCreate an Amazon Kinesis Data Firehose delivery stream to save the data in Amazon S3. Use an Amazon EMR cluster to process the data.\n\nAmazon EMR is typically used for big data processing and has additional operational overhead, which does not align with the requirement for real-time processing and minimizing operational overhead.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/kinesis/data-firehose\n\nhttps://aws.amazon.com/kinesis/data-analytics\n\nhttps://aws.amazon.com/blogs/big-data/analyzing-apache-parquet-optimized-data-using-amazon-kinesis-data-firehose-amazon-athena-and-amazon-redshift",
    "correctAnswerExplanations": [
      {
        "answer": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use an Amazon Kinesis Data Analytics application to process the data.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon S3, Amazon Redshift, and more. In our case, setting up a Kinesis Data Firehose delivery stream allows data to be ingested in real-time from the app and stored in Amazon S3, with automatic scaling to handle the volume of data from one million users. This setup provides the centralized repository needed. Kinesis Data Firehose also supports automatic data transformation and compression. It can convert incoming data to Apache Parquet format before storing it in Amazon S3, which meets the company's requirement of storing data in Parquet format."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real time, extract actionable insights, and respond to your business and customer needs promptly. It can process and analyze the data in near real-time, directly from the Kinesis Data Firehose, without any need to manually invoke AWS Lambda for data forwarding."
      },
      {
        "answer": "",
        "explanation": "Both Kinesis Data Firehose and Kinesis Data Analytics are fully managed services, which minimize operational overhead. Also, data stored in Amazon S3 is secure and can be encrypted using AWS Key Management Service (KMS) keys, satisfying the encryption requirement."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up an Amazon Kinesis data stream to save the data in Amazon S3. Use an Amazon Kinesis Data Analytics application to process the data. Invoke an AWS Lambda function to forward the data to the Kinesis Data Analytics application.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This approach is more complex and has additional operational overhead due to managing the Kinesis Data Stream and the AWS Lambda function. Kinesis Data Firehose can automatically deliver the data to S3, which simplifies the architecture and reduces management overhead."
      },
      {
        "answer": "Set up an Amazon Kinesis data stream to store the data in Amazon S3. Use an Amazon EMR cluster to process the data. Invoke an AWS Lambda function to forward the data to the EMR cluster.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon EMR is typically used for big data processing, not real-time data processing. It also requires more operational overhead in managing the EMR cluster, which contradicts the requirement of minimizing operational overhead."
      },
      {
        "answer": "Create an Amazon Kinesis Data Firehose delivery stream to save the data in Amazon S3. Use an Amazon EMR cluster to process the data.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon EMR is typically used for big data processing and has additional operational overhead, which does not align with the requirement for real-time processing and minimizing operational overhead."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-firehose",
      "https://aws.amazon.com/kinesis/data-analytics",
      "https://aws.amazon.com/blogs/big-data/analyzing-apache-parquet-optimized-data-using-amazon-kinesis-data-firehose-amazon-athena-and-amazon-redshift"
    ]
  },
  {
    "id": 15,
    "question": "An organization has deployed an application using AWS Gateway, AWS Lambda, and RDS databases featuring multi-AZ setups. The CEO wishes to ensure that all credentials for the database, API keys, and other secret information are encrypted and rotated regularly to ensure security compliance.\n\nWhich option is the most appropriate to meet these requirements?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Systems Manager Parameter Store with a SecureString data type for storing and managing database credentials, API keys, and other confidential information.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS Secrets Manager for storing and managing database credentials, API keys, and other confidential information.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use AWS KMS for storing and managing database credentials, API keys, and other confidential information.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS ACM for storing and managing database credentials, API keys, and other confidential information.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nUse AWS Secrets Manager for storing and managing database credentials, API keys, and other confidential information.\n\nAWS Secrets Manager protects access to applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Using Secrets Manager, you can secure and manage secrets used to access resources in the AWS Cloud, on third-party services, and on-premises. Secrets Manager also rotates credentials for Amazon RDS, Amazon DocumentDB, and Amazon Redshift, which aligns well with the organization's requirement of regular rotation.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Systems Manager Parameter Store with a SecureString data type for storing and managing database credentials, API keys, and other confidential information.\n\nAWS Systems Manager Parameter Store does provide secure, hierarchical storage for configuration data management and secrets management, but it does not offer functionality to rotate secrets. For such advanced secret management and rotation, AWS Secrets Manager is the recommended service.\n\n\n\n\nUse AWS ACM for storing and managing database credentials, API keys, and other confidential information.\n\nAWS Certificate Manager (ACM) is used to manage Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for your AWS-based websites and applications. It is not designed to manage and rotate database credentials or API keys.\n\n\n\n\nUse AWS KMS for storing and managing database credentials, API keys, and other confidential information.\n\nAWS Key Management Service (KMS) makes it easy to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. KMS can be used to encrypt secrets but it does not provide a secrets management system or rotation capabilities.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/secrets-manager",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Secrets Manager for storing and managing database credentials, API keys, and other confidential information.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Secrets Manager protects access to applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Using Secrets Manager, you can secure and manage secrets used to access resources in the AWS Cloud, on third-party services, and on-premises. Secrets Manager also rotates credentials for Amazon RDS, Amazon DocumentDB, and Amazon Redshift, which aligns well with the organization's requirement of regular rotation."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Systems Manager Parameter Store with a SecureString data type for storing and managing database credentials, API keys, and other confidential information.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Systems Manager Parameter Store does provide secure, hierarchical storage for configuration data management and secrets management, but it does not offer functionality to rotate secrets. For such advanced secret management and rotation, AWS Secrets Manager is the recommended service."
      },
      {
        "answer": "Use AWS ACM for storing and managing database credentials, API keys, and other confidential information.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Certificate Manager (ACM) is used to manage Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for your AWS-based websites and applications. It is not designed to manage and rotate database credentials or API keys."
      },
      {
        "answer": "Use AWS KMS for storing and managing database credentials, API keys, and other confidential information.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Key Management Service (KMS) makes it easy to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. KMS can be used to encrypt secrets but it does not provide a secrets management system or rotation capabilities."
      }
    ],
    "references": [
      "https://aws.amazon.com/secrets-manager"
    ]
  },
  {
    "id": 16,
    "question": "An organization plans to provide its developers with individual AWS accounts for project work. They want to ensure alerts are sent when the Amazon S3 storage usage for any account crosses a specified limit in a given month.\n\nWhat is the MOST cost-effective way a solutions architect can achieve this requirement?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Cost Explorer to create a monthly cost report by service, specifically for S3. Configure Cost Explorer to dispatch an Amazon Simple Email Service (Amazon SES) alert when the usage crosses the defined threshold.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Cost Explorer to generate a daily cost report by service, filtering it for S3. Set up Cost Explorer to send an Amazon Simple Email Service (Amazon SES) alert when the usage goes beyond the specified limit.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS Budgets to create a cost budget for each account. Set the time period to monthly and limit the scope to S3. Create an alert threshold for the budget and configure an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications when the set limit is exceeded.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate this report with Amazon Athena. Schedule an Athena query using Amazon EventBridge and configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when the threshold is exceeded.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nUse AWS Budgets to create a cost budget for each account. Set the time period to monthly and limit the scope to S3. Create an alert threshold for the budget and configure an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications when the set limit is exceeded.\n\nAWS Budgets gives you the ability to set custom cost and usage budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. In this scenario, you can set up a cost budget for each account, specifically for Amazon S3 usage. You can then define an alert threshold for this budget, so that when the Amazon S3 usage costs exceed the defined threshold, an alert is triggered.\n\nAlerts are sent via Amazon SNS, a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. The subscribing endpoints could be an email, a phone number for SMS, or any HTTP/HTTPS endpoint. This way, the organization can monitor the Amazon S3 usage of each account and get alerts when a specified limit is crossed.\n\nThis approach provides the most cost-effective way to monitor and control costs because it directly uses built-in AWS services without incurring additional data processing or analysis costs.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Cost Explorer to generate a daily cost report by service, filtering it for S3. Set up Cost Explorer to send an Amazon Simple Email Service (Amazon SES) alert when the usage goes beyond the specified limit.\n\nCost Explorer cannot trigger alerts or notifications when a threshold is crossed. It's used for visualizing, understanding, and managing AWS costs and usage over time, but doesn't have an in-built alerting capability.\n\n\n\n\nUse Cost Explorer to create a monthly cost report by service, specifically for S3. Configure Cost Explorer to dispatch an Amazon Simple Email Service (Amazon SES) alert when the usage crosses the defined threshold.\n\nCost Explorer does not have a built-in functionality to send notifications when a certain threshold is exceeded.\n\n\n\n\nUse AWS Cost and Usage Reports to create a report with hourly granularity. Integrate this report with Amazon Athena. Schedule an Athena query using Amazon EventBridge and configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when the threshold is exceeded.\n\nThis approach is unnecessarily complex and could be costly due to the use of several services. Additionally, maintaining this solution over time would require more effort compared to the straightforward AWS Budgets solution.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/aws-cost-management/aws-budgets\n\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Budgets to create a cost budget for each account. Set the time period to monthly and limit the scope to S3. Create an alert threshold for the budget and configure an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications when the set limit is exceeded.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Budgets gives you the ability to set custom cost and usage budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. In this scenario, you can set up a cost budget for each account, specifically for Amazon S3 usage. You can then define an alert threshold for this budget, so that when the Amazon S3 usage costs exceed the defined threshold, an alert is triggered."
      },
      {
        "answer": "",
        "explanation": "Alerts are sent via Amazon SNS, a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. The subscribing endpoints could be an email, a phone number for SMS, or any HTTP/HTTPS endpoint. This way, the organization can monitor the Amazon S3 usage of each account and get alerts when a specified limit is crossed."
      },
      {
        "answer": "",
        "explanation": "This approach provides the most cost-effective way to monitor and control costs because it directly uses built-in AWS services without incurring additional data processing or analysis costs."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Cost Explorer to generate a daily cost report by service, filtering it for S3. Set up Cost Explorer to send an Amazon Simple Email Service (Amazon SES) alert when the usage goes beyond the specified limit.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Cost Explorer cannot trigger alerts or notifications when a threshold is crossed. It's used for visualizing, understanding, and managing AWS costs and usage over time, but doesn't have an in-built alerting capability."
      },
      {
        "answer": "Use Cost Explorer to create a monthly cost report by service, specifically for S3. Configure Cost Explorer to dispatch an Amazon Simple Email Service (Amazon SES) alert when the usage crosses the defined threshold.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Cost Explorer does not have a built-in functionality to send notifications when a certain threshold is exceeded."
      },
      {
        "answer": "Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate this report with Amazon Athena. Schedule an Athena query using Amazon EventBridge and configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when the threshold is exceeded.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This approach is unnecessarily complex and could be costly due to the use of several services. Additionally, maintaining this solution over time would require more effort compared to the straightforward AWS Budgets solution."
      }
    ],
    "references": [
      "https://aws.amazon.com/aws-cost-management/aws-budgets",
      "https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html"
    ]
  },
  {
    "id": 17,
    "question": "A company recently migrated a multi-tier web application to AWS VPC. However, the security audit team informs that the principle of least privilege hasn't been applied for Amazon RDS security group inbound and outbound rules between the different application tiers.\n\nWhat should a solutions architect do to resolve this?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create security group rules using the subnet CIDR blocks as the source or destination.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create security group rules using the VPC CIDR blocks as the source or destination.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create security group rules using the RDS instance ID as the source or destination.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create security group rules using the security group ID as the source or destination.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate security group rules using the security group ID as the source or destination.\n\nAWS Security Groups act as a virtual firewall to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC could be assigned to a different set of security groups.\n\nIf you're applying the principle of least privilege for Amazon RDS (Relational Database Service), you should limit inbound and outbound traffic as much as possible to only what's necessary. You can accomplish this by creating security group rules using the security group ID as the source or destination. By using the security group ID instead of the IP address range or subnet, you're specifically allowing only the traffic from the instances associated with that particular security group. This way, even if an instance gets compromised, it won't affect the other instances that aren't associated with the same security group.\n\nMoreover, security groups are stateful — if you send a request from your instance, the response traffic for that request is allowed to return, regardless of outbound security group rules. This ensures that the communication between your different tiers of the application remains smooth and secure.\n\nThis approach provides granular control over your inter-tier communication within your multi-tier application, thereby enhancing security and adhering to the principle of least privilege.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate security group rules using the RDS instance ID as the source or destination.\n\nAWS does not allow to create security group rules using the RDS instance ID as the source or destination. Security group rules are always based on IP ranges or other security group IDs. Therefore, this option is incorrect.\n\n\n\n\nCreate security group rules using the VPC CIDR blocks as the source or destination.\n\nThis approach would generally allow too much traffic because it would allow all instances within the VPC's CIDR block to communicate with the RDS instance. This goes against the principle of least privilege, which advocates giving the minimum level of access necessary to function.\n\n\n\n\nCreate security group rules using the subnet CIDR blocks as the source or destination.\n\nThis option would allow all instances within the subnet's CIDR block to communicate with the RDS instance. This is broader access than what's typically necessary and doesn't fully enforce the principle of least privilege.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create security group rules using the security group ID as the source or destination.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Security Groups act as a virtual firewall to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC could be assigned to a different set of security groups."
      },
      {
        "answer": "",
        "explanation": "If you're applying the principle of least privilege for Amazon RDS (Relational Database Service), you should limit inbound and outbound traffic as much as possible to only what's necessary. You can accomplish this by creating security group rules using the security group ID as the source or destination. By using the security group ID instead of the IP address range or subnet, you're specifically allowing only the traffic from the instances associated with that particular security group. This way, even if an instance gets compromised, it won't affect the other instances that aren't associated with the same security group."
      },
      {
        "answer": "",
        "explanation": "Moreover, security groups are stateful — if you send a request from your instance, the response traffic for that request is allowed to return, regardless of outbound security group rules. This ensures that the communication between your different tiers of the application remains smooth and secure."
      },
      {
        "answer": "",
        "explanation": "This approach provides granular control over your inter-tier communication within your multi-tier application, thereby enhancing security and adhering to the principle of least privilege."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create security group rules using the RDS instance ID as the source or destination.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS does not allow to create security group rules using the RDS instance ID as the source or destination. Security group rules are always based on IP ranges or other security group IDs. Therefore, this option is incorrect."
      },
      {
        "answer": "Create security group rules using the VPC CIDR blocks as the source or destination.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This approach would generally allow too much traffic because it would allow all instances within the VPC's CIDR block to communicate with the RDS instance. This goes against the principle of least privilege, which advocates giving the minimum level of access necessary to function."
      },
      {
        "answer": "Create security group rules using the subnet CIDR blocks as the source or destination.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option would allow all instances within the subnet's CIDR block to communicate with the RDS instance. This is broader access than what's typically necessary and doesn't fully enforce the principle of least privilege."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html"
    ]
  },
  {
    "id": 18,
    "question": "A solutions architect is required to design a new microservice for a company's system. The microservice needs to be accessed via an HTTPS endpoint by clients. Furthermore, AWS Identity and Access Management (IAM) should be used to validate these calls. The logic for this microservice will be developed using a single AWS Lambda function, implemented in Python 3.8.\n\nWhich solution would be the MOST operationally efficient to deploy this function?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a Lambda function URL for the function and set AWS_IAM as the authentication type.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function and enable IAM authentication on the API.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Set up an Amazon CloudFront distribution. Deploy the function to CloudFront Functions and specify AWS_IAM as the authentication type.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge and incorporate IAM authentication logic within the Lambda@Edge function.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nCreate an Amazon API Gateway REST API. Configure the method to use the Lambda function and enable IAM authentication on the API.\n\nThe most operationally efficient method would be to use Amazon API Gateway. This service makes it easy for developers to create, deploy, secure, and manage APIs that expose HTTP endpoints to clients. You can directly link your API methods to your AWS Lambda functions and enable IAM authentication on these methods. With API Gateway, you can focus on writing business logic, not managing infrastructure.\n\nThe API Gateway service integrates seamlessly with AWS Lambda, a serverless compute service that lets you run your code without provisioning or managing servers. Lambda automatically scales your applications in response to incoming request traffic. You can set up your code to automatically trigger from other AWS services.\n\nEnabling IAM authentication on your API means that when a client sends a request to your API, AWS will handle the authentication process by checking the IAM policies of the calling identity. Only if the caller is allowed to invoke the method, the API Gateway will forward the request to your Lambda function. This approach adds a layer of security by ensuring only authorized calls are allowed to invoke your microservice.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a Lambda function URL for the function and set AWS_IAM as the authentication type.\n\nThis option is not possible because AWS Lambda itself does not provide a functionality to directly expose a HTTPS URL or enable IAM authentication for function invocation. This can be achieved using Amazon API Gateway.\n\n\n\n\nCreate an Amazon CloudFront distribution. Deploy the function to Lambda@Edge and incorporate IAM authentication logic within the Lambda@Edge function.\n\nLambda@Edge lets you run Node.js and Python lambda functions to customize content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. However, it is not the most operationally efficient solution as it is more suitable for content delivery networks (CDN) and does not support IAM authentication.\n\n\n\n\nSet up an Amazon CloudFront distribution. Deploy the function to CloudFront Functions and specify AWS_IAM as the authentication type.\n\nCloudFront Functions is a feature of Amazon CloudFront that enables you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. However, this option is more suitable for CDN use cases and does not support IAM authentication.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-access-control-iam.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function and enable IAM authentication on the API.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The most operationally efficient method would be to use Amazon API Gateway. This service makes it easy for developers to create, deploy, secure, and manage APIs that expose HTTP endpoints to clients. You can directly link your API methods to your AWS Lambda functions and enable IAM authentication on these methods. With API Gateway, you can focus on writing business logic, not managing infrastructure."
      },
      {
        "answer": "",
        "explanation": "The API Gateway service integrates seamlessly with AWS Lambda, a serverless compute service that lets you run your code without provisioning or managing servers. Lambda automatically scales your applications in response to incoming request traffic. You can set up your code to automatically trigger from other AWS services."
      },
      {
        "answer": "",
        "explanation": "Enabling IAM authentication on your API means that when a client sends a request to your API, AWS will handle the authentication process by checking the IAM policies of the calling identity. Only if the caller is allowed to invoke the method, the API Gateway will forward the request to your Lambda function. This approach adds a layer of security by ensuring only authorized calls are allowed to invoke your microservice."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a Lambda function URL for the function and set AWS_IAM as the authentication type.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option is not possible because AWS Lambda itself does not provide a functionality to directly expose a HTTPS URL or enable IAM authentication for function invocation. This can be achieved using Amazon API Gateway."
      },
      {
        "answer": "Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge and incorporate IAM authentication logic within the Lambda@Edge function.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Lambda@Edge lets you run Node.js and Python lambda functions to customize content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. However, it is not the most operationally efficient solution as it is more suitable for content delivery networks (CDN) and does not support IAM authentication."
      },
      {
        "answer": "Set up an Amazon CloudFront distribution. Deploy the function to CloudFront Functions and specify AWS_IAM as the authentication type.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "CloudFront Functions is a feature of Amazon CloudFront that enables you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. However, this option is more suitable for CDN use cases and does not support IAM authentication."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-access-control-iam.html"
    ]
  },
  {
    "id": 19,
    "question": "A media company plans to implement and test a blue-green deployment for its global video streaming application before a major live event in the next two days. A large number of viewers use mobile devices, which have a tendency for DNS caching. The company only has 48 hours until the event begins.\n\nAs a Solutions Architect, which of the following strategies would you suggest to distribute the release to the maximum number of users within the specified time frame?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Route 53 weighted routing to divide traffic between various release versions.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an Elastic Load Balancer to distribute traffic among the release versions evenly.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS CodeDeploy deployment strategies to manage the release versions.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Global Accelerator to assign a fraction of traffic to the new release version.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nUse AWS Global Accelerator to assign a fraction of traffic to the new release version.\n\nAWS Global Accelerator improves your application's availability and performance by using the AWS global network. It provides two static Anycast IP addresses that act as a single, fixed entry point to your applications hosted in any number of locations around the world. Global Accelerator can be used for a blue-green deployment by setting up two endpoint groups, one for the blue environment and one for the green environment. You can then shift the traffic from the blue to the green environment gradually by adjusting the traffic dial for each endpoint group.\n\nGlobal Accelerator is particularly effective in this case because it directs traffic based on the user's geographic location, which can be beneficial for a global video streaming application. It can also handle the sudden surge in traffic expected during a live event. Additionally, since Global Accelerator uses static IP addresses, it is not affected by DNS caching on mobile devices, which is a key requirement in this scenario. Global Accelerator changes traffic distribution almost instantly because it operates at the TCP or UDP level, not relying on DNS propagation. This is beneficial for mobile users who tend to cache DNS results, which could delay them from seeing the new version of the application.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Route 53 weighted routing to divide traffic between various release versions.\n\nThis option could work for a blue-green deployment; however, DNS changes can take some time to propagate, especially due to DNS caching on mobile devices. This would not be a suitable solution in this case given the 48-hour time limit.\n\n\n\n\nUse an Elastic Load Balancer to distribute traffic among the release versions evenly.\n\nAn Elastic Load Balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. While this does ensure high availability and fault tolerance, it doesn't help in controlling the distribution of traffic between different application versions, which is crucial for blue-green deployments.\n\n\n\n\nUse AWS CodeDeploy deployment strategies to manage the release versions.\n\nAWS CodeDeploy can handle application deployments, but it does not have built-in capabilities to handle the specific needs of blue-green deployments such as managing DNS routing or overcoming DNS caching issues, particularly on mobile devices.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/using-aws-global-accelerator-to-achieve-blue-green-deployments\n\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Global Accelerator to assign a fraction of traffic to the new release version.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Global Accelerator improves your application's availability and performance by using the AWS global network. It provides two static Anycast IP addresses that act as a single, fixed entry point to your applications hosted in any number of locations around the world. Global Accelerator can be used for a blue-green deployment by setting up two endpoint groups, one for the blue environment and one for the green environment. You can then shift the traffic from the blue to the green environment gradually by adjusting the traffic dial for each endpoint group."
      },
      {
        "answer": "",
        "explanation": "Global Accelerator is particularly effective in this case because it directs traffic based on the user's geographic location, which can be beneficial for a global video streaming application. It can also handle the sudden surge in traffic expected during a live event. Additionally, since Global Accelerator uses static IP addresses, it is not affected by DNS caching on mobile devices, which is a key requirement in this scenario. Global Accelerator changes traffic distribution almost instantly because it operates at the TCP or UDP level, not relying on DNS propagation. This is beneficial for mobile users who tend to cache DNS results, which could delay them from seeing the new version of the application."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Route 53 weighted routing to divide traffic between various release versions.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option could work for a blue-green deployment; however, DNS changes can take some time to propagate, especially due to DNS caching on mobile devices. This would not be a suitable solution in this case given the 48-hour time limit."
      },
      {
        "answer": "Use an Elastic Load Balancer to distribute traffic among the release versions evenly.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An Elastic Load Balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. While this does ensure high availability and fault tolerance, it doesn't help in controlling the distribution of traffic between different application versions, which is crucial for blue-green deployments."
      },
      {
        "answer": "Use AWS CodeDeploy deployment strategies to manage the release versions.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS CodeDeploy can handle application deployments, but it does not have built-in capabilities to handle the specific needs of blue-green deployments such as managing DNS routing or overcoming DNS caching issues, particularly on mobile devices."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/networking-and-content-delivery/using-aws-global-accelerator-to-achieve-blue-green-deployments",
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html"
    ]
  },
  {
    "id": 20,
    "question": "A company runs a multi-tier application on AWS using five Amazon EC2 instances. The company's requirement is to have the IP addresses of all healthy EC2 instances returned when a DNS query is made.\n\nWhich routing policy should be applied to meet this requirement?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Multivalue Answer Routing Policy",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Geo-proximity Routing Policy",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Simple Routing Policy",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Weighted Routing Policy",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nMultivalue Answer Routing Policy\n\nAmazon Route 53 is a scalable and highly available domain name system (DNS) web service. It provides reliable and cost-effective domain registration, DNS routing, and health checking of resources. Route 53 enables businesses to manage their domain names and route internet traffic to various resources, such as Amazon S3 buckets, EC2 instances, load balancers, and more.\n\nThe Multivalue Answer Routing Policy lets you have Route 53 respond to DNS queries with up to eight healthy records selected at random. By doing this, the traffic can be distributed across multiple resources, such as EC2 instances, to improve the availability and redundancy of your application. The records are chosen from the healthy records at random in response to each query. The Multivalue Answer Routing Policy allows you to integrate with health checks. Therefore, if any of the EC2 instances are unhealthy, the IP address of that instance will not be returned by the DNS query. This ensures that your application stays available even if one or more instances fail.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSimple Routing Policy\n\nSimple Routing Policy is a basic routing policy choice for the Amazon Route 53 service, allowing you to route internet traffic to a single resource doing the work. It cannot handle returning multiple healthy IP addresses.\n\n\n\n\nWeighted Routing Policy\n\nWeighted Routing Policy allows you to split your traffic based on different weights assigned. It doesn't return all healthy IPs but distributes incoming requests to multiple resources (such as EC2 instances) based on the weights.\n\n\n\n\nGeo-proximity Routing Policy\n\nGeo-proximity Routing Policy routes traffic based on the geographic location of your users and your resources, not based on the health of resources. It won't fulfill the company's requirement of returning the IP addresses of all healthy EC2 instances.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-multivalue.html",
    "correctAnswerExplanations": [
      {
        "answer": "Multivalue Answer Routing Policy",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Route 53 is a scalable and highly available domain name system (DNS) web service. It provides reliable and cost-effective domain registration, DNS routing, and health checking of resources. Route 53 enables businesses to manage their domain names and route internet traffic to various resources, such as Amazon S3 buckets, EC2 instances, load balancers, and more."
      },
      {
        "answer": "",
        "explanation": "The Multivalue Answer Routing Policy lets you have Route 53 respond to DNS queries with up to eight healthy records selected at random. By doing this, the traffic can be distributed across multiple resources, such as EC2 instances, to improve the availability and redundancy of your application. The records are chosen from the healthy records at random in response to each query. The Multivalue Answer Routing Policy allows you to integrate with health checks. Therefore, if any of the EC2 instances are unhealthy, the IP address of that instance will not be returned by the DNS query. This ensures that your application stays available even if one or more instances fail."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Simple Routing Policy",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Simple Routing Policy is a basic routing policy choice for the Amazon Route 53 service, allowing you to route internet traffic to a single resource doing the work. It cannot handle returning multiple healthy IP addresses."
      },
      {
        "answer": "Weighted Routing Policy",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Weighted Routing Policy allows you to split your traffic based on different weights assigned. It doesn't return all healthy IPs but distributes incoming requests to multiple resources (such as EC2 instances) based on the weights."
      },
      {
        "answer": "Geo-proximity Routing Policy",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Geo-proximity Routing Policy routes traffic based on the geographic location of your users and your resources, not based on the health of resources. It won't fulfill the company's requirement of returning the IP addresses of all healthy EC2 instances."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-multivalue.html"
    ]
  },
  {
    "id": 21,
    "question": "A retail company uses Amazon EC2 instances for its inventory management application, which handles requests for inventory updates coming from different branches. Recently, branch representatives reported that some of the update requests aren't being reflected in the central system. Amazon CloudWatch data indicates that EC2 instances have consistent CPU utilization near or at 100%. The company wants to increase system performance and adjust the system to match the load.\n\nWhat steps should a solutions architect take to fulfill these requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Clone the current instance and distribute the load between original and cloned instances using an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a Direct Connect connection between EC2 and the branches. Update the application to use the Direct Connect link.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a Direct Connect connection between EC2 and the branches. Update the application to use the Direct Connect link.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Set up an EC2 Auto Scaling group depending on the queue length. Update the application to fetch from the queue.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nRoute incoming requests to Amazon Simple Queue Service (Amazon SQS). Set up an EC2 Auto Scaling group depending on the queue length. Update the application to fetch from the queue.\n\nAmazon SQS can handle any volume of data and achieve any level of throughput, without losing messages or requiring other services to be available. Messages are not lost due to failures of an EC2 instance processing them since they remain in the queue till the successful processing is acknowledged. With EC2 Auto Scaling, the company can maintain application availability and scale Amazon EC2 capacity up or down automatically according to the defined conditions, in this case, the queue length. This makes sure the application has enough instances to handle the load while not over-provisioning and wasting resources. This is the best solution to ensure scalability, reliability, and decoupling of application components.\n\n\n\n\n\n\n\nIncorrect Options:\n\nClone the current instance and distribute the load between original and cloned instances using an Application Load Balancer.\n\nAlthough this approach will certainly distribute the load between the instances, it doesn't provide a long-term solution to the issue. The load may exceed the capacity of these two instances in the future, resulting in the same issue. Moreover, this option does not address the issue of lost update requests due to high CPU utilization.\n\n\n\n\nCreate a Direct Connect connection between EC2 and the branches. Update the application to use the Direct Connect link.\n\nAWS Direct Connect is a network service that provides a private network connection from a network to Amazon Web Services (AWS). However, Direct Connect won't help in the situation described as the problem is not with network connectivity but with high CPU utilization on the EC2 instances.\n\n\n\n\nStop the EC2 instances, change the instance type to a more large CPU and higher memory, then restart the instances.\n\nThis approach might temporarily alleviate the problem, it's not a long-term solution. The load may exceed even this larger instance's capacity, causing the problem to recur. This also doesn't offer high availability or fault tolerance and assumes that a single larger instance can handle all the load.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-benefits.html",
    "correctAnswerExplanations": [
      {
        "answer": "Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Set up an EC2 Auto Scaling group depending on the queue length. Update the application to fetch from the queue.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon SQS can handle any volume of data and achieve any level of throughput, without losing messages or requiring other services to be available. Messages are not lost due to failures of an EC2 instance processing them since they remain in the queue till the successful processing is acknowledged. With EC2 Auto Scaling, the company can maintain application availability and scale Amazon EC2 capacity up or down automatically according to the defined conditions, in this case, the queue length. This makes sure the application has enough instances to handle the load while not over-provisioning and wasting resources. This is the best solution to ensure scalability, reliability, and decoupling of application components."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Clone the current instance and distribute the load between original and cloned instances using an Application Load Balancer.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Although this approach will certainly distribute the load between the instances, it doesn't provide a long-term solution to the issue. The load may exceed the capacity of these two instances in the future, resulting in the same issue. Moreover, this option does not address the issue of lost update requests due to high CPU utilization."
      },
      {
        "answer": "Create a Direct Connect connection between EC2 and the branches. Update the application to use the Direct Connect link.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Direct Connect is a network service that provides a private network connection from a network to Amazon Web Services (AWS). However, Direct Connect won't help in the situation described as the problem is not with network connectivity but with high CPU utilization on the EC2 instances."
      },
      {
        "answer": "",
        "explanation": "<strong>Stop the EC2 instances, change the instance type to a more large CPU and higher memory, then restart the instances.</strong>"
      },
      {
        "answer": "",
        "explanation": "This approach might temporarily alleviate the problem, it's not a long-term solution. The load may exceed even this larger instance's capacity, causing the problem to recur. This also doesn't offer high availability or fault tolerance and assumes that a single larger instance can handle all the load."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-benefits.html"
    ]
  },
  {
    "id": 22,
    "question": "An organization’s regulatory group is planning to move its network drives to AWS. The network drives operate on an SMB file share managed by a Windows Server. A self-maintained Active Directory on-premises governs the access rights to the files and directories.\n\nThe organization plans to use Amazon FSx for Windows File Server as part of its solution. The company must ensure that the on-premises Active Directory groups continue to restrict access to the FSx for Windows File Server SMB regulatory shares, directories, and files post the transition to AWS. The firm has established an FSx for Windows File Server file system.\n\nWhich among the following would fulfill these requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set a tag having a Limit tag key and a Regulatory tag value. Map the Active Directory groups with IAM groups to control access.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Active Directory Connector for linking with the Active Directory. Map the Active Directory groups with IAM groups to control access.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Join the file system with the Active Directory to control access.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an IAM service-associated role directly linked to FSx for Windows File Server to control access.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nJoin the file system with the Active Directory to control access.\n\nAmazon FSx for Windows File Server is a fully managed service that provides fully featured Windows file shares, built on Windows Server. One of the core features of Amazon FSx is that it supports the Server Message Block (SMB) protocol, which is the same protocol that the on-premises network drives use.\n\nWhen an FSx for Windows File Server file system is joined to an Active Directory, it becomes a member of that directory. This way, the identities from the on-premises Active Directory can be used to control access to the file shares on FSx for Windows File Server, preserving the existing access control mechanisms. In other words, the Active Directory users and groups that currently have permissions to the network drives can continue to use those same permissions with the new FSx for Windows File Server file shares, ensuring seamless transition and continued regulatory compliance.\n\nThis setup provides strong security and ease of management, as it does not require changes to the existing Active Directory structure or the creation of additional IAM roles or groups.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an Active Directory Connector for linking with the Active Directory. Map the Active Directory groups with IAM groups to control access.\n\nCreating an Active Directory Connector can link AWS with an on-premises Active Directory. However, FSx for Windows File Server does not integrate with IAM for file-level or share-level permissions. Hence, mapping Active Directory groups with IAM groups will not effectively control access to FSx shares and directories.\n\n\n\n\nSet a tag having a Limit tag key and a Regulatory tag value. Map the Active Directory groups with IAM groups to control access.\n\nTagging can help manage resources at a high level but doesn't provide control for file-level or share-level permissions in Amazon FSx for Windows File Server. Also, Active Directory groups cannot be mapped directly to IAM groups for access control.\n\n\n\n\nCreate an IAM service-associated role directly linked to FSx for Windows File Server to control access.\n\nIAM service-linked roles can allow AWS services to access other resources they require to function. However, they do not offer a method for managing file-level or share-level permissions in Amazon FSx for Windows File Server.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/storage/using-amazon-fsx-for-windows-file-server-with-an-on-premises-active-directory\n\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html",
    "correctAnswerExplanations": [
      {
        "answer": "Join the file system with the Active Directory to control access.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon FSx for Windows File Server is a fully managed service that provides fully featured Windows file shares, built on Windows Server. One of the core features of Amazon FSx is that it supports the Server Message Block (SMB) protocol, which is the same protocol that the on-premises network drives use."
      },
      {
        "answer": "",
        "explanation": "When an FSx for Windows File Server file system is joined to an Active Directory, it becomes a member of that directory. This way, the identities from the on-premises Active Directory can be used to control access to the file shares on FSx for Windows File Server, preserving the existing access control mechanisms. In other words, the Active Directory users and groups that currently have permissions to the network drives can continue to use those same permissions with the new FSx for Windows File Server file shares, ensuring seamless transition and continued regulatory compliance."
      },
      {
        "answer": "",
        "explanation": "This setup provides strong security and ease of management, as it does not require changes to the existing Active Directory structure or the creation of additional IAM roles or groups."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create an Active Directory Connector for linking with the Active Directory. Map the Active Directory groups with IAM groups to control access.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Creating an Active Directory Connector can link AWS with an on-premises Active Directory. However, FSx for Windows File Server does not integrate with IAM for file-level or share-level permissions. Hence, mapping Active Directory groups with IAM groups will not effectively control access to FSx shares and directories."
      },
      {
        "answer": "Set a tag having a Limit tag key and a Regulatory tag value. Map the Active Directory groups with IAM groups to control access.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Tagging can help manage resources at a high level but doesn't provide control for file-level or share-level permissions in Amazon FSx for Windows File Server. Also, Active Directory groups cannot be mapped directly to IAM groups for access control."
      },
      {
        "answer": "Create an IAM service-associated role directly linked to FSx for Windows File Server to control access.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "IAM service-linked roles can allow AWS services to access other resources they require to function. However, they do not offer a method for managing file-level or share-level permissions in Amazon FSx for Windows File Server."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/storage/using-amazon-fsx-for-windows-file-server-with-an-on-premises-active-directory",
      "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html"
    ]
  },
  {
    "id": 23,
    "question": "A company uses Amazon RDS for its applications and plans to use RDS Read replicas to enhance application performance and data availability. The CTO requests information about how AWS applies data transfer charges when setting up RDS read replicas.\n\nWhich of the following correctly represents the application of charges for data transfer in the context of RDS read replicas?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Data transfer charges are applied when replicating data across multiple regions.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Data transfer charges for replicating data within the same region.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Data transfer charges for replicating data across multiple Availability Zones in different regions.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Data transfer charges for replicating data across multiple Availability Zones in the same region.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nData transfer charges are applied when replicating data across multiple regions.\n\nAmazon RDS Read Replicas are a feature that enables the creation of one or more copies of a database instance for read-heavy workloads. These replicas can handle read queries, offloading the primary database instance and improving performance. Read Replicas are automatically synchronized with the primary instance, ensuring data consistency. They can be used to scale read capacity, enhance application performance, and provide high availability.\n\nA Read Replica is billed similarly to a standard DB Instance, with the same rates. The pricing per \"DB Instance hour\" for a read replica is determined by its DB instance class, just like a standard DB instance. The data transfer costs associated with replicating data between the source DB instance and the read replica within the same AWS Region are not charged.\n\nBilling for a Read Replica begins upon successful creation, indicated by the status being listed as \"active.\" The Read Replica will be billed continuously at the regular Amazon RDS DB instance hour rates until you initiate a deletion command. When you use Amazon RDS Read Replicas, you are charged for data transfer from the primary RDS instance to the read replica. If the read replica is in a different region than the primary instance (cross-region replication), then you will incur data transfer costs. These costs are based on the amount of data transferred out of the source AWS Region.\n\n\n\n\n\n\n\nIncorrect Options:\n\nData transfer charges for replicating data within the same region.\n\nData transfer within the same region (including data transfer to a read replica) is free of charge in AWS. Therefore, this option is incorrect.\n\n\n\n\nData transfer charges for replicating data across multiple Availability Zones in the same region.\n\nAvailability Zones are within the same region, data transfer within the same region is free of charge. Hence, this option is incorrect.\n\n\n\n\nData transfer charges for replicating data across multiple Availability Zones in different regions.\n\nAvailability Zones cannot be in different regions. They are isolated locations within a region. Therefore, this option is incorrect.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/read-replicas\n\nhttps://aws.amazon.com/rds/faqs",
    "correctAnswerExplanations": [
      {
        "answer": "Data transfer charges are applied when replicating data across multiple regions.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon RDS Read Replicas are a feature that enables the creation of one or more copies of a database instance for read-heavy workloads. These replicas can handle read queries, offloading the primary database instance and improving performance. Read Replicas are automatically synchronized with the primary instance, ensuring data consistency. They can be used to scale read capacity, enhance application performance, and provide high availability."
      },
      {
        "answer": "",
        "explanation": "A Read Replica is billed similarly to a standard DB Instance, with the same rates. The pricing per \"DB Instance hour\" for a read replica is determined by its DB instance class, just like a standard DB instance. The data transfer costs associated with replicating data between the source DB instance and the read replica within the same AWS Region are not charged."
      },
      {
        "answer": "",
        "explanation": "Billing for a Read Replica begins upon successful creation, indicated by the status being listed as \"active.\" The Read Replica will be billed continuously at the regular Amazon RDS DB instance hour rates until you initiate a deletion command. When you use Amazon RDS Read Replicas, you are charged for data transfer from the primary RDS instance to the read replica. If the read replica is in a different region than the primary instance (cross-region replication), then you will incur data transfer costs. These costs are based on the amount of data transferred out of the source AWS Region."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Data transfer charges for replicating data within the same region.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Data transfer within the same region (including data transfer to a read replica) is free of charge in AWS. Therefore, this option is incorrect."
      },
      {
        "answer": "Data transfer charges for replicating data across multiple Availability Zones in the same region.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Availability Zones are within the same region, data transfer within the same region is free of charge. Hence, this option is incorrect."
      },
      {
        "answer": "Data transfer charges for replicating data across multiple Availability Zones in different regions.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Availability Zones cannot be in different regions. They are isolated locations within a region. Therefore, this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/features/read-replicas",
      "https://aws.amazon.com/rds/faqs"
    ]
  },
  {
    "id": 24,
    "question": "A media organization is using Amazon DynamoDB for its content management. Lately, due to the spike in user traffic, the content management team has noticed slow retrieval of data from the DynamoDB table and suggests adding a global secondary index.\n\nWhat combination of actions should the solutions architect consider before making this modification? (Select TWO.)",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Enable automatic backups on the DynamoDB table by setting the point-in-time recovery to ENABLED.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Enable stream on the DynamoDB table for capturing data modification.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy a Redshift cluster and define the AWS regions where the cluster will be available.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Make sure to complete any large-scale database operations currently running on the DynamoDB table.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Create the provisioned throughput settings for the global secondary index.",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nCreate the provisioned throughput settings for the global secondary index.\n\nCreating a Global Secondary Index (GSI), it's crucial to establish the provisioned throughput settings. These settings determine the read and write capacity units, which essentially control the performance of the index. Higher settings equate to higher performance, but at a higher cost. Thus, it's a balancing act between performance and cost. If the media organization is experiencing slow retrieval times, increasing the provisioned throughput on the new GSI might improve performance.\n\n\n\n\nMake sure to complete any large-scale database operations currently running on the DynamoDB table.\n\nLarge-scale database operations like a table scan, heavy write operations, or backups can consume significant system resources and impede the performance of the table and its indexes. Therefore, it's recommended to complete any large-scale operations before adding a GSI to prevent further performance degradation during its creation. It ensures that the table is in a stable state and can accommodate the additional load from the GSI.\n\n\n\n\n\n\n\nIncorrect Options:\n\nEnable stream on the DynamoDB table for capturing data modification.\n\nDynamoDB Streams is a feature that captures data modification events in DynamoDB, such as add, update, or delete operations. Enabling DynamoDB Streams doesn't improve data retrieval times or impact the creation or operation of a GSI.\n\n\n\n\nDeploy a Redshift cluster and define the AWS regions where the cluster will be available.\n\nRedshift is a completely different AWS service - it's a data warehousing solution that's designed for analytical workloads and running complex queries on large datasets. Redshift won't improve DynamoDB table read performance and isn't necessary for the creation of a GSI.\n\n\n\n\nEnable automatic backups on the DynamoDB table by setting the point-in-time recovery to ENABLED.\n\nEnabling automatic backups or point-in-time recovery is a good practice for data resilience and disaster recovery. However, it doesn't affect the performance of data retrieval from a DynamoDB table or the creation of a GSI.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create the provisioned throughput settings for the global secondary index.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Creating a Global Secondary Index (GSI), it's crucial to establish the provisioned throughput settings. These settings determine the read and write capacity units, which essentially control the performance of the index. Higher settings equate to higher performance, but at a higher cost. Thus, it's a balancing act between performance and cost. If the media organization is experiencing slow retrieval times, increasing the provisioned throughput on the new GSI might improve performance."
      },
      {
        "answer": "Make sure to complete any large-scale database operations currently running on the DynamoDB table.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Large-scale database operations like a table scan, heavy write operations, or backups can consume significant system resources and impede the performance of the table and its indexes. Therefore, it's recommended to complete any large-scale operations before adding a GSI to prevent further performance degradation during its creation. It ensures that the table is in a stable state and can accommodate the additional load from the GSI."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Enable stream on the DynamoDB table for capturing data modification.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "DynamoDB Streams is a feature that captures data modification events in DynamoDB, such as add, update, or delete operations. Enabling DynamoDB Streams doesn't improve data retrieval times or impact the creation or operation of a GSI."
      },
      {
        "answer": "Deploy a Redshift cluster and define the AWS regions where the cluster will be available.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Redshift is a completely different AWS service - it's a data warehousing solution that's designed for analytical workloads and running complex queries on large datasets. Redshift won't improve DynamoDB table read performance and isn't necessary for the creation of a GSI."
      },
      {
        "answer": "Enable automatic backups on the DynamoDB table by setting the point-in-time recovery to ENABLED.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Enabling automatic backups or point-in-time recovery is a good practice for data resilience and disaster recovery. However, it doesn't affect the performance of data retrieval from a DynamoDB table or the creation of a GSI."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html"
    ]
  },
  {
    "id": 25,
    "question": "An e-commerce company is Moving to AWS Cloud. The company stores its customer data in a MySQL database and requires a solution that ensures data availability and accessibility across multiple AWS Regions at all times.\n\nWhat solution would meet these requirements with the LEAST operational overhead?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the MySQL database to a MySQL cluster running on Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Migrate the MySQL database to an Amazon RDS for MySQL DB instance and create a read replica in another Region.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Migrate the MySQL database to an Amazon RDS for MySQL DB instance with the Multi-AZ feature enabled.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Migrate the MySQL database to an Amazon RDS for MySQL DB instance and configure DB snapshots to be copied to another Region.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nMigrate the MySQL database to an Amazon RDS for MySQL DB instance and create a read replica in another Region.\n\nThis solution ensures both data availability and accessibility across multiple AWS Regions with the least operational overhead. By using Amazon RDS for MySQL, the company can offload many of the time-consuming administration tasks associated with databases, such as backups, patch management, and replication. Creating a read replica in another region serves two purposes: it allows for high-availability in the case of a regional failure and it also allows for faster access to data for users located in that region, as it can serve read traffic.\n\nAmazon RDS provides a managed relational database service that makes it easy to set up, operate, and scale a relational database in the cloud. Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\n\nThe company needs to ensure data availability and accessibility across multiple AWS Regions, a read replica in another Region is a good solution. It provides a fully managed replication solution, reducing the operational overhead for the company. By creating a read replica in a separate Region, the company not only improves the availability and accessibility of its data but also enhances its disaster recovery capabilities. If the primary Region is unavailable due to any reason, the company can promote the read replica to a standalone database instance and direct all traffic to the new Region.\n\n\n\n\n\n\n\nIncorrect Options:\n\nMigrate the MySQL database to a MySQL cluster running on Amazon EC2 instances.\n\nRunning a MySQL cluster on EC2 instances requires significant operational overhead for managing, patching, and scaling the EC2 instances and the database software.\n\n\n\n\nMigrate the MySQL database to an Amazon RDS for MySQL DB instance with the Multi-AZ feature enabled.\n\nMulti-AZ deployments provide high availability and failover support for DB instances within a single region. However, this option does not provide accessibility across multiple regions.\n\n\n\n\nMigrate the MySQL database to an Amazon RDS for MySQL DB instance and configure DB snapshots to be copied to another Region.\n\nCopying DB snapshots to another region only ensures the availability of backup data in another region. It doesn't allow for active read access in the other region and does not provide high-availability in the case of regional failure.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/mysql\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
    "correctAnswerExplanations": [
      {
        "answer": "Migrate the MySQL database to an Amazon RDS for MySQL DB instance and create a read replica in another Region.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This solution ensures both data availability and accessibility across multiple AWS Regions with the least operational overhead. By using Amazon RDS for MySQL, the company can offload many of the time-consuming administration tasks associated with databases, such as backups, patch management, and replication. Creating a read replica in another region serves two purposes: it allows for high-availability in the case of a regional failure and it also allows for faster access to data for users located in that region, as it can serve read traffic."
      },
      {
        "answer": "",
        "explanation": "Amazon RDS provides a managed relational database service that makes it easy to set up, operate, and scale a relational database in the cloud. Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads."
      },
      {
        "answer": "",
        "explanation": "The company needs to ensure data availability and accessibility across multiple AWS Regions, a read replica in another Region is a good solution. It provides a fully managed replication solution, reducing the operational overhead for the company. By creating a read replica in a separate Region, the company not only improves the availability and accessibility of its data but also enhances its disaster recovery capabilities. If the primary Region is unavailable due to any reason, the company can promote the read replica to a standalone database instance and direct all traffic to the new Region."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Migrate the MySQL database to a MySQL cluster running on Amazon EC2 instances.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Running a MySQL cluster on EC2 instances requires significant operational overhead for managing, patching, and scaling the EC2 instances and the database software."
      },
      {
        "answer": "Migrate the MySQL database to an Amazon RDS for MySQL DB instance with the Multi-AZ feature enabled.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Multi-AZ deployments provide high availability and failover support for DB instances within a single region. However, this option does not provide accessibility across multiple regions."
      },
      {
        "answer": "Migrate the MySQL database to an Amazon RDS for MySQL DB instance and configure DB snapshots to be copied to another Region.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Copying DB snapshots to another region only ensures the availability of backup data in another region. It doesn't allow for active read access in the other region and does not provide high-availability in the case of regional failure."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/mysql",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"
    ]
  },
  {
    "id": 26,
    "question": "A software company has an AWS Lambda function that writes millions of objects to an Amazon S3 bucket daily. The company uses this S3 bucket as a data source for AWS Athena for data analytics. Encryption was not enabled on the S3 bucket before the objects were stored. A solutions architect needs to implement encryption for all the existing and future objects in the S3 bucket.\n\nWhich solution will fulfill these requirements with the LEAST amount of effort?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Navigate to Amazon S3 in the AWS Management Console. Search through the S3 bucket’s objects. Sort by the encryption field. Choose each unencrypted object and use the Modify button to apply default encryption settings to every unencrypted object in the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Enable default encryption settings on the S3 bucket. Use the S3 Inventory feature to generate a .csv file listing the unencrypted objects. Execute an S3 Batch Operations job that uses the copy command to encrypt these objects.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a new S3 bucket with default encryption settings. Move all existing objects to local storage temporarily and then upload them to the new S3 bucket.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Generate a new encryption key using AWS Key Management Service (AWS KMS). Alter the S3 bucket's settings to employ server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Enable versioning on the S3 bucket.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nEnable default encryption settings on the S3 bucket. Use the S3 Inventory feature to generate a .csv file listing the unencrypted objects. Execute an S3 Batch Operations job that uses the copy command to encrypt these objects.\n\nThis option is the most efficient one because it allows for the encryption of all existing objects in the S3 bucket in one operation. The S3 Batch Operations job will ensure all unencrypted objects are encrypted without having to manually select each one.\n\nFirstly, enabling default encryption settings on the S3 bucket ensures that all new objects added to the bucket will automatically be encrypted. For existing objects, AWS offers a feature called S3 Inventory, which can create a list (.csv file) of all objects in the bucket, including their encryption status. This is a simple, automated way to identify all unencrypted objects without needing to manually search through the bucket.\n\nNext, to encrypt these existing unencrypted objects, the best tool is S3 Batch Operations. This feature allows you to perform large-scale operations on S3 objects, such as copying, which in this case can be used to apply encryption. By using the copy command with encryption settings, you can effectively apply encryption to all identified unencrypted objects. This approach will require minimal manual effort, fulfill the requirement of encrypting all objects in the bucket, and won't disrupt the use of AWS Athena for data analytics.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a new S3 bucket with default encryption settings. Move all existing objects to local storage temporarily and then upload them to the new S3 bucket.\n\nThis method requires additional steps and also introduces a risk of data loss when moving large amounts of data.\n\n\n\n\nGenerate a new encryption key using AWS Key Management Service (AWS KMS). Alter the S3 bucket's settings to employ server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Enable versioning on the S3 bucket.\n\nThis option does not address the requirement to encrypt existing data, only future data.\n\n\n\n\nNavigate to Amazon S3 in the AWS Management Console. Search through the S3 bucket’s objects. Sort by the encryption field. Choose each unencrypted object and use the Modify button to apply default encryption settings to every unencrypted object in the S3 bucket.\n\nManually modifying each unencrypted object would be too time-consuming and impractical given the large number of objects in the bucket.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html\n\nhttps://aws.amazon.com/blogs/aws/new-amazon-s3-batch-operations",
    "correctAnswerExplanations": [
      {
        "answer": "Enable default encryption settings on the S3 bucket. Use the S3 Inventory feature to generate a .csv file listing the unencrypted objects. Execute an S3 Batch Operations job that uses the copy command to encrypt these objects.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option is the most efficient one because it allows for the encryption of all existing objects in the S3 bucket in one operation. The S3 Batch Operations job will ensure all unencrypted objects are encrypted without having to manually select each one."
      },
      {
        "answer": "",
        "explanation": "Firstly, enabling default encryption settings on the S3 bucket ensures that all new objects added to the bucket will automatically be encrypted. For existing objects, AWS offers a feature called S3 Inventory, which can create a list (.csv file) of all objects in the bucket, including their encryption status. This is a simple, automated way to identify all unencrypted objects without needing to manually search through the bucket."
      },
      {
        "answer": "",
        "explanation": "Next, to encrypt these existing unencrypted objects, the best tool is S3 Batch Operations. This feature allows you to perform large-scale operations on S3 objects, such as copying, which in this case can be used to apply encryption. By using the copy command with encryption settings, you can effectively apply encryption to all identified unencrypted objects. This approach will require minimal manual effort, fulfill the requirement of encrypting all objects in the bucket, and won't disrupt the use of AWS Athena for data analytics."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a new S3 bucket with default encryption settings. Move all existing objects to local storage temporarily and then upload them to the new S3 bucket.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This method requires additional steps and also introduces a risk of data loss when moving large amounts of data."
      },
      {
        "answer": "Generate a new encryption key using AWS Key Management Service (AWS KMS). Alter the S3 bucket's settings to employ server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Enable versioning on the S3 bucket.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option does not address the requirement to encrypt existing data, only future data."
      },
      {
        "answer": "Navigate to Amazon S3 in the AWS Management Console. Search through the S3 bucket’s objects. Sort by the encryption field. Choose each unencrypted object and use the Modify button to apply default encryption settings to every unencrypted object in the S3 bucket.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Manually modifying each unencrypted object would be too time-consuming and impractical given the large number of objects in the bucket."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html",
      "https://aws.amazon.com/blogs/aws/new-amazon-s3-batch-operations"
    ]
  },
  {
    "id": 27,
    "question": "A company's log management system generates hundreds of CSV files daily and store them into an Amazon S3 bucket. The company needs to convert these files into Apache Parquet format and store them in a dedicated processed data bucket.\n\nWhich solution would meet these requirements with the LEAST amount of development effort?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Batch to create a job definition with Python syntax to convert the data and deposit the data into the processed data bucket. Use the job definition to initiate a job. Specify a multi-node parallel job as the job type.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up an Amazon EMR cluster with Apache Flink installed. Write a Flink application to transform the data. Use EMR File System (EMRFS) to write files to the processed data bucket.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an AWS Glue crawler to identify the data. Set up an AWS Glue extract, transform, and load (ETL) job to convert the data. Specify the processed data bucket in the output stage.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an AWS Lambda function to convert the data and store the data into the processed data bucket. Configure an event trigger for the S3 bucket. Specify the Lambda function as the target for the event trigger.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nCreate an AWS Glue crawler to identify the data. Set up an AWS Glue extract, transform, and load (ETL) job to convert the data. Specify the processed data bucket in the output stage.\n\nAWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load your data for analytics. AWS Glue is the most appropriate choice because of the built-in functionality it provides for working with data transformations.\n\nFirst, you set up an AWS Glue crawler to explore your data, which could be in any of the numerous data formats. This crawler helps to discover the schema of the input data and catalog it, which is an important step for data preparation.\n\nNext, you set up an AWS Glue ETL job. This job will read the data (extract), apply any necessary transformations (transform), like converting the data from CSV format to Apache Parquet, and finally write the transformed data back to another location, such as an S3 bucket (load).\n\nThe whole process requires very little manual coding and can be easily set up and monitored through the AWS Management Console. This makes it a very efficient choice when you need to manage and transform large amounts of data with minimal development effort.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up an Amazon EMR cluster with Apache Flink installed. Write a Flink application to transform the data. Use EMR File System (EMRFS) to write files to the processed data bucket.\n\nThis is technically possible, it would require more development effort to write a custom Apache Flink application for this task, compared to using AWS Glue. So this solution violate requirements which is least amount of development effort.\n\n\n\n\nUse AWS Batch to create a job definition with Python syntax to convert the data and deposit the data into the processed data bucket. Use the job definition to initiate a job. Specify a multi-node parallel job as the job type.\n\nThis method is not the most efficient solution for this task as it requires writing Python scripts and manually managing the batch jobs, which increases the development effort.\n\n\n\n\nCreate an AWS Lambda function to convert the data and store the data into the processed data bucket. Configure an event trigger for the S3 bucket. Specify the Lambda function as the target for the event trigger.\n\nAWS Lambda could also do this, but the potential size and volume of the data may surpass Lambda's execution time and memory limits, leading to a failure of some transformations.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-data-to-apache-parquet.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create an AWS Glue crawler to identify the data. Set up an AWS Glue extract, transform, and load (ETL) job to convert the data. Specify the processed data bucket in the output stage.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load your data for analytics. AWS Glue is the most appropriate choice because of the built-in functionality it provides for working with data transformations."
      },
      {
        "answer": "",
        "explanation": "First, you set up an AWS Glue crawler to explore your data, which could be in any of the numerous data formats. This crawler helps to discover the schema of the input data and catalog it, which is an important step for data preparation."
      },
      {
        "answer": "",
        "explanation": "Next, you set up an AWS Glue ETL job. This job will read the data (extract), apply any necessary transformations (transform), like converting the data from CSV format to Apache Parquet, and finally write the transformed data back to another location, such as an S3 bucket (load)."
      },
      {
        "answer": "",
        "explanation": "The whole process requires very little manual coding and can be easily set up and monitored through the AWS Management Console. This makes it a very efficient choice when you need to manage and transform large amounts of data with minimal development effort."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up an Amazon EMR cluster with Apache Flink installed. Write a Flink application to transform the data. Use EMR File System (EMRFS) to write files to the processed data bucket.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This is technically possible, it would require more development effort to write a custom Apache Flink application for this task, compared to using AWS Glue. So this solution violate requirements which is least amount of development effort."
      },
      {
        "answer": "Use AWS Batch to create a job definition with Python syntax to convert the data and deposit the data into the processed data bucket. Use the job definition to initiate a job. Specify a multi-node parallel job as the job type.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This method is not the most efficient solution for this task as it requires writing Python scripts and manually managing the batch jobs, which increases the development effort."
      },
      {
        "answer": "Create an AWS Lambda function to convert the data and store the data into the processed data bucket. Configure an event trigger for the S3 bucket. Specify the Lambda function as the target for the event trigger.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Lambda could also do this, but the potential size and volume of the data may surpass Lambda's execution time and memory limits, leading to a failure of some transformations."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-data-to-apache-parquet.html"
    ]
  },
  {
    "id": 28,
    "question": "A small software enterprise recently moved its database management system to AWS, adopting Amazon EC2 instances in a single AWS Region. Now, they aim to restructure their system architecture to ensure maximum availability and fault tolerance. All active EC2 instances must receive user requests randomly.\n\nWhich two actions should the enterprise consider to achieve these goals? (Select TWO.)",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use an Amazon Route 53 geolocation routing policy.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an Amazon Route 53 latency-based routing policy.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use an Amazon Route 53 multivalue answer routing policy.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Deploy three EC2 instances: two instances in one Availability Zone and one instance in another Availability Zone.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Deploy four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nUse an Amazon Route 53 multivalue answer routing policy.\n\nA multivalue answer routing policy in Amazon Route 53 is used to route traffic approximately randomly to multiple resources, such as EC2 instances. This type of policy allows you to return multiple values in response to DNS queries, which include up to eight healthy records selected at random. This can effectively distribute user requests among multiple resources. By using this policy, the company can enhance its system's availability and fault tolerance because it does not rely on a single instance. Instead, user requests are distributed among several EC2 instances, reducing the likelihood of system overload and ensuring continuity of service if one instance fails.\n\n\n\n\nDeploy four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.\n\nAWS recommends distributing EC2 instances across multiple Availability Zones (AZs) within a Region for high availability and fault tolerance. By deploying two instances in one AZ and another two in a different AZ, the enterprise is reducing the risk of a single point of failure. If an AZ experiences issues, the instances in the other AZ can continue to handle user requests, ensuring maximum availability of their services. This distribution also ensures better load balancing of requests, since the traffic is distributed across more instances.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse an Amazon Route 53 latency-based routing policy.\n\nThis policy routes traffic based on the lowest network latency for your end user, which doesn't align with the company's requirement of random traffic distribution.\n\n\n\n\nUse an Amazon Route 53 geolocation routing policy.\n\nGeolocation routing policy routes traffic based on the geographic location of your users, which is not pertinent to the company's requirement for random traffic distribution.\n\n\n\n\nDeploy three RDS instances: two instances in one Availability Zone and one instance in another Availability Zone.\n\nThis option does distribute instances across different Availability Zones, it does not offer equal distribution, potentially leading to unequal load and reduced fault tolerance if one zone has more instances than the other.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-multivalue.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use an Amazon Route 53 multivalue answer routing policy.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A multivalue answer routing policy in Amazon Route 53 is used to route traffic approximately randomly to multiple resources, such as EC2 instances. This type of policy allows you to return multiple values in response to DNS queries, which include up to eight healthy records selected at random. This can effectively distribute user requests among multiple resources. By using this policy, the company can enhance its system's availability and fault tolerance because it does not rely on a single instance. Instead, user requests are distributed among several EC2 instances, reducing the likelihood of system overload and ensuring continuity of service if one instance fails."
      },
      {
        "answer": "Deploy four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS recommends distributing EC2 instances across multiple Availability Zones (AZs) within a Region for high availability and fault tolerance. By deploying two instances in one AZ and another two in a different AZ, the enterprise is reducing the risk of a single point of failure. If an AZ experiences issues, the instances in the other AZ can continue to handle user requests, ensuring maximum availability of their services. This distribution also ensures better load balancing of requests, since the traffic is distributed across more instances."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use an Amazon Route 53 latency-based routing policy.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This policy routes traffic based on the lowest network latency for your end user, which doesn't align with the company's requirement of random traffic distribution."
      },
      {
        "answer": "Use an Amazon Route 53 geolocation routing policy.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Geolocation routing policy routes traffic based on the geographic location of your users, which is not pertinent to the company's requirement for random traffic distribution."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy three RDS instances: two instances in one Availability Zone and one instance in another Availability Zone.</strong>"
      },
      {
        "answer": "",
        "explanation": "This option does distribute instances across different Availability Zones, it does not offer equal distribution, potentially leading to unequal load and reduced fault tolerance if one zone has more instances than the other."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-multivalue.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html"
    ]
  },
  {
    "id": 29,
    "question": "A company operates a global e-commerce platform on Amazon EC2 instances behind an Application Load Balancer. The platform uses Aurora PostgreSQL for data storage. The company is planning for a disaster recovery solution that can afford to have up to 30 minutes of downtime and potential data loss. The solution is not expected to handle any traffic when the primary infrastructure is operational.\n\nWhat is the most suitable approach that a solutions architect should recommend?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Host a minimal version of the platform in an alternate AWS Region. Use Amazon Route 53 for active-active failover configuration. Configure an Aurora Read Replica in the second Region.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Duplicate the primary infrastructure in a secondary AWS Region. Use Amazon Route 53 to arrange active-active failover. Create an Aurora database that is restored from the most recent snapshot.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS Backup for data backup. Use the backup to replicate the necessary infrastructure in a second AWS Region. Use Amazon Route 53 for active-passive failover configuration. Create a second primary Aurora instance in the secondary Region.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Implement the platform with the necessary infrastructure components. Use Amazon Route 53 to set up active-passive failover. Create an Aurora Read Replica in a different AWS Region.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nImplement the platform with the necessary infrastructure components. Use Amazon Route 53 to set up active-passive failover. Create an Aurora Read Replica in a different AWS Region.\n\nThis option addresses the Company’s requirements by creating a standby (passive) disaster recovery solution in another AWS Region. Amazon Route 53 is used to manage DNS failover, thereby directing traffic to the secondary site only in the event of a primary site failure. It also deploys an Aurora Read Replica in a different region for database resilience, ensuring data is available in the event of a disaster.\n\nIn this approach, the platform with the necessary infrastructure components is implemented, and Amazon Route 53 is used to set up active-passive failover. This is an effective strategy for disaster recovery as it designates a primary resource and a standby resource. Under normal conditions, traffic is only sent to the primary resource. If that resource becomes unavailable, Route 53 will begin to route traffic to the standby resource, in this case, the infrastructure in a different AWS Region.\n\nTo handle data storage, an Aurora Read Replica is created in a different AWS Region. RDS Read Replicas provide a way to replicate data across AWS Regions for a standby database. This solution supports the company's requirement that the disaster recovery solution does not handle traffic when the primary infrastructure is operational, and it provides a way to minimize data loss, should a disaster occur.\n\nSo, the combined use of active-passive failover and a Read Replica for the PostgreSQL database offers a cost-effective and efficient solution for the company's needs.\n\n\n\n\n\n\n\nIncorrect Options:\n\nHost a minimal version of the platform in an alternate AWS Region. Use Amazon Route 53 for active-active failover configuration. Configure an Aurora Read Replica in the second Region.\n\nThis option incorrectly suggests an active-active failover configuration. This would unnecessarily consume resources as traffic is not intended to be directed to the secondary site when the primary site is healthy.\n\n\n\n\nDuplicate the primary infrastructure in a secondary AWS Region. Use Amazon Route 53 to arrange active-active failover. Create an Aurora database that is restored from the most recent snapshot.\n\nThis option proposes an active-active failover strategy, which does not match the company's needs. In addition, restoring an Aurora database from the latest snapshot would not assure the least data loss.\n\n\n\n\nUse AWS Backup for data backup. Use the backup to replicate the necessary infrastructure in a second AWS Region. Use Amazon Route 53 for active-passive failover configuration. Create a second primary Aurora instance in the secondary Region.\n\nThis option is not as efficient because creating a second primary Aurora instance would not provide the required replication for disaster recovery.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html\n\nhttps://aws.amazon.com/blogs/database/cost-effective-disaster-recovery-for-amazon-aurora-databases-using-aws-backup",
    "correctAnswerExplanations": [
      {
        "answer": "Implement the platform with the necessary infrastructure components. Use Amazon Route 53 to set up active-passive failover. Create an Aurora Read Replica in a different AWS Region.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option addresses the Company’s requirements by creating a standby (passive) disaster recovery solution in another AWS Region. Amazon Route 53 is used to manage DNS failover, thereby directing traffic to the secondary site only in the event of a primary site failure. It also deploys an Aurora Read Replica in a different region for database resilience, ensuring data is available in the event of a disaster."
      },
      {
        "answer": "",
        "explanation": "In this approach, the platform with the necessary infrastructure components is implemented, and Amazon Route 53 is used to set up active-passive failover. This is an effective strategy for disaster recovery as it designates a primary resource and a standby resource. Under normal conditions, traffic is only sent to the primary resource. If that resource becomes unavailable, Route 53 will begin to route traffic to the standby resource, in this case, the infrastructure in a different AWS Region."
      },
      {
        "answer": "",
        "explanation": "To handle data storage, an Aurora Read Replica is created in a different AWS Region. RDS Read Replicas provide a way to replicate data across AWS Regions for a standby database. This solution supports the company's requirement that the disaster recovery solution does not handle traffic when the primary infrastructure is operational, and it provides a way to minimize data loss, should a disaster occur."
      },
      {
        "answer": "",
        "explanation": "So, the combined use of active-passive failover and a Read Replica for the PostgreSQL database offers a cost-effective and efficient solution for the company's needs."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Host a minimal version of the platform in an alternate AWS Region. Use Amazon Route 53 for active-active failover configuration. Configure an Aurora Read Replica in the second Region.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option incorrectly suggests an active-active failover configuration. This would unnecessarily consume resources as traffic is not intended to be directed to the secondary site when the primary site is healthy."
      },
      {
        "answer": "Duplicate the primary infrastructure in a secondary AWS Region. Use Amazon Route 53 to arrange active-active failover. Create an Aurora database that is restored from the most recent snapshot.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option proposes an active-active failover strategy, which does not match the company's needs. In addition, restoring an Aurora database from the latest snapshot would not assure the least data loss."
      },
      {
        "answer": "Use AWS Backup for data backup. Use the backup to replicate the necessary infrastructure in a second AWS Region. Use Amazon Route 53 for active-passive failover configuration. Create a second primary Aurora instance in the secondary Region.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option is not as efficient because creating a second primary Aurora instance would not provide the required replication for disaster recovery."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html",
      "https://aws.amazon.com/blogs/database/cost-effective-disaster-recovery-for-amazon-aurora-databases-using-aws-backup"
    ]
  },
  {
    "id": 30,
    "question": "A firm operates a web application that receives live data from multiple IoT devices. However, the current system faces challenges during peak traffic periods, resulting in timeout errors. The firm has identified that the Amazon RDS DB instance is unable to handle the varying volume of data sent by the IoT devices.\n\nIn order to address this issue and avoid data loss, the solutions architect is seeking a solution that can reduce the number of database connections during high-traffic periods.\n\nWhich of the following solutions will fulfill these requirements?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Update the application to write the incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. Use an AWS Lambda function that Amazon SNS invokes to transfer data from the topic to the database.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Modify the application to store the incoming data in an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function invoked by Amazon SQS to transfer data from the queue to the database.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Upscale the DB instance to a type with additional available memory.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Modify the DB instance into a Multi-AZ DB instance and configure the application to write to all the active RDS DB instances.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nModify the application to store the incoming data in an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function invoked by Amazon SQS to transfer data from the queue to the database.\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Therefore, SQS can be used to store the incoming IoT data during peak traffic, ensuring no data loss. Subsequently, an AWS Lambda function can be triggered by SQS to process the data in the queue and write it to the RDS database. This strategy will effectively reduce the number of database connections and handle the variable write traffic volume, fulfilling the requirements.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUpscale the DB instance to a type with additional available memory.\n\nUpscaling the DB instance might improve performance but doesn't fundamentally solve the problem of variable traffic and does not ensure no data loss during peak periods.\n\n\n\n\nModify the DB instance into a Multi-AZ DB instance and configure the application to write to all the active RDS DB instances.\n\nThe Multi-AZ feature in RDS is primarily for high availability and failover support for DB instances. It doesn't help with write traffic distribution since all writes go to the primary DB instance only.\n\n\n\n\nUpdate the application to write the incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. Use an AWS Lambda function that Amazon SNS invokes to transfer data from the topic to the database.\n\nSNS is a pub-sub messaging service, not intended to be used as a buffer for variable write traffic. Also, it does not guarantee the order of messages, which might be important in this case.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
    "correctAnswerExplanations": [
      {
        "answer": "Modify the application to store the incoming data in an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function invoked by Amazon SQS to transfer data from the queue to the database.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Therefore, SQS can be used to store the incoming IoT data during peak traffic, ensuring no data loss. Subsequently, an AWS Lambda function can be triggered by SQS to process the data in the queue and write it to the RDS database. This strategy will effectively reduce the number of database connections and handle the variable write traffic volume, fulfilling the requirements."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Upscale the DB instance to a type with additional available memory.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Upscaling the DB instance might improve performance but doesn't fundamentally solve the problem of variable traffic and does not ensure no data loss during peak periods."
      },
      {
        "answer": "Modify the DB instance into a Multi-AZ DB instance and configure the application to write to all the active RDS DB instances.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The Multi-AZ feature in RDS is primarily for high availability and failover support for DB instances. It doesn't help with write traffic distribution since all writes go to the primary DB instance only."
      },
      {
        "answer": "Update the application to write the incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. Use an AWS Lambda function that Amazon SNS invokes to transfer data from the topic to the database.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "SNS is a pub-sub messaging service, not intended to be used as a buffer for variable write traffic. Also, it does not guarantee the order of messages, which might be important in this case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"
    ]
  },
  {
    "id": 31,
    "question": "A software company is deploying an application on AWS. The application uses an Application Load Balancer (ALB) to distribute traffic to at least two Amazon EC2 instances within a single target group. Each environment has an Auto Scaling group associated with its EC2 instances. The company maintains a development environment and a production environment. The production environment experiences peaks of high traffic.\n\nHow can the development environment be set up in the MOST cost-effective manner?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Reduce the maximum number of EC2 instances in the development environment's Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Downsize the EC2 instances in both the development and production environments.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Modify the ALB's balancing algorithm to least outstanding requests.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Reconfigure the target group in the development environment to include just one EC2 instance as a target.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nReconfigure the target group in the development environment to include just one EC2 instance as a target.\n\nThis solution is the most cost-effective as it minimizes the number of EC2 instances running in the development environment. In a development environment, the need for high availability and redundancy is usually lower than in a production environment. Hence, it can operate with a single EC2 instance. By reducing the number of instances to just one, the company can save on the costs of running multiple instances. It's essential to keep in mind that while this approach reduces cost, it doesn't provide high availability.\n\n\n\n\n\n\n\nIncorrect Options:\n\nModify the ALB's balancing algorithm to least outstanding requests.\n\nThis solution won't have any significant impact on cost. The load balancer's algorithm is for distributing incoming requests, not for reducing the number of running instances or decreasing the costs.\n\n\n\n\nDownsize the EC2 instances in both the development and production environments.\n\nDownsizing EC2 instances may not be viable as it can impact the performance of the application, especially in the production environment which experiences high traffic peaks. Moreover, the downsizing might not be necessary for the development environment if it does not have high computational demands.\n\n\n\n\nReduce the maximum number of EC2 instances in the development environment's Auto Scaling group.\n\nThis could potentially reduce costs, it could also cause issues if the development environment needs to handle a temporary increase in load. Moreover, the company has only mentioned that it wants the application to run on at least two instances, and not necessarily more. Thus, the Auto Scaling group may not be scaling beyond two instances in the development environment anyway.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/autoscaling\n\nhttps://aws.amazon.com/elasticloadbalancing",
    "correctAnswerExplanations": [
      {
        "answer": "Reconfigure the target group in the development environment to include just one EC2 instance as a target.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This solution is the most cost-effective as it minimizes the number of EC2 instances running in the development environment. In a development environment, the need for high availability and redundancy is usually lower than in a production environment. Hence, it can operate with a single EC2 instance. By reducing the number of instances to just one, the company can save on the costs of running multiple instances. It's essential to keep in mind that while this approach reduces cost, it doesn't provide high availability."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Modify the ALB's balancing algorithm to least outstanding requests.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This solution won't have any significant impact on cost. The load balancer's algorithm is for distributing incoming requests, not for reducing the number of running instances or decreasing the costs."
      },
      {
        "answer": "Downsize the EC2 instances in both the development and production environments.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Downsizing EC2 instances may not be viable as it can impact the performance of the application, especially in the production environment which experiences high traffic peaks. Moreover, the downsizing might not be necessary for the development environment if it does not have high computational demands."
      },
      {
        "answer": "Reduce the maximum number of EC2 instances in the development environment's Auto Scaling group.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This could potentially reduce costs, it could also cause issues if the development environment needs to handle a temporary increase in load. Moreover, the company has only mentioned that it wants the application to run on at least two instances, and not necessarily more. Thus, the Auto Scaling group may not be scaling beyond two instances in the development environment anyway."
      }
    ],
    "references": [
      "https://aws.amazon.com/autoscaling",
      "https://aws.amazon.com/elasticloadbalancing"
    ]
  },
  {
    "id": 32,
    "question": "A media company operates a video streaming application. The application is structured in three tiers: an Amazon EC2 instance for the front-end, another EC2 instance for the application layer, and a third EC2 instance for a PostgreSQL database. A solutions architect is tasked with designing a solution that ensures high availability and scalability with minimal changes to the application's current architecture.\n\nWhich solution should the solutions architect recommend?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for both front-end and application layer. Migrate the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 for storing and delivering videos.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Host the front-end layer on Amazon S3. Use AWS Lambda functions for the application layer. Migrate the database to an Amazon Aurora PostgreSQL. Store and deliver videos using Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for both the front-end and application layer. Migrate the database to an Amazon RDS DB instance with multiple read replicas for video storage and delivery.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon S3 for the front-end layer hosting. Implement an Auto Scaling group of EC2 instances for the application layer. Move the database to a memory-optimized EC2 instance for storing and delivering videos.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nUse load-balanced Multi-AZ AWS Elastic Beanstalk environments for both front-end and application layer. Migrate the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 for storing and delivering videos.\n\nThe solution provides high availability and scalability with minimal changes to the current application architecture. AWS Elastic Beanstalk simplifies the deployment and scaling of applications, automatically managing the capacity, load balancing, scaling, and health monitoring. Configuring AWS Elastic Beanstalk environments in a load-balanced, Multi-AZ configuration can ensure high availability and scalability for both the front-end and application layers.\n\nFor the database layer, the Amazon RDS Multi-AZ deployment option is recommended. Amazon RDS Multi-AZ deployments provide enhanced availability and durability for database instances, making them a fit for production workloads. In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a standby in a different Availability Zone for failover support.\n\nAmazon S3 should be used for storing and delivering videos. Amazon S3 is a scalable object storage service designed for storing and retrieving data from anywhere on the web. It's perfect for media hosting due to its high durability, scalability, and easy-to-use management features. By storing and delivering videos through S3, the company can offload the heavy traffic related to video delivery from their EC2 instances, thereby increasing their application's scalability.\n\n\n\n\n\n\n\nIncorrect Options:\n\nHost the front-end layer on Amazon S3. Use AWS Lambda functions for the application layer. Migrate the database to an Amazon Aurora PostgreSQL. Store and deliver videos using Amazon S3.\n\nThis option would require significant changes to the application's current architecture and might not fully utilize the computational capabilities of EC2 instances in the application layer.\n\n\n\n\nUse load-balanced Multi-AZ AWS Elastic Beanstalk environments for both the front-end and application layer. Migrate the database to an Amazon RDS DB instance with multiple read replicas for video storage and delivery.\n\nUsing RDS with read replicas for video storage and delivery is inappropriate. Amazon S3 is a more appropriate choice for storing and delivering video content.\n\n\n\n\nUse Amazon S3 for the front-end layer hosting. Implement an Auto Scaling group of EC2 instances for the application layer. Move the database to a memory-optimized EC2 instance for storing and delivering videos.\n\nThis option proposes a valid solution for the application layer with Auto Scaling, using a memory-optimized EC2 instance for database hosting, video storage, and delivery is not optimal. EC2 is not typically used for video storage and delivery, and Amazon RDS is a better choice for database hosting.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\n\nhttps://docs.aws.amazon.com/rds/index.html\n\nhttps://docs.aws.amazon.com/s3/index.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for both front-end and application layer. Migrate the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 for storing and delivering videos.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The solution provides high availability and scalability with minimal changes to the current application architecture. AWS Elastic Beanstalk simplifies the deployment and scaling of applications, automatically managing the capacity, load balancing, scaling, and health monitoring. Configuring AWS Elastic Beanstalk environments in a load-balanced, Multi-AZ configuration can ensure high availability and scalability for both the front-end and application layers."
      },
      {
        "answer": "",
        "explanation": "For the database layer, the Amazon RDS Multi-AZ deployment option is recommended. Amazon RDS Multi-AZ deployments provide enhanced availability and durability for database instances, making them a fit for production workloads. In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a standby in a different Availability Zone for failover support."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 should be used for storing and delivering videos. Amazon S3 is a scalable object storage service designed for storing and retrieving data from anywhere on the web. It's perfect for media hosting due to its high durability, scalability, and easy-to-use management features. By storing and delivering videos through S3, the company can offload the heavy traffic related to video delivery from their EC2 instances, thereby increasing their application's scalability."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Host the front-end layer on Amazon S3. Use AWS Lambda functions for the application layer. Migrate the database to an Amazon Aurora PostgreSQL. Store and deliver videos using Amazon S3.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option would require significant changes to the application's current architecture and might not fully utilize the computational capabilities of EC2 instances in the application layer."
      },
      {
        "answer": "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for both the front-end and application layer. Migrate the database to an Amazon RDS DB instance with multiple read replicas for video storage and delivery.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Using RDS with read replicas for video storage and delivery is inappropriate. Amazon S3 is a more appropriate choice for storing and delivering video content."
      },
      {
        "answer": "Use Amazon S3 for the front-end layer hosting. Implement an Auto Scaling group of EC2 instances for the application layer. Move the database to a memory-optimized EC2 instance for storing and delivering videos.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option proposes a valid solution for the application layer with Auto Scaling, using a memory-optimized EC2 instance for database hosting, video storage, and delivery is not optimal. EC2 is not typically used for video storage and delivery, and Amazon RDS is a better choice for database hosting."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html",
      "https://docs.aws.amazon.com/rds/index.html",
      "https://docs.aws.amazon.com/s3/index.html"
    ]
  },
  {
    "id": 33,
    "question": "A company is integrating a third-party analytics service into its AWS environment. The service provider runs its software in its own AWS account, but the company doesn't want to provide the provider with direct IAM access to its environment.\n\nWhat should the solutions architect do to grant the necessary access?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an IAM role in the company’s account and grant permission to the service provider’s IAM role. Assign the necessary IAM policies to this role.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an IAM user in the company’s account with password fulfilling the complexity requirements. Attach the appropriate IAM policies to this user.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an IAM group in the company’s account and add the service provider’s IAM user to this group. Attach the necessary IAM policies to the group.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a new identity provider by selecting “AWS account” as the type in IAM. Input the service provider’s AWS account ID and username. Assign the necessary IAM policies to the provider.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate an IAM role in the company’s account and grant permission to the service provider’s IAM role. Assign the necessary IAM policies to this role.\n\nIAM (Identity and Access Management) roles are a secure way to grant permissions to entities that you trust, such as AWS services, applications running on EC2, or even users in another AWS account. Roles avoid the need to share and manage long-term credentials, thus enhancing security.\n\nIn our case, the company needs to provide access to a third-party service running in a separate AWS account. The optimal way to do this is by creating an IAM role in the company's AWS account and allowing the third-party service's IAM role to assume this role. This mechanism, known as role delegation, will enable the third-party service to perform actions in the company's AWS account under the permissions assigned to the role.\n\nThe role should be configured with the appropriate IAM policies that outline the precise permissions the service requires. By using this method, the company can ensure that the third-party service only has the necessary access and no more, adhering to the principle of least privilege, which is a fundamental aspect of cloud security.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an IAM user in the company’s account with password fulfilling the complexity requirements. Attach the appropriate IAM policies to this user.\n\nCreating a new IAM user and sharing credentials is not a recommended practice as it does not adhere to the principle of least privilege and poses a potential security risk.\n\n\n\n\nCreate an IAM group in the company’s account and add the service provider’s IAM user to this group. Attach the necessary IAM policies to the group.\n\nIAM groups cannot contain IAM users from other accounts, making this approach technically infeasible.\n\n\n\n\nCreate a new identity provider by selecting “AWS account” as the type in IAM. Input the service provider’s AWS account ID and username. Assign the necessary IAM policies to the provider.\n\nAWS does not support creating identity providers with an “AWS account” type. Identity providers are typically used for integrating with external identity systems, not for cross-account access.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create an IAM role in the company’s account and grant permission to the service provider’s IAM role. Assign the necessary IAM policies to this role.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "IAM (Identity and Access Management) roles are a secure way to grant permissions to entities that you trust, such as AWS services, applications running on EC2, or even users in another AWS account. Roles avoid the need to share and manage long-term credentials, thus enhancing security."
      },
      {
        "answer": "",
        "explanation": "In our case, the company needs to provide access to a third-party service running in a separate AWS account. The optimal way to do this is by creating an IAM role in the company's AWS account and allowing the third-party service's IAM role to assume this role. This mechanism, known as role delegation, will enable the third-party service to perform actions in the company's AWS account under the permissions assigned to the role."
      },
      {
        "answer": "",
        "explanation": "The role should be configured with the appropriate IAM policies that outline the precise permissions the service requires. By using this method, the company can ensure that the third-party service only has the necessary access and no more, adhering to the principle of least privilege, which is a fundamental aspect of cloud security."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create an IAM user in the company’s account with password fulfilling the complexity requirements. Attach the appropriate IAM policies to this user.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Creating a new IAM user and sharing credentials is not a recommended practice as it does not adhere to the principle of least privilege and poses a potential security risk."
      },
      {
        "answer": "Create an IAM group in the company’s account and add the service provider’s IAM user to this group. Attach the necessary IAM policies to the group.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "IAM groups cannot contain IAM users from other accounts, making this approach technically infeasible."
      },
      {
        "answer": "Create a new identity provider by selecting “AWS account” as the type in IAM. Input the service provider’s AWS account ID and username. Assign the necessary IAM policies to the provider.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS does not support creating identity providers with an “AWS account” type. Identity providers are typically used for integrating with external identity systems, not for cross-account access."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html"
    ]
  },
  {
    "id": 34,
    "question": "An organization runs a critical accounting application on an on-premises data center. The application uses an Oracle database that cannot be migrated to AWS due to customer preferences. The company wants to establish a secure, site-to-site VPN connection with its existing AWS environment in a VPC.\n\nWhich should be configured outside the VPC to ensure the successful setup of a site-to-site VPN connection?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "The internet-routable IP address for the external interface",
        "correct": true
      },
      {
        "id": 2,
        "answer": "A dedicated Elastic IP attached to the Virtual Private Gateway",
        "correct": false
      },
      {
        "id": 3,
        "answer": "The primary route table associated with the VPC",
        "correct": false
      },
      {
        "id": 4,
        "answer": "A public subnet allocated to a dedicated NAT gateway",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nThe internet-routable IP address for the external interface\n\nTo establish a site-to-site VPN connection between the on-premises data center and the AWS Virtual Private Cloud (VPC), the on-premises network must have an externally accessible IP address. This IP address is used to establish the VPN tunnel and facilitate secure communication between the on-premises network and the VPC.\n\nThe on-premises network typically has an edge device, such as a router or firewall, which connects to the internet and has an external interface with an internet-routable IP address. This IP address is used as the endpoint for the VPN tunnel. The configuration of the VPN tunnel on the AWS side requires the specification of the external IP address of the on-premises device.\n\nBy configuring the on-premises device with the internet-routable IP address for its external interface, the VPN tunnel can be established between the on-premises network and the AWS VPC. This allows secure communication between the critical accounting application running in the on-premises data center and the resources within the VPC.\n\n\n\n\n\n\n\nIncorrect Options:\n\nA public subnet allocated to a dedicated NAT gateway\n\nA public subnet or NAT gateway is not relevant for setting up a site-to-site VPN connection. NAT gateways enable instances in a private subnet to connect to the internet or other AWS services, but they are not involved in the configuration of VPN connections.\n\n\n\n\nThe primary route table associated with the VPC\n\nRoute table does not need to be directly configured when setting up a site-to-site VPN. The route table controls the routing for the subnet traffic within the VPC.\n\n\n\n\nA dedicated Elastic IP attached to the Virtual Private Gateway\n\nAn Elastic IP address is a static, internet-routable IP address, but it's not specifically required for setting up a site-to-site VPN connection. The VPN connection uses the Virtual Private Gateway and the Customer Gateway (on-premises) to establish the VPN tunnel.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/SetUpVPNConnections.html\n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html\n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html",
    "correctAnswerExplanations": [
      {
        "answer": "The internet-routable IP address for the external interface",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "To establish a site-to-site VPN connection between the on-premises data center and the AWS Virtual Private Cloud (VPC), the on-premises network must have an externally accessible IP address. This IP address is used to establish the VPN tunnel and facilitate secure communication between the on-premises network and the VPC."
      },
      {
        "answer": "",
        "explanation": "The on-premises network typically has an edge device, such as a router or firewall, which connects to the internet and has an external interface with an internet-routable IP address. This IP address is used as the endpoint for the VPN tunnel. The configuration of the VPN tunnel on the AWS side requires the specification of the external IP address of the on-premises device."
      },
      {
        "answer": "",
        "explanation": "By configuring the on-premises device with the internet-routable IP address for its external interface, the VPN tunnel can be established between the on-premises network and the AWS VPC. This allows secure communication between the critical accounting application running in the on-premises data center and the resources within the VPC."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "A public subnet allocated to a dedicated NAT gateway",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A public subnet or NAT gateway is not relevant for setting up a site-to-site VPN connection. NAT gateways enable instances in a private subnet to connect to the internet or other AWS services, but they are not involved in the configuration of VPN connections."
      },
      {
        "answer": "The primary route table associated with the VPC",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Route table does not need to be directly configured when setting up a site-to-site VPN. The route table controls the routing for the subnet traffic within the VPC."
      },
      {
        "answer": "A dedicated Elastic IP attached to the Virtual Private Gateway",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An Elastic IP address is a static, internet-routable IP address, but it's not specifically required for setting up a site-to-site VPN connection. The VPN connection uses the Virtual Private Gateway and the Customer Gateway (on-premises) to establish the VPN tunnel."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpn/latest/s2svpn/SetUpVPNConnections.html",
      "https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html",
      "https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html"
    ]
  },
  {
    "id": 35,
    "question": "A company has several Amazon EC2 instances for testing purposes. Each instance is hosted in an isolated VPC. The company’s security team wants to receive notifications when outbound connections to specific risky IPs are detected.\n\nWhat should you recommend to meet these requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change Notification, configure an Amazon Simple Notification Service (Amazon SNS) topic as a target, and subscribe the security team to the topic.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure an IAM instance profile to the EC2 instances with an IAM role that includes the AmazonSSMManagedInstanceCore policy.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure Amazon CloudWatch Application Insights to create AWS Systems Manager OpsItems when connections to risky IPs are detected.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Enable VPC flow logs, publish them to Amazon CloudWatch Logs, create necessary metric filters, and create an Amazon CloudWatch alarm with a notification action for when the alarm transitions to the ALARM state.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nEnable VPC flow logs, publish them to Amazon CloudWatch Logs, create necessary metric filters, and create an Amazon CloudWatch alarm with a notification action for when the alarm transitions to the ALARM state.\n\nVPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be useful for diagnostic and troubleshooting, for security and compliance auditing, and for monitoring the network traffic patterns of your applications.\n\nEnabling VPC Flow Logs will allow you to monitor and record all the outbound connections from Amazon EC2 instances. The log data collected can be published to Amazon CloudWatch Logs, a scalable, highly available, and fully managed log management service that can ingest application and system log data, and allows you to search, analyze, and visualize this data in near real-time.\n\nOnce the data is in CloudWatch Logs, you can then create specific metric filters. A metric filter extracts data from a log stream in CloudWatch Logs and turns it into a CloudWatch metric. So, in our case, you could create a metric filter to look for outbound connections to the risky IPs specified.\n\nAn Amazon CloudWatch Alarm can be set up based on the metric created by the filter. This alarm can be configured to transition to the ALARM state when certain conditions (like detecting a connection to a risky IP) are met. When the alarm is in the ALARM state, a notification action can be configured to notify the company's security team about the risky connection, fulfilling the requirement mentioned in the scenario.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure Amazon CloudWatch Application Insights to create AWS Systems Manager OpsItems when connections to risky IPs are detected.\n\nCloudWatch Application Insights doesn't monitor traffic or IP connections. It focuses on the application's logs and metrics to provide insights about performance and availability issues.\n\n\n\n\nConfigure an IAM instance profile to the EC2 instances with an IAM role that includes the AmazonSSMManagedInstanceCore policy.\n\nIAM roles are used for granting permissions, the AmazonSSMManagedInstanceCore policy is for using AWS Systems Manager with your instances and doesn't address the requirement of detecting and notifying about connections to risky IPs.\n\n\n\n\nConfigure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change Notification, configure an Amazon Simple Notification Service (Amazon SNS) topic as a target, and subscribe the security team to the topic.\n\nThis solution would only notify the security team of state changes to the EC2 instances, such as when an instance is stopped, started, or terminated. It would not provide notifications for connections to risky IPs.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html",
    "correctAnswerExplanations": [
      {
        "answer": "Enable VPC flow logs, publish them to Amazon CloudWatch Logs, create necessary metric filters, and create an Amazon CloudWatch alarm with a notification action for when the alarm transitions to the ALARM state.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be useful for diagnostic and troubleshooting, for security and compliance auditing, and for monitoring the network traffic patterns of your applications."
      },
      {
        "answer": "",
        "explanation": "Enabling VPC Flow Logs will allow you to monitor and record all the outbound connections from Amazon EC2 instances. The log data collected can be published to Amazon CloudWatch Logs, a scalable, highly available, and fully managed log management service that can ingest application and system log data, and allows you to search, analyze, and visualize this data in near real-time."
      },
      {
        "answer": "",
        "explanation": "Once the data is in CloudWatch Logs, you can then create specific metric filters. A metric filter extracts data from a log stream in CloudWatch Logs and turns it into a CloudWatch metric. So, in our case, you could create a metric filter to look for outbound connections to the risky IPs specified."
      },
      {
        "answer": "",
        "explanation": "An Amazon CloudWatch Alarm can be set up based on the metric created by the filter. This alarm can be configured to transition to the ALARM state when certain conditions (like detecting a connection to a risky IP) are met. When the alarm is in the ALARM state, a notification action can be configured to notify the company's security team about the risky connection, fulfilling the requirement mentioned in the scenario."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Configure Amazon CloudWatch Application Insights to create AWS Systems Manager OpsItems when connections to risky IPs are detected.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "CloudWatch Application Insights doesn't monitor traffic or IP connections. It focuses on the application's logs and metrics to provide insights about performance and availability issues."
      },
      {
        "answer": "Configure an IAM instance profile to the EC2 instances with an IAM role that includes the AmazonSSMManagedInstanceCore policy.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "IAM roles are used for granting permissions, the AmazonSSMManagedInstanceCore policy is for using AWS Systems Manager with your instances and doesn't address the requirement of detecting and notifying about connections to risky IPs."
      },
      {
        "answer": "Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change Notification, configure an Amazon Simple Notification Service (Amazon SNS) topic as a target, and subscribe the security team to the topic.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This solution would only notify the security team of state changes to the EC2 instances, such as when an instance is stopped, started, or terminated. It would not provide notifications for connections to risky IPs."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html"
    ]
  },
  {
    "id": 36,
    "question": "A business is preparing to operate on an Amazon EC2 instance and plans to use EBS volumes for data storage. The company intends to keep its data even if the EC2 instance is terminated. The root volume is deleted by default upon instance termination.\n\nWhich option should you take to ensure the volume persists after instance termination?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "During the Add Storage process, check the Delete On Termination option.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "When launching the instance, uncheck the Delete On Termination option.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "When launching the instance, check the Delete On Termination option.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "During the Add Storage process, uncheck the Delete On Termination option.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nDuring the Add Storage process, uncheck the Delete On Termination option.\n\nDuring the instance creation process, there's a stage to add storage, known as the Add Storage step. Here, you can modify the settings for your root EBS volume and additional EBS volumes. If you don't want the EBS volume to be deleted when the instance is terminated, you need to uncheck the \"Delete on Termination\" checkbox. By doing so, you are instructing AWS not to delete the volume when the instance is terminated.\n\n\n\n\n\n\n\nIncorrect Options:\n\nWhen launching the instance, uncheck the Delete On Termination option.\n\nThere's no option to check/uncheck \"Delete on Termination\" when launching the instance. This option is available during the \"Add Storage\" step of the instance creation process.\n\n\n\n\nWhen launching the instance, check the Delete On Termination option.\n\nThere's no option to check/uncheck \"Delete on Termination\" when launching the instance. This option is available during the \"Add Storage\" step of the instance creation process.\n\n\n\n\nDuring the Add Storage process, check the Delete On Termination option.\n\nChecking this option will lead to the deletion of the EBS volume when the instance is terminated, which is contrary to the requirement of preserving the volume data.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#preserving-volumes-on-termination",
    "correctAnswerExplanations": [
      {
        "answer": "During the Add Storage process, uncheck the Delete On Termination option.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "During the instance creation process, there's a stage to add storage, known as the Add Storage step. Here, you can modify the settings for your root EBS volume and additional EBS volumes. If you don't want the EBS volume to be deleted when the instance is terminated, you need to uncheck the \"Delete on Termination\" checkbox. By doing so, you are instructing AWS not to delete the volume when the instance is terminated."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "When launching the instance, uncheck the Delete On Termination option.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "There's no option to check/uncheck \"Delete on Termination\" when launching the instance. This option is available during the \"Add Storage\" step of the instance creation process."
      },
      {
        "answer": "When launching the instance, check the Delete On Termination option.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "There's no option to check/uncheck \"Delete on Termination\" when launching the instance. This option is available during the \"Add Storage\" step of the instance creation process."
      },
      {
        "answer": "During the Add Storage process, check the Delete On Termination option.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Checking this option will lead to the deletion of the EBS volume when the instance is terminated, which is contrary to the requirement of preserving the volume data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#preserving-volumes-on-termination"
    ]
  },
  {
    "id": 37,
    "question": "A company running a gaming platform has a web application that showcases player scores. This application operates on Amazon EC2 instances, behind an Application Load Balancer, while its data is stored in an Amazon RDS for MySQL database. Recently, users have been facing significant delays and interruptions due to database read performance issues. The company aims to enhance user experience while minimizing changes to the architecture.\n\nWhat would be the most appropriate action to meet these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use RDS Proxy between the application and the database.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon ElastiCache in front of the database.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Migrate the application from EC2 instances to AWS Lambda.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nUse RDS Proxy between the application and the database.\n\nAmazon RDS Proxy is a fully managed, highly available database proxy for Amazon Relational Database Service (RDS) that makes applications more scalable, more resilient to database failures, and more secure. It allows applications to pool and share database connections, reducing the overhead of establishing new connections and relieving the pressure on the database, which can significantly improve application scalability and latency.\n\nIn our case, using RDS Proxy between the application and the database can help manage the database connections more efficiently and reduce the impact of database read performance issues on the end users. It provides a solution that can improve the user experience by enhancing the database performance and availability without requiring significant changes to the existing architecture.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon ElastiCache in front of the database.\n\nWhile ElastiCache can provide advantages for caching frequently accessed data, it may not be suitable for our case due to the dynamic nature of the score. Moreover, using ElastiCache would require significant changes to the application's architecture. Additionally, it is worth considering that ElastiCache might not deliver the same level of read performance enhancement as RDS Proxy, particularly when the application heavily relies on complex queries or frequently updates the data.\n\n\n\n\nMigrate the application from EC2 instances to AWS Lambda.\n\nMigrating the application from EC2 instances to AWS Lambda would not address the database performance issues. Although Lambda can help scale the application, it doesn't resolve the underlying database read performance issue.\n\n\n\n\nMigrate the database from Amazon RDS for MySQL to Amazon DynamoDB.\n\nMigrating the database to DynamoDB would be a significant architectural change and may not necessarily improve the read performance. DynamoDB is a NoSQL database and is designed for different use cases than MySQL. It also involves a significant change in the data modeling and access patterns, contradicting the requirement to minimize architectural changes.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/proxy",
    "correctAnswerExplanations": [
      {
        "answer": "Use RDS Proxy between the application and the database.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon RDS Proxy is a fully managed, highly available database proxy for Amazon Relational Database Service (RDS) that makes applications more scalable, more resilient to database failures, and more secure. It allows applications to pool and share database connections, reducing the overhead of establishing new connections and relieving the pressure on the database, which can significantly improve application scalability and latency."
      },
      {
        "answer": "",
        "explanation": "In our case, using RDS Proxy between the application and the database can help manage the database connections more efficiently and reduce the impact of database read performance issues on the end users. It provides a solution that can improve the user experience by enhancing the database performance and availability without requiring significant changes to the existing architecture."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Amazon ElastiCache in front of the database.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "While ElastiCache can provide advantages for caching frequently accessed data, it may not be suitable for our case due to the dynamic nature of the score. Moreover, using ElastiCache would require significant changes to the application's architecture. Additionally, it is worth considering that ElastiCache might not deliver the same level of read performance enhancement as RDS Proxy, particularly when the application heavily relies on complex queries or frequently updates the data."
      },
      {
        "answer": "Migrate the application from EC2 instances to AWS Lambda.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Migrating the application from EC2 instances to AWS Lambda would not address the database performance issues. Although Lambda can help scale the application, it doesn't resolve the underlying database read performance issue."
      },
      {
        "answer": "Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Migrating the database to DynamoDB would be a significant architectural change and may not necessarily improve the read performance. DynamoDB is a NoSQL database and is designed for different use cases than MySQL. It also involves a significant change in the data modeling and access patterns, contradicting the requirement to minimize architectural changes."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/proxy"
    ]
  },
  {
    "id": 38,
    "question": "A company operates a multiplayer game on AWS, where lag can significantly impact the gaming experience. The game application is deployed globally in every AWS Region on Amazon EC2 instances. These instances are grouped into Auto Scaling groups and sit behind Application Load Balancers (ALBs). The solutions architect must implement a mechanism to continually monitor the application's health and route traffic towards healthy endpoints.\n\nWhat is the most suitable solution to achieve these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure an accelerator using AWS Global Accelerator. Add a listener for the application's listening port and attach it with a Regional endpoint in each region. Add the ALB as the endpoint.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an Amazon CloudFront distribution using Amazon S3 as the origin server. Configure the cache behavior to adhere to origin cache headers. Use AWS Lambda functions for traffic optimization.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon DynamoDB as the application's data storage. Create a DynamoDB Accelerator (DAX) cluster as the in-memory cache for DynamoDB to hold application data.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon CloudFront distribution using the ALB as the origin server. Set the cache behavior to follow origin cache headers. Use AWS Lambda functions to manage traffic flow.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nConfigure an accelerator using AWS Global Accelerator. Add a listener for the application's listening port and attach it with a Regional endpoint in each region. Add the ALB as the endpoint.\n\nAWS Global Accelerator improves the availability and performance of your applications by using the AWS global network. It provides static IP addresses that act as a fixed entry point to your applications and routes user traffic to the optimal AWS endpoint based on several factors like geographic proximity, health of the endpoint, and user-configurable routing policies.\n\nCreating an accelerator in Global Accelerator helps to direct user traffic to healthy regional endpoints where the application is running. By attaching a listener for the application's port and designating the ALBs as the regional endpoints, traffic is distributed across the different EC2 instances via the ALBs, which provides automatic routing to healthy instances.\n\nThis setup provides low-latency, high-availability performance necessary for the multiplayer gaming application. This solution is more appropriate compared to the other options as it focuses on performance enhancement, availability, and automatic routing to healthy endpoints rather than on caching or data storage.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an Amazon CloudFront distribution using the ALB as the origin server. Set the cache behavior to follow origin cache headers. Use AWS Lambda functions to manage traffic flow.\n\nCloudFront is a content delivery network (CDN) that speeds up the distribution of static and dynamic web content, but it does not support the concept of routing to the nearest healthy endpoint across multiple regions. CloudFront uses edge locations, not regional endpoints, and AWS Lambda@Edge can be used to customize content, it wouldn't provide the same seamless failover and global performance optimization as Global Accelerator.\n\n\n\n\nCreate an Amazon CloudFront distribution using Amazon S3 as the origin server. Configure the cache behavior to adhere to origin cache headers. Use AWS Lambda functions for traffic optimization.\n\nAmazon S3 is a storage service and wouldn't be appropriate as a primary hosting solution for a globally-distributed multiplayer game. It is mostly used for storing and retrieving data at any scale, but not for running applications or providing endpoints for a global accelerator.\n\n\n\n\nUse Amazon DynamoDB as the application's data storage. Create a DynamoDB Accelerator (DAX) cluster as the in-memory cache for DynamoDB to hold application data.\n\nDynamoDB is a fully-managed NoSQL database service, and DynamoDB Accelerator (DAX) provides fast, in-memory read performance. These are not designed to manage global traffic routing to an application deployed in multiple AWS regions.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/global-accelerator/index.html\n\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/about-accelerators.alb-accelerator.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure an accelerator using AWS Global Accelerator. Add a listener for the application's listening port and attach it with a Regional endpoint in each region. Add the ALB as the endpoint.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Global Accelerator improves the availability and performance of your applications by using the AWS global network. It provides static IP addresses that act as a fixed entry point to your applications and routes user traffic to the optimal AWS endpoint based on several factors like geographic proximity, health of the endpoint, and user-configurable routing policies."
      },
      {
        "answer": "",
        "explanation": "Creating an accelerator in Global Accelerator helps to direct user traffic to healthy regional endpoints where the application is running. By attaching a listener for the application's port and designating the ALBs as the regional endpoints, traffic is distributed across the different EC2 instances via the ALBs, which provides automatic routing to healthy instances."
      },
      {
        "answer": "",
        "explanation": "This setup provides low-latency, high-availability performance necessary for the multiplayer gaming application. This solution is more appropriate compared to the other options as it focuses on performance enhancement, availability, and automatic routing to healthy endpoints rather than on caching or data storage."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create an Amazon CloudFront distribution using the ALB as the origin server. Set the cache behavior to follow origin cache headers. Use AWS Lambda functions to manage traffic flow.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "CloudFront is a content delivery network (CDN) that speeds up the distribution of static and dynamic web content, but it does not support the concept of routing to the nearest healthy endpoint across multiple regions. CloudFront uses edge locations, not regional endpoints, and AWS Lambda@Edge can be used to customize content, it wouldn't provide the same seamless failover and global performance optimization as Global Accelerator."
      },
      {
        "answer": "Create an Amazon CloudFront distribution using Amazon S3 as the origin server. Configure the cache behavior to adhere to origin cache headers. Use AWS Lambda functions for traffic optimization.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 is a storage service and wouldn't be appropriate as a primary hosting solution for a globally-distributed multiplayer game. It is mostly used for storing and retrieving data at any scale, but not for running applications or providing endpoints for a global accelerator."
      },
      {
        "answer": "Use Amazon DynamoDB as the application's data storage. Create a DynamoDB Accelerator (DAX) cluster as the in-memory cache for DynamoDB to hold application data.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "DynamoDB is a fully-managed NoSQL database service, and DynamoDB Accelerator (DAX) provides fast, in-memory read performance. These are not designed to manage global traffic routing to an application deployed in multiple AWS regions."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/global-accelerator/index.html",
      "https://docs.aws.amazon.com/global-accelerator/latest/dg/about-accelerators.alb-accelerator.html"
    ]
  },
  {
    "id": 39,
    "question": "A media company's streaming application recently suffered from a malicious attack from a particular IP address that attempted to steal sensitive data. This application uses a CloudFront distribution connected to an ALB and has implemented AWS WAF for security.\n\nWhat steps should the cybersecurity team take to protect against such attacks in the future?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a rule to deny the attacking IP in the Security Groups associated with the application instances.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a ticket with AWS Support to take necessary action against the attacking IP address.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a rule to deny the attacking IP in the Network Access Control List (NACL) tied to the application instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an IP match condition in AWS WAF to block incoming requests from the offending IP address.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate an IP match condition in AWS WAF to block incoming requests from the offending IP address.\n\nAWS WAF (Web Application Firewall) protects web applications from common web exploits and attacks. It allows you to define customizable rules to filter and monitor incoming web traffic. AWS WAF can be integrated with Amazon CloudFront or Application Load Balancer, providing real-time protection against malicious requests, such as SQL injection, cross-site scripting, and distributed denial-of-service (DDoS) attacks. It helps improve the security and availability of your web applications running on AWS.\n\nTo protect against attacks from the malicious IP address, the cybersecurity team should create an IP match condition in AWS WAF. An IP match condition identifies the specific IP addresses or IP address ranges that AWS WAF should allow or block based on the rules that you've defined. This will ensure that any future requests from the same IP address are automatically blocked before reaching the application.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a rule to deny the attacking IP in the Network Access Control List (NACL) tied to the application instances.\n\nNACL is not the best place to deny specific IPs that are attacking your application. NACLs work at the subnet level, blocking an IP here could unintentionally affect other applications within the subnet.\n\n\n\n\nCreate a rule to deny the attacking IP in the Security Groups associated with the application instances.\n\nSecurity groups are stateful and operate at the instance level. Blocking an IP address at the security group level can be complicated and unwieldy, especially if there are many IP addresses to block or if the IP addresses change frequently.\n\n\n\n\nCreate a ticket with AWS Support to take necessary action against the attacking IP address.\n\nEven though AWS Support can provide assistance in resolving issues, it is ultimately the responsibility of the user to secure their own application. AWS WAF is designed specifically to protect your application from such threats.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create an IP match condition in AWS WAF to block incoming requests from the offending IP address.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS WAF (Web Application Firewall) protects web applications from common web exploits and attacks. It allows you to define customizable rules to filter and monitor incoming web traffic. AWS WAF can be integrated with Amazon CloudFront or Application Load Balancer, providing real-time protection against malicious requests, such as SQL injection, cross-site scripting, and distributed denial-of-service (DDoS) attacks. It helps improve the security and availability of your web applications running on AWS."
      },
      {
        "answer": "",
        "explanation": "To protect against attacks from the malicious IP address, the cybersecurity team should create an IP match condition in AWS WAF. An IP match condition identifies the specific IP addresses or IP address ranges that AWS WAF should allow or block based on the rules that you've defined. This will ensure that any future requests from the same IP address are automatically blocked before reaching the application."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a rule to deny the attacking IP in the Network Access Control List (NACL) tied to the application instances.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "NACL is not the best place to deny specific IPs that are attacking your application. NACLs work at the subnet level, blocking an IP here could unintentionally affect other applications within the subnet."
      },
      {
        "answer": "Create a rule to deny the attacking IP in the Security Groups associated with the application instances.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Security groups are stateful and operate at the instance level. Blocking an IP address at the security group level can be complicated and unwieldy, especially if there are many IP addresses to block or if the IP addresses change frequently."
      },
      {
        "answer": "Create a ticket with AWS Support to take necessary action against the attacking IP address.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Even though AWS Support can provide assistance in resolving issues, it is ultimately the responsibility of the user to secure their own application. AWS WAF is designed specifically to protect your application from such threats."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html"
    ]
  },
  {
    "id": 40,
    "question": "A solutions architect is tasked with designing a high availability multi-tier application. The application includes web, application, and database layers, and it is crucial that HTTPS content is delivered with the lowest latency possible.\n\nWhich solution meets these requirements while providing the highest level of security?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Use Amazon CloudFront for HTTPS content delivery using the EC2 instances as the origin.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Use Amazon CloudFront for HTTPS content delivery using the EC2 instances as the origin.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Use Amazon CloudFront for HTTPS content delivery using the public ALB as the origin.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Use Amazon CloudFront for HTTPS content delivery using the public ALB as the origin.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nConfigure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Use Amazon CloudFront for HTTPS content delivery using the public ALB as the origin.\n\nUsing an Application Load Balancer (ALB) in the public subnet enables traffic distribution across multiple Amazon EC2 instances, improving the application's availability. However, hosting the EC2 instances in private subnets rather than public ones adds an additional layer of security by shielding these instances from direct access to the internet. This approach minimizes the risk of unauthorized access and potential attacks on your EC2 instances.\n\nUsing Amazon CloudFront for HTTPS content delivery enhances the user experience by reducing latency. CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. It does so by caching content in edge locations around the world, bringing it closer to the users.\n\nIn this solution, CloudFront communicates with the ALB, not directly with the EC2 instances. This makes the EC2 instances even more secure, as they don't have to be exposed to the internet, even for communication with CloudFront.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Use Amazon CloudFront for HTTPS content delivery using the public ALB as the origin.\n\nRunning EC2 instances in public subnets means they have direct access to the internet, which increases their exposure to potential security threats. So, it's generally not recommended for applications requiring high security.\n\n\n\n\nConfigure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Use Amazon CloudFront for HTTPS content delivery using the EC2 instances as the origin.\n\nCloudFront can't use EC2 instances in private subnets as an origin because they aren't accessible from the internet. CloudFront requires a public IP address to pull content from the origin.\n\n\n\n\nConfigure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Use Amazon CloudFront for HTTPS content delivery using the EC2 instances as the origin.\n\nRunning EC2 instances in public subnets isn't recommended for high-security applications. Additionally, using EC2 instances as the origin for CloudFront rather than a load balancer does not offer the same level of distribution and failover capabilities.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/elasticloadbalancing/features",
    "correctAnswerExplanations": [
      {
        "answer": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Use Amazon CloudFront for HTTPS content delivery using the public ALB as the origin.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Using an Application Load Balancer (ALB) in the public subnet enables traffic distribution across multiple Amazon EC2 instances, improving the application's availability. However, hosting the EC2 instances in private subnets rather than public ones adds an additional layer of security by shielding these instances from direct access to the internet. This approach minimizes the risk of unauthorized access and potential attacks on your EC2 instances."
      },
      {
        "answer": "",
        "explanation": "Using Amazon CloudFront for HTTPS content delivery enhances the user experience by reducing latency. CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. It does so by caching content in edge locations around the world, bringing it closer to the users."
      },
      {
        "answer": "",
        "explanation": "In this solution, CloudFront communicates with the ALB, not directly with the EC2 instances. This makes the EC2 instances even more secure, as they don't have to be exposed to the internet, even for communication with CloudFront."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Use Amazon CloudFront for HTTPS content delivery using the public ALB as the origin.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Running EC2 instances in public subnets means they have direct access to the internet, which increases their exposure to potential security threats. So, it's generally not recommended for applications requiring high security."
      },
      {
        "answer": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Use Amazon CloudFront for HTTPS content delivery using the EC2 instances as the origin.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "CloudFront can't use EC2 instances in private subnets as an origin because they aren't accessible from the internet. CloudFront requires a public IP address to pull content from the origin."
      },
      {
        "answer": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Use Amazon CloudFront for HTTPS content delivery using the EC2 instances as the origin.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Running EC2 instances in public subnets isn't recommended for high-security applications. Additionally, using EC2 instances as the origin for CloudFront rather than a load balancer does not offer the same level of distribution and failover capabilities."
      }
    ],
    "references": [
      "https://aws.amazon.com/elasticloadbalancing/features"
    ]
  },
  {
    "id": 41,
    "question": "A company's application uses two NAT instances that are not able to support the application's traffic requirements. The solutions architect needs to provide a solution that ensures high availability, fault tolerance, and automatic scalability.\n\nWhat should the solutions architect propose?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Replace the two NAT instances with On-Demand Instances located in different Availability Zones and deploy an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Replace the two NAT instances with two NAT gateways spread across different Availability Zones.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Auto Scaling groups along with Application Load Balancers for the NAT instances distributed across different Availability Zones.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Replace the two NAT instances with two NAT gateways located in the same Availability Zone.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nReplace the two NAT instances with two NAT gateways spread across different Availability Zones.\n\nAmazon NAT Gateway is a highly available AWS managed service that operates at high throughput levels, provides automatic scalability, and requires less administrative effort and maintenance compared to a self-managed NAT instance. NAT Gateway helps instances in a private subnet to connect to the internet or other AWS services but prevents the internet from initiating connections with those instances. Distributing two NAT gateways across different Availability Zones ensures high availability and fault tolerance by providing redundancy and mitigating the risk of an Availability Zone failure.\n\n\n\n\n\n\n\nIncorrect Options:\n\nReplace the two NAT instances with two NAT gateways located in the same Availability Zone.\n\nHaving both NAT Gateways in the same Availability Zone does not provide fault tolerance. If the Availability Zone experiences an outage, both NAT Gateways would be unavailable.\n\n\n\n\nUse Auto Scaling groups along with Application Load Balancers for the NAT instances distributed across different Availability Zones.\n\nNAT instances can be a bottleneck for internet access and require manual scaling while also adding an overhead of managing Auto Scaling groups and Load Balancers. This solution is less optimal compared to using NAT gateways which are fully managed, scalable and provide better throughput.\n\n\n\n\nReplace the two NAT instances with On-Demand Instances located in different Availability Zones and deploy an Application Load Balancer.\n\nOn-Demand Instances and Application Load Balancers are not designed for the functionality that NAT Gateways or NAT Instances provide. They cannot perform Network Address Translation (NAT) and would not satisfy the application's traffic requirements.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
    "correctAnswerExplanations": [
      {
        "answer": "Replace the two NAT instances with two NAT gateways spread across different Availability Zones.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon NAT Gateway is a highly available AWS managed service that operates at high throughput levels, provides automatic scalability, and requires less administrative effort and maintenance compared to a self-managed NAT instance. NAT Gateway helps instances in a private subnet to connect to the internet or other AWS services but prevents the internet from initiating connections with those instances. Distributing two NAT gateways across different Availability Zones ensures high availability and fault tolerance by providing redundancy and mitigating the risk of an Availability Zone failure."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Replace the two NAT instances with two NAT gateways located in the same Availability Zone.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Having both NAT Gateways in the same Availability Zone does not provide fault tolerance. If the Availability Zone experiences an outage, both NAT Gateways would be unavailable."
      },
      {
        "answer": "Use Auto Scaling groups along with Application Load Balancers for the NAT instances distributed across different Availability Zones.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "NAT instances can be a bottleneck for internet access and require manual scaling while also adding an overhead of managing Auto Scaling groups and Load Balancers. This solution is less optimal compared to using NAT gateways which are fully managed, scalable and provide better throughput."
      },
      {
        "answer": "Replace the two NAT instances with On-Demand Instances located in different Availability Zones and deploy an Application Load Balancer.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "On-Demand Instances and Application Load Balancers are not designed for the functionality that NAT Gateways or NAT Instances provide. They cannot perform Network Address Translation (NAT) and would not satisfy the application's traffic requirements."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"
    ]
  },
  {
    "id": 42,
    "question": "A company has moved its data analysis platform to AWS and maintains an AWS Direct Connect connection. Employees at the headquarters use an analytical tool to perform queries on the data analysis platform. Each query response from the platform averages around 30 MB, and every webpage delivered by the analytical tool is approximately 300 KB. The platform does not support query result caching.\n\nWhich solution will result in the LOWEST data costs for the company?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Host the analytical tool on-premises and directly query the data analysis platform over the internet.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Host the analytical tool in the same AWS Region as the data analysis platform and access it via the internet.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Host the analytical tool in the same AWS Region as the data analysis platform and access it over a Direct Connect connection located in the same Region.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Host the analytical tool on-premises and directly query the data analysis platform over a Direct Connect connection located in the same AWS Region.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nHost the analytical tool in the same AWS Region as the data analysis platform and access it over a Direct Connect connection located in the same Region.\n\nAWS Direct Connect establishes a dedicated network connection from your premises to AWS. This connection can often reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections.\n\nThe analytical tool is hosted in the same AWS Region as the data analysis platform. This proximity ensures that inter-region data transfer costs are eliminated. As the data platform and the analytical tool are in the same region, the data does not need to leave the AWS network, leading to lower data transfer costs. Moreover, by accessing the tool over Direct Connect, the data transferred out to the headquarters (the webpage content) is subject to Direct Connect data transfer pricing, which is typically lower than Internet data transfer pricing.\n\n\n\n\n\n\n\nIncorrect Options:\n\nHost the analytical tool on-premises and directly query the data analysis platform over the internet.\n\nThis option incurs data transfer costs for both uploading the queries to AWS and downloading the results from AWS over the internet, which can be expensive.\n\n\n\n\nHost the analytical tool in the same AWS Region as the data analysis platform and access it via the internet.\n\nEven though the data transfer cost inside the same AWS region is free, accessing it over the internet would incur data transfer out cost, which would be higher compared to AWS Direct Connect.\n\n\n\n\nHost the analytical tool on-premises and directly query the data analysis platform over a Direct Connect connection located in the same AWS Region.\n\nThis option also incurs high data transfer costs because data transferred out from AWS to on-premises over AWS Direct Connect is billed.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html\n\nhttps://aws.amazon.com/directconnect/pricing",
    "correctAnswerExplanations": [
      {
        "answer": "Host the analytical tool in the same AWS Region as the data analysis platform and access it over a Direct Connect connection located in the same Region.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Direct Connect establishes a dedicated network connection from your premises to AWS. This connection can often reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections."
      },
      {
        "answer": "",
        "explanation": "The analytical tool is hosted in the same AWS Region as the data analysis platform. This proximity ensures that inter-region data transfer costs are eliminated. As the data platform and the analytical tool are in the same region, the data does not need to leave the AWS network, leading to lower data transfer costs. Moreover, by accessing the tool over Direct Connect, the data transferred out to the headquarters (the webpage content) is subject to Direct Connect data transfer pricing, which is typically lower than Internet data transfer pricing."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Host the analytical tool on-premises and directly query the data analysis platform over the internet.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option incurs data transfer costs for both uploading the queries to AWS and downloading the results from AWS over the internet, which can be expensive."
      },
      {
        "answer": "Host the analytical tool in the same AWS Region as the data analysis platform and access it via the internet.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Even though the data transfer cost inside the same AWS region is free, accessing it over the internet would incur data transfer out cost, which would be higher compared to AWS Direct Connect."
      },
      {
        "answer": "Host the analytical tool on-premises and directly query the data analysis platform over a Direct Connect connection located in the same AWS Region.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option also incurs high data transfer costs because data transferred out from AWS to on-premises over AWS Direct Connect is billed."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html",
      "https://aws.amazon.com/directconnect/pricing"
    ]
  },
  {
    "id": 43,
    "question": "An online gaming company has an application that collects high scores from users across the world. This data needs to be accessed quickly for leaderboard updates, with thousands of read and write requests per second. The data is small and doesn't need complex querying.\n\nWhich AWS service should the solutions architect recommend for this use case?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon DynamoDB with auto scaling to handle the high volume of read and write requests.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon FSx for high performance file storage.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon RDS with a Read Replica to handle the high volume of requests.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon S3 with transfer acceleration for high-speed data retrieval and storage.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nUse Amazon DynamoDB with auto scaling to handle the high volume of read and write requests.\n\nAmazon DynamoDB is a key-value and no-sql database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. It would be ideal for handling thousands of read and write requests per second due to its high throughput and low latency capabilities. The data size is small and doesn't require complex queries which makes it a perfect fit for a key-value store like DynamoDB.\n\nDynamoDB's auto scaling feature can automatically adjust read and write capacity in response to actual traffic patterns, ensuring that the database is able to maintain performance with high request volumes without manual intervention. Auto scaling allows DynamoDB to dynamically adjust provisioned throughput capacity on your behalf in response to actual traffic patterns, ensuring that the performance remains high while keeping costs down.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon RDS with a Read Replica to handle the high volume of requests.\n\nAmazon RDS is a relational database service which, while good for complex querying and transactions, it is not the best solution for the use case mentioned. Also, creating Read Replicas might help with read-heavy applications, but it doesn't equally balance the high write load as mentioned in our case.\n\n\n\n\nUse Amazon FSx for high performance file storage.\n\nAmazon FSx provides fully managed third-party file systems. It provides high-performance file storage, it isn't designed for the use case of handling thousands of read and write requests per second for small pieces of data, such as high scores in a gaming application.\n\n\n\n\nUse Amazon S3 with transfer acceleration for high-speed data retrieval and storage.\n\nAmazon S3 with transfer acceleration is good for fast, secure transfer over long distances between the client and the S3 bucket, it's not ideal for a use case with thousands of read and write requests per second for small pieces of data. S3 is object storage best suited for storing and retrieving any amount of data from anywhere on the web, not for transactional data with high IOPS.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon DynamoDB with auto scaling to handle the high volume of read and write requests.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and no-sql database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. It would be ideal for handling thousands of read and write requests per second due to its high throughput and low latency capabilities. The data size is small and doesn't require complex queries which makes it a perfect fit for a key-value store like DynamoDB."
      },
      {
        "answer": "",
        "explanation": "DynamoDB's auto scaling feature can automatically adjust read and write capacity in response to actual traffic patterns, ensuring that the database is able to maintain performance with high request volumes without manual intervention. Auto scaling allows DynamoDB to dynamically adjust provisioned throughput capacity on your behalf in response to actual traffic patterns, ensuring that the performance remains high while keeping costs down."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Amazon RDS with a Read Replica to handle the high volume of requests.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon RDS is a relational database service which, while good for complex querying and transactions, it is not the best solution for the use case mentioned. Also, creating Read Replicas might help with read-heavy applications, but it doesn't equally balance the high write load as mentioned in our case."
      },
      {
        "answer": "Use Amazon FSx for high performance file storage.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon FSx provides fully managed third-party file systems. It provides high-performance file storage, it isn't designed for the use case of handling thousands of read and write requests per second for small pieces of data, such as high scores in a gaming application."
      },
      {
        "answer": "Use Amazon S3 with transfer acceleration for high-speed data retrieval and storage.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 with transfer acceleration is good for fast, secure transfer over long distances between the client and the S3 bucket, it's not ideal for a use case with thousands of read and write requests per second for small pieces of data. S3 is object storage best suited for storing and retrieving any amount of data from anywhere on the web, not for transactional data with high IOPS."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"
    ]
  },
  {
    "id": 44,
    "question": "A media company is developing an application using a microservice architecture that needs to be run in multiple Amazon EC2 instances. The team requires a storage solution that can handle infrequently accessed media files while allowing simultaneous access from all the EC2 instances when required.\n\nWhat is the MOST cost-effective file storage service that can instantly access when needed?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon EFS Infrequent Access storage class",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Amazon S3 Glacier storage class",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon FSx for Windows File Server",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon EBS Provisioned IOPS (SSD) volumes",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nAmazon EFS Infrequent Access storage class\n\nThe Amazon EFS Infrequent Access (IA) storage class is a storage tier provided by Amazon Elastic File System (EFS) that offers cost savings for data with infrequent access patterns. It is designed for workloads where data is accessed less frequently but still requires the high durability, low latency, and scalability provided by EFS. With EFS IA, users benefit from a lower per-GB storage cost compared to the standard storage class.\n\nDespite being cost-effective for infrequently accessed data, it allows immediate access when the data is required, which is a perfect match for the given requirement. EFS IA offers the same high durability and high availability as the EFS Standard storage class, and you can access your data transparently across the EFS Standard and EFS IA storage classes.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAmazon S3 Glacier storage class\n\nAmazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed for data that is infrequently accessed and for which retrieval times of several hours are suitable. However, the requirement in this scenario is for immediate access when needed, which S3 Glacier cannot provide.\n\n\n\n\nAmazon EBS Provisioned IOPS (SSD) volumes\n\nAmazon EBS Provisioned IOPS SSD (io1 and io2) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that require low latency. They are not designed specifically for infrequently accessed data and it would not be as cost-effective as EFS IA for this use case.\n\n\n\n\nAmazon FSx for Windows File Server\n\nAmazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system. It allows multiple EC2 instances to access data simultaneously but it is not cost-effective storage of infrequently accessed data.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/efs/latest/ug/storage-classes.html",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon EFS Infrequent Access storage class",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The Amazon EFS Infrequent Access (IA) storage class is a storage tier provided by Amazon Elastic File System (EFS) that offers cost savings for data with infrequent access patterns. It is designed for workloads where data is accessed less frequently but still requires the high durability, low latency, and scalability provided by EFS. With EFS IA, users benefit from a lower per-GB storage cost compared to the standard storage class."
      },
      {
        "answer": "",
        "explanation": "Despite being cost-effective for infrequently accessed data, it allows immediate access when the data is required, which is a perfect match for the given requirement. EFS IA offers the same high durability and high availability as the EFS Standard storage class, and you can access your data transparently across the EFS Standard and EFS IA storage classes."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Amazon S3 Glacier storage class",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed for data that is infrequently accessed and for which retrieval times of several hours are suitable. However, the requirement in this scenario is for immediate access when needed, which S3 Glacier cannot provide."
      },
      {
        "answer": "Amazon EBS Provisioned IOPS (SSD) volumes",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon EBS Provisioned IOPS SSD (io1 and io2) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that require low latency. They are not designed specifically for infrequently accessed data and it would not be as cost-effective as EFS IA for this use case."
      },
      {
        "answer": "Amazon FSx for Windows File Server",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system. It allows multiple EC2 instances to access data simultaneously but it is not cost-effective storage of infrequently accessed data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html"
    ]
  },
  {
    "id": 45,
    "question": "A company operates an e-commerce platform on a single Amazon EC2 instance, hosting both the web server and the database software. The company intends to make its e-commerce platform highly available and scalable according to customer demand.\n\nWhat should a solutions architect suggest to achieve these requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the database to an Amazon RDS instance with automated backups. Manually set up an additional EC2 instance in the same Availability Zone. Configure an Application Load Balancer within the Availability Zone, designating the two instances as targets.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Move the database to a separate EC2 instance and schedule backups to Amazon S3. Generate an Amazon Machine Image (AMI) from the original EC2 instance. Configure an Application Load Balancer spanning two Availability Zones. Attach an Auto Scaling group that uses the AMI across both Availability Zones.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Move the database to an Amazon Aurora instance that has a read replica in the same Availability Zone as the current EC2 instance. Launch another EC2 instance manually within the same Availability Zone. Set up an Application Load Balancer and assign the two EC2 instances as targets.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Migrate the database to Amazon Aurora with a read replica in a different Availability Zone. Create an Amazon Machine Image (AMI) from the EC2 instance. Set up an Application Load Balancer across two Availability Zones. Connect an Auto Scaling group using the AMI across these two Availability Zones.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nMigrate the database to Amazon Aurora with a read replica in a different Availability Zone. Create an Amazon Machine Image (AMI) from the EC2 instance. Set up an Application Load Balancer across two Availability Zones. Connect an Auto Scaling group using the AMI across these two Availability Zones.\n\nAmazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. It provides superior fault tolerance by spreading replicas across multiple Availability Zones, which can greatly enhance the availability of the platform.\n\nCreating an Amazon Machine Image (AMI) from the EC2 instance allows you to create multiple instances with the same configuration, making it perfect for scalability.\n\nAn Application Load Balancer efficiently distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, in multiple Availability Zones. This increases the fault tolerance of your applications.\n\nAn Auto Scaling group automatically adjusts the number of EC2 instances in response to traffic patterns. This ensures that the number of EC2 instances you're using scales up during demand spikes to maintain performance, and scales down during demand drops to minimize costs.\n\nThis setup ensures high availability by distributing the load across multiple instances and Availability Zones and also ensures scalability by using Auto Scaling in response to demand changes.\n\n\n\n\n\n\n\nIncorrect Options:\n\nMigrate the database to an Amazon RDS instance with automated backups. Manually set up an additional EC2 instance in the same Availability Zone. Configure an Application Load Balancer within the Availability Zone, designating the two instances as targets.\n\nThis solution doesn't provide high availability as all the resources are in the same Availability Zone, which is a single point of failure. It also doesn't ensure scalability because manual instance management is used.\n\n\n\n\nMove the database to an Amazon Aurora instance that has a read replica in the same Availability Zone as the current EC2 instance. Launch another EC2 instance manually within the same Availability Zone. Set up an Application Load Balancer and assign the two EC2 instances as targets.\n\nThis solution also doesn't provide high availability since all resources are in the same Availability Zone, a single point of failure. It also lacks scalability due to manual instance management.\n\n\n\n\nMove the database to a separate EC2 instance and schedule backups to Amazon S3. Generate an Amazon Machine Image (AMI) from the original EC2 instance. Configure an Application Load Balancer spanning two Availability Zones. Attach an Auto Scaling group that uses the AMI across both Availability Zones.\n\nThis solution provides high availability and scalability for the web servers, it doesn't provide high availability for the database as it's still hosted on a single EC2 instance.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/aurora\n\nhttps://aws.amazon.com/ec2/autoscaling\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
    "correctAnswerExplanations": [
      {
        "answer": "Migrate the database to Amazon Aurora with a read replica in a different Availability Zone. Create an Amazon Machine Image (AMI) from the EC2 instance. Set up an Application Load Balancer across two Availability Zones. Connect an Auto Scaling group using the AMI across these two Availability Zones.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. It provides superior fault tolerance by spreading replicas across multiple Availability Zones, which can greatly enhance the availability of the platform."
      },
      {
        "answer": "",
        "explanation": "Creating an Amazon Machine Image (AMI) from the EC2 instance allows you to create multiple instances with the same configuration, making it perfect for scalability."
      },
      {
        "answer": "",
        "explanation": "An Application Load Balancer efficiently distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, in multiple Availability Zones. This increases the fault tolerance of your applications."
      },
      {
        "answer": "",
        "explanation": "An Auto Scaling group automatically adjusts the number of EC2 instances in response to traffic patterns. This ensures that the number of EC2 instances you're using scales up during demand spikes to maintain performance, and scales down during demand drops to minimize costs."
      },
      {
        "answer": "",
        "explanation": "This setup ensures high availability by distributing the load across multiple instances and Availability Zones and also ensures scalability by using Auto Scaling in response to demand changes."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Migrate the database to an Amazon RDS instance with automated backups. Manually set up an additional EC2 instance in the same Availability Zone. Configure an Application Load Balancer within the Availability Zone, designating the two instances as targets.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This solution doesn't provide high availability as all the resources are in the same Availability Zone, which is a single point of failure. It also doesn't ensure scalability because manual instance management is used."
      },
      {
        "answer": "Move the database to an Amazon Aurora instance that has a read replica in the same Availability Zone as the current EC2 instance. Launch another EC2 instance manually within the same Availability Zone. Set up an Application Load Balancer and assign the two EC2 instances as targets.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This solution also doesn't provide high availability since all resources are in the same Availability Zone, a single point of failure. It also lacks scalability due to manual instance management."
      },
      {
        "answer": "Move the database to a separate EC2 instance and schedule backups to Amazon S3. Generate an Amazon Machine Image (AMI) from the original EC2 instance. Configure an Application Load Balancer spanning two Availability Zones. Attach an Auto Scaling group that uses the AMI across both Availability Zones.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This solution provides high availability and scalability for the web servers, it doesn't provide high availability for the database as it's still hosted on a single EC2 instance."
      }
    ],
    "references": [
      "https://aws.amazon.com/aurora",
      "https://aws.amazon.com/ec2/autoscaling",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html"
    ]
  },
  {
    "id": 46,
    "question": "A business has recently acquired a startup, which is managed by an AWS account under an existing AWS Organization. The business intends to move the startup's AWS account from its current AWS Organization to its AWS Organization.\n\nWhat steps should they follow to achieve this?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Remove the startup's AWS account from the existing organization. Then, from the business's AWS Organization, send an invitation to the startup's AWS account and accept this invitation from the startup's AWS account.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Removing an AWS account from an AWS Organization is impossible. Instead, the business should create a new AWS account for the startup.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Remove the startup's AWS account from the existing AWS Organization. Then, from the startup's AWS account, send an invitation to the business's AWS Organization and accept it from there.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Start by sending an invitation to the startup's AWS account from the business's AWS Organization and then accept this invitation from the startup's AWS account. Afterward, remove the startup's AWS account from the existing AWS Organization.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nRemove the startup's AWS account from the existing organization. Then, from the business's AWS Organization, send an invitation to the startup's AWS account and accept this invitation from the startup's AWS account.\n\nAWS Organizations is a service for grouping and centrally managing multiple AWS accounts that your business owns. An AWS account can only be a part of a single AWS Organization at a time.\n\nIn order to move the startup's AWS account to the business's AWS Organization, the account first needs to be removed from its existing AWS Organization. After the account is successfully removed, the business's AWS Organization can send an invitation to the startup's AWS account. This invitation must be accepted from the startup's AWS account to be successfully added to the business's AWS Organization.\n\nThis process allows a seamless migration of the startup's AWS account to the business's AWS Organization, ensuring that all resources, permissions, and settings are maintained while gaining the benefits of the central governance, security, and billing settings of the business's AWS Organization.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStart by sending an invitation to the startup's AWS account from the business's AWS Organization and then accept this invitation from the startup's AWS account. Afterward, remove the startup's AWS account from the existing AWS Organization.\n\nAn account cannot be a member of two organizations at the same time. The invitation from the new organization will fail if the account is still part of the existing organization.\n\n\n\n\nRemove the startup's AWS account from the existing AWS Organization. Then, from the startup's AWS account, send an invitation to the business's AWS Organization and accept it from there.\n\nThe process to join an organization involves the organization sending an invitation to the account, not the other way around.\n\n\n\n\nRemoving an AWS account from an AWS Organization is impossible. Instead, the business should create a new AWS account for the startup.\n\nAn AWS account can be removed from an AWS Organization. Creating a new AWS account for the startup would involve unnecessary setup and configuration work.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_remove.html\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html",
    "correctAnswerExplanations": [
      {
        "answer": "Remove the startup's AWS account from the existing organization. Then, from the business's AWS Organization, send an invitation to the startup's AWS account and accept this invitation from the startup's AWS account.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Organizations is a service for grouping and centrally managing multiple AWS accounts that your business owns. An AWS account can only be a part of a single AWS Organization at a time."
      },
      {
        "answer": "",
        "explanation": "In order to move the startup's AWS account to the business's AWS Organization, the account first needs to be removed from its existing AWS Organization. After the account is successfully removed, the business's AWS Organization can send an invitation to the startup's AWS account. This invitation must be accepted from the startup's AWS account to be successfully added to the business's AWS Organization."
      },
      {
        "answer": "",
        "explanation": "This process allows a seamless migration of the startup's AWS account to the business's AWS Organization, ensuring that all resources, permissions, and settings are maintained while gaining the benefits of the central governance, security, and billing settings of the business's AWS Organization."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Start by sending an invitation to the startup's AWS account from the business's AWS Organization and then accept this invitation from the startup's AWS account. Afterward, remove the startup's AWS account from the existing AWS Organization.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An account cannot be a member of two organizations at the same time. The invitation from the new organization will fail if the account is still part of the existing organization."
      },
      {
        "answer": "Remove the startup's AWS account from the existing AWS Organization. Then, from the startup's AWS account, send an invitation to the business's AWS Organization and accept it from there.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The process to join an organization involves the organization sending an invitation to the account, not the other way around."
      },
      {
        "answer": "Removing an AWS account from an AWS Organization is impossible. Instead, the business should create a new AWS account for the startup.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An AWS account can be removed from an AWS Organization. Creating a new AWS account for the startup would involve unnecessary setup and configuration work."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_remove.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html"
    ]
  },
  {
    "id": 47,
    "question": "A company has a Python Flask application running as a pod on Amazon Elastic Kubernetes Service (Amazon EKS) in private subnets. This application must retrieve data from an Amazon S3 bucket. The solutions architect must ensure secure access to the S3 bucket without exposing traffic to the public internet.\n\nWhich two steps should the solutions architect take to achieve this objective? (Select TWO.)",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a VPC endpoint for Amazon S3.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Embed the access keys in the Python Flask application's code.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Allow outbound traffic to the S3 bucket through the private subnet’s network ACLs.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Assign an IAM role with necessary permissions to the EKS pod.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Assign an IAM user with necessary permissions to the EKS pod.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Options:\n\nAssign an IAM role with necessary permissions to the EKS pod.\n\nIAM (Identity and Access Management) roles are a secure way to grant permissions that determine what actions are allowed and denied by the entity (user, application, or service) that assumes the role. In this case, an IAM role with the necessary permissions to access the S3 bucket should be associated with the EKS pod. This role provides temporary security credentials that the applications in the pod can use to make AWS API requests. This method adheres to the best practice of not embedding AWS credentials directly into the code and ensures the principle of least privilege is followed.\n\n\n\n\nCreate a VPC endpoint for Amazon S3.\n\nTo enable your pod to connect to the S3 bucket privately (i.e., without traveling the Internet), you can create a VPC endpoint for S3. A VPC endpoint allows private connectivity between your VPC and supported AWS services. This endpoint routes the traffic between the entities within the AWS network, eliminating the need for an Internet Gateway, NAT device, VPN connection, or AWS Direct Connect connection. This approach further strengthens the security by ensuring the application's traffic is not exposed to the public Internet.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAssign an IAM user with necessary permissions to the EKS pod.\n\nIAM users should not be directly associated with EKS pods. Using IAM roles is the recommended method for granting permissions to AWS services from applications running on EKS.\n\n\n\n\nAllow outbound traffic to the S3 bucket through the private subnet’s network ACLs.\n\nModifying network ACLs alone wouldn't secure access to S3, as it requires proper IAM permissions and routing configurations, typically through a VPC endpoint.\n\n\n\n\nEmbed the access keys in the Python Flask application's code.\n\nEmbedding access keys in the code is a bad practice from a security perspective, as it can lead to key exposure and unauthorized access.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/about-aws/whats-new/2019/09/amazon-eks-adds-support-to-assign-iam-permissions-to-kubernetes-service-accounts\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-services-overview.html\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html",
    "correctAnswerExplanations": [
      {
        "answer": "Assign an IAM role with necessary permissions to the EKS pod.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "IAM (Identity and Access Management) roles are a secure way to grant permissions that determine what actions are allowed and denied by the entity (user, application, or service) that assumes the role. In this case, an IAM role with the necessary permissions to access the S3 bucket should be associated with the EKS pod. This role provides temporary security credentials that the applications in the pod can use to make AWS API requests. This method adheres to the best practice of not embedding AWS credentials directly into the code and ensures the principle of least privilege is followed."
      },
      {
        "answer": "Create a VPC endpoint for Amazon S3.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "To enable your pod to connect to the S3 bucket privately (i.e., without traveling the Internet), you can create a VPC endpoint for S3. A VPC endpoint allows private connectivity between your VPC and supported AWS services. This endpoint routes the traffic between the entities within the AWS network, eliminating the need for an Internet Gateway, NAT device, VPN connection, or AWS Direct Connect connection. This approach further strengthens the security by ensuring the application's traffic is not exposed to the public Internet."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Assign an IAM user with necessary permissions to the EKS pod.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "IAM users should not be directly associated with EKS pods. Using IAM roles is the recommended method for granting permissions to AWS services from applications running on EKS."
      },
      {
        "answer": "Allow outbound traffic to the S3 bucket through the private subnet’s network ACLs.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Modifying network ACLs alone wouldn't secure access to S3, as it requires proper IAM permissions and routing configurations, typically through a VPC endpoint."
      },
      {
        "answer": "Embed the access keys in the Python Flask application's code.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Embedding access keys in the code is a bad practice from a security perspective, as it can lead to key exposure and unauthorized access."
      }
    ],
    "references": [
      "https://aws.amazon.com/about-aws/whats-new/2019/09/amazon-eks-adds-support-to-assign-iam-permissions-to-kubernetes-service-accounts",
      "https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-services-overview.html",
      "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
    ]
  },
  {
    "id": 48,
    "question": "A corporation is introducing new data retention policies for all data stored in Amazon S3 buckets. The company must keep daily backups for a minimum period of 5 years. The backups must be consistent and retrievable.\n\nWhich solution should a solutions architect recommend to meet these requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure data logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 5 years.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure an AWS Database Migration Service (AWS DMS) replication task. Set up a replication instance, and configure a change data capture (CDC) task to stream data changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 5 years.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a backup vault in AWS Backup to retain S3 bucket backups. Set up a new backup plan with a daily schedule and an expiration period of 5 years after creation. Assign the S3 buckets to the backup plan.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure a daily snapshot schedule for the S3 buckets. Assign a snapshot retention policy of 5 years to each S3 bucket. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate a backup vault in AWS Backup to retain S3 bucket backups. Set up a new backup plan with a daily schedule and an expiration period of 5 years after creation. Assign the S3 buckets to the backup plan.\n\nAWS Backup is a fully managed backup service that makes it easy to centralize and automate the back up of data across AWS services. AWS Backup provides the ability to create backup plans that automate the backup scheduling, lifecycle management, and backup retention.\n\nThe corporation should create a backup vault in AWS Backup to store the backups securely. Then, they can create a backup plan with a daily schedule to ensure that backups are made each day. The backup plan should have an expiration period of 5 years, which aligns with the company's data retention policy. This ensures that each backup is retained for 5 years after it is created and then deleted.\n\nThis solution enables automated, consistent daily backups, without manual intervention, and aligns with the data retention requirements of the corporation.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure a daily snapshot schedule for the S3 buckets. Assign a snapshot retention policy of 5 years to each S3 bucket. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.\n\nAmazon S3 does not support snapshots like Amazon EBS does. Therefore, it's not possible to configure a snapshot schedule or retention policy for S3 buckets using Amazon DLM.\n\n\n\n\nConfigure data logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 5 years.\n\nCloudWatch Logs primarily capture log data from various AWS resources, and it's not suitable or designed to act as a backup solution for Amazon S3 data. This option will not ensure that all data in S3 is backed up and retrievable for 5 years.\n\n\n\n\nConfigure an AWS Database Migration Service (AWS DMS) replication task. Set up a replication instance, and configure a change data capture (CDC) task to stream data changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 5 years.\n\nAWS DMS is used for migrating databases to AWS and not suitable for backing up S3 data.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a backup vault in AWS Backup to retain S3 bucket backups. Set up a new backup plan with a daily schedule and an expiration period of 5 years after creation. Assign the S3 buckets to the backup plan.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Backup is a fully managed backup service that makes it easy to centralize and automate the back up of data across AWS services. AWS Backup provides the ability to create backup plans that automate the backup scheduling, lifecycle management, and backup retention."
      },
      {
        "answer": "",
        "explanation": "The corporation should create a backup vault in AWS Backup to store the backups securely. Then, they can create a backup plan with a daily schedule to ensure that backups are made each day. The backup plan should have an expiration period of 5 years, which aligns with the company's data retention policy. This ensures that each backup is retained for 5 years after it is created and then deleted."
      },
      {
        "answer": "",
        "explanation": "This solution enables automated, consistent daily backups, without manual intervention, and aligns with the data retention requirements of the corporation."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Configure a daily snapshot schedule for the S3 buckets. Assign a snapshot retention policy of 5 years to each S3 bucket. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 does not support snapshots like Amazon EBS does. Therefore, it's not possible to configure a snapshot schedule or retention policy for S3 buckets using Amazon DLM."
      },
      {
        "answer": "Configure data logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 5 years.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "CloudWatch Logs primarily capture log data from various AWS resources, and it's not suitable or designed to act as a backup solution for Amazon S3 data. This option will not ensure that all data in S3 is backed up and retrievable for 5 years."
      },
      {
        "answer": "Configure an AWS Database Migration Service (AWS DMS) replication task. Set up a replication instance, and configure a change data capture (CDC) task to stream data changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 5 years.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS DMS is used for migrating databases to AWS and not suitable for backing up S3 data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html"
    ]
  },
  {
    "id": 49,
    "question": "A web application hosted on an Amazon EC2 instance in VPC-A needs to retrieve data from a database running on another EC2 instance in VPC-B. These VPCs are under different AWS accounts. The solution architect is tasked with designing a secure connectivity solution from VPC-A to VPC-B, ensuring there is no single point of failure or bandwidth issues.\n\nWhich solution should the solution architect recommend?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a VPC peering connection between VPC-A and VPC-B.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Set up VPC gateway endpoints for the EC2 instance located in VPC-B.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a private virtual interface (VIF) for the EC2 instance in VPC-B and add the necessary routes from VPC-A.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Attach a virtual private gateway to VPC-B and set up routing from VPC-A.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nCreate a VPC peering connection between VPC-A and VPC-B.\n\nAmazon Virtual Private Cloud (VPC) allows you to launch AWS resources into a virtual network that you've defined. A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses.\n\nVPC peering connections can be created between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region (also known as inter-region VPC peering). Importantly, VPC peering does not rely on a single point of failure for communication or introduce bandwidth bottlenecks.\n\nTo ensure secure and reliable communication between the two EC2 instances in the different VPCs (VPC-A and VPC-B), you should create a VPC peering connection. Once the peering connection is created, you can update the route tables for each VPC to include routes that point to the IP address range of the other VPC (peered VPC), ensuring secure and direct communication.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up VPC gateway endpoints for the EC2 instance located in VPC-B.\n\nVPC Gateway Endpoints are primarily used for connecting to AWS services, not for direct connection between two EC2 instances residing in separate VPCs. They also do not work across different AWS accounts.\n\n\n\n\nAttach a virtual private gateway to VPC-B and set up routing from VPC-A.\n\nVirtual Private Gateway is used for creating a VPN connection between a VPC and an on-premises network, not for linking two VPCs, especially when they are under different AWS accounts.\n\n\n\n\nCreate a private virtual interface (VIF) for the EC2 instance in VPC-B and add the necessary routes from VPC-A.\n\nPrivate VIF is used to connect to AWS services in a VPC via AWS Direct Connect, not for direct EC2 to EC2 communication. This approach is also more complex and might not be cost-effective.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a VPC peering connection between VPC-A and VPC-B.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Virtual Private Cloud (VPC) allows you to launch AWS resources into a virtual network that you've defined. A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses."
      },
      {
        "answer": "",
        "explanation": "VPC peering connections can be created between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region (also known as inter-region VPC peering). Importantly, VPC peering does not rely on a single point of failure for communication or introduce bandwidth bottlenecks."
      },
      {
        "answer": "",
        "explanation": "To ensure secure and reliable communication between the two EC2 instances in the different VPCs (VPC-A and VPC-B), you should create a VPC peering connection. Once the peering connection is created, you can update the route tables for each VPC to include routes that point to the IP address range of the other VPC (peered VPC), ensuring secure and direct communication."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up VPC gateway endpoints for the EC2 instance located in VPC-B.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "VPC Gateway Endpoints are primarily used for connecting to AWS services, not for direct connection between two EC2 instances residing in separate VPCs. They also do not work across different AWS accounts."
      },
      {
        "answer": "Attach a virtual private gateway to VPC-B and set up routing from VPC-A.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Virtual Private Gateway is used for creating a VPN connection between a VPC and an on-premises network, not for linking two VPCs, especially when they are under different AWS accounts."
      },
      {
        "answer": "Create a private virtual interface (VIF) for the EC2 instance in VPC-B and add the necessary routes from VPC-A.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Private VIF is used to connect to AWS services in a VPC via AWS Direct Connect, not for direct EC2 to EC2 communication. This approach is also more complex and might not be cost-effective."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html"
    ]
  },
  {
    "id": 50,
    "question": "A media streaming company recently migrated their application to AWS using CloudFormation, with EC2 instances from the C5 family to handle high I/O operations. However, as the number of concurrent users increase during peak times, application response times increase noticeably. Users have reported buffering issues during these peak usage periods.\n\nWhich solution would MOST effectively improve performance and maintain operational efficiency?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Modify the CloudFormation templates and replace the EC2 instances to I3 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application performance metrics for future capacity planning.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Modify the CloudFormation templates and replace the EC2 instances to I3 instances. Use the native EC2 memory metrics in Amazon CloudWatch to monitor application performance for future capacity adjustments.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Migrate to T3 EC2 instances operating within an Auto Scaling group. Make these changes using the AWS Management Console.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Modify the CloudFormation templates to launch the EC2 instances within an Auto Scaling group. Manually increment the desired and maximum capacity of the Auto Scaling group as necessary.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nModify the CloudFormation templates and replace the EC2 instances to I3 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application performance metrics for future capacity planning.\n\nThe I3 instances are high I/O instances that are designed for very high random I/O performance. These instances could help in improving the streaming application's performance during peak times.\n\nAmazon CloudFormation helps you model and set up your Amazon Web Services resources so you can spend less time managing those resources and more time focusing on your applications. By adjusting CloudFormation templates to use I3 instances, the company ensures that infrastructure changes are tracked and repeatable.\n\nThe CloudWatch agent is a powerful tool for monitoring AWS environments. By deploying the CloudWatch agent to EC2 instances, the company can generate custom metrics specifically tailored to their application. This could include detailed insights on latency, error rates, or other key performance indicators (KPIs) relevant to the application.\n\nThis approach not only addresses the immediate performance issue but also improves the monitoring capabilities of the system, providing useful data for future capacity planning and troubleshooting. This way, the company can ensure its application scales efficiently and maintains high performance as usage grows.\n\n\n\n\n\n\n\nIncorrect Options:\n\nMigrate to T3 EC2 instances operating within an Auto Scaling group. Make these changes using the AWS Management Console.\n\nThis option doesn't leverage the capabilities of CloudFormation for infrastructure management, making it less operationally efficient.\n\n\n\n\nModify the CloudFormation templates to launch the EC2 instances within an Auto Scaling group. Manually increment the desired and maximum capacity of the Auto Scaling group as necessary.\n\nThis solution does not account for the high I/O demands of the application. Simply scaling without considering the instance type can lead to inefficient resource utilization.\n\n\n\n\nModify the CloudFormation templates and replace the EC2 instances to I3 instances. Use the native EC2 memory metrics in Amazon CloudWatch to monitor application performance for future capacity adjustments.\n\nThis option doesn't capture detailed performance metrics related to the application, which could lead to inaccurate capacity planning.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/cloudwatch\n\nhttps://aws.amazon.com/ec2/instance-types",
    "correctAnswerExplanations": [
      {
        "answer": "Modify the CloudFormation templates and replace the EC2 instances to I3 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application performance metrics for future capacity planning.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The I3 instances are high I/O instances that are designed for very high random I/O performance. These instances could help in improving the streaming application's performance during peak times."
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFormation helps you model and set up your Amazon Web Services resources so you can spend less time managing those resources and more time focusing on your applications. By adjusting CloudFormation templates to use I3 instances, the company ensures that infrastructure changes are tracked and repeatable."
      },
      {
        "answer": "",
        "explanation": "The CloudWatch agent is a powerful tool for monitoring AWS environments. By deploying the CloudWatch agent to EC2 instances, the company can generate custom metrics specifically tailored to their application. This could include detailed insights on latency, error rates, or other key performance indicators (KPIs) relevant to the application."
      },
      {
        "answer": "",
        "explanation": "This approach not only addresses the immediate performance issue but also improves the monitoring capabilities of the system, providing useful data for future capacity planning and troubleshooting. This way, the company can ensure its application scales efficiently and maintains high performance as usage grows."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Migrate to T3 EC2 instances operating within an Auto Scaling group. Make these changes using the AWS Management Console.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option doesn't leverage the capabilities of CloudFormation for infrastructure management, making it less operationally efficient."
      },
      {
        "answer": "Modify the CloudFormation templates to launch the EC2 instances within an Auto Scaling group. Manually increment the desired and maximum capacity of the Auto Scaling group as necessary.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This solution does not account for the high I/O demands of the application. Simply scaling without considering the instance type can lead to inefficient resource utilization."
      },
      {
        "answer": "Modify the CloudFormation templates and replace the EC2 instances to I3 instances. Use the native EC2 memory metrics in Amazon CloudWatch to monitor application performance for future capacity adjustments.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option doesn't capture detailed performance metrics related to the application, which could lead to inaccurate capacity planning."
      }
    ],
    "references": [
      "https://aws.amazon.com/cloudwatch",
      "https://aws.amazon.com/ec2/instance-types"
    ]
  },
  {
    "id": 51,
    "question": "A financial company operates a complex data analysis system on-premises with ETL jobs running on a Hadoop cluster. The company wants to migrate this workload to AWS Cloud and needs high availability for 25 EC2 instances per Availability Zone.\n\nAs a Solutions Architect, which EC2 placement group would you suggest for this distributed ETL workload?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Partition placement group",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Weighted placement group",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Cluster placement group",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Spread placement group",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nPartition placement group\n\nPartition placement groups provide a logical separation of instances within each group, reducing the impact of hardware failures and offering high availability and reliability. They are especially suitable for distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement group can have multiple partitions, each with its own set of EC2 instances on distinct hardware. Hence, a partition placement group would be the best fit for this use case, ensuring high availability for the EC2 instances running the distributed ETL workload.\n\nWhen you launch a new EC2 instance, the EC2 service tries to distribute your instances across the underlying hardware to lower the risk of simultaneous failures. You can use placement groups to guide the arrangement of a set of interrelated instances to accommodate your workload demands. Depending on your workload type, you can set up a placement group using one of these strategies:\n\nCluster - This method groups instances tightly within an Availability Zone, fostering the high-speed network performance required for intense inter-node communication typical of High Performance Computing (HPC) applications.\n\nPartition - This approach distributes your instances over logical partitions, ensuring that instance groups within one partition don't share the underlying hardware with groups in different partitions. It's a common choice for extensive distributed and replicated workloads such as Hadoop, Cassandra, and Kafka.\n\nSpread - This strategy deliberately disperses a limited group of instances across different underlying hardware to diminish the likelihood of correlated failures.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCluster placement group\n\nA cluster placement group is designed to provide low-latency network performance necessary for tightly-coupled node-to-node communication, typical of HPC applications. They do not offer the high availability required in this scenario as all instances are created in a single Availability Zone.\n\n\n\n\nSpread placement group\n\nSpread placement groups are designed to help reduce the risk of simultaneous failures that might occur when instances share the same racks. They are suitable for a small number of critical instances that should be kept separate from each other. They do not meet the requirement here as they only allow a maximum of seven instances per Availability Zone.\n\n\n\n\nWeighted placement group\n\nThis is a distraction. There are no such weight placement groups.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
    "correctAnswerExplanations": [
      {
        "answer": "Partition placement group",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Partition placement groups provide a logical separation of instances within each group, reducing the impact of hardware failures and offering high availability and reliability. They are especially suitable for distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement group can have multiple partitions, each with its own set of EC2 instances on distinct hardware. Hence, a partition placement group would be the best fit for this use case, ensuring high availability for the EC2 instances running the distributed ETL workload."
      },
      {
        "answer": "",
        "explanation": "When you launch a new EC2 instance, the EC2 service tries to distribute your instances across the underlying hardware to lower the risk of simultaneous failures. You can use placement groups to guide the arrangement of a set of interrelated instances to accommodate your workload demands. Depending on your workload type, you can set up a placement group using one of these strategies:"
      },
      {}
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Cluster placement group",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A cluster placement group is designed to provide low-latency network performance necessary for tightly-coupled node-to-node communication, typical of HPC applications. They do not offer the high availability required in this scenario as all instances are created in a single Availability Zone."
      },
      {
        "answer": "Spread placement group",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Spread placement groups are designed to help reduce the risk of simultaneous failures that might occur when instances share the same racks. They are suitable for a small number of critical instances that should be kept separate from each other. They do not meet the requirement here as they only allow a maximum of seven instances per Availability Zone."
      },
      {
        "answer": "Weighted placement group",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This is a distraction. There are no such weight placement groups."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"
    ]
  },
  {
    "id": 52,
    "question": "A company is planning to deploy a distributed web application. The infrastructure includes an ElastiCache cluster within a Data VPC and the application's EC2 instances within a separate App VPC. Both VPCs are hosted in the same Region.\n\nA solution is required for the EC2 instances to be able to communicate with the ElastiCache cluster.\n\nWhich of the following is the MOST cost-effective solution to this requirement?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a Transit Gateway. Adjust the route tables in the Data VPC and the App VPC to direct traffic via the Transit Gateway. Set an inbound rule in the Transit Gateway's security group to accept inbound connections from the application's security group.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a VPC peering connection between the two VPCs. In each VPC, add a route table entry for the peering connection. Set an inbound rule in the peering connection's security group to accept inbound connections from the application's security group.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a Transit Gateway. Adjust the route tables in the Data VPC and the App VPC to direct traffic via the Transit Gateway. Set an inbound rule in the ElastiCache cluster's security group to accept inbound connections from the application's security group.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a VPC peering connection between the two VPCs. In each VPC, add a route table entry for the peering connection. Set an inbound rule in the ElastiCache cluster's security group to accept inbound connections from the application's security group.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nCreate a VPC peering connection between the two VPCs. In each VPC, add a route table entry for the peering connection. Set an inbound rule in the ElastiCache cluster's security group to accept inbound connections from the application's security group.\n\nVPC Peering is a networking connection between two VPCs that enables routing using each VPC’s private IPv4 address or IPv6 address. You can create a VPC peering connection between your VPCs, or with a VPC in another AWS account within a single region. VPC peering is a cost-effective solution as it does not incur any extra data transfer costs beyond what is incurred for transferring data within the same region.\n\nSetting up a VPC peering connection between the Data VPC and the App VPC can allow the EC2 instances to communicate with the ElastiCache cluster. After the VPC peering connection is established, route table entries are added in each VPC to enable the routing of traffic via the peering connection. Setting an inbound rule in the ElastiCache cluster's security group to accept connections from the application's security group ensures that the required communication is securely permitted.\n\nThis solution is the most cost-effective as it does not incur the additional costs associated with a Transit Gateway, which is a more complex solution typically used for managing connectivity across many VPCs and VPNs.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a Transit Gateway. Adjust the route tables in the Data VPC and the App VPC to direct traffic via the Transit Gateway. Set an inbound rule in the ElastiCache cluster's security group to accept inbound connections from the application's security group.\n\nAWS Transit Gateway can connect multiple VPCs, it's typically used for large scale scenarios, such as hundreds of VPCs, and is more complex and expensive than VPC peering. For our case, where there are only two VPCs in the same region, VPC peering is a more cost-effective solution.\n\n\n\n\nCreate a VPC peering connection between the two VPCs. In each VPC, add a route table entry for the peering connection. Set an inbound rule in the peering connection's security group to accept inbound connections from the application's security group.\n\nVPC Peering connections don't have associated security groups. Therefore, setting an inbound rule on the peering connection's security group is not possible. Security groups are applied to EC2 instances or ElastiCache clusters, not to peering connections.\n\n\n\n\nCreate a Transit Gateway. Adjust the route tables in the Data VPC and the App VPC to direct traffic via the Transit Gateway. Set an inbound rule in the Transit Gateway's security group to accept inbound connections from the application's security group.\n\nTransit Gateways do not have security groups; thus, it is not possible to set inbound rules on a Transit Gateway's security group. The security groups are associated with resources like EC2 instances or ElastiCache clusters.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a VPC peering connection between the two VPCs. In each VPC, add a route table entry for the peering connection. Set an inbound rule in the ElastiCache cluster's security group to accept inbound connections from the application's security group.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "VPC Peering is a networking connection between two VPCs that enables routing using each VPC’s private IPv4 address or IPv6 address. You can create a VPC peering connection between your VPCs, or with a VPC in another AWS account within a single region. VPC peering is a cost-effective solution as it does not incur any extra data transfer costs beyond what is incurred for transferring data within the same region."
      },
      {
        "answer": "",
        "explanation": "Setting up a VPC peering connection between the Data VPC and the App VPC can allow the EC2 instances to communicate with the ElastiCache cluster. After the VPC peering connection is established, route table entries are added in each VPC to enable the routing of traffic via the peering connection. Setting an inbound rule in the ElastiCache cluster's security group to accept connections from the application's security group ensures that the required communication is securely permitted."
      },
      {
        "answer": "",
        "explanation": "This solution is the most cost-effective as it does not incur the additional costs associated with a Transit Gateway, which is a more complex solution typically used for managing connectivity across many VPCs and VPNs."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a Transit Gateway. Adjust the route tables in the Data VPC and the App VPC to direct traffic via the Transit Gateway. Set an inbound rule in the ElastiCache cluster's security group to accept inbound connections from the application's security group.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Transit Gateway can connect multiple VPCs, it's typically used for large scale scenarios, such as hundreds of VPCs, and is more complex and expensive than VPC peering. For our case, where there are only two VPCs in the same region, VPC peering is a more cost-effective solution."
      },
      {
        "answer": "Create a VPC peering connection between the two VPCs. In each VPC, add a route table entry for the peering connection. Set an inbound rule in the peering connection's security group to accept inbound connections from the application's security group.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "VPC Peering connections don't have associated security groups. Therefore, setting an inbound rule on the peering connection's security group is not possible. Security groups are applied to EC2 instances or ElastiCache clusters, not to peering connections."
      },
      {
        "answer": "Create a Transit Gateway. Adjust the route tables in the Data VPC and the App VPC to direct traffic via the Transit Gateway. Set an inbound rule in the Transit Gateway's security group to accept inbound connections from the application's security group.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Transit Gateways do not have security groups; thus, it is not possible to set inbound rules on a Transit Gateway's security group. The security groups are associated with resources like EC2 instances or ElastiCache clusters."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html"
    ]
  },
  {
    "id": 53,
    "question": "A database server running on an Amazon EC2 instance is situated in a private subnet in a VPC. This subnet does not have outbound internet access, but the database server needs to synchronize with an external time server for accurate timekeeping.\n\nWhat should a solutions architect do to meet these requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a VPC peering connection with another VPC that has an internet gateway. Configure the private subnet route table to use the peering connection as the default route.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway for the default route.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a NAT instance, and position it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as the default route.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a NAT gateway, and situate it in a public subnet. Adjust the private subnet route table to use the NAT gateway as the default route.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate a NAT gateway, and situate it in a public subnet. Adjust the private subnet route table to use the NAT gateway as the default route.\n\nA NAT gateway enables instances in a private subnet to connect to the internet but prevents the internet from initiating a connection with those instances. This is useful for use cases where resources in a private subnet need to download patches, updates, or, in this case, synchronize with an external time server. The NAT gateway itself is situated in a public subnet with a route out to an internet gateway. Configuring the private subnet route table to use the NAT gateway as the default route will provide the outbound internet access needed, without exposing the EC2 instances to inbound internet traffic.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway for the default route.\n\nAn internet gateway provides access to the internet. If you directly associate it with the private subnet's route table, the EC2 instances will be directly exposed to the internet, which is against the best practice of limiting exposure of critical resources.\n\n\n\n\nCreate a NAT instance, and position it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as the default route.\n\nA NAT instance can provide the necessary internet access, AWS recommends using a NAT gateway instead. NAT gateways are managed services that provide better bandwidth and require less administrative effort.\n\n\n\n\nCreate a VPC peering connection with another VPC that has an internet gateway. Configure the private subnet route table to use the peering connection as the default route.\n\nVPC peering allows for the routing of traffic between two VPCs as if they were in the same network, but it doesn't provide internet access. The route to the internet still needs to be provided, which in this case is not achievable by the proposed solution.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a NAT gateway, and situate it in a public subnet. Adjust the private subnet route table to use the NAT gateway as the default route.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A NAT gateway enables instances in a private subnet to connect to the internet but prevents the internet from initiating a connection with those instances. This is useful for use cases where resources in a private subnet need to download patches, updates, or, in this case, synchronize with an external time server. The NAT gateway itself is situated in a public subnet with a route out to an internet gateway. Configuring the private subnet route table to use the NAT gateway as the default route will provide the outbound internet access needed, without exposing the EC2 instances to inbound internet traffic."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway for the default route.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An internet gateway provides access to the internet. If you directly associate it with the private subnet's route table, the EC2 instances will be directly exposed to the internet, which is against the best practice of limiting exposure of critical resources."
      },
      {
        "answer": "Create a NAT instance, and position it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as the default route.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A NAT instance can provide the necessary internet access, AWS recommends using a NAT gateway instead. NAT gateways are managed services that provide better bandwidth and require less administrative effort."
      },
      {
        "answer": "Create a VPC peering connection with another VPC that has an internet gateway. Configure the private subnet route table to use the peering connection as the default route.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "VPC peering allows for the routing of traffic between two VPCs as if they were in the same network, but it doesn't provide internet access. The route to the internet still needs to be provided, which in this case is not achievable by the proposed solution."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"
    ]
  },
  {
    "id": 54,
    "question": "A company has an online hotel booking system that writes a booking request to a database and triggers a service to finalize the payment. Customers are facing timeouts during the booking process. When customers try to book again, multiple bookings are created for the same desired reservation.\n\nHow should a solutions architect redesign this workflow to prevent the creation of multiple bookings?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Store the booking in the database. Send a message that includes the booking number to Amazon Simple Notification Service (Amazon SNS). Configure the payment service to poll Amazon SNS, fetch the message, and finalize the booking.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up the web application to send a booking message to Amazon Kinesis Data Firehose. Configure the payment service to fetch the message from Kinesis Data Firehose and finalize the booking.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Store the booking in the database. Send a message with the booking number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to fetch the message and finalize the booking. Remove the message from the queue.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the tracked application path request. Use Lambda to interrogate the database, invoke the payment service, and feed in the booking information.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nStore the booking in the database. Send a message with the booking number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to fetch the message and finalize the booking. Remove the message from the queue.\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work. SQS FIFO (First In First Out) queues ensure that the order in which messages are sent and received is strictly preserved. Using SQS FIFO queues would help mitigate the problem of creating multiple bookings for the same reservation.\n\nWhen a booking is made, the booking information is stored in the database and a message containing the booking number is sent to the FIFO queue. The payment service is configured to fetch the message from the FIFO queue and finalize the booking.\n\nThis way, even if customers face timeouts during the booking process and try to book again, the FIFO queue ensures that the first booking is processed before any subsequent attempts. Once a booking is processed, its corresponding message is removed from the queue to prevent any duplicate processing. Even if the payment service fails to process a booking, the message remains in the queue (unless the visibility timeout period has passed), ensuring that no booking requests are lost and all are processed in order.\n\nThis strategy ensures the integrity of the booking process and avoids the creation of multiple bookings for the same desired reservation, providing a better user experience.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up the web application to send a booking message to Amazon Kinesis Data Firehose. Configure the payment service to fetch the message from Kinesis Data Firehose and finalize the booking.\n\nAmazon Kinesis Data Firehose is designed for near real-time data streaming and does not inherently guarantee that all data records will be processed exactly once. It's mainly used for ingesting large amounts of data into data stores like Amazon S3 or Amazon Redshift.\n\n\n\n\nCreate a rule in AWS CloudTrail to invoke an AWS Lambda function based on the tracked application path request. Use Lambda to interrogate the database, invoke the payment service, and feed in the booking information.\n\nAWS CloudTrail is used for governance, compliance, operational auditing, and risk auditing of your AWS account. Using it as a trigger for booking and payment services is not a suitable use case and could lead to performance issues.\n\n\n\n\nStore the booking in the database. Send a message that includes the booking number to Amazon Simple Notification Service (Amazon SNS). Configure the payment service to poll Amazon SNS, fetch the message, and finalize the booking.\n\nAmazon SNS is a pub-sub service and doesn't inherently offer the 'exactly once' processing semantics or the ability to preserve the order of transactions. It's not suitable for operations that need to be processed in order or exactly once.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
    "correctAnswerExplanations": [
      {
        "answer": "Store the booking in the database. Send a message with the booking number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to fetch the message and finalize the booking. Remove the message from the queue.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work. SQS FIFO (First In First Out) queues ensure that the order in which messages are sent and received is strictly preserved. Using SQS FIFO queues would help mitigate the problem of creating multiple bookings for the same reservation."
      },
      {
        "answer": "",
        "explanation": "When a booking is made, the booking information is stored in the database and a message containing the booking number is sent to the FIFO queue. The payment service is configured to fetch the message from the FIFO queue and finalize the booking."
      },
      {
        "answer": "",
        "explanation": "This way, even if customers face timeouts during the booking process and try to book again, the FIFO queue ensures that the first booking is processed before any subsequent attempts. Once a booking is processed, its corresponding message is removed from the queue to prevent any duplicate processing. Even if the payment service fails to process a booking, the message remains in the queue (unless the visibility timeout period has passed), ensuring that no booking requests are lost and all are processed in order."
      },
      {
        "answer": "",
        "explanation": "This strategy ensures the integrity of the booking process and avoids the creation of multiple bookings for the same desired reservation, providing a better user experience."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up the web application to send a booking message to Amazon Kinesis Data Firehose. Configure the payment service to fetch the message from Kinesis Data Firehose and finalize the booking.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is designed for near real-time data streaming and does not inherently guarantee that all data records will be processed exactly once. It's mainly used for ingesting large amounts of data into data stores like Amazon S3 or Amazon Redshift."
      },
      {
        "answer": "Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the tracked application path request. Use Lambda to interrogate the database, invoke the payment service, and feed in the booking information.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS CloudTrail is used for governance, compliance, operational auditing, and risk auditing of your AWS account. Using it as a trigger for booking and payment services is not a suitable use case and could lead to performance issues."
      },
      {
        "answer": "Store the booking in the database. Send a message that includes the booking number to Amazon Simple Notification Service (Amazon SNS). Configure the payment service to poll Amazon SNS, fetch the message, and finalize the booking.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon SNS is a pub-sub service and doesn't inherently offer the 'exactly once' processing semantics or the ability to preserve the order of transactions. It's not suitable for operations that need to be processed in order or exactly once."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"
    ]
  },
  {
    "id": 55,
    "question": "A retail company collects customer purchase data from thousands of point-of-sale (POS) systems via a RESTful web services application on an Amazon EC2 instance. The EC2 instance receives the raw data, transforms it, and stores it in an Amazon S3 bucket. The company plans to roll out more POS systems, with the total reaching millions. The company requires a highly scalable solution with minimized operational overhead.\n\nWhich combination of actions should a solutions architect suggest to fulfill these requirements? (Select TWO.)",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon API Gateway to channel the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source, sending data to Amazon S3.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Deploy additional EC2 instances to deal with the rising volume of incoming data.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon Route 53 for traffic distribution to various EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Glue to handle the raw data in Amazon S3.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Send the raw data to Amazon Simple Queue Service (Amazon SQQ). Use EC2 instances for data processing.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nUse AWS Glue to handle the raw data in Amazon S3.\n\nWith AWS Glue, the company can easily prepare and load their vast amounts of data for analytics. AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to move data between data stores. It can catalog data from various sources and formats, transform it, and load it into S3. AWS Glue is serverless, which implies there's no infrastructure to manage or setup. It automatically scales to meet your workloads and it's fault-tolerant, which matches the company's requirements for a highly scalable solution with minimized operational overhead.\n\n\n\n\nUse Amazon API Gateway to channel the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source, sending data to Amazon S3.\n\nAs the company's POS systems increase, the amount of raw data would scale up substantially. Amazon API Gateway is an AWS service for creating, deploying, and managing APIs. It can handle any volume of incoming data and can effectively channel this raw data to an Amazon Kinesis data stream. Kinesis Data Streams can collect and process large streams of data records in real-time. The data collected is available in milliseconds to enable real-time analytics use cases. Additionally, using Amazon Kinesis Data Firehose, the company can easily load massive volumes of streaming data into AWS, in this case, Amazon S3. Firehose automatically scales to match the throughput of your data and requires no ongoing administration, which aligns with the requirement of minimized operational overhead.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon Route 53 for traffic distribution to various EC2 instances.\n\nThis solution does not address the scalability requirement as effectively. Route 53 can distribute traffic, it won't help with the data processing scalability, and it does not minimize operational overhead in the context of data transformation and storage.\n\n\n\n\nDeploy additional EC2 instances to deal with the rising volume of incoming data.\n\nAdding more EC2 instances can theoretically accommodate more data, this approach doesn't minimize operational overhead and may not scale efficiently to accommodate millions of POS systems.\n\n\n\n\nSend the raw data to Amazon Simple Queue Service (Amazon SQQ). Use EC2 instances for data processing.\n\nAmazon SQS can decouple the data producers and consumers, but the processing would still rely on EC2 instances, which might not be as scalable as required and can increase operational overhead.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html\n\nhttps://docs.aws.amazon.com/streams/latest/dev/introduction.html\n\nhttps://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Glue to handle the raw data in Amazon S3.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "With AWS Glue, the company can easily prepare and load their vast amounts of data for analytics. AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to move data between data stores. It can catalog data from various sources and formats, transform it, and load it into S3. AWS Glue is serverless, which implies there's no infrastructure to manage or setup. It automatically scales to meet your workloads and it's fault-tolerant, which matches the company's requirements for a highly scalable solution with minimized operational overhead."
      },
      {
        "answer": "Use Amazon API Gateway to channel the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source, sending data to Amazon S3.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As the company's POS systems increase, the amount of raw data would scale up substantially. Amazon API Gateway is an AWS service for creating, deploying, and managing APIs. It can handle any volume of incoming data and can effectively channel this raw data to an Amazon Kinesis data stream. Kinesis Data Streams can collect and process large streams of data records in real-time. The data collected is available in milliseconds to enable real-time analytics use cases. Additionally, using Amazon Kinesis Data Firehose, the company can easily load massive volumes of streaming data into AWS, in this case, Amazon S3. Firehose automatically scales to match the throughput of your data and requires no ongoing administration, which aligns with the requirement of minimized operational overhead."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Amazon Route 53 for traffic distribution to various EC2 instances.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This solution does not address the scalability requirement as effectively. Route 53 can distribute traffic, it won't help with the data processing scalability, and it does not minimize operational overhead in the context of data transformation and storage."
      },
      {
        "answer": "Deploy additional EC2 instances to deal with the rising volume of incoming data.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Adding more EC2 instances can theoretically accommodate more data, this approach doesn't minimize operational overhead and may not scale efficiently to accommodate millions of POS systems."
      },
      {
        "answer": "Send the raw data to Amazon Simple Queue Service (Amazon SQQ). Use EC2 instances for data processing.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon SQS can decouple the data producers and consumers, but the processing would still rely on EC2 instances, which might not be as scalable as required and can increase operational overhead."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html",
      "https://docs.aws.amazon.com/streams/latest/dev/introduction.html",
      "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
    ]
  },
  {
    "id": 56,
    "question": "A video streaming company is collecting and analyzing viewer engagement data on premises. The company plans to move this functionality to AWS. The viewer engagement data store is expected to increase continuously and will reach petabytes in size. The company aims to create a highly available data acquisition solution that allows real-time analytics of both current and new data using SQL.\n\nWhich solution can achieve these requirements with the LEAST operational overhead?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Send the engagement data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to forward the data to an Amazon Redshift cluster.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Store the engagement data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as it arrives in the S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a data acquisition service on Amazon EC2 instances that are distributed across multiple Availability Zones. Configure the service to send data to an Amazon RDS Multi-AZ database.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Send the engagement data to an Amazon Kinesis data stream. Configure the stream to forward the data to an Amazon S3 bucket.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nSend the engagement data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to forward the data to an Amazon Redshift cluster.\n\nFor real-time analytics of both current and new data using SQL, Amazon Kinesis Data Firehose combined with Amazon Redshift is an ideal solution. Amazon Kinesis Data Firehose can capture, transform, and load streaming data into data lakes, data stores, and analytics tools. It can handle streaming data in real-time, making it perfect for the continuous engagement data the company is dealing with.\n\nBy configuring Kinesis Data Firehose to forward the data to an Amazon Redshift cluster, the data is made readily available for SQL-based analytics. Amazon Redshift is a fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to analyze all data using SQL. This configuration also provides excellent scalability as the data store grows in size.\n\nThis solution minimizes operational overhead. Both Kinesis Data Firehose and Redshift are fully managed services, meaning AWS handles the underlying infrastructure, freeing the company to focus on analyzing the data rather than managing the database.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSend the engagement data to an Amazon Kinesis data stream. Configure the stream to forward the data to an Amazon S3 bucket.\n\nThis approach could handle the streaming data, it doesn't provide immediate SQL-based analytics capabilities, as data in S3 isn't readily queryable using SQL.\n\n\n\n\nStore the engagement data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as it arrives in the S3 bucket.\n\nThis option would not be suitable for real-time analytics because running AWS Lambda functions for every data arrival could introduce significant latency, especially with petabytes of data.\n\n\n\n\nCreate a data acquisition service on Amazon EC2 instances that are distributed across multiple Availability Zones. Configure the service to send data to an Amazon RDS Multi-AZ database.\n\nAmazon RDS may not be the best choice for a real-time petabyte-scale data store, and this approach also introduces considerable operational overhead related to managing EC2 instances and the data ingestion service.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html",
    "correctAnswerExplanations": [
      {
        "answer": "Send the engagement data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to forward the data to an Amazon Redshift cluster.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "For real-time analytics of both current and new data using SQL, Amazon Kinesis Data Firehose combined with Amazon Redshift is an ideal solution. Amazon Kinesis Data Firehose can capture, transform, and load streaming data into data lakes, data stores, and analytics tools. It can handle streaming data in real-time, making it perfect for the continuous engagement data the company is dealing with."
      },
      {
        "answer": "",
        "explanation": "By configuring Kinesis Data Firehose to forward the data to an Amazon Redshift cluster, the data is made readily available for SQL-based analytics. Amazon Redshift is a fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to analyze all data using SQL. This configuration also provides excellent scalability as the data store grows in size."
      },
      {
        "answer": "",
        "explanation": "This solution minimizes operational overhead. Both Kinesis Data Firehose and Redshift are fully managed services, meaning AWS handles the underlying infrastructure, freeing the company to focus on analyzing the data rather than managing the database."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Send the engagement data to an Amazon Kinesis data stream. Configure the stream to forward the data to an Amazon S3 bucket.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This approach could handle the streaming data, it doesn't provide immediate SQL-based analytics capabilities, as data in S3 isn't readily queryable using SQL."
      },
      {
        "answer": "Store the engagement data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as it arrives in the S3 bucket.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option would not be suitable for real-time analytics because running AWS Lambda functions for every data arrival could introduce significant latency, especially with petabytes of data."
      },
      {
        "answer": "Create a data acquisition service on Amazon EC2 instances that are distributed across multiple Availability Zones. Configure the service to send data to an Amazon RDS Multi-AZ database.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon RDS may not be the best choice for a real-time petabyte-scale data store, and this approach also introduces considerable operational overhead related to managing EC2 instances and the data ingestion service."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html",
      "https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html"
    ]
  },
  {
    "id": 57,
    "question": "A media agency requires to transfer its daily video archives to Amazon S3, where different departments can reach them. The size of the transferred videos fluctuates between 2 GB and 5 GB. The pattern of reaching this content in S3 alters dynamically and often. The videos should be immediately accessible and should be kept reachable for a duration of 3 months. The agency seeks the most cost-saving strategy that doesn't compromise access time.\n\nWhich S3 storage class would be the most suitable for the agency's needs?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "S3 Standard",
        "correct": false
      },
      {
        "id": 2,
        "answer": "S3 Intelligent-Tiering",
        "correct": true
      },
      {
        "id": 3,
        "answer": "S3 Glacier Instant Retrieval",
        "correct": false
      },
      {
        "id": 4,
        "answer": "S3 Standard-Infrequent Access (S3 Standard-IA)",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nS3 Intelligent-Tiering\n\nS3 Intelligent-Tiering is a storage class designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It's suitable for data with unknown or changing access patterns. In the case of the media agency, the access patterns are variable and can change often, and the data needs to be immediately accessible. This makes S3 Intelligent-Tiering the most cost-effective and appropriate choice for this scenario.\n\n\n\n\n\n\n\nIncorrect Options:\n\nS3 Glacier Instant Retrieval\n\nGlacier Instant Retrieval is designed for long-term archiving of data that may need to be retrieved instantly. Although this storage class provides immediate access, it is not the most cost-effective option for data that only needs to be retained for three months.\n\n\n\n\nS3 Standard\n\nS3 Standard offers high durability, availability, and performance object storage for frequently accessed data. While it does provide immediate access, it can be more expensive than S3 Intelligent-Tiering, especially for data with variable access patterns.\n\n\n\n\nS3 Standard-Infrequent Access (S3 Standard-IA)\n\nS3 Standard-IA is designed for data that is accessed less frequently, but requires rapid access when needed. However, it includes a retrieval fee, which could increase the overall cost for data with variable and often changing access patterns.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes",
    "correctAnswerExplanations": [
      {
        "answer": "S3 Intelligent-Tiering",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "S3 Intelligent-Tiering is a storage class designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It's suitable for data with unknown or changing access patterns. In the case of the media agency, the access patterns are variable and can change often, and the data needs to be immediately accessible. This makes S3 Intelligent-Tiering the most cost-effective and appropriate choice for this scenario."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "S3 Glacier Instant Retrieval",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Glacier Instant Retrieval is designed for long-term archiving of data that may need to be retrieved instantly. Although this storage class provides immediate access, it is not the most cost-effective option for data that only needs to be retained for three months."
      },
      {
        "answer": "S3 Standard",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. While it does provide immediate access, it can be more expensive than S3 Intelligent-Tiering, especially for data with variable access patterns."
      },
      {
        "answer": "S3 Standard-Infrequent Access (S3 Standard-IA)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "S3 Standard-IA is designed for data that is accessed less frequently, but requires rapid access when needed. However, it includes a retrieval fee, which could increase the overall cost for data with variable and often changing access patterns."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "id": 58,
    "question": "A solutions architect is developing a serverless application using AWS services. This application will occasionally process data based on user interactions, with several hours of inactivity expected between requests. The data processing should start asynchronously and complete within a few seconds after the interaction occurs.\n\nWhich AWS service should the architect use for the data processing to meet these requirements most cost-effectively?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon EMR for big data processing",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon EC2 for traditional server-based processing",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Batch for job scheduling",
        "correct": false
      },
      {
        "id": 4,
        "answer": "AWS Lambda for event-driven processing",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nAWS Lambda for event-driven processing\n\nAWS Lambda is a serverless compute service that lets you run your code without provisioning or managing servers. It executes your code only when needed and scales automatically, from a few requests per day to thousands per second. This makes it incredibly useful for workloads that are sporadic or unpredictable, like the one described in the question.\n\nThe real advantage here is the cost-effectiveness. With AWS Lambda, you only pay for the compute time you consume - there is no charge when your code is not running. Thus, for applications with variable or sporadic usage, Lambda can significantly reduce costs. It's also a good fit for applications that need to be responsive, as it can start executing your code within milliseconds of an event.\n\nAlso, the fact that it handles all the operational aspects such as scaling, patching, and administration allows developers to focus purely on building great applications, enhancing productivity.\n\nSince the data processing needs to start asynchronously and complete within a few seconds after the interaction occurs, AWS Lambda is a perfect fit considering its responsiveness and cost-effectiveness.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAWS Batch for job scheduling\n\nAWS Batch is a powerful tool for scheduling and running batch computing workloads, but it isn't tailored for this use case. Batch jobs typically take longer to run and are less concerned with immediate response times, which makes it less suitable for the requirement of processing data within a few seconds after a request.\n\n\n\n\nAmazon EMR for big data processing\n\nAmazon EMR is excellent for big data processing tasks, it is overkill for the needs of this application. EMR clusters are designed to be long-lived and can be expensive to maintain, especially for sporadic workloads.\n\n\n\n\nAmazon EC2 for traditional server-based processing\n\nUsing Amazon EC2 instances would require constant running, leading to higher costs, especially during idle periods. It's not ideal for asynchronous processing of sporadic requests due to the overhead of instance management and scaling considerations.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/lambda",
    "correctAnswerExplanations": [
      {
        "answer": "AWS Lambda for event-driven processing",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Lambda is a serverless compute service that lets you run your code without provisioning or managing servers. It executes your code only when needed and scales automatically, from a few requests per day to thousands per second. This makes it incredibly useful for workloads that are sporadic or unpredictable, like the one described in the question."
      },
      {
        "answer": "",
        "explanation": "The real advantage here is the cost-effectiveness. With AWS Lambda, you only pay for the compute time you consume - there is no charge when your code is not running. Thus, for applications with variable or sporadic usage, Lambda can significantly reduce costs. It's also a good fit for applications that need to be responsive, as it can start executing your code within milliseconds of an event."
      },
      {
        "answer": "",
        "explanation": "Also, the fact that it handles all the operational aspects such as scaling, patching, and administration allows developers to focus purely on building great applications, enhancing productivity."
      },
      {
        "answer": "",
        "explanation": "Since the data processing needs to start asynchronously and complete within a few seconds after the interaction occurs, AWS Lambda is a perfect fit considering its responsiveness and cost-effectiveness."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "AWS Batch for job scheduling",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Batch is a powerful tool for scheduling and running batch computing workloads, but it isn't tailored for this use case. Batch jobs typically take longer to run and are less concerned with immediate response times, which makes it less suitable for the requirement of processing data within a few seconds after a request."
      },
      {
        "answer": "Amazon EMR for big data processing",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon EMR is excellent for big data processing tasks, it is overkill for the needs of this application. EMR clusters are designed to be long-lived and can be expensive to maintain, especially for sporadic workloads."
      },
      {
        "answer": "Amazon EC2 for traditional server-based processing",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Using Amazon EC2 instances would require constant running, leading to higher costs, especially during idle periods. It's not ideal for asynchronous processing of sporadic requests due to the overhead of instance management and scaling considerations."
      }
    ],
    "references": [
      "https://aws.amazon.com/lambda"
    ]
  },
  {
    "id": 59,
    "question": "An application runs on Amazon EC2 instances with Elastic IP addresses in VPC A. The application needs to access an Amazon RDS in VPC B. Both VPCs belong to the same AWS account.\n\nWhat solution will provide the most secure way to achieve the required connection?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Launch an EC2 instance with an Elastic IP address in VPC B and route all requests through this new EC2 instance.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Make the ElastiCache cluster publicly accessible and assign a public IP address to the ElastiCache nodes.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a security group for the Amazon RDS that allows all traffic from the public IP address of the application server in VPC A.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure a VPC peering connection between VPC A and VPC B.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nConfigure a VPC peering connection between VPC A and VPC B.\n\nVPC peering allows for private, secure communication between two VPCs. It creates a networking connection between two VPCs that enables instances in either VPC to communicate with each other as if they are within the same network. This communication occurs using private IP addresses, which is more secure than using public IPs. In this case, setting up a VPC peering connection between VPC A (where the EC2 instances are located) and VPC B (where the Amazon RDS is located) will enable the EC2 instances to securely access the RDS instance.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a security group for the Amazon RDS that allows all traffic from the public IP address of the application server in VPC A.\n\nThis option will expose your Amazon RDS instance to the public internet, which is not recommended and is less secure than VPC peering.\n\n\n\n\nMake the ElastiCache cluster publicly accessible and assign a public IP address to the ElastiCache nodes.\n\nThe question doesn't mention anything about an ElastiCache cluster, so this option is irrelevant. Besides, making your resources publicly accessible is generally not recommended from a security perspective.\n\n\n\n\nLaunch an EC2 instance with an Elastic IP address in VPC B and route all requests through this new EC2 instance.\n\nThis solution would unnecessarily expose an EC2 instance to the public and also add additional network latency because of the additional hop, thus being less secure and efficient than VPC peering.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure a VPC peering connection between VPC A and VPC B.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "VPC peering allows for private, secure communication between two VPCs. It creates a networking connection between two VPCs that enables instances in either VPC to communicate with each other as if they are within the same network. This communication occurs using private IP addresses, which is more secure than using public IPs. In this case, setting up a VPC peering connection between VPC A (where the EC2 instances are located) and VPC B (where the Amazon RDS is located) will enable the EC2 instances to securely access the RDS instance."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a security group for the Amazon RDS that allows all traffic from the public IP address of the application server in VPC A.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option will expose your Amazon RDS instance to the public internet, which is not recommended and is less secure than VPC peering."
      },
      {
        "answer": "Make the ElastiCache cluster publicly accessible and assign a public IP address to the ElastiCache nodes.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The question doesn't mention anything about an ElastiCache cluster, so this option is irrelevant. Besides, making your resources publicly accessible is generally not recommended from a security perspective."
      },
      {
        "answer": "Launch an EC2 instance with an Elastic IP address in VPC B and route all requests through this new EC2 instance.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This solution would unnecessarily expose an EC2 instance to the public and also add additional network latency because of the additional hop, thus being less secure and efficient than VPC peering."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html"
    ]
  },
  {
    "id": 60,
    "question": "A firm runs its PostgreSQL databases on Amazon EC2 instances, which it manages manually. The scaling and replication process as per the fluctuating demand has become complex for the firm. They are seeking a new solution that eases the addition or subtraction of compute capacity from its database layer as required. Additionally, the solution should enhance performance, scalability, and durability with minimum operational efforts.\n\nWhich solution will meet these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an EC2 Auto Scaling group for the database layer and migrate the current databases to this new environment.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Consolidate the databases into a single, larger PostgreSQL database and run it on larger EC2 instances.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nMigrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL.\n\nAmazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL and PostgreSQL-compatible editions), where the database will automatically start-up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database capacity. The migration of the firm's PostgreSQL databases to Amazon Aurora Serverless for Aurora PostgreSQL is a good solution because it would satisfy the firm's requirements for performance, scalability, durability, and minimal operational efforts. Moreover, since Aurora is a managed service, it takes care of the tedious administrative tasks like hardware provisioning, software patching, setup, configuration, or backups.\n\n\n\n\n\n\n\nIncorrect Options:\n\nMigrate the databases to Amazon Aurora Serverless for Aurora MySQL.\n\nThis wouldn't be the optimal solution because migrating from PostgreSQL to MySQL might result in compatibility issues due to differences in SQL dialect, features, and data types between these two database systems.\n\n\n\n\nConsolidate the databases into a single, larger PostgreSQL database and run it on larger EC2 instances.\n\nThis approach would not solve the problem of manual management or automatic scaling. It may also create a single point of failure and potentially introduce performance issues due to the increased load on a single database.\n\n\n\n\nCreate an EC2 Auto Scaling group for the database layer and migrate the current databases to this new environment.\n\nAlthough this approach might help manage compute capacity for the database layer, managing a database on EC2 still requires significant operational effort. This option does not address the need for reduced operational effort.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/aurora/serverless\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html",
    "correctAnswerExplanations": [
      {
        "answer": "Migrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL and PostgreSQL-compatible editions), where the database will automatically start-up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database capacity. The migration of the firm's PostgreSQL databases to Amazon Aurora Serverless for Aurora PostgreSQL is a good solution because it would satisfy the firm's requirements for performance, scalability, durability, and minimal operational efforts. Moreover, since Aurora is a managed service, it takes care of the tedious administrative tasks like hardware provisioning, software patching, setup, configuration, or backups."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This wouldn't be the optimal solution because migrating from PostgreSQL to MySQL might result in compatibility issues due to differences in SQL dialect, features, and data types between these two database systems."
      },
      {
        "answer": "Consolidate the databases into a single, larger PostgreSQL database and run it on larger EC2 instances.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This approach would not solve the problem of manual management or automatic scaling. It may also create a single point of failure and potentially introduce performance issues due to the increased load on a single database."
      },
      {
        "answer": "Create an EC2 Auto Scaling group for the database layer and migrate the current databases to this new environment.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Although this approach might help manage compute capacity for the database layer, managing a database on EC2 still requires significant operational effort. This option does not address the need for reduced operational effort."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/aurora/serverless",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html"
    ]
  },
  {
    "id": 61,
    "question": "A startup is building a web application and wants to mitigate typical web attacks such as SQL injection or cross-site scripting. The startup has a lean team with minimal resources for managing and updating their AWS infrastructure. They want to minimize their share of server security management.\n\nWhat should a solutions architect suggest to fulfill these needs?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon S3 to host the web application with public access.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure AWS WAF rules and associate them with their ALB.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Set up a new ALB that routes traffic to an Amazon EC2 instance with a third-party firewall, then forwards the traffic to the existing ALB.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy AWS Shield Advanced and designate the ALB as a safeguarded resource.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nConfigure AWS WAF rules and associate them with their ALB.\n\nWeb Application Firewall (AWS WAF) is a powerful service that protects web applications against common web threats such as SQL injection and cross-site scripting. These types of attacks can compromise the security of your application, affect its availability, or consume excessive resources, thereby causing service disruption or elevated costs.\n\nBy creating specific AWS WAF rules and associating with the Application Load Balancer (ALB), the startup can efficiently filter incoming traffic to their web application. This approach will block any malicious requests that match the patterns defined in the WAF rules before they even reach the application, hence reducing the risk of application-level attacks.\n\nImplementing this solution aligns with the startup's requirement to minimize their server security management responsibilities. AWS manages and updates the WAF service, freeing the startup's lean team from the complexities and operational overhead associated with maintaining their own security systems.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon S3 to host the web application with public access.\n\nAmazon S3 is a storage service, and it can host static websites, it is not appropriate for hosting applications with server-side processing. It also wouldn't provide any additional protection against the mentioned web attacks.\n\n\n\n\nDeploy AWS Shield Advanced and designate the ALB as a safeguarded resource.\n\nAWS Shield Advanced provides protection against larger-scaled DDoS attacks but does not specifically provide application-level protection against attacks such as SQL injection or cross-site scripting.\n\n\n\n\nSet up a new ALB that routes traffic to an Amazon EC2 instance with a third-party firewall, then forwards the traffic to the existing ALB.\n\nIt is a complex solution that increases operational overhead. This solution goes against the company's need to reduce the responsibility of managing and updating its AWS environment.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-rules.html\n\nhttps://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-managed-rule-groups.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure AWS WAF rules and associate them with their ALB.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Web Application Firewall (AWS WAF) is a powerful service that protects web applications against common web threats such as SQL injection and cross-site scripting. These types of attacks can compromise the security of your application, affect its availability, or consume excessive resources, thereby causing service disruption or elevated costs."
      },
      {
        "answer": "",
        "explanation": "By creating specific AWS WAF rules and associating with the Application Load Balancer (ALB), the startup can efficiently filter incoming traffic to their web application. This approach will block any malicious requests that match the patterns defined in the WAF rules before they even reach the application, hence reducing the risk of application-level attacks."
      },
      {
        "answer": "",
        "explanation": "Implementing this solution aligns with the startup's requirement to minimize their server security management responsibilities. AWS manages and updates the WAF service, freeing the startup's lean team from the complexities and operational overhead associated with maintaining their own security systems."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Amazon S3 to host the web application with public access.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 is a storage service, and it can host static websites, it is not appropriate for hosting applications with server-side processing. It also wouldn't provide any additional protection against the mentioned web attacks."
      },
      {
        "answer": "Deploy AWS Shield Advanced and designate the ALB as a safeguarded resource.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Shield Advanced provides protection against larger-scaled DDoS attacks but does not specifically provide application-level protection against attacks such as SQL injection or cross-site scripting."
      },
      {
        "answer": "Set up a new ALB that routes traffic to an Amazon EC2 instance with a third-party firewall, then forwards the traffic to the existing ALB.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "It is a complex solution that increases operational overhead. This solution goes against the company's need to reduce the responsibility of managing and updating its AWS environment. "
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/waf/latest/developerguide/waf-rules.html",
      "https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/waf-managed-rule-groups.html"
    ]
  },
  {
    "id": 62,
    "question": "A retail company is migrating its in-house MySQL database to Amazon RDS for PostgreSQL. The database serves multiple applications, which must be migrated sequentially with a month gap between each migration. The database is characterized by heavy read and write operations and data consistency must be maintained throughout the migration process.\n\nWhat should a solutions architect recommend?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Schema Conversion Tool in conjunction with AWS Database Migration Service (AWS DMS) using a memory-optimized replication instance. Create a full load plus change data capture (CDC) replication task and table mapping for all tables.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use AWS Snowball for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication task and a table mapping for all tables.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS Snowball for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a change data capture (CDC) replication task and a table mapping for all tables.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Schema Conversion Tool in conjunction with AWS Database Migration Service (AWS DMS) using a compute-optimized replication instance. Create a full load plus change data capture (CDC) replication task and table mapping for the most sizable tables.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nUse AWS Schema Conversion Tool in conjunction with AWS Database Migration Service (AWS DMS) using a memory-optimized replication instance. Create a full load plus change data capture (CDC) replication task and table mapping for all tables.\n\nThe company is migrating from MySQL to Amazon RDS for PostgreSQL and it requires the database to be fully operational during the migration process. Given this, the AWS Schema Conversion Tool (SCT) and AWS Database Migration Service (DMS) are ideal.\n\nAWS SCT simplifies heterogeneous database migrations by automatically converting the source database schema to match that of the target database. SCT also converts a majority of the custom code, including views, stored procedures, and functions, to match the target database.\n\nAWS DMS helps migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. AWS DMS supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations such as MySQL to PostgreSQL.\n\nA memory-optimized replication instance is recommended due to heavy read and write operations. Memory-optimized instances are designed to deliver fast performance for workloads that process large data sets in memory, which aligns with the scenario given.\n\nCreating a full load plus CDC replication task and table mapping for all tables ensures that all data gets migrated initially, and then DMS will continuously replicate changes on the source database (captured by CDC) to the target database. This ensures data consistency and availability during the migration process.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS Snowball for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a change data capture (CDC) replication task and a table mapping for all tables.\n\nAWS Snowball is used for offline data transfers, not for database migration. It doesn't support schema conversion or transformation of data from one database type to another, making it unsuitable for the task.\n\n\n\n\nUse AWS Snowball for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication task and a table mapping for all tables.\n\nAWS Snowball is inappropriate for this task as it doesn't support schema conversion or database transformations. AWS DMS is indeed used for migration and CDC, but AWS Snowball can't assist with the initial migration here.\n\n\n\n\nUse AWS Schema Conversion Tool in conjunction with AWS Database Migration Service (AWS DMS) using a compute-optimized replication instance. Create a full load plus change data capture (CDC) replication task and table mapping for the most sizable tables.\n\nCompute-optimized replication instances are usually suited for CPU-intensive tasks, but given the database's heavy read and write operations, a memory-optimized instance is more suitable. Also, creating a table mapping for only the most sizable tables could lead to data inconsistency if there are changes in smaller tables.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.Types.html\n\nhttps://aws.amazon.com/dms/schema-conversion-tool\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/Welcome.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Schema Conversion Tool in conjunction with AWS Database Migration Service (AWS DMS) using a memory-optimized replication instance. Create a full load plus change data capture (CDC) replication task and table mapping for all tables.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The company is migrating from MySQL to Amazon RDS for PostgreSQL and it requires the database to be fully operational during the migration process. Given this, the AWS Schema Conversion Tool (SCT) and AWS Database Migration Service (DMS) are ideal."
      },
      {
        "answer": "",
        "explanation": "AWS SCT simplifies heterogeneous database migrations by automatically converting the source database schema to match that of the target database. SCT also converts a majority of the custom code, including views, stored procedures, and functions, to match the target database."
      },
      {
        "answer": "",
        "explanation": "AWS DMS helps migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. AWS DMS supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations such as MySQL to PostgreSQL."
      },
      {
        "answer": "",
        "explanation": "A memory-optimized replication instance is recommended due to heavy read and write operations. Memory-optimized instances are designed to deliver fast performance for workloads that process large data sets in memory, which aligns with the scenario given."
      },
      {
        "answer": "",
        "explanation": "Creating a full load plus CDC replication task and table mapping for all tables ensures that all data gets migrated initially, and then DMS will continuously replicate changes on the source database (captured by CDC) to the target database. This ensures data consistency and availability during the migration process."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use AWS Snowball for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a change data capture (CDC) replication task and a table mapping for all tables.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Snowball is used for offline data transfers, not for database migration. It doesn't support schema conversion or transformation of data from one database type to another, making it unsuitable for the task."
      },
      {
        "answer": "Use AWS Snowball for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication task and a table mapping for all tables.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Snowball is inappropriate for this task as it doesn't support schema conversion or database transformations. AWS DMS is indeed used for migration and CDC, but AWS Snowball can't assist with the initial migration here."
      },
      {
        "answer": "Use AWS Schema Conversion Tool in conjunction with AWS Database Migration Service (AWS DMS) using a compute-optimized replication instance. Create a full load plus change data capture (CDC) replication task and table mapping for the most sizable tables.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Compute-optimized replication instances are usually suited for CPU-intensive tasks, but given the database's heavy read and write operations, a memory-optimized instance is more suitable. Also, creating a table mapping for only the most sizable tables could lead to data inconsistency if there are changes in smaller tables."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.Types.html",
      "https://aws.amazon.com/dms/schema-conversion-tool",
      "https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html"
    ]
  },
  {
    "id": 63,
    "question": "An organization runs multiple production applications, including resources from AWS Lambda, Amazon RDS, Amazon EC2, Amazon SNS, and Amazon SQS, distributed across various AWS Regions. All the resources are identified with a tag labeled \"project\" and a corresponding value for each project. A solutions architect is required to find the most efficient way to locate all these tagged components.\n\nWhat solution would fulfill these requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS CloudTrail to generate a compilation of resources with the \"project\" tag.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use the AWS CLI to query each service across all Regions for the tagged components.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon CloudWatch Logs Insights to perform a query that reports on the components tagged with the \"project\" tag.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use the AWS Resource Groups Tag Editor to perform a global search for resources tagged with the \"project\" tag.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nUse the AWS Resource Groups Tag Editor to perform a global search for resources tagged with the \"project\" tag.\n\nAWS Resource Groups helps you organize your AWS resources based on criteria that you define. Specifically, AWS Resource Groups and Tag Editor provide a unified view of AWS resources from different AWS services, which can be categorized and managed according to the tags that you assign to them.\n\nThe Tag Editor feature within AWS Resource Groups allows you to search globally across all your AWS resources based on their assigned tags. In our scenario, you can quickly and efficiently locate all resources that are tagged with the \"project\" tag. This can be done across all regions and across all the different services (EC2, Lambda, RDS, SNS, SQS) used.\n\nThis global, cross-region, and cross-service capability makes the Tag Editor the most efficient solution for this task. It provides a simple and direct method to get the information you need without the need for additional scripting or querying each service individually.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon CloudWatch Logs Insights to perform a query that reports on the components tagged with the \"project\" tag.\n\nAmazon CloudWatch Logs Insights designed to assist in the analysis, exploration, and visualization of your logs. It's not used for locating tagged resources.\n\n\n\n\nUse AWS CloudTrail to generate a compilation of resources with the \"project\" tag.\n\nAWS CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. It's not used for querying resource tags.\n\n\n\n\nUse the AWS CLI to query each service across all Regions for the tagged components.\n\nAWS CLI could technically be used to identify resources by their tags, it would require writing and running a script for each service in each region, which would be a highly manual and time-consuming process. Therefore, it wouldn't be the \"quickest solution\".\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/tag-editor/latest/userguide/tag-editor.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use the AWS Resource Groups Tag Editor to perform a global search for resources tagged with the \"project\" tag.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Resource Groups helps you organize your AWS resources based on criteria that you define. Specifically, AWS Resource Groups and Tag Editor provide a unified view of AWS resources from different AWS services, which can be categorized and managed according to the tags that you assign to them."
      },
      {
        "answer": "",
        "explanation": "The Tag Editor feature within AWS Resource Groups allows you to search globally across all your AWS resources based on their assigned tags. In our scenario, you can quickly and efficiently locate all resources that are tagged with the \"project\" tag. This can be done across all regions and across all the different services (EC2, Lambda, RDS, SNS, SQS) used."
      },
      {
        "answer": "",
        "explanation": "This global, cross-region, and cross-service capability makes the Tag Editor the most efficient solution for this task. It provides a simple and direct method to get the information you need without the need for additional scripting or querying each service individually."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Amazon CloudWatch Logs Insights to perform a query that reports on the components tagged with the \"project\" tag.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon CloudWatch Logs Insights designed to assist in the analysis, exploration, and visualization of your logs. It's not used for locating tagged resources."
      },
      {
        "answer": "Use AWS CloudTrail to generate a compilation of resources with the \"project\" tag.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. It's not used for querying resource tags."
      },
      {
        "answer": "Use the AWS CLI to query each service across all Regions for the tagged components.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS CLI could technically be used to identify resources by their tags, it would require writing and running a script for each service in each region, which would be a highly manual and time-consuming process. Therefore, it wouldn't be the \"quickest solution\"."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/tag-editor/latest/userguide/tag-editor.html"
    ]
  },
  {
    "id": 64,
    "question": "An organization is operating a web server on an Amazon EC2 instance within a private subnet, which has an Elastic IP address. The EC2 instance has the default security group assigned. The default network ACL has been configured to disallow all traffic. A Solutions Architect is tasked to make the web server available to all company offices globally on port 443.\n\nWhich two steps would accomplish this task? (Select TWO.)",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Modify the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Modify the network ACL to allow TCP port 443 from source 0.0.0.0/0",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Modify the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate a security group with a rule to allow TCP port 443 from source 0.0.0.0/0\n\nThe security group is instance-level firewalls that control the inbound and outbound traffic to an EC2 instance. So, to make the web server available to all the company offices globally, we need to configure the security group rules to allow incoming traffic on TCP port 443, which is the default port for HTTPS. This is exactly what the option is about creating a new security group with a rule that allows inbound traffic on port 443 from any IP address (0.0.0.0/0).\n\n\n\n\nModify the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0\n\nNetwork Access Control List (ACL) is another layer of security that controls traffic to and from the subnets to which they are associated. They operate at the subnet level. Currently, the network ACL is set to disallow all traffic. To make the web server available, we need to allow traffic on TCP port 443. But, since network ACLs are stateless, we must also define the return path for the traffic. For Linux-based systems, the ephemeral (short-lived) ports range from 32768 to 65535. Therefore, modifying the network ACL to allow inbound traffic on TCP port 443 from any IP (0.0.0.0/0) and allowing outbound traffic to the ephemeral ports completes the connection setup.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0\n\nSecurity group rules are always written in terms of the source of the traffic, not the destination. Hence, this option is incorrect.\n\n\n\n\nModify the network ACL to allow TCP port 443 from source 0.0.0.0/0\n\nModifying the network ACL to permit TCP port 443 from source 0.0.0.0/0 alone won't be sufficient. Network ACLs are stateless and don't track connections. Thus, it needs rules for return (outbound) traffic as well.\n\n\n\n\nModify the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0\n\nThis option incorrectly describes network ACL rules. Network ACL rules are separate for inbound and outbound traffic and do not contain a source and a destination in the same rule.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The security group is instance-level firewalls that control the inbound and outbound traffic to an EC2 instance. So, to make the web server available to all the company offices globally, we need to configure the security group rules to allow incoming traffic on TCP port 443, which is the default port for HTTPS. This is exactly what the option is about creating a new security group with a rule that allows inbound traffic on port 443 from any IP address (0.0.0.0/0)."
      },
      {
        "answer": "Modify the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Network Access Control List (ACL) is another layer of security that controls traffic to and from the subnets to which they are associated. They operate at the subnet level. Currently, the network ACL is set to disallow all traffic. To make the web server available, we need to allow traffic on TCP port 443. But, since network ACLs are stateless, we must also define the return path for the traffic. For Linux-based systems, the ephemeral (short-lived) ports range from 32768 to 65535. Therefore, modifying the network ACL to allow inbound traffic on TCP port 443 from any IP (0.0.0.0/0) and allowing outbound traffic to the ephemeral ports completes the connection setup."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Security group rules are always written in terms of the source of the traffic, not the destination. Hence, this option is incorrect."
      },
      {
        "answer": "Modify the network ACL to allow TCP port 443 from source 0.0.0.0/0",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Modifying the network ACL to permit TCP port 443 from source 0.0.0.0/0 alone won't be sufficient. Network ACLs are stateless and don't track connections. Thus, it needs rules for return (outbound) traffic as well."
      },
      {
        "answer": "Modify the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option incorrectly describes network ACL rules. Network ACL rules are separate for inbound and outbound traffic and do not contain a source and a destination in the same rule."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html"
    ]
  },
  {
    "id": 65,
    "question": "A tech startup is developing a complex web application using microservices architecture. The application will be hosted on AWS, and they want to use containerization for their services. The company wants to focus on application development and seeks to avoid any unnecessary overhead related to infrastructure management and scaling.\n\nWhich pair of actions should a solutions architect recommend to fulfill these requirements? (Select TWO.)",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Deploy Kubernetes worker nodes across multiple Availability Zones using Amazon EC2 instances and create a deployment that specifies two or more replicas for each microservice.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Deploy the Kubernetes master plane across multiple Availability Zones using Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy an Amazon Elastic Container Service (Amazon ECS) service using a Fargate launch type, with a specified minimum of 2 tasks.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Deploy an Amazon Elastic Container Service (Amazon ECS) service using an Amazon EC2 launch type, with a specified minimum of 2 tasks.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nDeploy an Amazon Elastic Container Service (Amazon ECS) cluster.\n\nAmazon ECS is a highly scalable and high-performance container orchestration service that supports Docker containers. It allows you to run and manage your Docker containerized applications without having to install and operate your own container orchestration software or manage the underlying infrastructure. Given the company wants to focus on application development and avoid infrastructure management overhead, Amazon ECS is a perfect fit.\n\n\n\n\nDeploy an Amazon Elastic Container Service (Amazon ECS) service using a Fargate launch type, with a specified minimum of 2 tasks.\n\nAmazon ECS on AWS Fargate removes the need to provision and manage servers and lets you specify and pay for resources per application, which is great for a company looking to focus on the application rather than the infrastructure. By specifying a minimum of 2 tasks, the application can enjoy high availability and fault tolerance, important characteristics for any complex web application.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDeploy the Kubernetes master plane across multiple Availability Zones using Amazon EC2 instances.\n\nThis would involve significant infrastructure management overhead and not an ideal option for a company that wants to focus more on application development. Deploying and managing a Kubernetes master plane across multiple AZs on EC2 instances would require significant operational effort.\n\n\n\n\nDeploy an Amazon Elastic Container Service (Amazon ECS) service using an Amazon EC2 launch type, with a specified minimum of 2 tasks.\n\nIt introduces additional overhead in terms of managing EC2 instances, which the company wants to avoid. With the EC2 launch type, you would have to manage the EC2 instances that the containers run on.\n\n\n\n\nDeploy Kubernetes worker nodes across multiple Availability Zones using Amazon EC2 instances and create a deployment that specifies two or more replicas for each microservice.\n\nThis approach provides scalability and redundancy for the microservices, it involves managing the underlying EC2 instances and the Kubernetes cluster itself, which the company wants to avoid.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html",
    "correctAnswerExplanations": [
      {
        "answer": "Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon ECS is a highly scalable and high-performance container orchestration service that supports Docker containers. It allows you to run and manage your Docker containerized applications without having to install and operate your own container orchestration software or manage the underlying infrastructure. Given the company wants to focus on application development and avoid infrastructure management overhead, Amazon ECS is a perfect fit."
      },
      {
        "answer": "Deploy an Amazon Elastic Container Service (Amazon ECS) service using a Fargate launch type, with a specified minimum of 2 tasks.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon ECS on AWS Fargate removes the need to provision and manage servers and lets you specify and pay for resources per application, which is great for a company looking to focus on the application rather than the infrastructure. By specifying a minimum of 2 tasks, the application can enjoy high availability and fault tolerance, important characteristics for any complex web application."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Deploy the Kubernetes master plane across multiple Availability Zones using Amazon EC2 instances.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This would involve significant infrastructure management overhead and not an ideal option for a company that wants to focus more on application development. Deploying and managing a Kubernetes master plane across multiple AZs on EC2 instances would require significant operational effort."
      },
      {
        "answer": "Deploy an Amazon Elastic Container Service (Amazon ECS) service using an Amazon EC2 launch type, with a specified minimum of 2 tasks.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "It introduces additional overhead in terms of managing EC2 instances, which the company wants to avoid. With the EC2 launch type, you would have to manage the EC2 instances that the containers run on."
      },
      {
        "answer": "Deploy Kubernetes worker nodes across multiple Availability Zones using Amazon EC2 instances and create a deployment that specifies two or more replicas for each microservice.",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This approach provides scalability and redundancy for the microservices, it involves managing the underlying EC2 instances and the Kubernetes cluster itself, which the company wants to avoid."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html"
    ]
  }
]