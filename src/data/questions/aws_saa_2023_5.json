[
  {
    "id": 1,
    "question": "An organization is designing a service for customer feedback via SMS. The company needs to dispatch satisfaction survey links to its customers and allows them to respond via SMS. The responses need to be stored for two years for further analysis.\n\nWhich solution will meet these requirements?",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Simple Queue Service (Amazon SQS) to send the SMS surveys. Utilize AWS Lambda to process the responses.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Establish an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Construct an Amazon Pinpoint campaign. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Develop an Amazon Connect contact flow to dispatch the SMS surveys. Utilize AWS Lambda to process the incoming responses.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nConstruct an Amazon Pinpoint campaign. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.\n\nAmazon Pinpoint is a fully managed service for sending targeted messages to customers across multiple communication channels, including SMS. By constructing an Amazon Pinpoint campaign, the organization can easily dispatch SMS surveys to customers and collect their responses.\n\nTo store and analyze the responses, Amazon Pinpoint can be configured to send events to an Amazon Kinesis data stream. The data stream enables real-time analysis and provides a durable storage mechanism. The organization can set up a consumer application or process, such as AWS Lambda or Amazon Kinesis Data Analytics, to process and analyze the responses stored in the data stream.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDevelop an Amazon Connect contact flow to dispatch the SMS surveys. Utilize AWS Lambda to process the incoming responses.\n\nAmazon Connect is primarily used for creating contact center workflows and not ideal for sending SMS.\n\n\n\n\nUse Amazon Simple Queue Service (Amazon SQS) to send the SMS surveys. Utilize AWS Lambda to process the responses.\n\nAmazon SQS is a message queuing service for reliably communicating among distributed software components and microservices - but not designed specifically for SMS communication.\n\n\n\n\nEstablish an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.\n\nAmazon SNS is a service for pub/sub messaging and mobile notifications. It could be used for sending SMS but doesn't support two-way SMS communication.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/pinpoint/latest/userguide/channels-sms-two-way.html\n\nhttps://docs.aws.amazon.com/streams/latest/dev/introduction.html",
    "correctAnswerExplanations": [
      {
        "answer": "Construct an Amazon Pinpoint campaign. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.",
        "explanation": "Amazon Pinpoint is a fully managed service for sending targeted messages to customers across multiple communication channels, including SMS. By constructing an Amazon Pinpoint campaign, the organization can easily dispatch SMS surveys to customers and collect their responses."
      },
      {
        "answer": "Establish an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.",
        "explanation": "Amazon SNS is a service for pub/sub messaging and mobile notifications. It could be used for sending SMS but doesn't support two-way SMS communication."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Develop an Amazon Connect contact flow to dispatch the SMS surveys. Utilize AWS Lambda to process the incoming responses.",
        "explanation": "Amazon Connect is primarily used for creating contact center workflows and not ideal for sending SMS."
      },
      {
        "answer": "Use Amazon Simple Queue Service (Amazon SQS) to send the SMS surveys. Utilize AWS Lambda to process the responses.",
        "explanation": "Amazon SQS is a message queuing service for reliably communicating among distributed software components and microservices - but not designed specifically for SMS communication."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/pinpoint/latest/userguide/channels-sms-two-way.html",
      "https://docs.aws.amazon.com/streams/latest/dev/introduction.html"
    ]
  },
  {
    "id": 2,
    "question": "A healthcare analytics firm is developing a new application that processes large datasets to provide real-time insights. The application must be able to scale on-demand to handle the unpredictable load and must also be cost-effective. The company has limited DevOps resources and prefers a solution with minimal operational overhead.\n\nWhich of the following solutions is the MOST cost-effective for this scenario?",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Develop the application using AWS Lambda functions and Amazon DynamoDB.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Spot instances for running the application and store the data in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Run the application in Docker containers managed by Amazon ECS on EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy the application on Amazon EC2 instances with Auto Scaling groups.",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nDevelop the application using AWS Lambda functions and Amazon DynamoDB.\n\nUsing serverless services like AWS Lambda and Amazon DynamoDB provides the most cost-effective solution for this scenario. AWS Lambda is a serverless compute service that automatically manages the underlying compute resources, making it a perfect fit for applications with unpredictable loads. It charges for the compute time you consume, and you pay only for the time your code is running. Amazon DynamoDB, a serverless NoSQL database, can handle high levels of request traffic and automatically scales up and down to meet demand, allowing for cost optimization.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDeploy the application on Amazon EC2 instances with Auto Scaling groups.\n\nWhile this provides scalability, it introduces more operational overhead compared to serverless services, and EC2 instances may not be as cost-effective as serverless options due to their pricing model.\n\n\n\n\nUse Spot instances for running the application and store the data in Amazon S3.\n\nSpot instances provide cost savings but are not guaranteed to be available, which could affect the application's performance and availability. Amazon S3 is excellent for storing data, but it's not designed for real-time processing of large datasets.\n\n\n\n\nRun the application in Docker containers managed by Amazon ECS on EC2 instances.\n\nWhile this option provides scalability, it also requires managing EC2 instances and Docker containers, resulting in more operational overhead compared to serverless solutions.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/lambda/\n\nhttps://aws.amazon.com/dynamodb/",
    "correctAnswerExplanations": [
      {
        "answer": "Develop the application using AWS Lambda functions and Amazon DynamoDB.",
        "explanation": "Using serverless services like AWS Lambda and Amazon DynamoDB provides the most cost-effective solution for this scenario. AWS Lambda is a serverless compute service that automatically manages the underlying compute resources, making it a perfect fit for applications with unpredictable loads. It charges for the compute time you consume, and you pay only for the time your code is running. Amazon DynamoDB, a serverless NoSQL database, can handle high levels of request traffic and automatically scales up and down to meet demand, allowing for cost optimization."
      },
      {
        "answer": "Deploy the application on Amazon EC2 instances with Auto Scaling groups.",
        "explanation": "While this provides scalability, it introduces more operational overhead compared to serverless services, and EC2 instances may not be as cost-effective as serverless options due to their pricing model."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Spot instances for running the application and store the data in Amazon S3.",
        "explanation": "Spot instances provide cost savings but are not guaranteed to be available, which could affect the application's performance and availability. Amazon S3 is excellent for storing data, but it's not designed for real-time processing of large datasets."
      },
      {
        "answer": "Run the application in Docker containers managed by Amazon ECS on EC2 instances.",
        "explanation": "While this option provides scalability, it also requires managing EC2 instances and Docker containers, resulting in more operational overhead compared to serverless solutions."
      }
    ],
    "references": [
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/dynamodb/"
    ]
  },
  {
    "id": 3,
    "question": "A media streaming company is architecting a resilient system. The platform operates on a custom Linux kernel and exclusively supports UDP-based traffic. The company needs the front-end layer to offer the optimal user experience. This layer must ensure low latency, direct traffic to the nearest edge location, and provide static IP addresses for entry into the application endpoints.\n\nWhat should a solutions architect do to fulfill these requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure Amazon CloudFront to direct requests to a Network Load Balancer. Employ AWS Lambda for the application within an AWS Application Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up Amazon API Gateway to route requests to an Application Load Balancer. Utilize Amazon EC2 instances for the application within an EC2 Auto Scaling group.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS Global Accelerator to direct requests to a Network Load Balancer. Deploy Amazon EC2 instances for the application within an EC2 Auto Scaling group.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Set up Amazon Route 53 to route requests to an Application Load Balancer. Utilize AWS Lambda for the application within AWS Application Auto Scaling.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nUse AWS Global Accelerator to direct requests to a Network Load Balancer. Deploy Amazon EC2 instances for the application within an EC2 Auto Scaling group.\n\nAWS Global Accelerator is a networking service that improves the availability and performance of the applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as Network Load Balancers, Application Load Balancers, EC2 Instances, and Elastic IPs. Global Accelerator uses the AWS global network to optimize the path from your users to your applications, improving the performance of your TCP and UDP traffic. The EC2 Auto Scaling group ensures that you have a consistent number of EC2 instances running for your application, which aids in providing a reliable, scalable, and cost-efficient computing capacity.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up Amazon Route 53 to route requests to an Application Load Balancer. Utilize AWS Lambda for the application within AWS Application Auto Scaling.\n\nRoute 53 and Application Load Balancer do not support UDP traffic. Also, AWS Lambda is not suitable for applications that require persistent network connections.\n\n\n\n\nConfigure Amazon CloudFront to direct requests to a Network Load Balancer. Employ AWS Lambda for the application within an AWS Application Auto Scaling group.\n\nCloudFront does not support UDP traffic. AWS Lambda is not suitable for applications that require persistent network connections.\n\n\n\n\nSet up Amazon API Gateway to route requests to an Application Load Balancer. Utilize Amazon EC2 instances for the application within an EC2 Auto Scaling group.\n\nAmazon API Gateway and Application Load Balancer do not support UDP traffic.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Global Accelerator to direct requests to a Network Load Balancer. Deploy Amazon EC2 instances for the application within an EC2 Auto Scaling group.",
        "explanation": "AWS Global Accelerator is a networking service that improves the availability and performance of the applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as Network Load Balancers, Application Load Balancers, EC2 Instances, and Elastic IPs. Global Accelerator uses the AWS global network to optimize the path from your users to your applications, improving the performance of your TCP and UDP traffic. The EC2 Auto Scaling group ensures that you have a consistent number of EC2 instances running for your application, which aids in providing a reliable, scalable, and cost-efficient computing capacity."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up Amazon Route 53 to route requests to an Application Load Balancer. Utilize AWS Lambda for the application within AWS Application Auto Scaling.",
        "explanation": "Route 53 and Application Load Balancer do not support UDP traffic. Also, AWS Lambda is not suitable for applications that require persistent network connections."
      },
      {
        "answer": "Configure Amazon CloudFront to direct requests to a Network Load Balancer. Employ AWS Lambda for the application within an AWS Application Auto Scaling group.",
        "explanation": "CloudFront does not support UDP traffic. AWS Lambda is not suitable for applications that require persistent network connections."
      },
      {
        "answer": "Set up Amazon API Gateway to route requests to an Application Load Balancer. Utilize Amazon EC2 instances for the application within an EC2 Auto Scaling group.",
        "explanation": "Amazon API Gateway and Application Load Balancer do not support UDP traffic."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html"
    ]
  },
  {
    "id": 4,
    "question": "A financial firm is planning to upgrade its traditional transaction processing application that currently operates on Amazon EC2 instances. The application processes transactions in order, however, the sequence of results is irrelevant. The application is designed on a monolithic architecture, and scaling is possible only by increasing the instance size.\n\nThe firm's engineers plan to redevelop the application using a microservices architecture on Amazon Elastic Kubernetes Service (EKS).\n\nWhat would a solutions architect advise for communication between the microservices?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up an Amazon Simple Queue Service (SQS) queue. Include code in the transaction originators to send data to the queue. Include code in the transaction handlers to process data from the queue.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use an Amazon Simple Notification Service (SNS) topic. Include code in the transaction originators to publish notifications to the topic. Include code in the transaction handlers to subscribe to the topic.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an AWS Lambda function to transmit messages. Include code in the transaction originators to invoke the Lambda function with a data object. Include code in the transaction handlers to receive a data object passed from the Lambda function.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon DynamoDB table. Activate DynamoDB Streams. Include code in the transaction originators to insert data into the table. Include code in the transaction handlers to utilize the DynamoDB Streams API to recognize new table entries and fetch the data.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nSet up an Amazon Simple Queue Service (SQS) queue. Include code in the transaction originators to send data to the queue. Include code in the transaction handlers to process data from the queue.\n\nAmazon SQS is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse an Amazon Simple Notification Service (SNS) topic. Include code in the transaction originators to publish notifications to the topic. Include code in the transaction handlers to subscribe to the topic.\n\nAmazon SNS is used for pub/sub messaging and mobile notifications. It's not an ideal solution for communication between microservices in this context, as it's more suited for fan-out architecture where multiple subscribers need to receive the same message.\n\n\n\n\nCreate an AWS Lambda function to transmit messages. Include code in the transaction originators to invoke the Lambda function with a data object. Include code in the transaction handlers to receive a data object passed from the Lambda function.\n\nAWS Lambda is a compute service that lets you run code without provisioning or managing servers. However, using Lambda for message passing between microservices could add unnecessary complexity and could also lead to issues with cold starts and execution time limits.\n\n\n\n\nCreate an Amazon DynamoDB table. Activate DynamoDB Streams. Include code in the transaction originators to insert data into the table. Include code in the transaction handlers to utilize the DynamoDB Streams API to recognize new table entries and fetch the data.\n\nWhile it is possible to use DynamoDB and DynamoDB Streams for communication between microservices, it is not the most suitable or efficient method. DynamoDB is a NoSQL database service for any scale, but using it primarily for inter-service communication might add unnecessary cost and complexity.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html",
    "correctAnswerExplanations": [
      {
        "answer": "Set up an Amazon Simple Queue Service (SQS) queue. Include code in the transaction originators to send data to the queue. Include code in the transaction handlers to process data from the queue.",
        "explanation": "Amazon SQS is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use an Amazon Simple Notification Service (SNS) topic. Include code in the transaction originators to publish notifications to the topic. Include code in the transaction handlers to subscribe to the topic.",
        "explanation": "Amazon SNS is used for pub/sub messaging and mobile notifications. It's not an ideal solution for communication between microservices in this context, as it's more suited for fan-out architecture where multiple subscribers need to receive the same message."
      },
      {
        "answer": "Create an AWS Lambda function to transmit messages. Include code in the transaction originators to invoke the Lambda function with a data object. Include code in the transaction handlers to receive a data object passed from the Lambda function.",
        "explanation": "AWS Lambda is a compute service that lets you run code without provisioning or managing servers. However, using Lambda for message passing between microservices could add unnecessary complexity and could also lead to issues with cold starts and execution time limits."
      },
      {
        "answer": "Create an Amazon DynamoDB table. Activate DynamoDB Streams. Include code in the transaction originators to insert data into the table. Include code in the transaction handlers to utilize the DynamoDB Streams API to recognize new table entries and fetch the data.",
        "explanation": "While it is possible to use DynamoDB and DynamoDB Streams for communication between microservices, it is not the most suitable or efficient method. DynamoDB is a NoSQL database service for any scale, but using it primarily for inter-service communication might add unnecessary cost and complexity."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html"
    ]
  },
  {
    "id": 5,
    "question": "A software company is deploying an application using Amazon Elastic Kubernetes Service (EKS). This application analyzes video files and then uses the Amazon S3 API to store the analysis results in Amazon S3.\n\nWhat can a solutions architect do to ensure the application has the necessary permissions to access Amazon S3?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Update the Amazon EKS role in AWS IAM to allow read/write access from Amazon EKS, and then redeploy the Kubernetes pod.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Develop a security group that permits access from Amazon EKS to Amazon S3, and modify the launch configuration used by the EKS worker nodes.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Generate an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the EKS worker nodes while logged in as this user.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an IAM role with S3 permissions, and then specify that role in the Kubernetes service account used by the pod.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate an IAM role with S3 permissions, and then specify that role in the Kubernetes service account used by the pod.\n\nBy creating an IAM role with the appropriate S3 permissions, the solutions architect can define granular access controls for the application's interaction with Amazon S3. This ensures that the application can read/write the analysis results in Amazon S3 securely. In Kubernetes, the IAM role can be associated with a Kubernetes service account. By specifying the created IAM role in the service account, the EKS cluster grants the necessary permissions to the pods associated with that service account. This enables the pods to access Amazon S3 using the S3 API.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUpdate the Amazon EKS role in AWS IAM to allow read/write access from Amazon EKS, and then redeploy the Kubernetes pod.\n\nUpdating the EKS role would not provide the necessary S3 permissions to the application running in the pods.\n\n\n\n\nDevelop a security group that permits access from Amazon EKS to Amazon S3, and modify the launch configuration used by the EKS worker nodes.\n\nThis is not the right approach as security groups can't be used to grant access to Amazon S3; they are used to control inbound and outbound traffic at the instance level.\n\n\n\n\nGenerate an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the EKS worker nodes while logged in as this user.\n\nIAM users are not meant to provide permissions to applications running on EC2 instances or EKS pods.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create an IAM role with S3 permissions, and then specify that role in the Kubernetes service account used by the pod.",
        "explanation": "By creating an IAM role with the appropriate S3 permissions, the solutions architect can define granular access controls for the application's interaction with Amazon S3. This ensures that the application can read/write the analysis results in Amazon S3 securely. In Kubernetes, the IAM role can be associated with a Kubernetes service account. By specifying the created IAM role in the service account, the EKS cluster grants the necessary permissions to the pods associated with that service account. This enables the pods to access Amazon S3 using the S3 API."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Update the Amazon EKS role in AWS IAM to allow read/write access from Amazon EKS, and then redeploy the Kubernetes pod.",
        "explanation": "Updating the EKS role would not provide the necessary S3 permissions to the application running in the pods."
      },
      {
        "answer": "Develop a security group that permits access from Amazon EKS to Amazon S3, and modify the launch configuration used by the EKS worker nodes.",
        "explanation": "This is not the right approach as security groups can't be used to grant access to Amazon S3; they are used to control inbound and outbound traffic at the instance level."
      },
      {
        "answer": "Generate an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the EKS worker nodes while logged in as this user.",
        "explanation": "IAM users are not meant to provide permissions to applications running on EC2 instances or EKS pods."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html"
    ]
  },
  {
    "id": 6,
    "question": "A software firm plans to move its Linux-based application to AWS. The application requires a shared Linux file system accessible by multiple Amazon EC2 Linux instances across multiple Availability Zones.\n\nWhat should a solutions architect do to meet this requirement?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon Elastic Block Store (Amazon EBS) volume with the necessary size. Connect each EC2 instance to the volume. Mount the file system within the volume to each Linux instance.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure Amazon FSx for Windows File Server. Connect the Amazon FSx file system to each Linux instance.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure AWS Storage Gateway in volume gateway mode. Mount the volume to each Linux instance.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure a file system using Amazon Elastic File System (Amazon EFS). Mount the EFS file system to each Linux instance.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nConfigure a file system using Amazon Elastic File System (Amazon EFS). Mount the EFS file system to each Linux instance.\n\nAmazon EFS provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files. Hence, it meets the requirement of a shared Linux file system across multiple instances and Availability Zones.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure Amazon FSx for Windows File Server. Connect the Amazon FSx file system to each Linux instance.\n\nAmazon FSx for Windows File Server provides a Windows file system and is not designed to work with Linux-based applications.\n\n\n\n\nConfigure AWS Storage Gateway in volume gateway mode. Mount the volume to each Linux instance.\n\nAWS Storage Gateway's volume gateway mode provides block storage backed by Amazon S3. It does not provide a shared file system required by the application.\n\n\n\n\nCreate an Amazon Elastic Block Store (Amazon EBS) volume with the necessary size. Connect each EC2 instance to the volume. Mount the file system within the volume to each Linux instance.\n\nAmazon EBS provides block-level storage volumes for use with Amazon EC2 instances. It does not provide a shared file system that can be simultaneously attached to multiple instances.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/efs/latest/ug/mounting-fs.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure a file system using Amazon Elastic File System (Amazon EFS). Mount the EFS file system to each Linux instance.",
        "explanation": "Amazon EFS provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files. Hence, it meets the requirement of a shared Linux file system across multiple instances and Availability Zones."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Configure Amazon FSx for Windows File Server. Connect the Amazon FSx file system to each Linux instance.",
        "explanation": "Amazon FSx for Windows File Server provides a Windows file system and is not designed to work with Linux-based applications."
      },
      {
        "answer": "Configure AWS Storage Gateway in volume gateway mode. Mount the volume to each Linux instance.",
        "explanation": "AWS Storage Gateway's volume gateway mode provides block storage backed by Amazon S3. It does not provide a shared file system required by the application."
      },
      {
        "answer": "Create an Amazon Elastic Block Store (Amazon EBS) volume with the necessary size. Connect each EC2 instance to the volume. Mount the file system within the volume to each Linux instance.",
        "explanation": "Amazon EBS provides block-level storage volumes for use with Amazon EC2 instances. It does not provide a shared file system that can be simultaneously attached to multiple instances."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/efs/latest/ug/mounting-fs.html"
    ]
  },
  {
    "id": 7,
    "question": "A video streaming company has a large library of short video clips stored in Amazon S3 Standard. Each clip is at least 256 KB in size. The company's library has millions of clips, but viewers rarely access clips older than 60 days. The company wants to optimize storage costs while ensuring that the most popular clips are readily available.\n\nWhat is the MOST cost-effective action the company can take to meet these requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use S3 inventory to manage objects and transition them to S3 Standard-Infrequent Access (S3 Standard-IA) after 60 days.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Move the clips to S3 Intelligent-Tiering and configure it to shift objects to a more economical storage tier after 60 days.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure an S3 Lifecycle policy that transfers the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 60 days.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Set the initial storage tier of the objects to S3 Standard-Infrequent Access (S3 Standard-IA).",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nConfigure an S3 Lifecycle policy that transfers the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 60 days.\n\nAn S3 Lifecycle policy provides an automated way to transition objects between different storage classes or expire objects at the end of their lifecycle. This policy can be used to move objects from S3 Standard, which is designed for frequently accessed data, to S3 Standard-IA, which is designed for less frequently accessed data but requires rapid access when needed. It is cost-effective and ideal for the company's use case.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet the initial storage tier of the objects to S3 Standard-Infrequent Access (S3 Standard-IA).\n\nS3 Standard-IA is meant for less frequently accessed data, but the objects in the scenario are accessed frequently for the first 60 days. Hence, this is not an optimal solution.\n\n\n\n\nMove the clips to S3 Intelligent-Tiering and configure it to shift objects to a more economical storage tier after 60 days.\n\nWhile S3 Intelligent-Tiering automatically moves objects between tiers based on access patterns, it introduces additional monitoring and automation fees that could make it less cost-effective for the use case presented.\n\n\n\n\nUse S3 inventory to manage objects and transition them to S3 Standard-Infrequent Access (S3 Standard-IA) after 60 days.\n\nS3 Inventory provides a CSV or ORC file listing of objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or prefix, but it doesn't automatically transition objects between storage classes.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure an S3 Lifecycle policy that transfers the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 60 days.",
        "explanation": "An S3 Lifecycle policy provides an automated way to transition objects between different storage classes or expire objects at the end of their lifecycle. This policy can be used to move objects from S3 Standard, which is designed for frequently accessed data, to S3 Standard-IA, which is designed for less frequently accessed data but requires rapid access when needed. It is cost-effective and ideal for the company's use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set the initial storage tier of the objects to S3 Standard-Infrequent Access (S3 Standard-IA).",
        "explanation": "S3 Standard-IA is meant for less frequently accessed data, but the objects in the scenario are accessed frequently for the first 60 days. Hence, this is not an optimal solution."
      },
      {
        "answer": "Move the clips to S3 Intelligent-Tiering and configure it to shift objects to a more economical storage tier after 60 days.",
        "explanation": "While S3 Intelligent-Tiering automatically moves objects between tiers based on access patterns, it introduces additional monitoring and automation fees that could make it less cost-effective for the use case presented."
      },
      {
        "answer": "Use S3 inventory to manage objects and transition them to S3 Standard-Infrequent Access (S3 Standard-IA) after 60 days.",
        "explanation": "S3 Inventory provides a CSV or ORC file listing of objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or prefix, but it doesn't automatically transition objects between storage classes."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html"
    ]
  },
  {
    "id": 8,
    "question": "A university has a vast collection of historical handwritten manuscripts. The university plans to digitize the entire collection and add hundreds of new manuscripts every day. The university's digitization team will scan these documents and upload them to the AWS Cloud.\n\nA solutions architect is tasked with implementing a solution to interpret the manuscripts, extract key academic information, and store them in a way that allows SQL queries to be run on the data. The solution needs to prioritize scalability and operational efficiency.\n\nWhich combination of steps should the solutions architect take to fulfill these requirements? (Select TWO.)",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an Auto Scaling group of Amazon EC2 instances to run a custom application that processes the scanned files and extracts the academic information.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up an AWS Lambda function that triggers when new documents are uploaded. Use Amazon Textract to convert the documents to raw text. Use Amazon Comprehend to detect and extract relevant academic information from the text.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Set up an AWS Lambda function that triggers when new documents are uploaded. Use Amazon Rekognition to convert the documents to raw text. Use Amazon Transcribe to detect and extract relevant academic information from the text.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Save the manuscript information to an Amazon S3 bucket and use Amazon Athena to query the data.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Save the manuscript information to an Amazon EC2 instance running a PostgreSQL database.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nSave the manuscript information to an Amazon S3 bucket and use Amazon Athena to query the data.\n\nBy saving the manuscript information to an Amazon S3 bucket, the solution can benefit from the scalable and durable storage provided by S3. Amazon Athena, a serverless query service, can then be used to run SQL queries directly on the data stored in S3. This allows for efficient querying without the need to manage database infrastructure.\n\n\n\n\nSet up an AWS Lambda function that triggers when new documents are uploaded. Use Amazon Textract to convert the documents to raw text. Use Amazon Comprehend to detect and extract relevant academic information from the text.\n\nUsing AWS Lambda, the solution can trigger document processing whenever new manuscripts are uploaded to the S3 bucket. Amazon Textract, an OCR service, can convert the documents to raw text, which can then be passed to Amazon Comprehend for natural language processing to detect and extract relevant academic information. This combination of services enables the interpretation and extraction of key information from the manuscripts.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSave the manuscript information to an Amazon EC2 instance running a PostgreSQL database.\n\nThis option would not be suitable for scaling up as the volume of manuscripts increases over time. The operational efficiency would also be compromised due to the management overhead.\n\n\n\n\nCreate an Auto Scaling group of Amazon EC2 instances to run a custom application that processes the scanned files and extracts the academic information.\n\nThis solution could work, but it would require significant operational overhead in terms of maintaining the EC2 instances and the custom application.\n\n\n\n\nSet up an AWS Lambda function that triggers when new documents are uploaded. Use Amazon Rekognition to convert the documents to raw text. Use Amazon Transcribe to detect and extract relevant academic information from the text.\n\nAmazon Rekognition and Amazon Transcribe are not designed for text extraction from scanned documents, making this option ineffective.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory-athena-query.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html\n\nhttps://docs.aws.amazon.com/textract/latest/dg/what-is.html\n\nhttps://docs.aws.amazon.com/comprehend/latest/dg/what-is.html",
    "correctAnswerExplanations": [
      {
        "answer": "Save the manuscript information to an Amazon S3 bucket and use Amazon Athena to query the data.",
        "explanation": "By saving the manuscript information to an Amazon S3 bucket, the solution can benefit from the scalable and durable storage provided by S3. Amazon Athena, a serverless query service, can then be used to run SQL queries directly on the data stored in S3. This allows for efficient querying without the need to manage database infrastructure."
      },
      {
        "answer": "Set up an AWS Lambda function that triggers when new documents are uploaded. Use Amazon Textract to convert the documents to raw text. Use Amazon Comprehend to detect and extract relevant academic information from the text.",
        "explanation": "Using AWS Lambda, the solution can trigger document processing whenever new manuscripts are uploaded to the S3 bucket. Amazon Textract, an OCR service, can convert the documents to raw text, which can then be passed to Amazon Comprehend for natural language processing to detect and extract relevant academic information. This combination of services enables the interpretation and extraction of key information from the manuscripts."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Save the manuscript information to an Amazon EC2 instance running a PostgreSQL database.",
        "explanation": "This option would not be suitable for scaling up as the volume of manuscripts increases over time. The operational efficiency would also be compromised due to the management overhead."
      },
      {
        "answer": "Create an Auto Scaling group of Amazon EC2 instances to run a custom application that processes the scanned files and extracts the academic information.",
        "explanation": "This solution could work, but it would require significant operational overhead in terms of maintaining the EC2 instances and the custom application."
      },
      {
        "answer": "Set up an AWS Lambda function that triggers when new documents are uploaded. Use Amazon Rekognition to convert the documents to raw text. Use Amazon Transcribe to detect and extract relevant academic information from the text.",
        "explanation": "Amazon Rekognition and Amazon Transcribe are not designed for text extraction from scanned documents, making this option ineffective."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory-athena-query.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html",
      "https://docs.aws.amazon.com/textract/latest/dg/what-is.html",
      "https://docs.aws.amazon.com/comprehend/latest/dg/what-is.html"
    ]
  },
  {
    "id": 9,
    "question": "A solutions architect is designing the architecture for a new, highly interactive web application that will be hosted on AWS. The application will run on Amazon EC2 On-Demand Instances, which will scale up and down in response to load, distributed across multiple Availability Zones. The architecture needs to handle real-time user data such as user preferences and activities, which should be consistent across different instances. A Network Load Balancer will manage the traffic. The company is open to making necessary code modifications.\n\nWhat should the solutions architect recommend to ensure user data consistency across instances?",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage user data.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon ElastiCache to store and manage user data.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Session Manager from AWS Systems Manager to manage user data.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use sticky sessions feature of the Network Load Balancer to manage user data.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nUse Amazon ElastiCache to store and manage user data.\n\nAmazon ElastiCache provides a high-performance, in-memory data store and cache, which is ideal for managing real-time user data across multiple instances. It helps in maintaining data consistency across different instances, enabling seamless user experience even when instances scale up and down.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse sticky sessions feature of the Network Load Balancer to manage user data.\n\nSticky sessions tie a user's session to a specific instance, which is not suitable for an environment with frequent scaling. Also, Network Load Balancer does not support sticky sessions.\n\n\n\n\nUse Session Manager from AWS Systems Manager to manage user data.\n\nAWS Session Manager is primarily used for secure access and management of EC2 instances. It doesn't handle real-time user data storage or management.\n\n\n\n\nUse the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage user data.\n\nThe GetSessionToken API operation in AWS STS is used for providing temporary security credentials, not for managing real-time user data.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/elasticache/",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon ElastiCache to store and manage user data.",
        "explanation": "Amazon ElastiCache provides a high-performance, in-memory data store and cache, which is ideal for managing real-time user data across multiple instances. It helps in maintaining data consistency across different instances, enabling seamless user experience even when instances scale up and down."
      },
      {
        "answer": "Use Session Manager from AWS Systems Manager to manage user data.",
        "explanation": "AWS Session Manager is primarily used for secure access and management of EC2 instances. It doesn't handle real-time user data storage or management."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use sticky sessions feature of the Network Load Balancer to manage user data.",
        "explanation": "Sticky sessions tie a user's session to a specific instance, which is not suitable for an environment with frequent scaling. Also, Network Load Balancer does not support sticky sessions."
      },
      {
        "answer": "Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage user data.",
        "explanation": "The GetSessionToken API operation in AWS STS is used for providing temporary security credentials, not for managing real-time user data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticache/"
    ]
  },
  {
    "id": 10,
    "question": "A company is developing an IoT application. The IoT devices send telemetry data every minute, and a data processing application receives this data for analysis. The company is looking for an AWS service to manage data transmission between the devices and the application. The IoT devices can send about 500 messages per minute. The data processing might take up to 48 hours, and if any messages fail to process, they should be retained to avoid affecting the processing of the remaining data.\n\nWhich solution is MOST suitable and operationally efficient for these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Integrate the IoT devices and data processing application with an Amazon Simple Queue Service (Amazon SQS) queue. Set up a dead-letter queue to collect the messages that failed to process.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use an Amazon Kinesis data stream to receive the messages from the IoT devices. Integrate the data processing application with the Kinesis Client Library (KCL).",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Subscribe the data processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications for processing. Configure the IoT devices to write to the SNS topic.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up an Amazon EC2 instance running a MongoDB database. Configure both IoT devices and the application to use the instance. Store, process, and delete the messages, respectively.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nIntegrate the IoT devices and data processing application with an Amazon Simple Queue Service (Amazon SQS) queue. Set up a dead-letter queue to collect the messages that failed to process.\n\nAmazon SQS is a fully managed message queuing service that enables decoupling and scaling microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. The dead-letter queue feature in SQS allows storing the messages that failed to process. This ensures that failed messages do not affect the processing of subsequent messages, making SQS the most suitable solution for the given scenario.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse an Amazon Kinesis data stream to receive the messages from the IoT devices. Integrate the data processing application with the Kinesis Client Library (KCL).\n\nThis is not an optimal solution as Kinesis is generally used for real-time data streaming and processing, and doesn't inherently provide the capability to retain failed messages for reprocessing.\n\n\n\n\nSet up an Amazon EC2 instance running a MongoDB database. Configure both IoT devices and the application to use the instance. Store, process, and delete the messages, respectively.\n\nRunning a database on an EC2 instance introduces additional operational overhead and is not a recommended approach for message queuing scenarios.\n\n\n\n\nSubscribe the data processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications for processing. Configure the IoT devices to write to the SNS topic.\n\nAmazon SNS is more suitable for pub-sub messaging and mobile notifications. It does not inherently provide a mechanism for retaining and reprocessing failed messages.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html",
    "correctAnswerExplanations": [
      {
        "answer": "Integrate the IoT devices and data processing application with an Amazon Simple Queue Service (Amazon SQS) queue. Set up a dead-letter queue to collect the messages that failed to process.",
        "explanation": "Amazon SQS is a fully managed message queuing service that enables decoupling and scaling microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. The dead-letter queue feature in SQS allows storing the messages that failed to process. This ensures that failed messages do not affect the processing of subsequent messages, making SQS the most suitable solution for the given scenario."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use an Amazon Kinesis data stream to receive the messages from the IoT devices. Integrate the data processing application with the Kinesis Client Library (KCL).",
        "explanation": "This is not an optimal solution as Kinesis is generally used for real-time data streaming and processing, and doesn't inherently provide the capability to retain failed messages for reprocessing."
      },
      {
        "answer": "Set up an Amazon EC2 instance running a MongoDB database. Configure both IoT devices and the application to use the instance. Store, process, and delete the messages, respectively.",
        "explanation": "Running a database on an EC2 instance introduces additional operational overhead and is not a recommended approach for message queuing scenarios."
      },
      {
        "answer": "Subscribe the data processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications for processing. Configure the IoT devices to write to the SNS topic.",
        "explanation": "Amazon SNS is more suitable for pub-sub messaging and mobile notifications. It does not inherently provide a mechanism for retaining and reprocessing failed messages."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
    ]
  },
  {
    "id": 11,
    "question": "A media company has a lightweight JavaScript application that processes XML files and writes the results to an on-site NoSQL database. The application executes hundreds of times daily. The company intends to migrate the application to the AWS Cloud, aiming for a solution that emphasizes scalability and operational efficiency while ensuring high availability.\n\nWhich solution would meet these requirements?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Store the XML documents as messages in an Amazon Simple Queue Service (SQS) queue. Deploy the JavaScript code as a container on an Amazon Elastic Container Service (ECS) cluster configured with the Amazon EC2 launch type. Use the container to process the SQS messages. Store the results in an Amazon RDS instance.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Store the XML files in an Amazon S3 bucket. Create an AWS Lambda function to run the JavaScript code to process the documents as they are added to the S3 bucket. Store the results in an Amazon DynamoDB table.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Store the XML files in an Amazon Elastic Block Store (EBS) volume. Use the EBS Multi-Attach feature to link the volume to multiple Amazon EC2 instances. Run the JavaScript code on the EC2 instances to process the documents. Store the results in an Amazon RDS instance.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Store the XML files in an Amazon S3 bucket. Execute the JavaScript code on multiple Amazon EC2 instances to process the documents. Store the results in an Amazon DynamoDB table.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nStore the XML files in an Amazon S3 bucket. Create an AWS Lambda function to run the JavaScript code to process the documents as they are added to the S3 bucket. Store the results in an Amazon DynamoDB table.\n\nThis solution fulfills all the requirements. AWS Lambda, a serverless compute service, allows the company to run code without provisioning or managing servers. It automatically scales applications by running code in response to triggers such as changes to data in an Amazon S3 bucket. It also pairs well with DynamoDB, a managed NoSQL database service, which provides fast and predictable performance with seamless scalability, fulfilling the high availability and scalability requirements.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStore the XML files in an Amazon S3 bucket. Execute the JavaScript code on multiple Amazon EC2 instances to process the documents. Store the results in an Amazon DynamoDB table.\n\nWhile this option is technically feasible, it fails to minimize operational overhead as the company needs to manage the EC2 instances.\n\n\n\n\nStore the XML files in an Amazon Elastic Block Store (EBS) volume. Use the EBS Multi-Attach feature to link the volume to multiple Amazon EC2 instances. Run the JavaScript code on the EC2 instances to process the documents. Store the results in an Amazon RDS instance.\n\nThis option does not offer the best solution. EBS Multi-Attach is not designed for this type of use case, and using Amazon RDS, a relational database service, for storing the results does not align with the original NoSQL database requirement.\n\n\n\n\nStore the XML documents as messages in an Amazon Simple Queue Service (SQS) queue. Deploy the JavaScript code as a container on an Amazon Elastic Container Service (ECS) cluster configured with the Amazon EC2 launch type. Use the container to process the SQS messages. Store the results in an Amazon RDS instance.\n\nThough SQS and ECS are robust services, this option introduces unnecessary complexity and does not minimize operational overhead. Additionally, RDS is not the best fit for the company's NoSQL requirement.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3.html\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html",
    "correctAnswerExplanations": [
      {
        "answer": "Store the XML files in an Amazon S3 bucket. Create an AWS Lambda function to run the JavaScript code to process the documents as they are added to the S3 bucket. Store the results in an Amazon DynamoDB table.",
        "explanation": "This solution fulfills all the requirements. AWS Lambda, a serverless compute service, allows the company to run code without provisioning or managing servers. It automatically scales applications by running code in response to triggers such as changes to data in an Amazon S3 bucket. It also pairs well with DynamoDB, a managed NoSQL database service, which provides fast and predictable performance with seamless scalability, fulfilling the high availability and scalability requirements."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Store the XML files in an Amazon S3 bucket. Execute the JavaScript code on multiple Amazon EC2 instances to process the documents. Store the results in an Amazon DynamoDB table.",
        "explanation": "While this option is technically feasible, it fails to minimize operational overhead as the company needs to manage the EC2 instances."
      },
      {
        "answer": "Store the XML files in an Amazon Elastic Block Store (EBS) volume. Use the EBS Multi-Attach feature to link the volume to multiple Amazon EC2 instances. Run the JavaScript code on the EC2 instances to process the documents. Store the results in an Amazon RDS instance.",
        "explanation": "This option does not offer the best solution. EBS Multi-Attach is not designed for this type of use case, and using Amazon RDS, a relational database service, for storing the results does not align with the original NoSQL database requirement."
      },
      {
        "answer": "Store the XML documents as messages in an Amazon Simple Queue Service (SQS) queue. Deploy the JavaScript code as a container on an Amazon Elastic Container Service (ECS) cluster configured with the Amazon EC2 launch type. Use the container to process the SQS messages. Store the results in an Amazon RDS instance.",
        "explanation": "Though SQS and ECS are robust services, this option introduces unnecessary complexity and does not minimize operational overhead. Additionally, RDS is not the best fit for the company's NoSQL requirement."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html"
    ]
  },
  {
    "id": 12,
    "question": "A financial services company runs a transaction-processing application that utilizes Amazon API Gateway and an AWS Lambda function. This application stores data in an Amazon Aurora MySQL database. During peak trading hours, an abrupt increase in user transactions led to some users experiencing timeouts, and the application failed to process those transactions.\n\nA solutions architect discovered that the CPU and memory usage were high on the database due to an excess number of open connections. The solutions architect needs to mitigate the timeout issues while minimally modifying the application.\n\nWhich solution would fulfill these requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Transfer the data from Aurora MySQL to Amazon DynamoDB using AWS Database Migration Service (AWS DMS). Alter the Lambda function to use the DynamoDB table.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a read replica for the database in a different AWS Region. Use query string parameters in API Gateway to direct traffic to the read replica.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up provisioned concurrency for the Lambda function. Modify the database to be a global database across multiple AWS Regions.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon RDS Proxy to establish a proxy for the database. Adjust the Lambda function to use the RDS Proxy endpoint instead of the database endpoint.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nUse Amazon RDS Proxy to establish a proxy for the database. Adjust the Lambda function to use the RDS Proxy endpoint instead of the database endpoint.\n\nAmazon RDS Proxy allows applications to pool and share database connections, improving the ability to manage a surge in connections and enhancing database efficiency. By changing the Lambda function to use the RDS Proxy endpoint, the application can handle a greater number of connections without increasing CPU and memory usage.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up provisioned concurrency for the Lambda function. Modify the database to be a global database across multiple AWS Regions.\n\nWhile provisioned concurrency can help Lambda functions scale, the problem here is database connections, not Lambda scaling. Making the database global would not directly address the issue of too many open connections.\n\n\n\n\nCreate a read replica for the database in a different AWS Region. Use query string parameters in API Gateway to direct traffic to the read replica.\n\nRead replicas are used for read-heavy database workloads. Since the problem is about processing transactions (which involve writes), a read replica wouldn't solve the issue.\n\n\n\n\nTransfer the data from Aurora MySQL to Amazon DynamoDB using AWS Database Migration Service (AWS DMS). Alter the Lambda function to use the DynamoDB table.\n\nWhile DynamoDB can scale to handle high request rates, migrating the entire database is a significant change and may not be feasible or desirable depending on the application's requirements.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/proxy/\n\nhttps://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon RDS Proxy to establish a proxy for the database. Adjust the Lambda function to use the RDS Proxy endpoint instead of the database endpoint.",
        "explanation": "Amazon RDS Proxy allows applications to pool and share database connections, improving the ability to manage a surge in connections and enhancing database efficiency. By changing the Lambda function to use the RDS Proxy endpoint, the application can handle a greater number of connections without increasing CPU and memory usage."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up provisioned concurrency for the Lambda function. Modify the database to be a global database across multiple AWS Regions.",
        "explanation": "While provisioned concurrency can help Lambda functions scale, the problem here is database connections, not Lambda scaling. Making the database global would not directly address the issue of too many open connections."
      },
      {
        "answer": "Create a read replica for the database in a different AWS Region. Use query string parameters in API Gateway to direct traffic to the read replica.",
        "explanation": "Read replicas are used for read-heavy database workloads. Since the problem is about processing transactions (which involve writes), a read replica wouldn't solve the issue."
      },
      {
        "answer": "Transfer the data from Aurora MySQL to Amazon DynamoDB using AWS Database Migration Service (AWS DMS). Alter the Lambda function to use the DynamoDB table.",
        "explanation": "While DynamoDB can scale to handle high request rates, migrating the entire database is a significant change and may not be feasible or desirable depending on the application's requirements."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/proxy/",
      "https://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/"
    ]
  },
  {
    "id": 13,
    "question": "A software development company aims to migrate its existing monolithic application from an on-premises environment to AWS. While the company intends to retain much of its front-end and back-end code, they seek to decompose the application into several smaller services, with each service managed by a different team. The company requires a highly scalable solution that reduces operational complexity. Which solution will fulfill these requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Deploy the application on AWS Lambda. Interface the application with Amazon API Gateway.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Run the application using AWS Amplify. Link the application to an Amazon API Gateway API that integrates with AWS Lambda.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Run the application on Amazon Elastic Kubernetes Service (Amazon EKS). Set up an Application Load Balancer with Amazon EKS as the target.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Host the application on Amazon EC2 instances. Establish an Application Load Balancer with EC2 instances in an Auto Scaling group as targets.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nRun the application on Amazon Elastic Kubernetes Service (Amazon EKS). Set up an Application Load Balancer with Amazon EKS as the target.\n\nAmazon EKS provides a managed Kubernetes service that allows you to run containerized applications without the need to install, operate, and maintain your own Kubernetes control plane or worker nodes. This makes it suitable for decomposing a monolithic application into microservices, as it simplifies container management, scales easily, and reduces operational overhead. The use of an Application Load Balancer helps distribute incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses, in multiple Availability Zones.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDeploy the application on AWS Lambda. Interface the application with Amazon API Gateway.\n\nWhile Lambda and API Gateway are great for serverless architectures, they may not be the best fit for migrating a monolithic application without significant refactoring of the application code.\n\n\n\n\nRun the application using AWS Amplify. Link the application to an Amazon API Gateway API that integrates with AWS Lambda.\n\nAWS Amplify is ideal for building scalable mobile and web applications, but migrating a monolithic application to this architecture would require significant code changes.\n\n\n\n\nHost the application on Amazon EC2 instances. Establish an Application Load Balancer with EC2 instances in an Auto Scaling group as targets.\n\nThis option may work but doesn't provide as much scalability and operational simplicity as Amazon EKS. Hosting on EC2 instances doesn't inherently support the microservices architecture the company is aiming for.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html",
    "correctAnswerExplanations": [
      {
        "answer": "Run the application on Amazon Elastic Kubernetes Service (Amazon EKS). Set up an Application Load Balancer with Amazon EKS as the target.",
        "explanation": "Amazon EKS provides a managed Kubernetes service that allows you to run containerized applications without the need to install, operate, and maintain your own Kubernetes control plane or worker nodes. This makes it suitable for decomposing a monolithic application into microservices, as it simplifies container management, scales easily, and reduces operational overhead. The use of an Application Load Balancer helps distribute incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses, in multiple Availability Zones."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Deploy the application on AWS Lambda. Interface the application with Amazon API Gateway.",
        "explanation": "While Lambda and API Gateway are great for serverless architectures, they may not be the best fit for migrating a monolithic application without significant refactoring of the application code."
      },
      {
        "answer": "Run the application using AWS Amplify. Link the application to an Amazon API Gateway API that integrates with AWS Lambda.",
        "explanation": "AWS Amplify is ideal for building scalable mobile and web applications, but migrating a monolithic application to this architecture would require significant code changes."
      },
      {
        "answer": "Host the application on Amazon EC2 instances. Establish an Application Load Balancer with EC2 instances in an Auto Scaling group as targets.",
        "explanation": "This option may work but doesn't provide as much scalability and operational simplicity as Amazon EKS. Hosting on EC2 instances doesn't inherently support the microservices architecture the company is aiming for."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html"
    ]
  },
  {
    "id": 14,
    "question": "A media company hosts a video streaming application on a single Amazon EC2 On-Demand Instance. The streaming software is written in Node.js and uses a PostgreSQL database. The streaming software, the web server that supports Node.js, and the database server are all hosted on the EC2 instance. During peak usage times, the application's performance deteriorates and 5xx errors are reported. The company requires a solution that allows the application to scale smoothly.\n\nWhich option would address these requirements most cost-effectively?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Transfer the database to an Amazon Aurora PostgreSQL DB instance. Generate an AMI of the streaming application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template. Configure the launch template to use a Spot Fleet. Connect an Application Load Balancer to the Auto Scaling group.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Move the database to an Amazon RDS for PostgreSQL DB instance. Generate an AMI of the streaming application. Use this AMI to initiate a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Move the database to an Amazon Aurora PostgreSQL DB instance. Develop an AWS Lambda function to stop the EC2 instance and alter the instance type. Configure an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization exceeds 75%.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Transfer the database to an Amazon RDS for PostgreSQL DB instance. Generate an AMI of the streaming application. Use this AMI to initiate a second EC2 On-Demand Instance. Employ an Application Load Balancer to distribute the load among the EC2 instances.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nTransfer the database to an Amazon Aurora PostgreSQL DB instance. Generate an AMI of the streaming application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template. Configure the launch template to use a Spot Fleet. Connect an Application Load Balancer to the Auto Scaling group.\n\nAmazon Aurora provides higher performance than traditional RDS and separating the database from the application server allows for better resource management. Using an Auto Scaling group with a Spot Fleet optimizes costs while managing the scaling of the EC2 instances. Application Load Balancer efficiently distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, in multiple Availability Zones. Therefore, this option is the most cost-effective and scalable solution.\n\n\n\n\n\n\n\nIncorrect Options:\n\nTransfer the database to an Amazon RDS for PostgreSQL DB instance. Generate an AMI of the streaming application. Use this AMI to initiate a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load among the EC2 instances.\n\nWhile this option provides scalability, it is not as cost-effective as using Spot Instances through a Spot Fleet.\n\n\n\n\nMove the database to an Amazon RDS for PostgreSQL DB instance. Generate an AMI of the streaming application. Use this AMI to initiate a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.\n\nUsing Amazon Route 53 for load balancing is less efficient compared to using an Application Load Balancer or an Auto Scaling group.\n\n\n\n\nMove the database to an Amazon Aurora PostgreSQL DB instance. Develop an AWS Lambda function to stop the EC2 instance and alter the instance type. Configure an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization exceeds 75%.\n\nThis option does not provide a scalable solution and could lead to downtime while the EC2 instance is stopped and its type is changed.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.PostgreSQL.html\n\nhttps://aws.amazon.com/ec2/autoscaling/",
    "correctAnswerExplanations": [
      {
        "answer": "Transfer the database to an Amazon Aurora PostgreSQL DB instance. Generate an AMI of the streaming application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template. Configure the launch template to use a Spot Fleet. Connect an Application Load Balancer to the Auto Scaling group.",
        "explanation": "Amazon Aurora provides higher performance than traditional RDS and separating the database from the application server allows for better resource management. Using an Auto Scaling group with a Spot Fleet optimizes costs while managing the scaling of the EC2 instances. Application Load Balancer efficiently distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, in multiple Availability Zones. Therefore, this option is the most cost-effective and scalable solution."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Transfer the database to an Amazon RDS for PostgreSQL DB instance. Generate an AMI of the streaming application. Use this AMI to initiate a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load among the EC2 instances.",
        "explanation": "While this option provides scalability, it is not as cost-effective as using Spot Instances through a Spot Fleet."
      },
      {
        "answer": "Move the database to an Amazon RDS for PostgreSQL DB instance. Generate an AMI of the streaming application. Use this AMI to initiate a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.",
        "explanation": "Using Amazon Route 53 for load balancing is less efficient compared to using an Application Load Balancer or an Auto Scaling group."
      },
      {
        "answer": "Move the database to an Amazon Aurora PostgreSQL DB instance. Develop an AWS Lambda function to stop the EC2 instance and alter the instance type. Configure an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization exceeds 75%.",
        "explanation": "This option does not provide a scalable solution and could lead to downtime while the EC2 instance is stopped and its type is changed."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.PostgreSQL.html",
      "https://aws.amazon.com/ec2/autoscaling/"
    ]
  },
  {
    "id": 15,
    "question": "A tech company runs a critical web application on four backend servers in an Amazon EC2 Auto Scaling group in a single Availability Zone (AZ) behind a Network Load Balancer (NLB). A solutions architect is tasked with redesigning the infrastructure to be highly available without altering the application.\n\nWhich design should the solutions architect select to ensure high availability?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an Auto Scaling template that can be used to swiftly deploy more instances in an alternative Region.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Redesign the Auto Scaling group to distribute two instances across each of two different Regions.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Modify the NLB in front of the EC2 instances to utilize a least connections load balancing algorithm for the backend tier.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Adjust the Auto Scaling group to distribute two instances across each of two different Availability Zones.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nAdjust the Auto Scaling group to distribute two instances across each of two different Availability Zones.\n\nHigh availability is best achieved by distributing resources across multiple Availability Zones within a Region. By modifying the Auto Scaling group to distribute two instances across each of two different Availability Zones, the application will be highly available. If an instance fails in one AZ, the other AZ can handle the application's load, ensuring that the application remains available.\n\n\n\n\n\n\n\nIncorrect Options:\n\nRedesign the Auto Scaling group to distribute two instances across each of two different Regions.\n\nWhile this may add a level of redundancy, it is not the most cost-effective or operationally efficient solution. Managing resources across multiple regions may increase complexity and cost.\n\n\n\n\nCreate an Auto Scaling template that can be used to swiftly deploy more instances in an alternative Region.\n\nWhile having a template for quick deployment is good for disaster recovery scenarios, it does not provide high availability in real-time. It is a reactive approach rather than a proactive one.\n\n\n\n\nModify the NLB in front of the EC2 instances to utilize a least connections load balancing algorithm for the backend tier.\n\nWhile adjusting the load balancing algorithm may optimize distribution of traffic, it does not inherently increase high availability of the application.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html",
    "correctAnswerExplanations": [
      {
        "answer": "Adjust the Auto Scaling group to distribute two instances across each of two different Availability Zones.",
        "explanation": "High availability is best achieved by distributing resources across multiple Availability Zones within a Region. By modifying the Auto Scaling group to distribute two instances across each of two different Availability Zones, the application will be highly available. If an instance fails in one AZ, the other AZ can handle the application's load, ensuring that the application remains available."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Redesign the Auto Scaling group to distribute two instances across each of two different Regions.",
        "explanation": "While this may add a level of redundancy, it is not the most cost-effective or operationally efficient solution. Managing resources across multiple regions may increase complexity and cost."
      },
      {
        "answer": "Create an Auto Scaling template that can be used to swiftly deploy more instances in an alternative Region.",
        "explanation": "While having a template for quick deployment is good for disaster recovery scenarios, it does not provide high availability in real-time. It is a reactive approach rather than a proactive one."
      },
      {
        "answer": "Modify the NLB in front of the EC2 instances to utilize a least connections load balancing algorithm for the backend tier.",
        "explanation": "While adjusting the load balancing algorithm may optimize distribution of traffic, it does not inherently increase high availability of the application."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html"
    ]
  },
  {
    "id": 16,
    "question": "A media broadcasting company is designing its live show transcription service on AWS. The company requires a solution that can handle multiple speaker recognition and generate time-coded transcript files. The company wants to analyze the transcript files to identify popular topics and trends. The transcript files must be stored for 5 years for content analysis purposes.\n\nWhich solution will meet these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Transcribe for multiple speaker recognition. Use Amazon QuickSight for transcript file analysis.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use machine learning models for transcript file analysis.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon Comprehend for transcript file analysis.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nUse Amazon Transcribe for multiple speaker recognition. Use Amazon QuickSight for transcript file analysis.\n\nAmazon Transcribe is the appropriate service for multiple speaker recognition and transcription. Its output can be analyzed using Amazon QuickSight to identify patterns and trends in the conversations, making this the right solution for the company's requirements.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use machine learning models for transcript file analysis.\n\nAmazon Rekognition is primarily used for image and video analysis, not for audio transcription or speaker recognition, making this option incorrect.\n\n\n\n\nUse Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis.\n\nAmazon Translate is used for language translation, not for speaker recognition or transcription. Therefore, this option doesn't meet the company's requirements.\n\n\n\n\nUse Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon Comprehend for transcript file analysis.\n\nAmazon Rekognition is not suitable for multiple speaker recognition or transcription, making this option incorrect.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/transcribe/\n\nhttps://aws.amazon.com/quicksight/",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon Transcribe for multiple speaker recognition. Use Amazon QuickSight for transcript file analysis.",
        "explanation": "Amazon Transcribe is the appropriate service for multiple speaker recognition and transcription. Its output can be analyzed using Amazon QuickSight to identify patterns and trends in the conversations, making this the right solution for the company's requirements."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use machine learning models for transcript file analysis.",
        "explanation": "Amazon Rekognition is primarily used for image and video analysis, not for audio transcription or speaker recognition, making this option incorrect."
      },
      {
        "answer": "Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis.",
        "explanation": "Amazon Translate is used for language translation, not for speaker recognition or transcription. Therefore, this option doesn't meet the company's requirements."
      },
      {
        "answer": "Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon Comprehend for transcript file analysis.",
        "explanation": "Amazon Rekognition is not suitable for multiple speaker recognition or transcription, making this option incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/transcribe/",
      "https://aws.amazon.com/quicksight/"
    ]
  },
  {
    "id": 17,
    "question": "A company runs a Java-based application on a local Linux server, and it stores data using a MySQL database. The company plans to migrate to AWS while minimizing the changes needed for the application. The AWS application environment must offer high availability.\n\nWhich combination of steps should the company follow to meet these requirements? (Select TWO.)",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Database Migration Service (AWS DMS) to migrate from the MySQL database to Amazon DynamoDB in a Multi-AZ deployment.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Rehost the application on AWS Elastic Beanstalk with the Java platform in a Multi-AZ deployment.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Refactor the application into a serverless architecture with AWS Lambda functions running Java.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Replatform the application to run on Amazon EC2 with the Windows Server Amazon Machine Image (AMI).",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Use AWS Database Migration Service (AWS DMS) to migrate from the MySQL database to MySQL on Amazon RDS in a Multi-AZ deployment.",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nRehost the application on AWS Elastic Beanstalk with the Java platform in a Multi-AZ deployment.\n\nAWS Elastic Beanstalk provides an easy way to deploy and manage applications on AWS without requiring significant changes to the application's architecture. By rehosting the application on Elastic Beanstalk with the Java platform in a Multi-AZ deployment, the application benefits from automatic scaling, load balancing, and high availability across multiple availability zones.\n\n\n\n\nUse AWS Database Migration Service (AWS DMS) to migrate from the MySQL database to MySQL on Amazon RDS in a Multi-AZ deployment.\n\nAWS DMS enables seamless and minimal-downtime database migration from on-premises or cloud databases to AWS. By using AWS DMS to migrate the MySQL database to MySQL on Amazon RDS in a Multi-AZ deployment, the company can take advantage of the managed database service provided by Amazon RDS, which offers high availability and automated backups.\n\n\n\n\n\n\n\nIncorrect Options:\n\nRefactor the application into a serverless architecture with AWS Lambda functions running Java.\n\nRefactoring the application would involve significant development changes, which the company wants to avoid.\n\n\n\n\nReplatform the application to run on Amazon EC2 with the Windows Server Amazon Machine Image (AMI).\n\nThe application currently runs on a Linux server, so replatforming to a Windows Server would necessitate significant changes, which the company is trying to avoid.\n\n\n\n\nUse AWS Database Migration Service (AWS DMS) to migrate from the MySQL database to Amazon DynamoDB in a Multi-AZ deployment.\n\nMoving from a relational database (MySQL) to a NoSQL database (DynamoDB) would require significant changes to the application code to adjust the data access patterns, which the company wishes to avoid.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_Java.html\n\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.mysql2rds.html",
    "correctAnswerExplanations": [
      {
        "answer": "Rehost the application on AWS Elastic Beanstalk with the Java platform in a Multi-AZ deployment.",
        "explanation": "AWS Elastic Beanstalk provides an easy way to deploy and manage applications on AWS without requiring significant changes to the application's architecture. By rehosting the application on Elastic Beanstalk with the Java platform in a Multi-AZ deployment, the application benefits from automatic scaling, load balancing, and high availability across multiple availability zones."
      },
      {
        "answer": "Use AWS Database Migration Service (AWS DMS) to migrate from the MySQL database to MySQL on Amazon RDS in a Multi-AZ deployment.",
        "explanation": "AWS DMS enables seamless and minimal-downtime database migration from on-premises or cloud databases to AWS. By using AWS DMS to migrate the MySQL database to MySQL on Amazon RDS in a Multi-AZ deployment, the company can take advantage of the managed database service provided by Amazon RDS, which offers high availability and automated backups."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Refactor the application into a serverless architecture with AWS Lambda functions running Java.",
        "explanation": "Refactoring the application would involve significant development changes, which the company wants to avoid."
      },
      {
        "answer": "Replatform the application to run on Amazon EC2 with the Windows Server Amazon Machine Image (AMI).",
        "explanation": "The application currently runs on a Linux server, so replatforming to a Windows Server would necessitate significant changes, which the company is trying to avoid."
      },
      {
        "answer": "Use AWS Database Migration Service (AWS DMS) to migrate from the MySQL database to Amazon DynamoDB in a Multi-AZ deployment.",
        "explanation": "Moving from a relational database (MySQL) to a NoSQL database (DynamoDB) would require significant changes to the application code to adjust the data access patterns, which the company wishes to avoid."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_Java.html",
      "https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.mysql2rds.html"
    ]
  },
  {
    "id": 18,
    "question": "A legal firm needs to store client case files on Amazon S3. The S3 bucket must allow certain lawyers to upload new files and limit all other users to read-only access. No users should have the ability to alter or remove any files from the bucket. All files must be retained in the bucket for at least 2 years after their upload date.\n\nWhich solution will satisfy these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Apply S3 Object Lock in compliance mode with a retention period of 730 days.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use an IAM role to prohibit all users from deleting or altering objects in the S3 bucket. Apply an S3 bucket policy to exclusively permit the IAM role.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Implement S3 Object Lock in governance mode with a legal hold of 2 years.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure the S3 bucket to activate an AWS Lambda function each time an object is uploaded. Set up the function to track the hash of the stored object so that modified objects can be flagged accordingly.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nApply S3 Object Lock in compliance mode with a retention period of 730 days.\n\nS3 Object Lock in compliance mode is a perfect fit for this scenario. It prevents objects from being deleted or overwritten for a fixed amount of time (in this case, 2 years or 730 days) by anyone, including the root user. This ensures the immutability of objects for the required retention period, fulfilling the firm's requirements.\n\n\n\n\n\n\n\nIncorrect Options:\n\nImplement S3 Object Lock in governance mode with a legal hold of 2 years.\n\nS3 Object Lock in governance mode provides the same protection as compliance mode but allows users with specific IAM permissions to remove the lock. Therefore, it does not guarantee absolute protection against alteration or deletion.\n\n\n\n\nUse an IAM role to prohibit all users from deleting or altering objects in the S3 bucket. Apply an S3 bucket policy to exclusively permit the IAM role.\n\nThis option doesn't provide an absolute guarantee against deletion or alteration of objects because IAM policies and bucket policies can be changed by users with sufficient permissions.\n\n\n\n\nConfigure the S3 bucket to activate an AWS Lambda function each time an object is uploaded. Set up the function to track the hash of the stored object so that modified objects can be flagged accordingly.\n\nThis option doesn't prevent objects from being altered or deleted. It merely flags modified objects, which doesn't meet the requirement of preventing modification or deletion in the first place.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html",
    "correctAnswerExplanations": [
      {
        "answer": "Apply S3 Object Lock in compliance mode with a retention period of 730 days.",
        "explanation": "S3 Object Lock in compliance mode is a perfect fit for this scenario. It prevents objects from being deleted or overwritten for a fixed amount of time (in this case, 2 years or 730 days) by anyone, including the root user. This ensures the immutability of objects for the required retention period, fulfilling the firm's requirements."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Implement S3 Object Lock in governance mode with a legal hold of 2 years.",
        "explanation": "S3 Object Lock in governance mode provides the same protection as compliance mode but allows users with specific IAM permissions to remove the lock. Therefore, it does not guarantee absolute protection against alteration or deletion."
      },
      {
        "answer": "Use an IAM role to prohibit all users from deleting or altering objects in the S3 bucket. Apply an S3 bucket policy to exclusively permit the IAM role.",
        "explanation": "This option doesn't provide an absolute guarantee against deletion or alteration of objects because IAM policies and bucket policies can be changed by users with sufficient permissions."
      },
      {
        "answer": "Configure the S3 bucket to activate an AWS Lambda function each time an object is uploaded. Set up the function to track the hash of the stored object so that modified objects can be flagged accordingly.",
        "explanation": "This option doesn't prevent objects from being altered or deleted. It merely flags modified objects, which doesn't meet the requirement of preventing modification or deletion in the first place."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html"
    ]
  },
  {
    "id": 19,
    "question": "An enterprise's IT department wants to restrict access to certain AWS resources across all its AWS accounts. All these accounts are part of a substantial organization managed by AWS Organizations. The solution needs to be scalable and maintainable from a single control point.\n\nWhich method should a solutions architect employ to achieve this?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create cross-account IAM roles in each account to deny access to specific resources.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Implement a Network Access Control List (NACL) to regulate access to the resources.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a Service Control Policy (SCP) at the root Organizational Unit (OU) to restrict access to the resources.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Set up a security group to manage access and associate it with the necessary IAM roles.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate a Service Control Policy (SCP) at the root Organizational Unit (OU) to restrict access to the resources.\n\nService Control Policies (SCP) are a type of policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines. You can attach SCPs to an OU, which will then apply to all accounts within that OU, providing a scalable, single-point solution.\n\n\n\n\n\n\n\nIncorrect Options:\n\nImplement a Network Access Control List (NACL) to regulate access to the resources.\n\nNACLs are stateless and operate at the subnet level. They are not suitable for controlling access to specific services or actions across multiple AWS accounts.\n\n\n\n\nSet up a security group to manage access and associate it with the necessary IAM roles.\n\nSecurity groups operate at the instance level and are primarily used to control inbound and outbound traffic, not to manage service-level permissions across multiple AWS accounts.\n\n\n\n\nCreate cross-account IAM roles in each account to deny access to specific resources.\n\nThis approach is not scalable as it requires managing permissions individually in each account. It also does not provide a single point for maintaining permissions.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a Service Control Policy (SCP) at the root Organizational Unit (OU) to restrict access to the resources.",
        "explanation": "Service Control Policies (SCP) are a type of policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines. You can attach SCPs to an OU, which will then apply to all accounts within that OU, providing a scalable, single-point solution."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Implement a Network Access Control List (NACL) to regulate access to the resources.",
        "explanation": "NACLs are stateless and operate at the subnet level. They are not suitable for controlling access to specific services or actions across multiple AWS accounts."
      },
      {
        "answer": "Set up a security group to manage access and associate it with the necessary IAM roles.",
        "explanation": "Security groups operate at the instance level and are primarily used to control inbound and outbound traffic, not to manage service-level permissions across multiple AWS accounts."
      },
      {
        "answer": "Create cross-account IAM roles in each account to deny access to specific resources.",
        "explanation": "This approach is not scalable as it requires managing permissions individually in each account. It also does not provide a single point for maintaining permissions."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
    ]
  },
  {
    "id": 20,
    "question": "A financial services company has a currency conversion API that experiences a surge in usage during international financial events. The API performs real-time conversions based on fluctuating exchange rates. A solutions architect needs to design a solution that can handle the high demand during these events while remaining cost-efficient during normal operation.\n\nWhich of the following strategies should the solutions architect adopt?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Develop a REST API using Amazon API Gateway that interfaces with an API hosted on an Amazon EC2 instance. API Gateway receives and forwards the currency types and amounts to the EC2 instance for conversion.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Build a REST API using Amazon API Gateway that takes in currency types and amounts. The API Gateway forwards these requests to AWS Lambda to perform the conversion operations.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Set up an Application Load Balancer with two Amazon EC2 instances behind it. The EC2 instances will perform the conversions based on the received currency types and amounts.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon EC2 instance and host the API there. The EC2 instance will perform the necessary conversions when the API is called.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nBuild a REST API using Amazon API Gateway that takes in currency types and amounts. The API Gateway forwards these requests to AWS Lambda to perform the conversion operations.\n\nAWS Lambda is a serverless computing service that runs your code in response to events and automatically manages the underlying compute resources for you. Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. Together, these two services can handle the sudden increase in traffic during financial events while remaining cost-efficient during normal operation times.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an Amazon EC2 instance and host the API there. The EC2 instance will perform the necessary conversions when the API is called.\n\nThis solution is not the most cost-effective or scalable. EC2 instances would need to be manually scaled to meet demand, which could lead to either over-provisioning or under-provisioning of resources.\n\n\n\n\nSet up an Application Load Balancer with two Amazon EC2 instances behind it. The EC2 instances will perform the conversions based on the received currency types and amounts.\n\nWhile this solution provides some scalability, it does not offer the same level of elasticity and cost-effectiveness as a serverless architecture with API Gateway and Lambda.\n\n\n\n\nDevelop a REST API using Amazon API Gateway that interfaces with an API hosted on an Amazon EC2 instance. API Gateway receives and forwards the currency types and amounts to the EC2 instance for conversion.\n\nThis approach combines the scalability of API Gateway with the less optimal scalability of EC2, which would require manual management to scale up and down based on demand.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html",
    "correctAnswerExplanations": [
      {
        "answer": "Build a REST API using Amazon API Gateway that takes in currency types and amounts. The API Gateway forwards these requests to AWS Lambda to perform the conversion operations.",
        "explanation": "AWS Lambda is a serverless computing service that runs your code in response to events and automatically manages the underlying compute resources for you. Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. Together, these two services can handle the sudden increase in traffic during financial events while remaining cost-efficient during normal operation times."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create an Amazon EC2 instance and host the API there. The EC2 instance will perform the necessary conversions when the API is called.",
        "explanation": "This solution is not the most cost-effective or scalable. EC2 instances would need to be manually scaled to meet demand, which could lead to either over-provisioning or under-provisioning of resources."
      },
      {
        "answer": "Set up an Application Load Balancer with two Amazon EC2 instances behind it. The EC2 instances will perform the conversions based on the received currency types and amounts.",
        "explanation": "While this solution provides some scalability, it does not offer the same level of elasticity and cost-effectiveness as a serverless architecture with API Gateway and Lambda."
      },
      {
        "answer": "Develop a REST API using Amazon API Gateway that interfaces with an API hosted on an Amazon EC2 instance. API Gateway receives and forwards the currency types and amounts to the EC2 instance for conversion.",
        "explanation": "This approach combines the scalability of API Gateway with the less optimal scalability of EC2, which would require manual management to scale up and down based on demand."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html"
    ]
  },
  {
    "id": 21,
    "question": "A retail company accepts online orders from customers via a web application hosted on Amazon EC2 instances. The order details are published to an Amazon Simple Queue Service (Amazon SQS) queue by the web application. Another application running on EC2 instances then processes the orders and sends confirmation emails to the customers. After successful processing, this application stores the order information in an Amazon DynamoDB database.\n\nAs the company grows, customers report that their order confirmation emails are taking longer to arrive.\n\nWhat should a solutions architect suggest to solve this issue?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Add an Auto Scaling group for the application that processes orders and sends confirmation emails. Configure the Auto Scaling group to scale based on the depth of the SQS queue.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Add an Amazon CloudFront distribution, setting the origin as the web application that accepts the order requests.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Add DynamoDB Accelerator (DAX) cluster to improve the DynamoDB database's performance.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Add an Amazon API Gateway API in front of the web application that accepts the order requests.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nAdd an Auto Scaling group for the application that processes orders and sends confirmation emails. Configure the Auto Scaling group to scale based on the depth of the SQS queue.\n\nThis is the correct option because the delay in confirmation emails indicates that the application processing the orders and sending the emails is unable to keep up with the incoming requests. By implementing Auto Scaling based on the SQS queue depth, more instances can be added when the queue depth increases, ensuring timely processing of orders and sending of confirmation emails.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAdd DynamoDB Accelerator (DAX) cluster to improve the DynamoDB database's performance.\n\nThe delay is not due to the DynamoDB database performance but due to the inability of the application to process the orders in a timely manner.\n\n\n\n\nAdd an Amazon API Gateway API in front of the web application that accepts the order requests.\n\nIt doesn't address the issue of delayed order processing and email confirmations. API Gateway would primarily help in managing the incoming requests but not processing them faster.\n\n\n\n\nAdd an Amazon CloudFront distribution, setting the origin as the web application that accepts the order requests.\n\nCloudFront primarily improves the loading speed of static and dynamic web content, but it doesn't help in scaling the backend processing application which is causing the delay in sending order confirmation emails.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html",
    "correctAnswerExplanations": [
      {
        "answer": "Add an Auto Scaling group for the application that processes orders and sends confirmation emails. Configure the Auto Scaling group to scale based on the depth of the SQS queue.",
        "explanation": "This is the correct option because the delay in confirmation emails indicates that the application processing the orders and sending the emails is unable to keep up with the incoming requests. By implementing Auto Scaling based on the SQS queue depth, more instances can be added when the queue depth increases, ensuring timely processing of orders and sending of confirmation emails."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Add DynamoDB Accelerator (DAX) cluster to improve the DynamoDB database's performance.",
        "explanation": "The delay is not due to the DynamoDB database performance but due to the inability of the application to process the orders in a timely manner."
      },
      {
        "answer": "Add an Amazon API Gateway API in front of the web application that accepts the order requests.",
        "explanation": "It doesn't address the issue of delayed order processing and email confirmations. API Gateway would primarily help in managing the incoming requests but not processing them faster."
      },
      {
        "answer": "Add an Amazon CloudFront distribution, setting the origin as the web application that accepts the order requests.",
        "explanation": "CloudFront primarily improves the loading speed of static and dynamic web content, but it doesn't help in scaling the backend processing application which is causing the delay in sending order confirmation emails."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html"
    ]
  },
  {
    "id": 22,
    "question": "A small business hosts a blog with static content, presently located in an on-premises server. The blog is updated sporadically by an administrator who uses an SFTP client to upload new posts. The business plans to move its blog to AWS and leverage Amazon CloudFront. The business's solutions architect has already set up a CloudFront distribution. The architect is tasked with creating a cost-effective and resilient hosting architecture to serve as the CloudFront origin.\n\nWhich solution is the most appropriate?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a private Amazon S3 bucket. Apply an S3 bucket policy to permit access from a CloudFront origin access identity (OAI). Upload blog content using the AWS CLI.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Deploy an AWS Auto Scaling group for Amazon EC2 instances. Implement an Application Load Balancer. Upload blog content using an SFTP client.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up a virtual server with Amazon Lightsail. Configure the web server in the Lightsail instance. Upload blog content using an SFTP client.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a public Amazon S3 bucket. Set up AWS Transfer for SFTP. Configure the S3 bucket for website hosting. Upload blog content using the SFTP client.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nCreate a private Amazon S3 bucket. Apply an S3 bucket policy to permit access from a CloudFront origin access identity (OAI). Upload blog content using the AWS CLI.\n\nThis solution provides high scalability, durability, and cost-effectiveness for hosting static content. By establishing a private S3 bucket, the blog content remains securely stored and inaccessible to the public directly. The S3 bucket policy can be configured to allow access only from the CloudFront OAI, ensuring that the content is served exclusively through CloudFront.\n\nUsing the AWS CLI to upload blog content provides a convenient and scriptable method for uploading new posts. The administrator can continue using their familiar SFTP client to upload content to the S3 bucket, and then the changes will be automatically propagated to CloudFront through the S3 bucket and the configured CloudFront distribution.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up a virtual server with Amazon Lightsail. Configure the web server in the Lightsail instance. Upload blog content using an SFTP client.\n\nThis option is not the most cost-effective or resilient for hosting static content, as it involves managing a server.\n\n\n\n\nDeploy an AWS Auto Scaling group for Amazon EC2 instances. Implement an Application Load Balancer. Upload blog content using an SFTP client.\n\nThis solution is overkill for a static blog site, and it's not the most cost-effective solution as it involves running EC2 instances and a load balancer.\n\n\n\n\nCreate a public Amazon S3 bucket. Set up AWS Transfer for SFTP. Configure the S3 bucket for website hosting. Upload blog content using the SFTP client.\n\nSetting up AWS Transfer for SFTP adds unnecessary complexity and cost for a static blog site. Also, using a public S3 bucket could lead to security concerns.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\n\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a private Amazon S3 bucket. Apply an S3 bucket policy to permit access from a CloudFront origin access identity (OAI). Upload blog content using the AWS CLI.",
        "explanation": "This solution provides high scalability, durability, and cost-effectiveness for hosting static content. By establishing a private S3 bucket, the blog content remains securely stored and inaccessible to the public directly. The S3 bucket policy can be configured to allow access only from the CloudFront OAI, ensuring that the content is served exclusively through CloudFront."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up a virtual server with Amazon Lightsail. Configure the web server in the Lightsail instance. Upload blog content using an SFTP client.",
        "explanation": "This option is not the most cost-effective or resilient for hosting static content, as it involves managing a server."
      },
      {
        "answer": "Deploy an AWS Auto Scaling group for Amazon EC2 instances. Implement an Application Load Balancer. Upload blog content using an SFTP client.",
        "explanation": "This solution is overkill for a static blog site, and it's not the most cost-effective solution as it involves running EC2 instances and a load balancer."
      },
      {
        "answer": "Create a public Amazon S3 bucket. Set up AWS Transfer for SFTP. Configure the S3 bucket for website hosting. Upload blog content using the SFTP client.",
        "explanation": "Setting up AWS Transfer for SFTP adds unnecessary complexity and cost for a static blog site. Also, using a public S3 bucket could lead to security concerns."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html",
      "https://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html"
    ]
  },
  {
    "id": 23,
    "question": "A solutions architect is designing a new media content delivery network using Amazon CloudFront for a music streaming application. The application uses HTTPS, but the company's policy mandates an additional layer of security for protecting the premium content which should be accessible to premium subscribers only.\n\nWhich strategy should the solutions architect follow?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up CloudFront and specify the Viewer Protocol Policy setting to HTTPS Only for the Origin Protocol Policy.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up a CloudFront signed cookie.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up a CloudFront field-level encryption profile.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up a CloudFront signed URL.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nSet up a CloudFront signed URL.\n\nIn the case of the music streaming application, we want to protect individual pieces of content - the music files - which are only accessible to premium users. A signed URL is perfect for this scenario because you can control who can access the content served by CloudFront. With a signed URL, you can control access to individual files, such as music files, and you can set the time after which the URL will no longer work - this is known as the expiration time. This provides an additional layer of security on top of HTTPS.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up a CloudFront signed cookie.\n\nThis option would not be the best for this scenario. Signed cookies are used when you want to provide access to multiple restricted files, for example, all files in a specific directory. They do not offer the same level of granular access control as signed URLs, which is required in this case.\n\n\n\n\nSet up a CloudFront field-level encryption profile.\n\nField-level encryption is a feature where CloudFront encrypts specific POST parameters at the edge locations. It's used mainly in form submissions and not suitable for securing media file access at a granular level.\n\n\n\n\nSet up CloudFront and specify the Viewer Protocol Policy setting to HTTPS Only for the Origin Protocol Policy.\n\nWhile this does increase security by ensuring that all communication between viewers and CloudFront is over HTTPS, it does not provide the additional layer of security needed to restrict access to premium content.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html",
    "correctAnswerExplanations": [
      {
        "answer": "Set up a CloudFront signed URL.",
        "explanation": "In the case of the music streaming application, we want to protect individual pieces of content - the music files - which are only accessible to premium users. A signed URL is perfect for this scenario because you can control who can access the content served by CloudFront. With a signed URL, you can control access to individual files, such as music files, and you can set the time after which the URL will no longer work - this is known as the expiration time. This provides an additional layer of security on top of HTTPS."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up a CloudFront signed cookie.",
        "explanation": "This option would not be the best for this scenario. Signed cookies are used when you want to provide access to multiple restricted files, for example, all files in a specific directory. They do not offer the same level of granular access control as signed URLs, which is required in this case."
      },
      {
        "answer": "Set up a CloudFront field-level encryption profile.",
        "explanation": "Field-level encryption is a feature where CloudFront encrypts specific POST parameters at the edge locations. It's used mainly in form submissions and not suitable for securing media file access at a granular level."
      },
      {
        "answer": "Set up CloudFront and specify the Viewer Protocol Policy setting to HTTPS Only for the Origin Protocol Policy.",
        "explanation": "While this does increase security by ensuring that all communication between viewers and CloudFront is over HTTPS, it does not provide the additional layer of security needed to restrict access to premium content."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html"
    ]
  },
  {
    "id": 24,
    "question": "A financial institution plans to shift its data center from on-premises to AWS. The firm's regulatory requirements state that they can only use the eu-west-2 region. The institution's IT administrators are not allowed to establish VPC connections to the internet.\n\nWhich solutions will satisfy these constraints? (Select TWO.)",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Activate AWS Config to implement managed rules to detect and alert for internet gateways and to alert for new resources deployed outside of eu-west-2.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS Organizations to configure Service Control Policies (SCPs) that prevent VPCs from accessing the internet. Block access to all AWS Regions except eu-west-2.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Set up an outbound rule for the network ACL in each VPC to block all traffic from 0.0.0.0/0. Create an IAM policy for every user to prevent the use of any AWS Region other than eu-west-2.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Implement rules in AWS WAF to prevent internet access. Restrict access to all AWS Regions excluding eu-west-2 in the AWS account settings.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Use AWS Control Tower to establish data residency guardrails to block internet connectivity and deny access to all AWS Regions except for eu-west-2.",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nUse AWS Control Tower to establish data residency guardrails to block internet connectivity and deny access to all AWS Regions except for eu-west-2.\n\nAWS Control Tower provides the easiest way to set up and govern a new, secure, multi-account AWS environment. It can establish guardrails, which are high-level rules that provide ongoing governance. In this case, guardrails can be set to deny internet access and allow only the eu-west-2 region.\n\n\n\n\nUse AWS Organizations to configure Service Control Policies (SCPs) that prevent VPCs from accessing the internet. Block access to all AWS Regions except eu-west-2.\n\nAWS Organizations allows you to centrally manage and govern your environment as you grow and scale your AWS resources. Using SCPs, you can restrict what actions can be performed by entities (users and roles) in the member accounts of an organization. Thus, it can help to fulfill the requirements.\n\n\n\n\n\n\n\nIncorrect Options:\n\nImplement rules in AWS WAF to prevent internet access. Restrict access to all AWS Regions excluding eu-west-2 in the AWS account settings.\n\nAWS WAF is a web application firewall that helps protect web applications from common web exploits. However, it doesn't have the capability to restrict VPCs from accessing the internet or limiting the usage of specific AWS regions.\n\n\n\n\nSet up an outbound rule for the network ACL in each VPC to block all traffic from 0.0.0.0/0. Create an IAM policy for every user to prevent the use of any AWS Region other than eu-west-2.\n\nThis method would be overly complex and difficult to manage in a large organization. Additionally, IAM policies don't inherently restrict network access at the VPC level.\n\n\n\n\nActivate AWS Config to implement managed rules to detect and alert for internet gateways and to alert for new resources deployed outside of eu-west-2.\n\nWhile AWS Config can help detect and alert for changes, it doesn't prevent actions from happening. It is more of a reactive measure than a proactive one.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/new-for-aws-control-tower-region-deny-and-guardrails-to-help-you-meet-data-residency-requirements/\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_vpc.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Control Tower to establish data residency guardrails to block internet connectivity and deny access to all AWS Regions except for eu-west-2.",
        "explanation": "AWS Control Tower provides the easiest way to set up and govern a new, secure, multi-account AWS environment. It can establish guardrails, which are high-level rules that provide ongoing governance. In this case, guardrails can be set to deny internet access and allow only the eu-west-2 region."
      },
      {
        "answer": "Use AWS Organizations to configure Service Control Policies (SCPs) that prevent VPCs from accessing the internet. Block access to all AWS Regions except eu-west-2.",
        "explanation": "AWS Organizations allows you to centrally manage and govern your environment as you grow and scale your AWS resources. Using SCPs, you can restrict what actions can be performed by entities (users and roles) in the member accounts of an organization. Thus, it can help to fulfill the requirements."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Implement rules in AWS WAF to prevent internet access. Restrict access to all AWS Regions excluding eu-west-2 in the AWS account settings.",
        "explanation": "AWS WAF is a web application firewall that helps protect web applications from common web exploits. However, it doesn't have the capability to restrict VPCs from accessing the internet or limiting the usage of specific AWS regions."
      },
      {
        "answer": "Set up an outbound rule for the network ACL in each VPC to block all traffic from 0.0.0.0/0. Create an IAM policy for every user to prevent the use of any AWS Region other than eu-west-2.",
        "explanation": "This method would be overly complex and difficult to manage in a large organization. Additionally, IAM policies don't inherently restrict network access at the VPC level."
      },
      {
        "answer": "Activate AWS Config to implement managed rules to detect and alert for internet gateways and to alert for new resources deployed outside of eu-west-2.",
        "explanation": "While AWS Config can help detect and alert for changes, it doesn't prevent actions from happening. It is more of a reactive measure than a proactive one."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/new-for-aws-control-tower-region-deny-and-guardrails-to-help-you-meet-data-residency-requirements/",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_vpc.html"
    ]
  },
  {
    "id": 25,
    "question": "An enterprise is hosting a widely used public-facing application on AWS behind a Network Load Balancer (NLB). Due to recent cyber threats, the company wants to secure its application against DDoS attacks.\n\nWhat should the solutions architect recommend to enhance security?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Activate AWS WAF (Web Application Firewall) to thwart attacks.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy Amazon GuardDuty for DDoS protection.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Enable AWS Shield Advanced for DDoS mitigation.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use Amazon Macie to monitor the NLB.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nEnable AWS Shield Advanced for DDoS mitigation.\n\nAWS Shield Advanced provides cost-effective, advanced protection against Distributed Denial of Service (DDoS) attacks for applications running on Amazon Web Services (AWS). AWS Shield Advanced offers advanced DDoS protection, web application firewall integration, cost protection, and 24/7 DDoS response team access to safeguard your applications from large and sophisticated attacks.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDeploy Amazon GuardDuty for DDoS protection.\n\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior, but it does not provide DDoS protection.\n\n\n\n\nActivate AWS WAF (Web Application Firewall) to thwart attacks.\n\nAWS WAF is a web application firewall that helps protect web applications from common web exploits but doesn't offer DDoS protection.\n\n\n\n\nUse Amazon Macie to monitor the NLB.\n\nAmazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data but does not provide DDoS protection.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/shield/features/",
    "correctAnswerExplanations": [
      {
        "answer": "Enable AWS Shield Advanced for DDoS mitigation.",
        "explanation": "AWS Shield Advanced provides cost-effective, advanced protection against Distributed Denial of Service (DDoS) attacks for applications running on Amazon Web Services (AWS). AWS Shield Advanced offers advanced DDoS protection, web application firewall integration, cost protection, and 24/7 DDoS response team access to safeguard your applications from large and sophisticated attacks."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Deploy Amazon GuardDuty for DDoS protection.",
        "explanation": "Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior, but it does not provide DDoS protection."
      },
      {
        "answer": "Activate AWS WAF (Web Application Firewall) to thwart attacks.",
        "explanation": "AWS WAF is a web application firewall that helps protect web applications from common web exploits but doesn't offer DDoS protection."
      },
      {
        "answer": "Use Amazon Macie to monitor the NLB.",
        "explanation": "Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data but does not provide DDoS protection."
      }
    ],
    "references": [
      "https://aws.amazon.com/shield/features/"
    ]
  },
  {
    "id": 26,
    "question": "A streaming media company has a content processing pipeline that involves the following components:\n\nAn Amazon Kinesis stream that receives real-time video uploads\n\nAn AWS Lambda function that transcodes the video and stores it\n\nThe processing pipeline occasionally encounters failures due to network instability. When this happens, the corresponding video content is not processed unless the company manually restarts the task.\n\nWhat can a solutions architect do to ensure that all video uploads are processed eventually?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure the Lambda function to be deployed across multiple Availability Zones.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Adjust the Lambda function's configuration to increase the CPU and memory allocations for the function.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. Alter the Lambda function to process messages in the queue.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Modify the Kinesis stream's retry policy to increase both the number of retries and the delay between retries.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nSet up an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. Alter the Lambda function to process messages in the queue.\n\nThis strategy ensures that in case of a failure, the video upload information is queued for reprocessing. It utilizes the durability of SQS to store messages until they are processed successfully, ensuring no loss of video uploads due to intermittent network issues.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure the Lambda function to be deployed across multiple Availability Zones.\n\nAWS Lambda functions are not configured per Availability Zone; they run within a region and are designed to be highly available by default.\n\n\n\n\nAdjust the Lambda function's configuration to increase the CPU and memory allocations for the function.\n\nWhile this might help with some types of processing issues, it would not necessarily solve problems related to network instability.\n\n\n\n\nModify the Kinesis stream's retry policy to increase both the number of retries and the delay between retries.\n\nAmazon Kinesis Data Streams doesn't have a built-in retry policy like this.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
    "correctAnswerExplanations": [
      {
        "answer": "Set up an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. Alter the Lambda function to process messages in the queue.",
        "explanation": "This strategy ensures that in case of a failure, the video upload information is queued for reprocessing. It utilizes the durability of SQS to store messages until they are processed successfully, ensuring no loss of video uploads due to intermittent network issues."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Configure the Lambda function to be deployed across multiple Availability Zones.",
        "explanation": "AWS Lambda functions are not configured per Availability Zone; they run within a region and are designed to be highly available by default."
      },
      {
        "answer": "Adjust the Lambda function's configuration to increase the CPU and memory allocations for the function.",
        "explanation": "While this might help with some types of processing issues, it would not necessarily solve problems related to network instability."
      },
      {
        "answer": "Modify the Kinesis stream's retry policy to increase both the number of retries and the delay between retries.",
        "explanation": "Amazon Kinesis Data Streams doesn't have a built-in retry policy like this."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html"
    ]
  },
  {
    "id": 27,
    "question": "A solutions architect is working with a company to reduce the cost of running a multi-tier web application on AWS. The application will use Amazon EC2 instances for the ingestion layer, AWS Fargate for the application layer, and AWS Lambda for the serverless compute layer.\n\nThe EC2 instances will handle the unpredictable and interruptible ingestion transactions. The application layer hosted on Fargate and the serverless compute layer on Lambda will have consistent utilization throughout the year.\n\nWhich combination of purchasing options will yield the MOST cost-effective solution for operating this application? (Select TWO.)",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Purchase a 1-year Compute Savings Plan for the application layer and serverless compute layer.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Purchase a 1-year EC2 instance Savings Plan for the application layer and serverless compute layer.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Purchase 1-year All Upfront Reserved instances for the ingestion layer.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Spot Instances for the ingestion layer.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Use On-Demand Instances for the ingestion layer.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nUse Spot Instances for the ingestion layer.\n\nUsing Spot Instances for the ingestion layer is cost-effective because Spot Instances offer significant savings compared to On-Demand Instances. Spot Instances allow you to bid on spare EC2 capacity and take advantage of unused resources, making them ideal for unpredictable and interruptible workloads like the ingestion layer.\n\n\n\n\nPurchase a 1-year Compute Savings Plan for the application layer and serverless compute layer.\n\nPurchasing a 1-year Compute Savings Plan for the application layer and serverless compute layer provides a discounted pricing model for consistent utilization workloads. Compute Savings Plans offer cost savings compared to On-Demand pricing for a commitment of usage over a 1-year term.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse On-Demand Instances for the ingestion layer.\n\nOn-Demand instances are more expensive than Spot instances and do not offer the same cost savings for interruptible, unpredictable workloads.\n\n\n\n\nPurchase 1-year All Upfront Reserved instances for the ingestion layer.\n\nReserved Instances are not suitable for unpredictable, interruptible workloads as they are designed for consistent usage.\n\n\n\n\nPurchase a 1-year EC2 instance Savings Plan for the application layer and serverless compute layer.\n\nThe EC2 instance Savings Plan only applies to EC2 instances, not Fargate or Lambda services.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/\n\nhttps://aws.amazon.com/ec2/spot/\n\nhttps://aws.amazon.com/savingsplans/\n\nhttps://aws.amazon.com/ec2/pricing/",
    "correctAnswerExplanations": [
      {
        "answer": "Use Spot Instances for the ingestion layer.",
        "explanation": "Using Spot Instances for the ingestion layer is cost-effective because Spot Instances offer significant savings compared to On-Demand Instances. Spot Instances allow you to bid on spare EC2 capacity and take advantage of unused resources, making them ideal for unpredictable and interruptible workloads like the ingestion layer."
      },
      {
        "answer": "Purchase a 1-year Compute Savings Plan for the application layer and serverless compute layer.",
        "explanation": "Purchasing a 1-year Compute Savings Plan for the application layer and serverless compute layer provides a discounted pricing model for consistent utilization workloads. Compute Savings Plans offer cost savings compared to On-Demand pricing for a commitment of usage over a 1-year term."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use On-Demand Instances for the ingestion layer.",
        "explanation": "On-Demand instances are more expensive than Spot instances and do not offer the same cost savings for interruptible, unpredictable workloads."
      },
      {
        "answer": "Purchase 1-year All Upfront Reserved instances for the ingestion layer.",
        "explanation": "Reserved Instances are not suitable for unpredictable, interruptible workloads as they are designed for consistent usage."
      },
      {
        "answer": "Purchase a 1-year EC2 instance Savings Plan for the application layer and serverless compute layer.",
        "explanation": "The EC2 instance Savings Plan only applies to EC2 instances, not Fargate or Lambda services."
      }
    ],
    "references": [
      "https://aws.amazon.com/ec2/pricing/reserved-instances/",
      "https://aws.amazon.com/ec2/spot/",
      "https://aws.amazon.com/savingsplans/",
      "https://aws.amazon.com/ec2/pricing/"
    ]
  },
  {
    "id": 28,
    "question": "A software development company has an AWS account with AWS Direct Connect connections facilitating access to their on-site data center. All non-VPC traffic is directed to the virtual private gateway. The development team has recently deployed an AWS Lambda function, which requires access to a proprietary application hosted in a private subnet within the on-site data center.\n\nWhich solution would best facilitate these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure the AWS Lambda function to operate within the VPC, applying the appropriate security group.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Update the VPC's route tables to allow the AWS Lambda function to access the on-site data center via Direct Connect.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Elastic IP address, then configure the AWS Lambda function to route traffic via this Elastic IP address, negating the need for an elastic network interface.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a VPN connection from AWS to the on-site data center and direct traffic from the AWS Lambda function through the VPN.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nConfigure the AWS Lambda function to operate within the VPC, applying the appropriate security group.\n\nBy configuring the AWS Lambda function to operate within the Virtual Private Cloud (VPC), the function can be placed in the same VPC as the private subnet hosting the proprietary application. This allows the Lambda function to securely access the application without exposing it to the public internet.\n\nOperating within the VPC ensures that the Lambda function can communicate with resources in the private subnet using private IP addresses. The appropriate security group can be applied to the Lambda function's network interface to control inbound and outbound traffic, providing the necessary security measures.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a VPN connection from AWS to the on-site data center and direct traffic from the AWS Lambda function through the VPN.\n\nEstablishing a VPN connection from AWS to the on-site data center and directing traffic from the Lambda function through the VPN would introduce unnecessary complexity. Since the proprietary application is already hosted within the private subnet of the on-site data center, there is no need for additional VPN connections.\n\n\n\n\nUpdate the VPC's route tables to allow the AWS Lambda function to access the on-site data center via Direct Connect.\n\nUpdating the VPC's route tables to allow the Lambda function to access the on-site data center via Direct Connect is not necessary. Direct Connect is typically used to establish dedicated network connections between AWS and on-premises data centers, but since the Lambda function can operate within the VPC, it can communicate with the private subnet directly.\n\n\n\n\nCreate an Elastic IP address, then configure the AWS Lambda function to route traffic via this Elastic IP address, negating the need for an elastic network interface.\n\nCreating an Elastic IP address and configuring the Lambda function to route traffic via that IP address is not applicable in this scenario. Elastic IP addresses are typically used for static public IP assignment, and they don't provide a solution for accessing resources within a private subnet.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure the AWS Lambda function to operate within the VPC, applying the appropriate security group.",
        "explanation": "By configuring the AWS Lambda function to operate within the Virtual Private Cloud (VPC), the function can be placed in the same VPC as the private subnet hosting the proprietary application. This allows the Lambda function to securely access the application without exposing it to the public internet."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a VPN connection from AWS to the on-site data center and direct traffic from the AWS Lambda function through the VPN.",
        "explanation": "Establishing a VPN connection from AWS to the on-site data center and directing traffic from the Lambda function through the VPN would introduce unnecessary complexity. Since the proprietary application is already hosted within the private subnet of the on-site data center, there is no need for additional VPN connections."
      },
      {
        "answer": "Update the VPC's route tables to allow the AWS Lambda function to access the on-site data center via Direct Connect.",
        "explanation": "Updating the VPC's route tables to allow the Lambda function to access the on-site data center via Direct Connect is not necessary. Direct Connect is typically used to establish dedicated network connections between AWS and on-premises data centers, but since the Lambda function can operate within the VPC, it can communicate with the private subnet directly."
      },
      {
        "answer": "Create an Elastic IP address, then configure the AWS Lambda function to route traffic via this Elastic IP address, negating the need for an elastic network interface.",
        "explanation": "Creating an Elastic IP address and configuring the Lambda function to route traffic via that IP address is not applicable in this scenario. Elastic IP addresses are typically used for static public IP assignment, and they don't provide a solution for accessing resources within a private subnet."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html"
    ]
  },
  {
    "id": 29,
    "question": "An organization operates a mission-critical application on an array of Amazon EC2 instances. The application retrieves data from an Amazon DynamoDB table and concurrently processes the records. The record volume varies and often experiences irregular traffic. The application needs to process records consistently without any service disruption.\n\nWhich solution is the MOST cost-effective to meet these requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Reserved Instances for the standard capacity and Spot Instances to handle the additional capacity.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Exclusively use Reserved Instances to handle the peak capacity requirement.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Exclusively use Spot Instances to handle the peak capacity requirement.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Reserved Instances for the standard capacity and On-Demand Instances to handle the additional capacity.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nUse Reserved Instances for the standard capacity and On-Demand Instances to handle the additional capacity.\n\nReserved Instances are best for baseline capacity as they offer significant discounts over On-Demand pricing and provide a capacity reservation. For variable workloads, On-Demand Instances give you the flexibility to handle spikes in demand without any long-term commitments. This approach ensures that the application has the capacity it needs when it needs it, thereby ensuring continuous processing of records without any downtime, and is the most cost-effective solution.\n\n\n\n\n\n\n\nIncorrect Options:\n\nExclusively use Spot Instances to handle the peak capacity requirement.\n\nThis is not a reliable solution as Spot Instances can be interrupted with little notice when demand for capacity rises. This could lead to downtime, which is not acceptable for the application.\n\n\n\n\nExclusively use Reserved Instances to handle the peak capacity requirement.\n\nThis is not cost-effective because it means paying for maximum capacity all the time, even during periods of low demand.\n\n\n\n\nUse Reserved Instances for the standard capacity and Spot Instances to handle the additional capacity.\n\nThis could lead to interrupted processing during peak loads as Spot Instances can be terminated with little notice when demand rises. This could cause downtime, which is not acceptable for the application.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/\n\nhttps://aws.amazon.com/ec2/pricing/on-demand/\n\nhttps://aws.amazon.com/ec2/spot/\n\nhttps://aws.amazon.com/ec2/pricing/",
    "correctAnswerExplanations": [
      {
        "answer": "Use Reserved Instances for the standard capacity and On-Demand Instances to handle the additional capacity.",
        "explanation": "Reserved Instances are best for baseline capacity as they offer significant discounts over On-Demand pricing and provide a capacity reservation. For variable workloads, On-Demand Instances give you the flexibility to handle spikes in demand without any long-term commitments. This approach ensures that the application has the capacity it needs when it needs it, thereby ensuring continuous processing of records without any downtime, and is the most cost-effective solution."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Exclusively use Spot Instances to handle the peak capacity requirement.",
        "explanation": "This is not a reliable solution as Spot Instances can be interrupted with little notice when demand for capacity rises. This could lead to downtime, which is not acceptable for the application."
      },
      {
        "answer": "Exclusively use Reserved Instances to handle the peak capacity requirement.",
        "explanation": "This is not cost-effective because it means paying for maximum capacity all the time, even during periods of low demand."
      },
      {
        "answer": "Use Reserved Instances for the standard capacity and Spot Instances to handle the additional capacity.",
        "explanation": "This could lead to interrupted processing during peak loads as Spot Instances can be terminated with little notice when demand rises. This could cause downtime, which is not acceptable for the application."
      }
    ],
    "references": [
      "https://aws.amazon.com/ec2/pricing/reserved-instances/",
      "https://aws.amazon.com/ec2/pricing/on-demand/",
      "https://aws.amazon.com/ec2/spot/",
      "https://aws.amazon.com/ec2/pricing/"
    ]
  },
  {
    "id": 30,
    "question": "A company operates a popular online multiplayer game. Due to its growing user base, the company's game session creation system is facing scaling issues during peak hours. The current architecture includes:\n\nA group of Amazon EC2 instances in an Amazon EC2 Auto Scaling group to receive game session requests.\n\nAnother group of EC2 instances in an Amazon EC2 Auto Scaling group to manage game session servers.\n\nThe game session request process is quick, but the game session server management process can take longer. It is critical not to lose any data during scaling events.\n\nA solutions architect needs to ensure that both the game session request and the game session server management processes can scale adequately during peak traffic hours. The solution must optimize the company’s use of AWS resources.\n\nWhich solution will meet these requirements?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon CloudWatch metrics to monitor each EC2 instance's CPU usage in the Auto Scaling groups. Configure a CloudWatch alarm to trigger an Amazon Simple Notification Service (Amazon SNS) topic that creates additional Auto Scaling groups on demand.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up two Amazon Simple Queue Service (Amazon SQS) queues: one for game session requests and another for game session server management. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Set up two Amazon Simple Queue Service (Amazon SQS) queues: one for game session requests and another for game session server management. Configure the EC2 instances to poll their respective queue. Scale the Auto Scaling groups based on notifications from the queues.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon CloudWatch metrics to monitor each EC2 instance's CPU usage in the Auto Scaling groups. Set the minimum capacity of each Auto Scaling group according to peak workload values.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nSet up two Amazon Simple Queue Service (Amazon SQS) queues: one for game session requests and another for game session server management. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric.\n\nBy using SQS queues, the game session requests and game session server management tasks can be decoupled, ensuring scalability and minimizing data loss. The EC2 instances can poll their respective queues to retrieve and process messages, allowing for distributed and parallel processing.\n\nTo optimize the use of AWS resources and ensure adequate scaling, a metric based on a backlog per instance calculation can be established. This metric measures the number of messages in the queue per instance and can be used to determine if additional instances are required. Scaling the Auto Scaling groups based on this metric allows for dynamic scaling based on the workload.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon CloudWatch metrics to monitor each EC2 instance's CPU usage in the Auto Scaling groups. Set the minimum capacity of each Auto Scaling group according to peak workload values.\n\nThis option is not the optimal solution because setting the minimum capacity of each Auto Scaling group according to peak workload values may lead to overprovisioning and underutilization of resources during non-peak periods.\n\n\n\n\nUse Amazon CloudWatch metrics to monitor each EC2 instance's CPU usage in the Auto Scaling groups. Configure a CloudWatch alarm to trigger an Amazon Simple Notification Service (Amazon SNS) topic that creates additional Auto Scaling groups on demand.\n\nThis option is not the best solution because triggering an Amazon SNS topic to create additional Auto Scaling groups may not provide the necessary fine-grained control over scaling and may result in resource inefficiencies.\n\n\n\n\nSet up two Amazon Simple Queue Service (Amazon SQS) queues: one for game session requests and another for game session server management. Configure the EC2 instances to poll their respective queue. Scale the Auto Scaling groups based on notifications from the queues.\n\nWhile this option correctly separates the tasks into different queues, it doesn't provide an effective metric for scaling. Scaling based on notifications from the queues may not accurately reflect the amount of work to be done.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html",
    "correctAnswerExplanations": [
      {
        "answer": "Set up two Amazon Simple Queue Service (Amazon SQS) queues: one for game session requests and another for game session server management. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric.",
        "explanation": "By using SQS queues, the game session requests and game session server management tasks can be decoupled, ensuring scalability and minimizing data loss. The EC2 instances can poll their respective queues to retrieve and process messages, allowing for distributed and parallel processing."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Amazon CloudWatch metrics to monitor each EC2 instance's CPU usage in the Auto Scaling groups. Set the minimum capacity of each Auto Scaling group according to peak workload values.",
        "explanation": "This option is not the optimal solution because setting the minimum capacity of each Auto Scaling group according to peak workload values may lead to overprovisioning and underutilization of resources during non-peak periods."
      },
      {
        "answer": "Use Amazon CloudWatch metrics to monitor each EC2 instance's CPU usage in the Auto Scaling groups. Configure a CloudWatch alarm to trigger an Amazon Simple Notification Service (Amazon SNS) topic that creates additional Auto Scaling groups on demand.",
        "explanation": "This option is not the best solution because triggering an Amazon SNS topic to create additional Auto Scaling groups may not provide the necessary fine-grained control over scaling and may result in resource inefficiencies."
      },
      {
        "answer": "Set up two Amazon Simple Queue Service (Amazon SQS) queues: one for game session requests and another for game session server management. Configure the EC2 instances to poll their respective queue. Scale the Auto Scaling groups based on notifications from the queues.",
        "explanation": "While this option correctly separates the tasks into different queues, it doesn't provide an effective metric for scaling. Scaling based on notifications from the queues may not accurately reflect the amount of work to be done."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html"
    ]
  },
  {
    "id": 31,
    "question": "A multinational corporation collects time-series data from IoT devices across several geographical regions. The corporation also gathers static data from various company databases. The company wants to centralize all the data for comprehensive analytics. The data must be processed upon arrival and staged in different Amazon S3 buckets. The company will perform ad-hoc queries on this data and use a visualization tool to display key performance indicators (KPIs).\n\nWhich two steps should the company take to meet these requirements with minimal operational overhead? (Select TWO.)",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Athena for ad-hoc queries and Amazon QuickSight to visualize KPIs.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use blueprints in AWS Lake Formation to identify data that can be ingested into a data lake. Utilize AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format. Load the data into multiple Amazon OpenSearch Service (Amazon Elasticsearch Service) clusters.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Kinesis Data Analytics for ad-hoc queries and Amazon QuickSight to visualize KPIs.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Create custom AWS Lambda functions to transfer the individual records from the databases to an Amazon Redshift cluster.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nUse Amazon Athena for ad-hoc queries and Amazon QuickSight to visualize KPIs.\n\nAmazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. You can quickly run ad-hoc queries on large datasets and get results in seconds. Athena fits the requirement of performing ad-hoc queries on the staged data in S3.\n\nAmazon QuickSight is a scalable, serverless, embeddable, machine learning-powered business intelligence (BI) service built for the cloud. QuickSight lets you easily create and publish interactive BI dashboards that include Machine Learning-powered insights. This aligns perfectly with the corporation's requirement to visualize their KPIs.\n\n\n\n\nUse blueprints in AWS Lake Formation to identify data that can be ingested into a data lake. Utilize AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.\n\nWith Lake Formation blueprints, you can quickly create a data lake by defining where your data resides and what data access and security policies you want to apply. On other hand, AWS Glue can discover and catalog metadata about your data stored in S3, making the data readily searchable and queryable across multiple AWS services. Once the data is in Amazon S3, it can be stored in Parquet format, which is a columnar storage file format optimized for use with big data processing frameworks such as Apache Hadoop, Apache Spark, and others. The use of Parquet format will provide efficient and performant querying with Athena.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon Kinesis Data Analytics for ad-hoc queries and Amazon QuickSight to visualize KPIs.\n\nKinesis Data Analytics is primarily used for processing streaming data in real-time, not for ad-hoc querying of data in S3.\n\n\n\n\nCreate custom AWS Lambda functions to transfer the individual records from the databases to an Amazon Redshift cluster.\n\nThis approach would add unnecessary complexity and operational overhead. It also doesn't cover the requirement to stage the data in S3.\n\n\n\n\nUse an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format. Load the data into multiple Amazon OpenSearch Service (Amazon Elasticsearch Service) clusters.\n\nThis option is not suitable for large-scale analytical workloads and ad-hoc queries that the corporation needs to perform. Amazon Elasticsearch Service is primarily used for log and event data analysis.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/big-data/enhance-analytics-with-google-trends-data-using-aws-glue-amazon-athena-and-amazon-quicksight/",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon Athena for ad-hoc queries and Amazon QuickSight to visualize KPIs.",
        "explanation": "Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. You can quickly run ad-hoc queries on large datasets and get results in seconds. Athena fits the requirement of performing ad-hoc queries on the staged data in S3."
      },
      {
        "answer": "Use blueprints in AWS Lake Formation to identify data that can be ingested into a data lake. Utilize AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.",
        "explanation": "With Lake Formation blueprints, you can quickly create a data lake by defining where your data resides and what data access and security policies you want to apply. On other hand, AWS Glue can discover and catalog metadata about your data stored in S3, making the data readily searchable and queryable across multiple AWS services. Once the data is in Amazon S3, it can be stored in Parquet format, which is a columnar storage file format optimized for use with big data processing frameworks such as Apache Hadoop, Apache Spark, and others. The use of Parquet format will provide efficient and performant querying with Athena."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Amazon Kinesis Data Analytics for ad-hoc queries and Amazon QuickSight to visualize KPIs.",
        "explanation": "Kinesis Data Analytics is primarily used for processing streaming data in real-time, not for ad-hoc querying of data in S3."
      },
      {
        "answer": "Create custom AWS Lambda functions to transfer the individual records from the databases to an Amazon Redshift cluster.",
        "explanation": "This approach would add unnecessary complexity and operational overhead. It also doesn't cover the requirement to stage the data in S3."
      },
      {
        "answer": "Use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format. Load the data into multiple Amazon OpenSearch Service (Amazon Elasticsearch Service) clusters.",
        "explanation": "This option is not suitable for large-scale analytical workloads and ad-hoc queries that the corporation needs to perform. Amazon Elasticsearch Service is primarily used for log and event data analysis."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/enhance-analytics-with-google-trends-data-using-aws-glue-amazon-athena-and-amazon-quicksight/"
    ]
  },
  {
    "id": 32,
    "question": "An organization has a web application that uses Python and Node.js. The organization is planning to migrate the application from a physical server to AWS. The organization needs the capacity to frequently test new application features. They also require a highly available and managed solution that necessitates minimal operational involvement.\n\nWhich solution will meet these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to alternate between multiple Elastic Beanstalk environments for feature testing.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Deploy the web application to Amazon EC2 instances that are set up with Python and Node.js. Use Auto Scaling groups and an Application Load Balancer to manage the website's availability.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Containerize the web application. Deploy the web application to Amazon EC2 instances. Use the AWS Load Balancer Controller to dynamically direct traffic between containers that hold the new application features for testing.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up an Amazon S3 bucket. Activate static web hosting on the S3 bucket. Upload the static content to the S3 bucket. Use AWS Lambda to process all dynamic content.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nDeploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to alternate between multiple Elastic Beanstalk environments for feature testing.\n\nAWS Elastic Beanstalk is a fully managed service that simplifies the deployment and scaling of web applications. It supports multiple languages, including Python and Node.js, which are needed for this application. URL swapping is a feature in Elastic Beanstalk that allows you to quickly switch between different environments, which is ideal for testing new application features.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up an Amazon S3 bucket. Activate static web hosting on the S3 bucket. Upload the static content to the S3 bucket. Use AWS Lambda to process all dynamic content.\n\nWhile this approach could work for a static website, it would not be suitable for a dynamic web application that uses Python and Node.js.\n\n\n\n\nDeploy the web application to Amazon EC2 instances that are set up with Python and Node.js. Use Auto Scaling groups and an Application Load Balancer to manage the website's availability.\n\nWhile this solution would provide high availability, it would not offer the managed environment and ease of feature testing that Elastic Beanstalk does.\n\n\n\n\nContainerize the web application. Deploy the web application to Amazon EC2 instances. Use the AWS Load Balancer Controller to dynamically direct traffic between containers that hold the new application features for testing.\n\nThis option involves a lot more operational overhead due to managing the container orchestration and load balancer controller.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/elasticbeanstalk/\n\nhttps://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html\n\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html",
    "correctAnswerExplanations": [
      {
        "answer": "Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to alternate between multiple Elastic Beanstalk environments for feature testing.",
        "explanation": "AWS Elastic Beanstalk is a fully managed service that simplifies the deployment and scaling of web applications. It supports multiple languages, including Python and Node.js, which are needed for this application. URL swapping is a feature in Elastic Beanstalk that allows you to quickly switch between different environments, which is ideal for testing new application features."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up an Amazon S3 bucket. Activate static web hosting on the S3 bucket. Upload the static content to the S3 bucket. Use AWS Lambda to process all dynamic content.",
        "explanation": "While this approach could work for a static website, it would not be suitable for a dynamic web application that uses Python and Node.js."
      },
      {
        "answer": "Deploy the web application to Amazon EC2 instances that are set up with Python and Node.js. Use Auto Scaling groups and an Application Load Balancer to manage the website's availability.",
        "explanation": "While this solution would provide high availability, it would not offer the managed environment and ease of feature testing that Elastic Beanstalk does."
      },
      {
        "answer": "Containerize the web application. Deploy the web application to Amazon EC2 instances. Use the AWS Load Balancer Controller to dynamically direct traffic between containers that hold the new application features for testing.",
        "explanation": "This option involves a lot more operational overhead due to managing the container orchestration and load balancer controller."
      }
    ],
    "references": [
      "https://aws.amazon.com/elasticbeanstalk/",
      "https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html"
    ]
  },
  {
    "id": 33,
    "question": "A media company runs a live video streaming service where users can comment in real-time. The comments are received via an Amazon API Gateway and processed by an AWS Lambda function, which then writes the comments to Amazon DynamoDB for storage before they are displayed to other users. Despite provisioning maximum allowable DynamoDB throughput within its budget, the company is facing service disruptions and is losing user comments.\n\nWhat can a solutions architect recommend to mitigate this problem without affecting the existing users?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Implement request throttling on the API Gateway with client-side throttling limits.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy DynamoDB Accelerator (DAX) and use Lambda to buffer writes to DynamoDB.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create a Global Secondary Index (GSI) in DynamoDB for the table storing user comments.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nUse Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.\n\nThis solution can decouple the write process and handle temporary bursts in traffic. Lambda function can consume the messages from the SQS queue and write them to DynamoDB at a rate that can be handled by the provisioned throughput. This approach allows for smoother traffic management and handles temporary spikes in write requests. The SQS queue acts as a buffer, ensuring that comments are not lost due to service disruptions or exceeding the provisioned throughput of DynamoDB.\n\n\n\n\n\n\n\nIncorrect Options:\n\nImplement request throttling on the API Gateway with client-side throttling limits.\n\nThrottling at the API Gateway might prevent some legitimate user comments from being processed during high traffic periods, negatively impacting the user experience.\n\n\n\n\nDeploy DynamoDB Accelerator (DAX) and use Lambda to buffer writes to DynamoDB.\n\nDAX is primarily used to speed up read operations from DynamoDB, not write operations. Therefore, it will not help solve the problem of losing user comments during high write demand.\n\n\n\n\nCreate a Global Secondary Index (GSI) in DynamoDB for the table storing user comments.\n\nCreating a GSI would not help in this situation. GSIs are used for flexible querying and not for managing write throughput.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.",
        "explanation": "This solution can decouple the write process and handle temporary bursts in traffic. Lambda function can consume the messages from the SQS queue and write them to DynamoDB at a rate that can be handled by the provisioned throughput. This approach allows for smoother traffic management and handles temporary spikes in write requests. The SQS queue acts as a buffer, ensuring that comments are not lost due to service disruptions or exceeding the provisioned throughput of DynamoDB."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Implement request throttling on the API Gateway with client-side throttling limits.",
        "explanation": "Throttling at the API Gateway might prevent some legitimate user comments from being processed during high traffic periods, negatively impacting the user experience."
      },
      {
        "answer": "Deploy DynamoDB Accelerator (DAX) and use Lambda to buffer writes to DynamoDB.",
        "explanation": "DAX is primarily used to speed up read operations from DynamoDB, not write operations. Therefore, it will not help solve the problem of losing user comments during high write demand."
      },
      {
        "answer": "Create a Global Secondary Index (GSI) in DynamoDB for the table storing user comments.",
        "explanation": "Creating a GSI would not help in this situation. GSIs are used for flexible querying and not for managing write throughput."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"
    ]
  },
  {
    "id": 34,
    "question": "A financial institution is running its operations on AWS. It needs to connect to a specific banking service provided by a third-party vendor. The service is located in the vendor's VPC. As per the institution's security protocols, the connection needs to be private, limited to the specific service, and can only be initiated from the institution's VPC.\n\nWhich solution fulfills these requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up a NAT gateway in a public subnet of the institution's VPC and update the route table to enable connection to the specific service.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Request the vendor to create a virtual private gateway in its VPC and employ AWS PrivateLink for connection to the specific service.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Request the vendor to create a VPC endpoint for the specific service and use AWS PrivateLink for the connection.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create a VPC peering connection between the institution's VPC and the vendor's VPC, and update the route table to enable connection to the specific service.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nRequest the vendor to create a VPC endpoint for the specific service and use AWS PrivateLink for the connection.\n\nAWS PrivateLink allows you to securely access services hosted on AWS or by third-party vendors over private connections that stay within the AWS network. By requesting the vendor to create a VPC endpoint for the specific banking service, you can establish a private and secure connection from your institution's VPC directly to the vendor's VPC without traversing the public internet. This VPC endpoint acts as a gateway for accessing the specific service, limiting the communication to the authorized resources within your VPC.\n\nAWS PrivateLink ensures that the connection remains private, isolated, and encrypted, meeting the security protocols of your financial institution. It enables you to securely connect to the banking service without exposing any resources or data to the public internet.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a VPC peering connection between the institution's VPC and the vendor's VPC, and update the route table to enable connection to the specific service.\n\nVPC peering would establish a broader connection between the two VPCs, not limited to the specific service, which contradicts the requirements.\n\n\n\n\nRequest the vendor to create a virtual private gateway in its VPC and employ AWS PrivateLink for connection to the specific service.\n\nVirtual Private Gateways are typically used for VPN or Direct Connect, and they don't provide service-specific access like AWS PrivateLink.\n\n\n\n\nSet up a NAT gateway in a public subnet of the institution's VPC and update the route table to enable connection to the specific service.\n\nNAT gateways provide internet access to instances in a private subnet, but they don't ensure a private connection to a specific service in a different VPC.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/what-is-privatelink.html",
    "correctAnswerExplanations": [
      {
        "answer": "Request the vendor to create a VPC endpoint for the specific service and use AWS PrivateLink for the connection.",
        "explanation": "AWS PrivateLink allows you to securely access services hosted on AWS or by third-party vendors over private connections that stay within the AWS network. By requesting the vendor to create a VPC endpoint for the specific banking service, you can establish a private and secure connection from your institution's VPC directly to the vendor's VPC without traversing the public internet. This VPC endpoint acts as a gateway for accessing the specific service, limiting the communication to the authorized resources within your VPC."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a VPC peering connection between the institution's VPC and the vendor's VPC, and update the route table to enable connection to the specific service.",
        "explanation": "VPC peering would establish a broader connection between the two VPCs, not limited to the specific service, which contradicts the requirements."
      },
      {
        "answer": "Request the vendor to create a virtual private gateway in its VPC and employ AWS PrivateLink for connection to the specific service.",
        "explanation": "Virtual Private Gateways are typically used for VPN or Direct Connect, and they don't provide service-specific access like AWS PrivateLink."
      },
      {
        "answer": "Set up a NAT gateway in a public subnet of the institution's VPC and update the route table to enable connection to the specific service.",
        "explanation": "NAT gateways provide internet access to instances in a private subnet, but they don't ensure a private connection to a specific service in a different VPC."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/privatelink/what-is-privatelink.html"
    ]
  },
  {
    "id": 35,
    "question": "A company is developing a mobile application on AWS. The application uses Amazon Cognito for user management and retrieves data from Amazon S3 via a REST API hosted on Amazon API Gateway. The company wants to ensure secure access to the REST API while minimizing the development overhead.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Include the user's mobile number in each request header. Validate the user's access rights using an AWS Lambda function.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Implement an AWS Lambda authorizer in API Gateway to verify each user request.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Generate an API key for each user, which must be included in each request. Verify the key using an AWS Lambda function.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure an Amazon Cognito user pool authorizer in API Gateway to authenticate each request.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nConfigure an Amazon Cognito user pool authorizer in API Gateway to authenticate each request.\n\nBy using an Amazon Cognito user pool authorizer in API Gateway, the company can directly leverage the user management capabilities of Cognito, thereby ensuring secure access to the API. This approach will also reduce the development overhead because it eliminates the need for custom code to authenticate requests.\n\n\n\n\n\n\n\nIncorrect Options:\n\nImplement an AWS Lambda authorizer in API Gateway to verify each user request.\n\nThis option would involve additional development and maintenance overhead due to the need for a custom Lambda function to handle authentication.\n\n\n\n\nGenerate an API key for each user, which must be included in each request. Verify the key using an AWS Lambda function.\n\nThis option would also require extra development and maintenance effort to manage API keys and a Lambda function for key validation.\n\n\n\n\nInclude the user's mobile number in each request header. Validate the user's access rights using an AWS Lambda function.\n\nThis option would introduce unnecessary complexity and security risk by exposing personal user information in the request headers. Moreover, it would require additional development effort to manage a Lambda function for access validation.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure an Amazon Cognito user pool authorizer in API Gateway to authenticate each request.",
        "explanation": "By using an Amazon Cognito user pool authorizer in API Gateway, the company can directly leverage the user management capabilities of Cognito, thereby ensuring secure access to the API. This approach will also reduce the development overhead because it eliminates the need for custom code to authenticate requests."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Implement an AWS Lambda authorizer in API Gateway to verify each user request.",
        "explanation": "This option would involve additional development and maintenance overhead due to the need for a custom Lambda function to handle authentication."
      },
      {
        "answer": "Generate an API key for each user, which must be included in each request. Verify the key using an AWS Lambda function.",
        "explanation": "This option would also require extra development and maintenance effort to manage API keys and a Lambda function for key validation."
      },
      {
        "answer": "Include the user's mobile number in each request header. Validate the user's access rights using an AWS Lambda function.",
        "explanation": "This option would introduce unnecessary complexity and security risk by exposing personal user information in the request headers. Moreover, it would require additional development effort to manage a Lambda function for access validation."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html"
    ]
  },
  {
    "id": 36,
    "question": "A corporation plans to transition its PostgreSQL database from a local data center to AWS. The firm recently suffered a significant database failure, which had a profound business impact. The corporation now seeks a robust AWS database solution that minimizes data loss and ensures every transaction is stored on at least two nodes.\n\nWhich approach satisfies these criteria?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up an Amazon RDS PostgreSQL DB instance and establish a read replica in a distinct AWS Region to synchronize data replication.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon Aurora PostgreSQL instance with synchronous replication across three nodes in three Availability Zones.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Launch an Amazon EC2 instance with a PostgreSQL engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS PostgreSQL DB instance.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure an Amazon RDS PostgreSQL DB instance with Multi-AZ deployment to enable synchronous data replication.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nConfigure an Amazon RDS PostgreSQL DB instance with Multi-AZ deployment to enable synchronous data replication.\n\nMulti-AZ (Availability Zone) deployment in Amazon RDS ensures high availability and durability for database instances. With Multi-AZ, a synchronous standby replica of the primary database is created in a different Availability Zone. Every transaction committed to the primary database is synchronously replicated to the standby replica, ensuring data consistency and minimizing data loss in the event of a failure.\n\nIn case of a database failure, Amazon RDS automatically promotes the standby replica to become the new primary database, minimizing the downtime and providing a seamless failover experience. This ensures that the database remains highly available and the data is stored on at least two nodes (primary and standby replica) in separate Availability Zones.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an Amazon Aurora PostgreSQL instance with synchronous replication across three nodes in three Availability Zones.\n\nWhile Amazon Aurora provides replication across multiple Availability Zones, it does not provide synchronous replication for PostgreSQL databases. Aurora’s replication is asynchronous, which may not meet the corporation’s requirement for minimal data loss.\n\n\n\n\nSet up an Amazon RDS PostgreSQL DB instance and establish a read replica in a distinct AWS Region to synchronize data replication.\n\nAmazon RDS Read Replicas provide asynchronous replication and are primarily used for read-heavy database workloads. It won't provide the synchronous replication required for minimizing data loss in this scenario.\n\n\n\n\nLaunch an Amazon EC2 instance with a PostgreSQL engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS PostgreSQL DB instance.\n\nThis is a complex and unnecessary solution. It is also prone to errors and does not provide the same level of reliability and automated failover that Amazon RDS Multi-AZ deployments offer.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/multi-az/\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure an Amazon RDS PostgreSQL DB instance with Multi-AZ deployment to enable synchronous data replication.",
        "explanation": "Multi-AZ (Availability Zone) deployment in Amazon RDS ensures high availability and durability for database instances. With Multi-AZ, a synchronous standby replica of the primary database is created in a different Availability Zone. Every transaction committed to the primary database is synchronously replicated to the standby replica, ensuring data consistency and minimizing data loss in the event of a failure."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create an Amazon Aurora PostgreSQL instance with synchronous replication across three nodes in three Availability Zones.",
        "explanation": "While Amazon Aurora provides replication across multiple Availability Zones, it does not provide synchronous replication for PostgreSQL databases. Aurora’s replication is asynchronous, which may not meet the corporation’s requirement for minimal data loss."
      },
      {
        "answer": "Set up an Amazon RDS PostgreSQL DB instance and establish a read replica in a distinct AWS Region to synchronize data replication.",
        "explanation": "Amazon RDS Read Replicas provide asynchronous replication and are primarily used for read-heavy database workloads. It won't provide the synchronous replication required for minimizing data loss in this scenario."
      },
      {
        "answer": "Launch an Amazon EC2 instance with a PostgreSQL engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS PostgreSQL DB instance.",
        "explanation": "This is a complex and unnecessary solution. It is also prone to errors and does not provide the same level of reliability and automated failover that Amazon RDS Multi-AZ deployments offer."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
    ]
  },
  {
    "id": 37,
    "question": "An organization runs a software demo for potential clients via a web application that is only active 8 hours a day. The organization uses Amazon RDS for PostgreSQL DB instance to store client data and wants to optimize costs. How can a solutions architect help achieve this goal?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up an IAM policy for AWS Systems Manager Automation, create an IAM role for this policy, update the role's trust relationship, and configure automatic start and stop for the DB instance.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy an Amazon EC2 instance, create an IAM role with Amazon RDS access, attach the role to the EC2 instance, and configure a cron job to start and stop the EC2 instance following the desired schedule.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon ElastiCache for Memcached cache cluster, allowing users to access data from the cache when the DB instance is inactive. Purge the cache once the DB instance is restarted.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Develop AWS Lambda functions to start and stop the DB instance. Create Amazon EventBridge (Amazon CloudWatch Events) scheduled rules to invoke the Lambda functions and set these functions as event targets for the rules.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nDevelop AWS Lambda functions to start and stop the DB instance. Create Amazon EventBridge (Amazon CloudWatch Events) scheduled rules to invoke the Lambda functions and set these functions as event targets for the rules.\n\nAWS Lambda is a compute service that runs code without provisioning or managing servers. Lambda can be set to start and stop the RDS instances at specific times. Moreover, Amazon EventBridge makes it easy to connect applications together using data from Software-as-a-Service(SaaS), AWS services, and one's own applications. EventBridge can be used to set a schedule to trigger these Lambda functions, effectively starting and stopping the instances at desired times, leading to cost optimization.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up an IAM policy for AWS Systems Manager Automation, create an IAM role for this policy, update the role's trust relationship, and configure automatic start and stop for the DB instance.\n\nAWS Systems Manager doesn't natively support the automatic start and stop of RDS instances. Therefore, this option wouldn't work.\n\n\n\n\nCreate an Amazon ElastiCache for Memcached cache cluster, allowing users to access data from the cache when the DB instance is inactive. Purge the cache once the DB instance is restarted.\n\nThis option does not address the need to start and stop the RDS instance to save costs. Furthermore, caching is not a substitute for a relational database like RDS PostgreSQL.\n\n\n\n\nDeploy an Amazon EC2 instance, create an IAM role with Amazon RDS access, attach the role to the EC2 instance, and configure a cron job to start and stop the EC2 instance following the desired schedule.\n\nThis option mentions starting and stopping an EC2 instance, not an RDS instance. Hence, it's not a solution for the given requirement.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/database/schedule-amazon-rds-stop-and-start-using-aws-lambda",
    "correctAnswerExplanations": [
      {
        "answer": "Develop AWS Lambda functions to start and stop the DB instance. Create Amazon EventBridge (Amazon CloudWatch Events) scheduled rules to invoke the Lambda functions and set these functions as event targets for the rules.",
        "explanation": "AWS Lambda is a compute service that runs code without provisioning or managing servers. Lambda can be set to start and stop the RDS instances at specific times. Moreover, Amazon EventBridge makes it easy to connect applications together using data from Software-as-a-Service(SaaS), AWS services, and one's own applications. EventBridge can be used to set a schedule to trigger these Lambda functions, effectively starting and stopping the instances at desired times, leading to cost optimization."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up an IAM policy for AWS Systems Manager Automation, create an IAM role for this policy, update the role's trust relationship, and configure automatic start and stop for the DB instance.",
        "explanation": "AWS Systems Manager doesn't natively support the automatic start and stop of RDS instances. Therefore, this option wouldn't work."
      },
      {
        "answer": "Create an Amazon ElastiCache for Memcached cache cluster, allowing users to access data from the cache when the DB instance is inactive. Purge the cache once the DB instance is restarted.",
        "explanation": "This option does not address the need to start and stop the RDS instance to save costs. Furthermore, caching is not a substitute for a relational database like RDS PostgreSQL."
      },
      {
        "answer": "Deploy an Amazon EC2 instance, create an IAM role with Amazon RDS access, attach the role to the EC2 instance, and configure a cron job to start and stop the EC2 instance following the desired schedule.",
        "explanation": "This option mentions starting and stopping an EC2 instance, not an RDS instance. Hence, it's not a solution for the given requirement."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/database/schedule-amazon-rds-stop-and-start-using-aws-lambda"
    ]
  },
  {
    "id": 38,
    "question": "An application is hosted on Amazon EC2 instances within private subnets and requires access to an Amazon S3 bucket for object storage.\n\nWhat is the MOST secure way to enable access to the S3 bucket while ensuring that the traffic does not leave the AWS network?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use an internet gateway attached to the VPC.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use a NAT gateway in a public subnet.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use a VPC endpoint for S3.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use a NAT instance in a private subnet.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nUse a VPC endpoint for S3.\n\nVPC endpoints allow private connectivity between your VPC and supported AWS services. In this case, a VPC endpoint for S3 would enable secure, private communication between the EC2 instances in the VPC and the S3 bucket, all within the AWS network. This is the most secure method as it does not require internet access and thus reduces exposure to external threats.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse an internet gateway attached to the VPC.\n\nThis method would allow the EC2 instances to access the S3 bucket over the public internet, which is not the most secure method and does not meet the requirement to keep traffic within the AWS network.\n\n\n\n\nUse a NAT gateway in a public subnet.\n\nNAT gateways allow instances in a private subnet to access the internet, but they don't allow inbound connections initiated by external entities. While this would provide access to S3, it would still involve sending traffic over the public internet, which is not as secure as a VPC endpoint and does not keep traffic within the AWS network.\n\n\n\n\nUse a NAT instance in a private subnet.\n\nLike a NAT gateway, a NAT instance provides instances in a private subnet with internet access but does not allow inbound connections. This method would not keep traffic within the AWS network and is not the most secure option.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/\n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use a VPC endpoint for S3.",
        "explanation": "VPC endpoints allow private connectivity between your VPC and supported AWS services. In this case, a VPC endpoint for S3 would enable secure, private communication between the EC2 instances in the VPC and the S3 bucket, all within the AWS network. This is the most secure method as it does not require internet access and thus reduces exposure to external threats."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use an internet gateway attached to the VPC.",
        "explanation": "This method would allow the EC2 instances to access the S3 bucket over the public internet, which is not the most secure method and does not meet the requirement to keep traffic within the AWS network."
      },
      {
        "answer": "Use a NAT gateway in a public subnet.",
        "explanation": "NAT gateways allow instances in a private subnet to access the internet, but they don't allow inbound connections initiated by external entities. While this would provide access to S3, it would still involve sending traffic over the public internet, which is not as secure as a VPC endpoint and does not keep traffic within the AWS network."
      },
      {
        "answer": "Use a NAT instance in a private subnet.",
        "explanation": "Like a NAT gateway, a NAT instance provides instances in a private subnet with internet access but does not allow inbound connections. This method would not keep traffic within the AWS network and is not the most secure option."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/",
      "https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html"
    ]
  },
  {
    "id": 39,
    "question": "A financial firm operates a data processing application on a group of Amazon EC2 On-Demand Instances behind an Application Load Balancer. The application experiences heavy workload during the first three business days of each month, moderate and steady usage for the rest of the month, and minimal usage over the weekends.\n\nThe firm aims to reduce its EC2 expenses without compromising the application's availability.\n\nWhich option would best meet these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Reserved Instances for the baseline level of usage, and use Spot Instances for any extra capacity needed by the application.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Dedicated Instances for the baseline level of usage, and use On-Demand Instances for any extra capacity needed by the application.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use On-Demand Instances for the baseline level of usage, and use Spot Instances for any extra capacity needed by the application.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Spot Instances for the entire workload.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nUse Reserved Instances for the baseline level of usage, and use Spot Instances for any extra capacity needed by the application.\n\nReserved Instances provide a significant discount (up to 75%) compared to On-Demand instance pricing and are best for steady-state usage. Spot Instances allow you to take advantage of unused EC2 capacity in the AWS cloud and are available at up to a 90% discount compared to On-Demand prices. For applications with flexible start and end times, or that are only feasible at very low compute prices, Spot Instances can significantly optimize costs. This approach is the most cost-effective.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Spot Instances for the entire workload.\n\nWhile Spot Instances are cost-effective, they are not guaranteed to always be available, which can compromise the application's availability.\n\n\n\n\nUse On-Demand Instances for the baseline level of usage, and use Spot Instances for any extra capacity needed by the application.\n\nWhile this solution may work, it wouldn't be as cost-effective, where Reserved Instances are used for the baseline level of usage.\n\n\n\n\nUse Dedicated Instances for the baseline level of usage, and use On-Demand Instances for any extra capacity needed by the application.\n\nDedicated Instances are typically more expensive and don't provide the cost savings that Reserved Instances do. This option would not be as cost-effective.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/ec2/pricing/",
    "correctAnswerExplanations": [
      {
        "answer": "Use Reserved Instances for the baseline level of usage, and use Spot Instances for any extra capacity needed by the application.",
        "explanation": "Reserved Instances provide a significant discount (up to 75%) compared to On-Demand instance pricing and are best for steady-state usage. Spot Instances allow you to take advantage of unused EC2 capacity in the AWS cloud and are available at up to a 90% discount compared to On-Demand prices. For applications with flexible start and end times, or that are only feasible at very low compute prices, Spot Instances can significantly optimize costs. This approach is the most cost-effective."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Spot Instances for the entire workload.",
        "explanation": "While Spot Instances are cost-effective, they are not guaranteed to always be available, which can compromise the application's availability."
      },
      {
        "answer": "Use On-Demand Instances for the baseline level of usage, and use Spot Instances for any extra capacity needed by the application.",
        "explanation": "While this solution may work, it wouldn't be as cost-effective, where Reserved Instances are used for the baseline level of usage."
      },
      {
        "answer": "Use Dedicated Instances for the baseline level of usage, and use On-Demand Instances for any extra capacity needed by the application.",
        "explanation": "Dedicated Instances are typically more expensive and don't provide the cost savings that Reserved Instances do. This option would not be as cost-effective."
      }
    ],
    "references": [
      "https://aws.amazon.com/ec2/pricing/"
    ]
  },
  {
    "id": 40,
    "question": "A company is using Amazon RDS instances for its applications. The company currently backs up RDS snapshots to the same AWS Region where the RDS instances were created. The company wants to design a system that captures AWS API calls and sends alerts whenever the Amazon RDS CreateDBSnapshot API operation is invoked within the company's account.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the CreateDBSnapshot API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a CreateDBSnapshot API call is detected.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic when a CreateDBSnapshot API call is detected.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that triggers when updated logs are delivered to Amazon S3. Use Amazon Athena to create a new table and to query on CreateDBSnapshot when an API call is detected.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Develop an AWS Lambda function to inspect AWS CloudTrail logs and to send an alert when a CreateDBSnapshot API call is detected.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nCreate an Amazon EventBridge (Amazon CloudWatch Events) rule for the CreateDBSnapshot API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a CreateDBSnapshot API call is detected.\n\nAmazon EventBridge allows for easy monitoring and event-driven workflows by capturing events from various AWS services. By creating an EventBridge rule specifically for the CreateDBSnapshot API call, the company can detect and respond to this specific event.\n\nConfiguring the target of the EventBridge rule as an Amazon SNS topic enables sending alerts when the CreateDBSnapshot API call is detected. Amazon SNS is a fully managed pub/sub messaging service that supports sending notifications via various channels, including email, SMS, and HTTP endpoints.\n\nThis solution minimizes operational overhead as it leverages existing AWS services (EventBridge and SNS) to handle the event detection and alerting. It does not require the development and management of custom Lambda functions or additional data processing layers.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDevelop an AWS Lambda function to inspect AWS CloudTrail logs and to send an alert when a CreateDBSnapshot API call is detected.\n\nThis is not the most efficient solution as it involves querying CloudTrail logs, which can be operationally complex and time-consuming.\n\n\n\n\nSet up AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that triggers when updated logs are delivered to Amazon S3. Use Amazon Athena to create a new table and to query on CreateDBSnapshot when an API call is detected.\n\nThis solution is operationally complex as it requires setting up an Athena table and querying it each time new logs arrive, which is not efficient for real-time alerting.\n\n\n\n\nConfigure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic when a CreateDBSnapshot API call is detected.\n\nThis solution is unnecessarily complex for this requirement. SQS is typically used for decoupling components in a distributed system, which is not required here.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/monitor-ami-events.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the CreateDBSnapshot API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a CreateDBSnapshot API call is detected.",
        "explanation": "Amazon EventBridge allows for easy monitoring and event-driven workflows by capturing events from various AWS services. By creating an EventBridge rule specifically for the CreateDBSnapshot API call, the company can detect and respond to this specific event."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Develop an AWS Lambda function to inspect AWS CloudTrail logs and to send an alert when a CreateDBSnapshot API call is detected.",
        "explanation": "This is not the most efficient solution as it involves querying CloudTrail logs, which can be operationally complex and time-consuming."
      },
      {
        "answer": "Set up AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that triggers when updated logs are delivered to Amazon S3. Use Amazon Athena to create a new table and to query on CreateDBSnapshot when an API call is detected.",
        "explanation": "This solution is operationally complex as it requires setting up an Athena table and querying it each time new logs arrive, which is not efficient for real-time alerting."
      },
      {
        "answer": "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic when a CreateDBSnapshot API call is detected.",
        "explanation": "This solution is unnecessarily complex for this requirement. SQS is typically used for decoupling components in a distributed system, which is not required here."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/monitor-ami-events.html"
    ]
  },
  {
    "id": 41,
    "question": "A media company is developing a video processing application on-premises and plans to transition this application to AWS. The application is expected to serve millions of users soon after deployment. The company is uncertain about managing the large-scale deployment of containers. The company needs to deploy the containerized application in a highly resilient architecture with minimal operational overhead.\n\nWhich solution is MOST appropriate for these requirements?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 for running the containers. Implement target tracking for automatic scaling based on demand.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with AWS Fargate for running the containers. Implement target tracking for automatic scaling based on demand.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an Amazon EC2 Amazon Machine Image (AMI) that includes the container image. Launch EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon CloudWatch alarm to scale out EC2 instances when the average CPU utilization threshold is breached.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Store container images in a repository hosted on an Amazon EC2 instance. Run the containers on EC2 instances distributed across multiple Availability Zones. Monitor average CPU utilization using Amazon CloudWatch and manually launch new EC2 instances as required.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nStore container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with AWS Fargate for running the containers. Implement target tracking for automatic scaling based on demand.\n\nAmazon ECR is a fully managed container registry that makes it easy to store, manage, share, and deploy container images. Amazon EKS runs the Kubernetes management infrastructure across multiple AWS Availability Zones, ensuring high availability and eliminating a single point of failure. AWS Fargate is a serverless compute engine for containers that works with both Amazon ECS and Amazon EKS. Fargate removes the need to provision and manage servers, letting you focus on designing and building your applications. Lastly, target tracking scaling policies for Amazon ECS services increase or decrease the desired count of tasks in your service based on a target value for a specific CloudWatch metric. This solution meets all the company's needs.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStore container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 for running the containers. Implement target tracking for automatic scaling based on demand.\n\nThis option would require additional operational management because using Amazon EC2 for running containers would require managing the server infrastructure, which the company wants to avoid.\n\n\n\n\nStore container images in a repository hosted on an Amazon EC2 instance. Run the containers on EC2 instances distributed across multiple Availability Zones. Monitor average CPU utilization using Amazon CloudWatch and manually launch new EC2 instances as required.\n\nThis solution is not appropriate because it introduces significant manual overhead and does not utilize container orchestration services such as Amazon ECS or Amazon EKS, which are specifically designed to manage containerized applications.\n\n\n\n\nCreate an Amazon EC2 Amazon Machine Image (AMI) that includes the container image. Launch EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon CloudWatch alarm to scale out EC2 instances when the average CPU utilization threshold is breached.\n\nThis solution is also not suitable because it doesn't leverage AWS container orchestration services and could introduce unnecessary complexity and management overhead.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/ecr/\n\nhttps://aws.amazon.com/eks/\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/fargate-getting-started.html\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/userguide/service-autoscaling-targettracking.html",
    "correctAnswerExplanations": [
      {
        "answer": "Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with AWS Fargate for running the containers. Implement target tracking for automatic scaling based on demand.",
        "explanation": "Amazon ECR is a fully managed container registry that makes it easy to store, manage, share, and deploy container images. Amazon EKS runs the Kubernetes management infrastructure across multiple AWS Availability Zones, ensuring high availability and eliminating a single point of failure. AWS Fargate is a serverless compute engine for containers that works with both Amazon ECS and Amazon EKS. Fargate removes the need to provision and manage servers, letting you focus on designing and building your applications. Lastly, target tracking scaling policies for Amazon ECS services increase or decrease the desired count of tasks in your service based on a target value for a specific CloudWatch metric. This solution meets all the company's needs."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 for running the containers. Implement target tracking for automatic scaling based on demand.",
        "explanation": "This option would require additional operational management because using Amazon EC2 for running containers would require managing the server infrastructure, which the company wants to avoid."
      },
      {
        "answer": "Store container images in a repository hosted on an Amazon EC2 instance. Run the containers on EC2 instances distributed across multiple Availability Zones. Monitor average CPU utilization using Amazon CloudWatch and manually launch new EC2 instances as required.",
        "explanation": "This solution is not appropriate because it introduces significant manual overhead and does not utilize container orchestration services such as Amazon ECS or Amazon EKS, which are specifically designed to manage containerized applications."
      },
      {
        "answer": "Create an Amazon EC2 Amazon Machine Image (AMI) that includes the container image. Launch EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon CloudWatch alarm to scale out EC2 instances when the average CPU utilization threshold is breached.",
        "explanation": "This solution is also not suitable because it doesn't leverage AWS container orchestration services and could introduce unnecessary complexity and management overhead."
      }
    ],
    "references": [
      "https://aws.amazon.com/ecr/",
      "https://aws.amazon.com/eks/",
      "https://docs.aws.amazon.com/eks/latest/userguide/fargate-getting-started.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/userguide/service-autoscaling-targettracking.html"
    ]
  },
  {
    "id": 42,
    "question": "A digital media company hosts an online streaming service that uses AWS CloudFront and AWS Lambda@Edge. The service has recently experienced a surge in illegitimate requests from botnets.\n\nWhat measures should a solutions architect implement to prevent unauthorized access? (Select TWO.)",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an IAM role for every user trying to access the streaming service. Users will assume this role when requesting the service.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a rate limiting policy with an API key that is distributed solely to legitimate users.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Change the existing public content delivery network (CDN) to a private one. Update the DNS records to direct users to the new CDN endpoint.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Incorporate logic within the Lambda@Edge function to reject requests from known fraudulent IP addresses.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Implement an AWS Shield Advanced rule to target and filter out malicious requests.",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate a rate limiting policy with an API key that is distributed solely to legitimate users.\n\nThis method allows the company to control the number of requests each user can make, minimizing the impact of malicious requests. Sharing the API key only with legitimate users ensures that only these users can make requests to the service.\n\n\n\n\nImplement an AWS Shield Advanced rule to target and filter out malicious requests.\n\nAWS Shield Advanced offers cost-effective DDoS protection for applications running on AWS. By creating specific rules that target and filter out malicious requests, the architect can significantly reduce the effect of botnet attacks on the streaming service.\n\n\n\n\n\n\n\nIncorrect Options:\n\nIncorporate logic within the Lambda@Edge function to reject requests from known fraudulent IP addresses.\n\nWhile this approach could potentially help in blocking some fraudulent requests, it does not provide a comprehensive solution against distributed botnet attacks, which often originate from various IP addresses.\n\n\n\n\nChange the existing public content delivery network (CDN) to a private one. Update the DNS records to direct users to the new CDN endpoint.\n\nThis strategy would likely disrupt service for all users and would not necessarily solve the problem of fraudulent requests.\n\n\n\n\nCreate an IAM role for every user trying to access the streaming service. Users will assume this role when requesting the service.\n\nThis option is impractical and does not provide a viable defense against botnet attacks. Moreover, managing IAM roles for each user can become complex and hard to maintain.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/shield-policies.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a rate limiting policy with an API key that is distributed solely to legitimate users.",
        "explanation": "This method allows the company to control the number of requests each user can make, minimizing the impact of malicious requests. Sharing the API key only with legitimate users ensures that only these users can make requests to the service."
      },
      {
        "answer": "Implement an AWS Shield Advanced rule to target and filter out malicious requests.",
        "explanation": "AWS Shield Advanced offers cost-effective DDoS protection for applications running on AWS. By creating specific rules that target and filter out malicious requests, the architect can significantly reduce the effect of botnet attacks on the streaming service."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Incorporate logic within the Lambda@Edge function to reject requests from known fraudulent IP addresses.",
        "explanation": "While this approach could potentially help in blocking some fraudulent requests, it does not provide a comprehensive solution against distributed botnet attacks, which often originate from various IP addresses."
      },
      {
        "answer": "Change the existing public content delivery network (CDN) to a private one. Update the DNS records to direct users to the new CDN endpoint.",
        "explanation": "This strategy would likely disrupt service for all users and would not necessarily solve the problem of fraudulent requests."
      },
      {
        "answer": "Create an IAM role for every user trying to access the streaming service. Users will assume this role when requesting the service.",
        "explanation": "This option is impractical and does not provide a viable defense against botnet attacks. Moreover, managing IAM roles for each user can become complex and hard to maintain."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html",
      "https://docs.aws.amazon.com/waf/latest/developerguide/shield-policies.html"
    ]
  },
  {
    "id": 43,
    "question": "A media company uses Amazon Aurora MySQL DB cluster for storing user interaction data. They must retain all the data for 7 years and completely delete it post that. Additionally, they need to maintain logs of all database-related operations indefinitely. Currently, they have Aurora's automated backups enabled.\n\nWhich combination of actions should the solution architect take to fulfill these requirements? (Select TWO.)",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Backup for capturing the backups and retaining them for 7 years.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Configure Amazon CloudWatch Logs export for the DB cluster.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a lifecycle policy for the automated backups.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set the automated backup retention to 7 years.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Implement a manual snapshot of the DB cluster.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Options:\n\nConfigure Amazon CloudWatch Logs export for the DB cluster.\n\nCloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. By configuring Amazon CloudWatch Logs for the Aurora DB cluster, you can collect and store database activity logs indefinitely, fulfilling the audit requirement of the company.\n\n\n\n\nUse AWS Backup for capturing the backups and retaining them for 7 years.\n\nAWS Backup is a fully managed backup service that makes it easy to centralize and automate the backup of data across AWS services. By using AWS Backup, you can define backup policies, set the retention period as 7 years, and automate backup tasks. This will ensure that the user interaction data in the Aurora DB cluster is backed up and retained for the desired period.\n\n\n\n\n\n\n\nIncorrect Options:\n\nImplement a manual snapshot of the DB cluster.\n\nWhile manual snapshots can be a good way to capture the DB state at a certain point, they do not provide an automated, scheduled backup solution that the company needs for continuous data backup and retention over 7 years.\n\n\n\n\nCreate a lifecycle policy for the automated backups.\n\nAurora's automated backups do not support lifecycle policies. Lifecycle policies are typically associated with S3 and EBS snapshots, and they can't be applied to Aurora's automated backups.\n\n\n\n\nSet the automated backup retention to 7 years.\n\nAmazon Aurora's automated backup retention period can only be set up to a maximum of 35 days, so it's not possible to set it for 7 years to meet the company's data retention requirement.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.CloudWatch.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_PIT.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure Amazon CloudWatch Logs export for the DB cluster.",
        "explanation": "CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. By configuring Amazon CloudWatch Logs for the Aurora DB cluster, you can collect and store database activity logs indefinitely, fulfilling the audit requirement of the company."
      },
      {
        "answer": "Use AWS Backup for capturing the backups and retaining them for 7 years.",
        "explanation": "AWS Backup is a fully managed backup service that makes it easy to centralize and automate the backup of data across AWS services. By using AWS Backup, you can define backup policies, set the retention period as 7 years, and automate backup tasks. This will ensure that the user interaction data in the Aurora DB cluster is backed up and retained for the desired period."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Implement a manual snapshot of the DB cluster.",
        "explanation": "While manual snapshots can be a good way to capture the DB state at a certain point, they do not provide an automated, scheduled backup solution that the company needs for continuous data backup and retention over 7 years."
      },
      {
        "answer": "Create a lifecycle policy for the automated backups.",
        "explanation": "Aurora's automated backups do not support lifecycle policies. Lifecycle policies are typically associated with S3 and EBS snapshots, and they can't be applied to Aurora's automated backups."
      },
      {
        "answer": "Set the automated backup retention to 7 years.",
        "explanation": "Amazon Aurora's automated backup retention period can only be set up to a maximum of 35 days, so it's not possible to set it for 7 years to meet the company's data retention requirement."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.CloudWatch.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_PIT.html"
    ]
  },
  {
    "id": 44,
    "question": "A media company is moving their video processing application from an on-premises data center to Amazon EC2. A solutions architect needs to design a system for monitoring and alarming infrastructure metrics. If memory utilization exceeds 70% for a brief period, no action is required. However, if memory utilization exceeds 70% and network bandwidth is concurrently high, an immediate response is necessary. The solutions architect also needs to minimize false alerts.\n\nWhat should the solutions architect implement to fulfill these requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Implement Amazon CloudWatch Synthetics canaries to monitor the application and trigger an alarm.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up Amazon CloudWatch dashboards to visualize metrics and respond swiftly to issues.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create Amazon CloudWatch composite alarms where applicable.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use singular Amazon CloudWatch metric alarms with multiple metric thresholds where feasible.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nCreate Amazon CloudWatch composite alarms where applicable.\n\nAmazon CloudWatch composite alarms allow you to combine multiple metrics with arbitrary mathematical expressions to create an alarm condition. In this scenario, the composite alarm could be set to trigger when both memory utilization exceeds 70% and network bandwidth is concurrently high, which aligns with the requirement. This approach reduces the likelihood of false alarms as it only alerts when both conditions are met.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up Amazon CloudWatch dashboards to visualize metrics and respond swiftly to issues.\n\nWhile CloudWatch dashboards are useful for visualizing metrics, they do not automatically trigger alarms or actions when certain conditions are met.\n\n\n\n\nImplement Amazon CloudWatch Synthetics canaries to monitor the application and trigger an alarm.\n\nCloudWatch Synthetics is primarily used for creating canaries or scripts that simulate user flows and monitor application endpoints, but it does not handle infrastructure metrics like memory utilization or network bandwidth.\n\n\n\n\nUse singular Amazon CloudWatch metric alarms with multiple metric thresholds where feasible.\n\nWhile CloudWatch metric alarms can monitor a single metric, they cannot correlate between different metrics to trigger an alarm, which is a requirement in this case.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_Composite_Alarm.html\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Synthetics_Canaries.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create Amazon CloudWatch composite alarms where applicable.",
        "explanation": "Amazon CloudWatch composite alarms allow you to combine multiple metrics with arbitrary mathematical expressions to create an alarm condition. In this scenario, the composite alarm could be set to trigger when both memory utilization exceeds 70% and network bandwidth is concurrently high, which aligns with the requirement. This approach reduces the likelihood of false alarms as it only alerts when both conditions are met."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up Amazon CloudWatch dashboards to visualize metrics and respond swiftly to issues.",
        "explanation": "While CloudWatch dashboards are useful for visualizing metrics, they do not automatically trigger alarms or actions when certain conditions are met."
      },
      {
        "answer": "Implement Amazon CloudWatch Synthetics canaries to monitor the application and trigger an alarm.",
        "explanation": "CloudWatch Synthetics is primarily used for creating canaries or scripts that simulate user flows and monitor application endpoints, but it does not handle infrastructure metrics like memory utilization or network bandwidth."
      },
      {
        "answer": "Use singular Amazon CloudWatch metric alarms with multiple metric thresholds where feasible.",
        "explanation": "While CloudWatch metric alarms can monitor a single metric, they cannot correlate between different metrics to trigger an alarm, which is a requirement in this case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_Composite_Alarm.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Synthetics_Canaries.html"
    ]
  },
  {
    "id": 45,
    "question": "A financial firm recently transitioned to Amazon Aurora as the data storage solution for its global trading application. When comprehensive analytics are executed, developers report that the trading application's performance is suboptimal. Upon analyzing metrics in Amazon CloudWatch, a solutions architect discovers that the ReadIOPS and CPUUtilization metrics experience a surge when quarterly analytics are performed.\n\nWhat is the MOST cost-effective solution?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Shift the quarterly analytics workload to Amazon Redshift.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Upgrade the Aurora database to a larger instance class.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Transfer the quarterly analytics to an Aurora Replica.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Enhance the Provisioned IOPS on the Aurora instance.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nTransfer the quarterly analytics to an Aurora Replica.\n\nAurora Replicas provide a powerful, cost-effective solution for offloading read traffic from your primary database instance. By directing the data-intensive analytics workload to a read replica, you can free up resources on the primary Aurora instance, preventing it from becoming a performance bottleneck. This helps maintain the responsiveness of the trading application during high-load periods, like when quarterly analytics are run, without the need for expensive upgrades or increases in provisioned IOPS.\n\n\n\n\n\n\n\nIncorrect Options:\n\nShift the quarterly analytics workload to Amazon Redshift.\n\nAlthough Amazon Redshift is a powerful data warehousing solution ideal for complex analytical queries, migrating the analytics workload to Redshift might incur additional cost and complexity, making it less cost-effective than using an Aurora Replica.\n\n\n\n\nUpgrade the Aurora database to a larger instance class.\n\nWhile this could potentially alleviate the issue, it is not the most cost-effective solution. It would increase costs continuously, even though the performance issue only occurs when running quarterly analytics.\n\n\n\n\nEnhance the Provisioned IOPS on the Aurora instance.\n\nIncreasing the provisioned IOPS can be expensive and may not be necessary if the high IOPS and CPU utilization only occur during the quarterly analytics.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html",
    "correctAnswerExplanations": [
      {
        "answer": "Transfer the quarterly analytics to an Aurora Replica.",
        "explanation": "Aurora Replicas provide a powerful, cost-effective solution for offloading read traffic from your primary database instance. By directing the data-intensive analytics workload to a read replica, you can free up resources on the primary Aurora instance, preventing it from becoming a performance bottleneck. This helps maintain the responsiveness of the trading application during high-load periods, like when quarterly analytics are run, without the need for expensive upgrades or increases in provisioned IOPS."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Shift the quarterly analytics workload to Amazon Redshift.",
        "explanation": "Although Amazon Redshift is a powerful data warehousing solution ideal for complex analytical queries, migrating the analytics workload to Redshift might incur additional cost and complexity, making it less cost-effective than using an Aurora Replica."
      },
      {
        "answer": "Upgrade the Aurora database to a larger instance class.",
        "explanation": "While this could potentially alleviate the issue, it is not the most cost-effective solution. It would increase costs continuously, even though the performance issue only occurs when running quarterly analytics."
      },
      {
        "answer": "Enhance the Provisioned IOPS on the Aurora instance.",
        "explanation": "Increasing the provisioned IOPS can be expensive and may not be necessary if the high IOPS and CPU utilization only occur during the quarterly analytics."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html"
    ]
  },
  {
    "id": 46,
    "question": "A solutions architect is tasked with securely storing an API key that an application utilizes to communicate with a third-party service. The application is hosted on an Amazon EC2 instance. The solutions architect plans to leverage AWS Systems Manager Parameter Store for secure parameter storage.\n\nWhat should the solutions architect do to fulfill this requirement?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an IAM role with read access to the Parameter Store parameter, and grant Decrypt access to the AWS Key Management Service (AWS KMS) key used for encrypting the parameter. Associate this IAM role with the EC2 instance.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify the third-party service as a principal in the trust policy.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up an IAM trust relationship between the EC2 instance and the third-party service. Include Systems Manager as a principal in the trust policy.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an IAM policy that permits read access to the Parameter Store parameter, and allows Decrypt access to the AWS Key Management Service (AWS KMS) key employed for parameter encryption. Attach this IAM policy to the EC2 instance.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate an IAM role with read access to the Parameter Store parameter, and grant Decrypt access to the AWS Key Management Service (AWS KMS) key used for encrypting the parameter. Associate this IAM role with the EC2 instance.\n\nAWS Systems Manager Parameter Store allows secure storage of sensitive data such as API keys, passwords, and configuration settings. To access a parameter stored in Parameter Store, IAM roles or policies are used to provide controlled access.\n\nIn this case, creating an IAM role with read access to the Parameter Store parameter ensures that the EC2 instance can retrieve the API key. Granting Decrypt access to the AWS KMS key used for encryption ensures that the parameter is securely decrypted. By associating the IAM role with the EC2 instance, the instance can assume the role and access the API key securely.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an IAM policy that permits read access to the Parameter Store parameter, and allows Decrypt access to the AWS Key Management Service (AWS KMS) key employed for parameter encryption. Attach this IAM policy to the EC2 instance.\n\nYou can't directly attach IAM policies to EC2 instances. Hence, this option is incorrect.\n\n\n\n\nCreate an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify the third-party service as a principal in the trust policy.\n\nTrust relationships are used to establish trust between different entities, such as IAM roles and AWS services, but not AWS services, service parameters, or third-party services.\n\n\n\n\nSet up an IAM trust relationship between the EC2 instance and the third-party service. Include Systems Manager as a principal in the trust policy.\n\nTrust relationships are used to establish trust between different entities, such as IAM roles and AWS services, but not AWS services, EC2 instances, or third-party services.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create an IAM role with read access to the Parameter Store parameter, and grant Decrypt access to the AWS Key Management Service (AWS KMS) key used for encrypting the parameter. Associate this IAM role with the EC2 instance.",
        "explanation": "AWS Systems Manager Parameter Store allows secure storage of sensitive data such as API keys, passwords, and configuration settings. To access a parameter stored in Parameter Store, IAM roles or policies are used to provide controlled access."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create an IAM policy that permits read access to the Parameter Store parameter, and allows Decrypt access to the AWS Key Management Service (AWS KMS) key employed for parameter encryption. Attach this IAM policy to the EC2 instance.",
        "explanation": "You can't directly attach IAM policies to EC2 instances. Hence, this option is incorrect."
      },
      {
        "answer": "Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify the third-party service as a principal in the trust policy.",
        "explanation": "Trust relationships are used to establish trust between different entities, such as IAM roles and AWS services, but not AWS services, service parameters, or third-party services."
      },
      {
        "answer": "Set up an IAM trust relationship between the EC2 instance and the third-party service. Include Systems Manager as a principal in the trust policy.",
        "explanation": "Trust relationships are used to establish trust between different entities, such as IAM roles and AWS services, but not AWS services, EC2 instances, or third-party services."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html"
    ]
  },
  {
    "id": 47,
    "question": "A corporation uses Amazon S3 for data storage. They've recently onboarded a new client who will need to use FTPS to transfer data files. The task for a solutions architect is to design a highly available FTPS solution that minimizes operational effort.\n\nWhich solution will satisfy these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Transfer Family to set up an FTPS-enabled server with a publicly accessible endpoint. Set the S3 data storage as the destination.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Launch Amazon EC2 instances in a private subnet within a VPC. Place a Network Load Balancer (NLB) in front of the EC2 instances. Establish an FTPS listener port for the NLB. Share the NLB hostname with the new client. Run a cron job script on the EC2 instances to transfer files to the S3 data storage.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use an Amazon S3 File Gateway as an FTPS server. Make the S3 File Gateway endpoint URL available to the new client.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure an Amazon EC2 instance in a private subnet within a VPC. Instruct the new client to transfer files to the EC2 instance using a VPN. Execute a cron job script on the EC2 instance to move files to the S3 data storage.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nUse AWS Transfer Family to set up an FTPS-enabled server with a publicly accessible endpoint. Set the S3 data storage as the destination.\n\nAWS Transfer Family is a fully managed service that provides a secure way to transfer files over FTP, FTPS, and SFTP protocols. By using AWS Transfer Family, the corporation can set up an FTPS-enabled server with a publicly accessible endpoint, allowing the new client to securely transfer data files.\n\nThe advantage of using AWS Transfer Family is that it eliminates the need to provision and manage infrastructure for the FTPS server. It is a serverless solution that automatically scales to handle incoming file transfers, ensuring high availability without requiring operational effort.\n\nBy setting the S3 data storage as the destination for the file transfers, the corporation can leverage the durability, scalability, and low-cost storage provided by Amazon S3. The files transferred via FTPS can be directly stored in S3, simplifying the data storage process.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse an Amazon S3 File Gateway as an FTPS server. Make the S3 File Gateway endpoint URL available to the new client.\n\nAmazon S3 File Gateway does not support the FTPS protocol. It provides a seamless way to connect to the cloud to store application data files and backup images as durable objects on Amazon S3 cloud storage.\n\n\n\n\nConfigure an Amazon EC2 instance in a private subnet within a VPC. Instruct the new client to transfer files to the EC2 instance using a VPN. Execute a cron job script on the EC2 instance to move files to the S3 data storage.\n\nThis solution is operationally intensive and does not provide the high availability required. The EC2 instance would be a single point of failure, and the setup and maintenance of a VPN would add complexity.\n\n\n\n\nLaunch Amazon EC2 instances in a private subnet within a VPC. Place a Network Load Balancer (NLB) in front of the EC2 instances. Establish an FTPS listener port for the NLB. Share the NLB hostname with the new client. Run a cron job script on the EC2 instances to transfer files to the S3 data storage.\n\nThis solution involves more operational overhead than using the AWS Transfer Family, as it requires manual setup of EC2 instances, NLB, and cron jobs.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/aws-transfer-family/\n\nhttps://docs.aws.amazon.com/transfer/latest/userguide/create-server-ftps.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Transfer Family to set up an FTPS-enabled server with a publicly accessible endpoint. Set the S3 data storage as the destination.",
        "explanation": "AWS Transfer Family is a fully managed service that provides a secure way to transfer files over FTP, FTPS, and SFTP protocols. By using AWS Transfer Family, the corporation can set up an FTPS-enabled server with a publicly accessible endpoint, allowing the new client to securely transfer data files."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use an Amazon S3 File Gateway as an FTPS server. Make the S3 File Gateway endpoint URL available to the new client.",
        "explanation": "Amazon S3 File Gateway does not support the FTPS protocol. It provides a seamless way to connect to the cloud to store application data files and backup images as durable objects on Amazon S3 cloud storage."
      },
      {
        "answer": "Configure an Amazon EC2 instance in a private subnet within a VPC. Instruct the new client to transfer files to the EC2 instance using a VPN. Execute a cron job script on the EC2 instance to move files to the S3 data storage.",
        "explanation": "This solution is operationally intensive and does not provide the high availability required. The EC2 instance would be a single point of failure, and the setup and maintenance of a VPN would add complexity."
      },
      {
        "answer": "Launch Amazon EC2 instances in a private subnet within a VPC. Place a Network Load Balancer (NLB) in front of the EC2 instances. Establish an FTPS listener port for the NLB. Share the NLB hostname with the new client. Run a cron job script on the EC2 instances to transfer files to the S3 data storage.",
        "explanation": "This solution involves more operational overhead than using the AWS Transfer Family, as it requires manual setup of EC2 instances, NLB, and cron jobs."
      }
    ],
    "references": [
      "https://aws.amazon.com/aws-transfer-family/",
      "https://docs.aws.amazon.com/transfer/latest/userguide/create-server-ftps.html"
    ]
  },
  {
    "id": 48,
    "question": "A corporation intends to move its records to an Amazon DynamoDB table. The records must be encrypted in the table, and the encryption key should be automatically rotated each year.\n\nWhich solution will fulfill these requirements with the MINIMUM operational overhead?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the records to the DynamoDB table. Use server-side encryption with DynamoDB managed encryption keys (SSE-DynamoDB). Use the built-in key rotation feature of SSE-DynamoDB.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Encrypt the records with a customer-provided key before moving them to the DynamoDB table. Generate an AWS Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the DynamoDB table's default encryption behavior to use the customer managed KMS key. Migrate the records to the DynamoDB table.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Generate an AWS Key Management Service (AWS KMS) customer managed key. Set the DynamoDB table's default encryption behavior to use the customer managed KMS key. Transfer the records to the DynamoDB table. Manually rotate the KMS key every year.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nCreate an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the DynamoDB table's default encryption behavior to use the customer managed KMS key. Migrate the records to the DynamoDB table.\n\nCreate an AWS Key Management Service (AWS KMS) customer managed key, enable automatic key rotation, and set the DynamoDB table's default encryption behavior to use the customer managed KMS key. Then, migrate the records to the DynamoDB table.\n\nBy using a customer managed key in AWS KMS, the corporation has control over the encryption keys and can configure key rotation. Enabling automatic key rotation ensures that the encryption key used to encrypt the records in DynamoDB is automatically rotated each year, reducing operational effort.\n\nSetting the DynamoDB table's default encryption behavior to use the customer managed KMS key ensures that all records in the table are encrypted with the specified key. This provides a consistent and streamlined approach to encryption within DynamoDB.\n\n\n\n\n\n\n\nIncorrect Options:\n\nMigrate the records to the DynamoDB table. Use server-side encryption with DynamoDB managed encryption keys (SSE-DynamoDB). Use the built-in key rotation feature of SSE-DynamoDB.\n\nDynamoDB doesn't have a built-in key rotation feature.\n\nWhile server-side encryption with DynamoDB managed encryption keys (SSE-DynamoDB) provides encryption, it does not have a built-in key rotation feature. SSE-DynamoDB manages the encryption keys on behalf of the user, and key rotation is not explicitly supported.\n\n\n\n\nGenerate an AWS Key Management Service (AWS KMS) customer managed key. Set the DynamoDB table's default encryption behavior to use the customer managed KMS key. Transfer the records to the DynamoDB table. Manually rotate the KMS key every year.\n\nManually rotating the AWS KMS key every year introduces operational effort and does not align with the goal of minimizing operational overhead. The requirement specifies that key rotation should be automatic.\n\n\n\n\nEncrypt the records with a customer-provided key before moving them to the DynamoDB table. Generate an AWS Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation.\n\nEncrypting the records with a customer-provided key and then importing the key material into AWS KMS does not provide automatic key rotation. Additionally, generating an AWS KMS key without key material is not a valid approach. AWS KMS requires key material for key creation.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the DynamoDB table's default encryption behavior to use the customer managed KMS key. Migrate the records to the DynamoDB table.",
        "explanation": "Create an AWS Key Management Service (AWS KMS) customer managed key, enable automatic key rotation, and set the DynamoDB table's default encryption behavior to use the customer managed KMS key. Then, migrate the records to the DynamoDB table."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Migrate the records to the DynamoDB table. Use server-side encryption with DynamoDB managed encryption keys (SSE-DynamoDB). Use the built-in key rotation feature of SSE-DynamoDB.",
        "explanation": "DynamoDB doesn't have a built-in key rotation feature."
      },
      {
        "answer": "Generate an AWS Key Management Service (AWS KMS) customer managed key. Set the DynamoDB table's default encryption behavior to use the customer managed KMS key. Transfer the records to the DynamoDB table. Manually rotate the KMS key every year.",
        "explanation": "Manually rotating the AWS KMS key every year introduces operational effort and does not align with the goal of minimizing operational overhead. The requirement specifies that key rotation should be automatic."
      },
      {
        "answer": "Encrypt the records with a customer-provided key before moving them to the DynamoDB table. Generate an AWS Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation.",
        "explanation": "Encrypting the records with a customer-provided key and then importing the key material into AWS KMS does not provide automatic key rotation. Additionally, generating an AWS KMS key without key material is not a valid approach. AWS KMS requires key material for key creation."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html"
    ]
  },
  {
    "id": 49,
    "question": "A law firm needs to store case files. A case is usually active for 3 years. Throughout the 3-year duration, the firm needs to ensure that the files cannot be modified or removed. The firm also needs to secure the files at rest and automatically renew the encryption keys each year.\n\nWhich combination of actions should a solutions architect take to meet these requirements with the minimum operational effort? (Select TWO.)",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys. Enable key rotation.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Enable key rotation.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use server-side encryption with AWS Key Management Service (AWS KMS) customer provided (imported) keys. Enable key rotation.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Store the files in Amazon S3. Use S3 Object Lock in governance mode.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Store the files in Amazon S3. Use S3 Object Lock in compliance mode.",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nStore the files in Amazon S3. Use S3 Object Lock in compliance mode.\n\nBy storing the files in Amazon S3 and enabling S3 Object Lock in compliance mode, the law firm can prevent the files from being modified or deleted for the specified retention period. In compliance mode, the retention period cannot be shortened or modified by any user, including those with root or administrative access.\n\n\n\n\nUse server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys. Enable key rotation.\n\nServer-side encryption with AWS KMS customer managed keys ensures that the files are secured at rest. By enabling key rotation, the encryption keys used for encrypting the data in Amazon S3 are automatically renewed each year, meeting the requirement of automatically renewing the encryption keys.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStore the files in Amazon S3. Use S3 Object Lock in governance mode.\n\nGovernance mode allows users with specific IAM permissions to override the lock settings, which does not meet the firm's requirement of preventing modification or deletion.\n\n\n\n\nUse server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Enable key rotation.\n\nSSE-S3 does not support key rotation, therefore it does not meet the requirement for automatic annual key rotation.\n\n\n\n\nUse server-side encryption with AWS Key Management Service (AWS KMS) customer provided (imported) keys. Enable key rotation.\n\nCustomer provided keys (imported keys) in AWS KMS do not support automatic key rotation, therefore it does not meet the requirement for automatic annual key rotation.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html",
    "correctAnswerExplanations": [
      {
        "answer": "Store the files in Amazon S3. Use S3 Object Lock in compliance mode.",
        "explanation": "By storing the files in Amazon S3 and enabling S3 Object Lock in compliance mode, the law firm can prevent the files from being modified or deleted for the specified retention period. In compliance mode, the retention period cannot be shortened or modified by any user, including those with root or administrative access."
      },
      {
        "answer": "Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys. Enable key rotation.",
        "explanation": "Server-side encryption with AWS KMS customer managed keys ensures that the files are secured at rest. By enabling key rotation, the encryption keys used for encrypting the data in Amazon S3 are automatically renewed each year, meeting the requirement of automatically renewing the encryption keys."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Store the files in Amazon S3. Use S3 Object Lock in governance mode.",
        "explanation": "Governance mode allows users with specific IAM permissions to override the lock settings, which does not meet the firm's requirement of preventing modification or deletion."
      },
      {
        "answer": "Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Enable key rotation.",
        "explanation": "SSE-S3 does not support key rotation, therefore it does not meet the requirement for automatic annual key rotation."
      },
      {
        "answer": "Use server-side encryption with AWS Key Management Service (AWS KMS) customer provided (imported) keys. Enable key rotation.",
        "explanation": "Customer provided keys (imported keys) in AWS KMS do not support automatic key rotation, therefore it does not meet the requirement for automatic annual key rotation."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk",
      "https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html"
    ]
  },
  {
    "id": 50,
    "question": "A business operates a containerized application on a Docker Swarm cluster within its own data center. MongoDB is used for data storage. The business intends to migrate some of these environments to AWS, but no code alterations or deployment method changes are possible at this time. The business requires a solution that minimizes operational overhead.\n\nWhich solution meets these requirements?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and Amazon DynamoDB for data storage.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon DocumentDB for data storage.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for compute and Amazon DynamoDB for data storage.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for compute and MongoDB on EC2 for data storage.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option\n\nUse Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon DocumentDB for data storage.\n\nBy using EKS with Fargate, the company can migrate its Docker Swarm cluster to a Kubernetes cluster on AWS without making any code alterations. The operational overhead is minimized as AWS manages the infrastructure and scaling aspects of the cluster, allowing the business to focus on the application.\n\nFor data storage, Amazon DocumentDB is a fully managed MongoDB-compatible database service, can be used. This provides a seamless migration path as it is compatible with the existing MongoDB setup. The company can leverage the benefits of managed database service, such as automated backups, high availability, and scalability, without the need for manual management.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for compute and MongoDB on EC2 for data storage.\n\nUsing Amazon ECS with EC2 worker nodes and MongoDB on EC2 would require operational overhead for managing the underlying EC2 instances and MongoDB deployments.\n\n\n\n\nUse Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and Amazon DynamoDB for data storage.\n\nUsing Amazon ECS with AWS Fargate for compute and Amazon DynamoDB for data storage would require significant changes to the application, as DynamoDB is a NoSQL database and may not be compatible with the existing MongoDB data model.\n\n\n\n\nUse Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for compute and Amazon DynamoDB for data storage.\n\nUsing Amazon EKS with EC2 worker nodes and Amazon DynamoDB for data storage would introduce operational overhead for managing the EC2 instances and may not align with the requirement of minimizing operational overhead.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/fargate.html\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon DocumentDB for data storage.",
        "explanation": "By using EKS with Fargate, the company can migrate its Docker Swarm cluster to a Kubernetes cluster on AWS without making any code alterations. The operational overhead is minimized as AWS manages the infrastructure and scaling aspects of the cluster, allowing the business to focus on the application."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for compute and MongoDB on EC2 for data storage.",
        "explanation": "Using Amazon ECS with EC2 worker nodes and MongoDB on EC2 would require operational overhead for managing the underlying EC2 instances and MongoDB deployments."
      },
      {
        "answer": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and Amazon DynamoDB for data storage.",
        "explanation": "Using Amazon ECS with AWS Fargate for compute and Amazon DynamoDB for data storage would require significant changes to the application, as DynamoDB is a NoSQL database and may not be compatible with the existing MongoDB data model."
      },
      {
        "answer": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for compute and Amazon DynamoDB for data storage.",
        "explanation": "Using Amazon EKS with EC2 worker nodes and Amazon DynamoDB for data storage would introduce operational overhead for managing the EC2 instances and may not align with the requirement of minimizing operational overhead."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/eks/latest/userguide/fargate.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html"
    ]
  },
  {
    "id": 51,
    "question": "A digital media firm is developing a new streaming platform. The company aims to eliminate server maintenance and patching while ensuring high availability and fast scalability to meet variable user demands.\n\nWhich solution aligns with these requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Host all platform content on Amazon EC2 instances. Employ Auto Scaling to scale the EC2 instances. Use an Application Load Balancer for traffic distribution. Implement Amazon DynamoDB with provisioned write capacity for the database.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Store media files in Amazon S3. Manage media processing through Amazon API Gateway and AWS Lambda. Use Amazon Aurora with Aurora Auto Scaling for the database. Configure Amazon CloudFront for content delivery.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Host media files in Amazon S3. Handle media processing with Amazon API Gateway and AWS Lambda. Use Amazon DynamoDB with on-demand capacity for the database. Use Amazon CloudFront to distribute the content.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Host all platform content on Amazon EC2 instances. Implement an Auto Scaling group to scale the EC2 instances. Employ an Application Load Balancer for traffic distribution. Utilize Amazon Aurora with Aurora Auto Scaling for the database.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nHost media files in Amazon S3. Handle media processing with Amazon API Gateway and AWS Lambda. Use Amazon DynamoDB with on-demand capacity for the database. Use Amazon CloudFront to distribute the content.\n\nAmazon S3 is a highly scalable and durable object storage service that provides reliable storage for media files. It eliminates the need for server maintenance and patching, as S3 handles all infrastructure management.\n\nMedia processing can be handled through Amazon API Gateway, which acts as a secure entry point for the streaming platform. AWS Lambda, a serverless compute service, can be used to execute the media processing tasks without the need for managing servers.\n\nFor the database, Amazon DynamoDB with on-demand capacity is a suitable choice as it automatically scales to accommodate variable user demands. With on-demand capacity, DynamoDB handles the scalability and provisioning of resources transparently.\n\nTo distribute the content and ensure fast delivery, Amazon CloudFront, a content delivery network (CDN), can be configured. CloudFront caches the content at edge locations worldwide, providing low-latency access to users.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStore media files in Amazon S3. Manage media processing through Amazon API Gateway and AWS Lambda. Use Amazon Aurora with Aurora Auto Scaling for the database. Configure Amazon CloudFront for content delivery.\n\nAmazon Aurora Auto Scaling does not scale as quickly as Amazon DynamoDB's on-demand capacity, potentially leading to slower response times to changes in user demand.\n\n\n\n\nHost all platform content on Amazon EC2 instances. Employ Auto Scaling to scale the EC2 instances. Use an Application Load Balancer for traffic distribution. Implement Amazon DynamoDB with provisioned write capacity for the database.\n\nHosting all platform content on Amazon EC2 instances increases server maintenance and patching, conflicting with the requirement to minimize these tasks.\n\n\n\n\nHost all platform content on Amazon EC2 instances. Implement an Auto Scaling group to scale the EC2 instances. Employ an Application Load Balancer for traffic distribution. Utilize Amazon Aurora with Aurora Auto Scaling for the database.\n\nHosting all platform content on Amazon EC2 instances increases server maintenance and patching. Moreover, Aurora Auto Scaling may not respond to user demand changes as quickly as DynamoDB's on-demand capacity.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html\n\nhttps://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html",
    "correctAnswerExplanations": [
      {
        "answer": "Host media files in Amazon S3. Handle media processing with Amazon API Gateway and AWS Lambda. Use Amazon DynamoDB with on-demand capacity for the database. Use Amazon CloudFront to distribute the content.",
        "explanation": "Amazon S3 is a highly scalable and durable object storage service that provides reliable storage for media files. It eliminates the need for server maintenance and patching, as S3 handles all infrastructure management."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Store media files in Amazon S3. Manage media processing through Amazon API Gateway and AWS Lambda. Use Amazon Aurora with Aurora Auto Scaling for the database. Configure Amazon CloudFront for content delivery.",
        "explanation": "Amazon Aurora Auto Scaling does not scale as quickly as Amazon DynamoDB's on-demand capacity, potentially leading to slower response times to changes in user demand."
      },
      {
        "answer": "Host all platform content on Amazon EC2 instances. Employ Auto Scaling to scale the EC2 instances. Use an Application Load Balancer for traffic distribution. Implement Amazon DynamoDB with provisioned write capacity for the database.",
        "explanation": "Hosting all platform content on Amazon EC2 instances increases server maintenance and patching, conflicting with the requirement to minimize these tasks."
      },
      {
        "answer": "Host all platform content on Amazon EC2 instances. Implement an Auto Scaling group to scale the EC2 instances. Employ an Application Load Balancer for traffic distribution. Utilize Amazon Aurora with Aurora Auto Scaling for the database.",
        "explanation": "Hosting all platform content on Amazon EC2 instances increases server maintenance and patching. Moreover, Aurora Auto Scaling may not respond to user demand changes as quickly as DynamoDB's on-demand capacity."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/",
      "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html",
      "https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html"
    ]
  },
  {
    "id": 52,
    "question": "An organization is operating its applications on Amazon EC2 instances and storing data on Amazon S3 in a single AWS Region. The organization plans to keep a disaster recovery copy of its data in a separate Region.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Backup to copy EC2 instance backups and S3 backups to the separate Region.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create Amazon Elastic Block Store (Amazon EBS) snapshots. Copy the EBS snapshots to the separate Region. Configure S3 Cross-Region Replication (CRR) for data backup in the separate Region.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Generate Amazon Machine Images (AMIs) of the EC2 instances. Copy the AMIs to the separate Region. Configure S3 Cross-Region Replication (CRR) for data backup in the separate Region.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 instance backups and S3 backups to the separate Region.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nUse AWS Backup to copy EC2 instance backups and S3 backups to the separate Region.\n\nAWS Backup is a fully managed backup service that simplifies the process of backing up data across AWS services. It provides a centralized console to manage and automate backups, making it easier to implement disaster recovery strategies.\n\nBy using AWS Backup, the organization can create backup plans that include both EC2 instances and S3 buckets. These backup plans can then be configured to automatically copy the backups to a separate Region. This ensures that a disaster recovery copy of the data is maintained in a different Region, providing a higher level of data protection.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 instance backups and S3 backups to the separate Region.\n\nAmazon DLM only manages EBS snapshots and does not manage backups for S3, hence this option will not meet the requirement.\n\n\n\n\nGenerate Amazon Machine Images (AMIs) of the EC2 instances. Copy the AMIs to the separate Region. Configure S3 Cross-Region Replication (CRR) for data backup in the separate Region.\n\nThis option involves additional steps and overhead, such as creating AMIs and setting up S3 CRR, which is not as straightforward and automated as using AWS Backup.\n\n\n\n\nCreate Amazon Elastic Block Store (Amazon EBS) snapshots. Copy the EBS snapshots to the separate Region. Configure S3 Cross-Region Replication (CRR) for data backup in the separate Region.\n\nThis option also involves additional steps and operational overhead, such as manually creating and copying EBS snapshots and configuring S3 CRR.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Backup to copy EC2 instance backups and S3 backups to the separate Region.",
        "explanation": "AWS Backup is a fully managed backup service that simplifies the process of backing up data across AWS services. It provides a centralized console to manage and automate backups, making it easier to implement disaster recovery strategies."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 instance backups and S3 backups to the separate Region.",
        "explanation": "Amazon DLM only manages EBS snapshots and does not manage backups for S3, hence this option will not meet the requirement."
      },
      {
        "answer": "Generate Amazon Machine Images (AMIs) of the EC2 instances. Copy the AMIs to the separate Region. Configure S3 Cross-Region Replication (CRR) for data backup in the separate Region.",
        "explanation": "This option involves additional steps and overhead, such as creating AMIs and setting up S3 CRR, which is not as straightforward and automated as using AWS Backup."
      },
      {
        "answer": "Create Amazon Elastic Block Store (Amazon EBS) snapshots. Copy the EBS snapshots to the separate Region. Configure S3 Cross-Region Replication (CRR) for data backup in the separate Region.",
        "explanation": "This option also involves additional steps and operational overhead, such as manually creating and copying EBS snapshots and configuring S3 CRR."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html"
    ]
  },
  {
    "id": 53,
    "question": "A software firm is designing a blogging platform that will incorporate a load-balanced user interface and a NoSQL database. A solutions architect is tasked with designing a highly available solution with minimal manual management.\n\nWhich solutions fulfill these requirements? (Select TWO.)",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type to handle the dynamic application load.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Set up an Amazon EC2 instance-based Kubernetes cluster to handle the dynamic application load.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon RDS with a read replica in another Availability Zone for the NoSQL database.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon DynamoDB with global tables for the NoSQL database.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Deploy the application on AWS Lambda and configure it to scale automatically based on demand.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nUse Amazon DynamoDB with global tables for the NoSQL database.\n\nAmazon DynamoDB is a fully managed NoSQL database service that provides automatic scaling, high availability, and durability. Global tables in DynamoDB allow for multi-region replication, ensuring data availability in different geographic regions. By using global tables, the data is automatically replicated across multiple regions, providing a highly available and globally distributed database for the blogging platform.\n\n\n\n\nCreate an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type to handle the dynamic application load.\n\nAmazon ECS is a highly scalable container orchestration service. By using the Fargate launch type, the infrastructure management is abstracted, and containers are automatically provisioned and scaled based on demand. This eliminates the need for manual management of underlying EC2 instances, allowing for a highly available and scalable application infrastructure.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDeploy the application on AWS Lambda and configure it to scale automatically based on demand.\n\nDeploying the application on AWS Lambda and configuring it to scale automatically based on demand is suitable for serverless function-based applications. However, it does not suitable for a blogging platform web application.\n\n\n\n\nSet up an Amazon EC2 instance-based Kubernetes cluster to handle the dynamic application load.\n\nSetting up an Amazon EC2 instance-based Kubernetes cluster requires manual management of the infrastructure, which contradicts the requirement for minimal manual management. Additionally, it does not align with the serverless requirements of the solution.\n\n\n\n\nUse Amazon RDS with a read replica in another Availability Zone for the NoSQL database.\n\nUsing Amazon RDS with a read replica in another Availability Zone is a suitable solution for a relational database, but it does not address the requirement for a NoSQL database. Additionally, it does not align with the serverless and load-balanced user interface requirements of the solution.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/dynamodb/\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon DynamoDB with global tables for the NoSQL database.",
        "explanation": "Amazon DynamoDB is a fully managed NoSQL database service that provides automatic scaling, high availability, and durability. Global tables in DynamoDB allow for multi-region replication, ensuring data availability in different geographic regions. By using global tables, the data is automatically replicated across multiple regions, providing a highly available and globally distributed database for the blogging platform."
      },
      {
        "answer": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type to handle the dynamic application load.",
        "explanation": "Amazon ECS is a highly scalable container orchestration service. By using the Fargate launch type, the infrastructure management is abstracted, and containers are automatically provisioned and scaled based on demand. This eliminates the need for manual management of underlying EC2 instances, allowing for a highly available and scalable application infrastructure."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Deploy the application on AWS Lambda and configure it to scale automatically based on demand.",
        "explanation": "Deploying the application on AWS Lambda and configuring it to scale automatically based on demand is suitable for serverless function-based applications. However, it does not suitable for a blogging platform web application."
      },
      {
        "answer": "Set up an Amazon EC2 instance-based Kubernetes cluster to handle the dynamic application load.",
        "explanation": "Setting up an Amazon EC2 instance-based Kubernetes cluster requires manual management of the infrastructure, which contradicts the requirement for minimal manual management. Additionally, it does not align with the serverless requirements of the solution."
      },
      {
        "answer": "Use Amazon RDS with a read replica in another Availability Zone for the NoSQL database.",
        "explanation": "Using Amazon RDS with a read replica in another Availability Zone is a suitable solution for a relational database, but it does not address the requirement for a NoSQL database. Additionally, it does not align with the serverless and load-balanced user interface requirements of the solution."
      }
    ],
    "references": [
      "https://aws.amazon.com/dynamodb/",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html"
    ]
  },
  {
    "id": 54,
    "question": "A company runs its mission-critical web application on Amazon EC2 instances within a VPC. The instances are behind an Amazon CloudFront distribution. The company's new policy mandates that the application can only be accessed from a particular geographic region.\n\nWhich configuration should be adopted to enforce this requirement?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Update the Network ACL associated with the VPC subnet.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure AWS WAF on the CloudFront distribution.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Adjust the CloudFront distribution's security group settings.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Modify the security group associated with the EC2 instances.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nConfigure AWS WAF on the CloudFront distribution.\n\nAWS WAF (Web Application Firewall) can be used to create a geolocation rule that allows or blocks requests based on the country that the requests originated from. This would allow the company to restrict access to their application to a specific country. This aligns with the requirement, making it the correct option.\n\n\n\n\n\n\n\nIncorrect Options:\n\nModify the security group associated with the EC2 instances.\n\nSecurity groups act as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level. However, they cannot restrict traffic based on geographic location.\n\n\n\n\nUpdate the Network ACL associated with the VPC subnet.\n\nNetwork Access Control Lists (ACLs) provide a rule-based tool for controlling inbound and outbound traffic at the subnet level. However, similar to security groups, they do not have the capability to restrict traffic based on geographic location.\n\n\n\n\nConfigure the CloudFront distribution's security group settings.\n\nCloudFront does not use security groups. Instead, AWS WAF can be integrated with CloudFront to control access to the application.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure AWS WAF on the CloudFront distribution.",
        "explanation": "AWS WAF (Web Application Firewall) can be used to create a geolocation rule that allows or blocks requests based on the country that the requests originated from. This would allow the company to restrict access to their application to a specific country. This aligns with the requirement, making it the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Modify the security group associated with the EC2 instances.",
        "explanation": "Security groups act as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level. However, they cannot restrict traffic based on geographic location."
      },
      {
        "answer": "Update the Network ACL associated with the VPC subnet.",
        "explanation": "Network Access Control Lists (ACLs) provide a rule-based tool for controlling inbound and outbound traffic at the subnet level. However, similar to security groups, they do not have the capability to restrict traffic based on geographic location."
      },
      {
        "answer": "Configure the CloudFront distribution's security group settings.",
        "explanation": "CloudFront does not use security groups. Instead, AWS WAF can be integrated with CloudFront to control access to the application."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html"
    ]
  },
  {
    "id": 55,
    "question": "A health analytics firm hosts its application on AWS Cloud, generating approximately 500 MB of data each month in CSV format. The firm is considering a disaster recovery solution for data backup. The data must be retrievable within seconds if necessary and must be retained for 60 days.\n\nWhich solution would be the MOST cost-effective for these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon S3 Standard",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Amazon RDS for MySQL",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon S3 Glacier",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nAmazon S3 Standard\n\nAmazon S3 Standard provides high durability, availability, and performance object storage for frequently accessed data. Due to its design for 99.99% availability and millisecond latency, it is suitable for this use case. The cost-effectiveness of S3 Standard stems from its pay-as-you-go pricing, which is ideal for this scenario where data generation is not massive (around 500 MB per month). Moreover, the data retention requirement of 60 days can easily be managed by configuring S3 Lifecycle policies.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAmazon DynamoDB\n\nAlthough DynamoDB provides fast data access, it is primarily designed for NoSQL database use cases, not as a cost-effective storage solution for CSV data.\n\n\n\n\nAmazon S3 Glacier\n\nWhile S3 Glacier is cost-effective for long-term data storage, it doesn't provide millisecond access to data. Retrieval times range from a few minutes to several hours.\n\n\n\n\nAmazon RDS for MySQL\n\nAmazon RDS for MySQL is a relational database service, which is not the most cost-effective choice for storing CSV files that need to be accessed quickly. RDS is more suited for transactional workloads where the ability to perform complex queries is required.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes/\n\nhttps://aws.amazon.com/s3/pricing/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon S3 Standard",
        "explanation": "Amazon S3 Standard provides high durability, availability, and performance object storage for frequently accessed data. Due to its design for 99.99% availability and millisecond latency, it is suitable for this use case. The cost-effectiveness of S3 Standard stems from its pay-as-you-go pricing, which is ideal for this scenario where data generation is not massive (around 500 MB per month). Moreover, the data retention requirement of 60 days can easily be managed by configuring S3 Lifecycle policies."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Amazon DynamoDB",
        "explanation": "Although DynamoDB provides fast data access, it is primarily designed for NoSQL database use cases, not as a cost-effective storage solution for CSV data."
      },
      {
        "answer": "Amazon S3 Glacier",
        "explanation": "While S3 Glacier is cost-effective for long-term data storage, it doesn't provide millisecond access to data. Retrieval times range from a few minutes to several hours."
      },
      {
        "answer": "Amazon RDS for MySQL",
        "explanation": "Amazon RDS for MySQL is a relational database service, which is not the most cost-effective choice for storing CSV files that need to be accessed quickly. RDS is more suited for transactional workloads where the ability to perform complex queries is required."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/s3/pricing/"
    ]
  },
  {
    "id": 56,
    "question": "A company operates an application on a large cluster of Amazon EC2 instances. The application constantly retrieves and adds entries into an Amazon DynamoDB table. The DynamoDB table size continuously increases, but the application only requires data from the last 60 days. The company seeks a solution that reduces cost and development time.\n\nWhich solution meets these requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use an AWS CloudFormation template to set up the entire solution. Redeploy the CloudFormation stack every 60 days, and delete the original stack.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an EC2 instance running a tracking application from AWS Marketplace. Configure the tracking application to use Amazon DynamoDB Streams to store the timestamp when a new item is added in the table. Use a script on the EC2 instance to delete items older than 60 days based on the timestamp.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up Amazon DynamoDB Streams to trigger an AWS Lambda function when a new item is added in the table. Configure the Lambda function to remove items in the table that are older than 60 days.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Modify the application to include an attribute with a value of the current timestamp plus 60 days to each new item created in the table. Set up DynamoDB to use this attribute as the TTL attribute.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nModify the application to include an attribute with a value of the current timestamp plus 60 days to each new item created in the table. Set up DynamoDB to use this attribute as the TTL attribute.\n\nBy adding an attribute with a value of the current timestamp plus 60 days, the application effectively adds an expiry time to each item. DynamoDB's TTL attribute can then be used to automatically delete items after this expiry time, which reduces cost and requires minimal development effort.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse an AWS CloudFormation template to set up the entire solution. Redeploy the CloudFormation stack every 60 days, and delete the original stack.\n\nIt would involve unnecessary redeployment and deletion of the entire stack, not just the old items in the DynamoDB table.\n\n\n\n\nUse an EC2 instance running a tracking application from AWS Marketplace. Configure the tracking application to use Amazon DynamoDB Streams to store the timestamp when a new item is added in the table. Use a script on the EC2 instance to delete items older than 60 days based on the timestamp.\n\nThis option adds extra cost and complexity with the EC2 instance and tracking application and does not minimize development effort.\n\n\n\n\nSet up Amazon DynamoDB Streams to trigger an AWS Lambda function when a new item is added in the table. Configure the Lambda function to remove items in the table that are older than 60 days.\n\nIt would invoke the Lambda function for every new item, even though only a fraction of items would be older than 60 days.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html",
    "correctAnswerExplanations": [
      {
        "answer": "Modify the application to include an attribute with a value of the current timestamp plus 60 days to each new item created in the table. Set up DynamoDB to use this attribute as the TTL attribute.",
        "explanation": "By adding an attribute with a value of the current timestamp plus 60 days, the application effectively adds an expiry time to each item. DynamoDB's TTL attribute can then be used to automatically delete items after this expiry time, which reduces cost and requires minimal development effort."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use an AWS CloudFormation template to set up the entire solution. Redeploy the CloudFormation stack every 60 days, and delete the original stack.",
        "explanation": "It would involve unnecessary redeployment and deletion of the entire stack, not just the old items in the DynamoDB table."
      },
      {
        "answer": "Use an EC2 instance running a tracking application from AWS Marketplace. Configure the tracking application to use Amazon DynamoDB Streams to store the timestamp when a new item is added in the table. Use a script on the EC2 instance to delete items older than 60 days based on the timestamp.",
        "explanation": "This option adds extra cost and complexity with the EC2 instance and tracking application and does not minimize development effort."
      },
      {
        "answer": "Set up Amazon DynamoDB Streams to trigger an AWS Lambda function when a new item is added in the table. Configure the Lambda function to remove items in the table that are older than 60 days.",
        "explanation": "It would invoke the Lambda function for every new item, even though only a fraction of items would be older than 60 days."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"
    ]
  },
  {
    "id": 57,
    "question": "A multinational e-commerce business maintains an online store that offers a unique shopping experience by presenting a blend of static product images and dynamic customer reviews. The website communicates securely over HTTPS via an API server that operates on an Amazon EC2 instance, tucked behind an Application Load Balancer (ALB). The business aims to provide this blend of content to its international customer base with the minimum possible latency.\n\nWhat should a solutions architect do to design the system in a way that ensures the LOWEST latency for all customers?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Roll out the application stack across two AWS Regions. Use an Amazon Route 53 geolocation routing policy to distribute all content from the closest Regional ALB.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up the entire application stack in a single AWS Region. Employ Amazon CloudFront to disseminate the static content, while the dynamic content is served straight from the ALB.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Place the application stack across two AWS Regions. Implement an Amazon Route 53 latency routing policy to deliver all content from the nearest Regional ALB.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Launch the entire application stack in one AWS Region. Use Amazon CloudFront to distribute both static and dynamic content, designating the ALB as the source.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nLaunch the entire application stack in one AWS Region. Use Amazon CloudFront to distribute both static and dynamic content, designating the ALB as the source.\n\nCloudFront is a global Content Delivery Network (CDN) service that securely delivers data, videos, applications, and APIs to viewers with low latency and high transfer speeds. It can handle both static and dynamic content, caching the static content at edge locations closer to users, and maintaining persistent connections with the origin servers for dynamic content. This reduces the round-trip time, lowering latency. Using the ALB as the origin for CloudFront ensures that user requests are load-balanced across the EC2 instances, enhancing availability and fault tolerance.\n\n\n\n\n\n\n\nIncorrect Options:\n\nPlace the application stack across two AWS Regions. Implement an Amazon Route 53 latency routing policy to deliver all content from the nearest Regional ALB.\n\nLatency-based routing can help reduce latency, but it's not the most efficient solution for this use case. It involves maintaining two separate instances of the application stack, which may lead to increased complexity and cost.\n\n\n\n\nSet up the entire application stack in a single AWS Region. Employ Amazon CloudFront to disseminate the static content, while the dynamic content is served straight from the ALB.\n\nServing dynamic content directly from the ALB instead of using CloudFront can lead to higher latency for users who are geographically distant from the AWS Region where the ALB is deployed.\n\n\n\n\nRoll out the application stack across two AWS Regions. Use an Amazon Route 53 geolocation routing policy to distribute all content from the closest Regional ALB.\n\nThe geolocation routing policy increases complexity and cost by requiring the maintenance of the application stack in two separate regions. Furthermore, geolocation is not always the best indicator of the quickest route, and therefore may not always ensure the lowest latency.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
    "correctAnswerExplanations": [
      {
        "answer": "Launch the entire application stack in one AWS Region. Use Amazon CloudFront to distribute both static and dynamic content, designating the ALB as the source.",
        "explanation": "CloudFront is a global Content Delivery Network (CDN) service that securely delivers data, videos, applications, and APIs to viewers with low latency and high transfer speeds. It can handle both static and dynamic content, caching the static content at edge locations closer to users, and maintaining persistent connections with the origin servers for dynamic content. This reduces the round-trip time, lowering latency. Using the ALB as the origin for CloudFront ensures that user requests are load-balanced across the EC2 instances, enhancing availability and fault tolerance."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Place the application stack across two AWS Regions. Implement an Amazon Route 53 latency routing policy to deliver all content from the nearest Regional ALB.",
        "explanation": "Latency-based routing can help reduce latency, but it's not the most efficient solution for this use case. It involves maintaining two separate instances of the application stack, which may lead to increased complexity and cost."
      },
      {
        "answer": "Set up the entire application stack in a single AWS Region. Employ Amazon CloudFront to disseminate the static content, while the dynamic content is served straight from the ALB.",
        "explanation": "Serving dynamic content directly from the ALB instead of using CloudFront can lead to higher latency for users who are geographically distant from the AWS Region where the ALB is deployed."
      },
      {
        "answer": "Roll out the application stack across two AWS Regions. Use an Amazon Route 53 geolocation routing policy to distribute all content from the closest Regional ALB.",
        "explanation": "The geolocation routing policy increases complexity and cost by requiring the maintenance of the application stack in two separate regions. Furthermore, geolocation is not always the best indicator of the quickest route, and therefore may not always ensure the lowest latency."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html"
    ]
  },
  {
    "id": 58,
    "question": "A financial institution must preserve transaction records for a decade due to regulatory requirements. The institution's team regularly accesses records from the last 30 days for auditing, but seldom retrieves records older than this period. The system generates over 12 TB of transaction data every month.\n\nWhich storage solution would meet these requirements in the most cost-effective manner?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Store the records in Amazon S3 and employ S3 Lifecycle policies to transition records older than 1 month to S3 Glacier Deep Archive.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Keep the records in Amazon CloudWatch Logs and use AWS Backup to transition records older than 1 month to S3 Glacier Deep Archive.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Save the records in Amazon S3 and use AWS Backup to transfer records older than 1 month to S3 Glacier Deep Archive.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Keep the records in Amazon CloudWatch Logs and utilize Amazon S3 Lifecycle policies to transition records older than 1 month to S3 Glacier Deep Archive.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nStore the records in Amazon S3 and employ S3 Lifecycle policies to transition records older than 1 month to S3 Glacier Deep Archive.\n\nS3 provides a scalable and durable storage for web-scale computing. For long-term retention of rarely accessed data, S3 Glacier Deep Archive is the most cost-effective storage service. S3 Lifecycle policies automate the transition of data between storage classes based on the data's age, thereby reducing costs without compromising data availability.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSave the records in Amazon S3 and use AWS Backup to transfer records older than 1 month to S3 Glacier Deep Archive.\n\nWhile AWS Backup can be used for backup and restore, it's not as efficient or cost-effective as using S3 lifecycle policies for transitioning objects to Glacier.\n\n\n\n\nKeep the records in Amazon CloudWatch Logs and use AWS Backup to transition records older than 1 month to S3 Glacier Deep Archive.\n\nCloudWatch Logs is not designed to store data for extended periods, and it would be more expensive than using S3.\n\n\n\n\nKeep the records in Amazon CloudWatch Logs and utilize Amazon S3 Lifecycle policies to transition records older than 1 month to S3 Glacier Deep Archive.\n\nCloudWatch Logs are not designed for long-term data storage, and Amazon S3 Lifecycle policies cannot directly apply to CloudWatch Logs.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html",
    "correctAnswerExplanations": [
      {
        "answer": "Store the records in Amazon S3 and employ S3 Lifecycle policies to transition records older than 1 month to S3 Glacier Deep Archive.",
        "explanation": "S3 provides a scalable and durable storage for web-scale computing. For long-term retention of rarely accessed data, S3 Glacier Deep Archive is the most cost-effective storage service. S3 Lifecycle policies automate the transition of data between storage classes based on the data's age, thereby reducing costs without compromising data availability."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Save the records in Amazon S3 and use AWS Backup to transfer records older than 1 month to S3 Glacier Deep Archive.",
        "explanation": "While AWS Backup can be used for backup and restore, it's not as efficient or cost-effective as using S3 lifecycle policies for transitioning objects to Glacier."
      },
      {
        "answer": "Keep the records in Amazon CloudWatch Logs and use AWS Backup to transition records older than 1 month to S3 Glacier Deep Archive.",
        "explanation": "CloudWatch Logs is not designed to store data for extended periods, and it would be more expensive than using S3."
      },
      {
        "answer": "Keep the records in Amazon CloudWatch Logs and utilize Amazon S3 Lifecycle policies to transition records older than 1 month to S3 Glacier Deep Archive.",
        "explanation": "CloudWatch Logs are not designed for long-term data storage, and Amazon S3 Lifecycle policies cannot directly apply to CloudWatch Logs."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html"
    ]
  },
  {
    "id": 59,
    "question": "A bioinformatics company aims to leverage high-performance computing (HPC) infrastructure on AWS for genome sequencing analysis. The company's HPC workloads run on Linux. Each HPC workflow runs on hundreds of Amazon EC2 Spot Instances, is short-lived, and generates thousands of output files that are ultimately stored in persistent storage for further analysis and long-term use.\n\nThe company needs a cloud storage solution that allows the transfer of on-premises data to long-term persistent storage to ensure data accessibility for processing by all EC2 instances. The solution should also provide a high-performance file system that integrates with persistent storage to read and write datasets and output files.\n\nWhich combination of AWS services fulfills these requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon FSx for Windows File Server integrated with Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS).",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon FSx for Lustre integrated with Amazon S3.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp2) volume.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nAmazon FSx for Lustre integrated with Amazon S3.\n\nAmazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance storage for computing workloads like the HPC tasks the company needs to perform. Lustre is a type of parallel distributed file system, often used for large-scale cluster computing. Furthermore, FSx for Lustre is designed to work with Amazon S3, enabling easy and efficient data transfer between long-term storage (S3) and the high-performance file system, which is crucial for the company's workloads. Therefore, this option is ideal for the company's requirements.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAmazon FSx for Windows File Server integrated with Amazon S3.\n\nThis option is not suitable as the company's HPC workloads run on Linux, not Windows. Amazon FSx for Windows File Server is a fully managed native Microsoft Windows file system built on Windows Server.\n\n\n\n\nAmazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS).\n\nAmazon S3 Glacier is a low-cost storage service for data archiving and long-term backup, and is not designed for high-performance computing workloads. Also, it doesn't integrate directly with EBS for high-performance file operations.\n\n\n\n\nAmazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp2) volume.\n\nWhile Amazon S3 and EBS are robust AWS services, this option doesn't provide a high-performance file system like Lustre, which is specifically designed for high-performance, large-scale workloads.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/fsx/lustre/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon FSx for Lustre integrated with Amazon S3.",
        "explanation": "Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance storage for computing workloads like the HPC tasks the company needs to perform. Lustre is a type of parallel distributed file system, often used for large-scale cluster computing. Furthermore, FSx for Lustre is designed to work with Amazon S3, enabling easy and efficient data transfer between long-term storage (S3) and the high-performance file system, which is crucial for the company's workloads. Therefore, this option is ideal for the company's requirements."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Amazon FSx for Windows File Server integrated with Amazon S3.",
        "explanation": "This option is not suitable as the company's HPC workloads run on Linux, not Windows. Amazon FSx for Windows File Server is a fully managed native Microsoft Windows file system built on Windows Server."
      },
      {
        "answer": "Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS).",
        "explanation": "Amazon S3 Glacier is a low-cost storage service for data archiving and long-term backup, and is not designed for high-performance computing workloads. Also, it doesn't integrate directly with EBS for high-performance file operations."
      },
      {
        "answer": "Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp2) volume.",
        "explanation": "While Amazon S3 and EBS are robust AWS services, this option doesn't provide a high-performance file system like Lustre, which is specifically designed for high-performance, large-scale workloads."
      }
    ],
    "references": [
      "https://aws.amazon.com/fsx/lustre/"
    ]
  },
  {
    "id": 60,
    "question": "A firm is developing a serverless application using AWS Lambda and Amazon API Gateway. The application exposes several APIs to external users. The firm wants to shield the application from threats like Cross-Site Scripting (XSS) and also wants to identify and neutralize voluminous and complex DDoS attacks.\n\nWhich pair of solutions offers the HIGHEST degree of protection? (Select TWO.)",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Adopt AWS Shield Standard with AWS Lambda.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS Shield Advanced with Amazon API Gateway.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use AWS WAF to guard AWS Lambda.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Integrate Amazon GuardDuty with AWS Shield Standard.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Use AWS WAF to protect Amazon API Gateway.",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Options:\n\nUse AWS Shield Advanced with Amazon API Gateway.\n\nAWS Shield Advanced offers cost-effective, advanced protection against larger and more sophisticated DDoS attacks. Implementing Shield Advanced with API Gateway is a strong measure to safeguard the application from DDoS threats.\n\n\n\n\nUse AWS WAF to protect Amazon API Gateway.\n\nAWS WAF is a web application firewall service that helps protect your applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. Applying AWS WAF to API Gateway aids in mitigating threats like XSS.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS WAF to guard AWS Lambda.\n\nAWS WAF cannot be directly implemented with AWS Lambda. It is usually used with services like Amazon CloudFront or Amazon API Gateway. Hence, this option is incorrect.\n\n\n\n\nIntegrate Amazon GuardDuty with AWS Shield Standard.\n\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior. However, it doesn't offer direct protection against DDoS attacks or web exploits like XSS. Thus, this option is incorrect.\n\n\n\n\nAdopt AWS Shield Standard with AWS Lambda.\n\nAWS Shield Standard cannot be directly used with AWS Lambda. It is commonly used with resources like Elastic Load Balancers, Amazon CloudFront distributions, or Amazon Route 53 hosted zones. Therefore, this option is incorrect.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/shield/features/\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-aws-waf.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Shield Advanced with Amazon API Gateway.",
        "explanation": "AWS Shield Advanced offers cost-effective, advanced protection against larger and more sophisticated DDoS attacks. Implementing Shield Advanced with API Gateway is a strong measure to safeguard the application from DDoS threats."
      },
      {
        "answer": "Use AWS WAF to protect Amazon API Gateway.",
        "explanation": "AWS WAF is a web application firewall service that helps protect your applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. Applying AWS WAF to API Gateway aids in mitigating threats like XSS."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use AWS WAF to guard AWS Lambda.",
        "explanation": "AWS WAF cannot be directly implemented with AWS Lambda. It is usually used with services like Amazon CloudFront or Amazon API Gateway. Hence, this option is incorrect."
      },
      {
        "answer": "Integrate Amazon GuardDuty with AWS Shield Standard.",
        "explanation": "Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior. However, it doesn't offer direct protection against DDoS attacks or web exploits like XSS. Thus, this option is incorrect."
      },
      {
        "answer": "Adopt AWS Shield Standard with AWS Lambda.",
        "explanation": "AWS Shield Standard cannot be directly used with AWS Lambda. It is commonly used with resources like Elastic Load Balancers, Amazon CloudFront distributions, or Amazon Route 53 hosted zones. Therefore, this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/shield/features/",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-aws-waf.html"
    ]
  },
  {
    "id": 61,
    "question": "A company is looking to minimize the cost of its existing multi-environment web application. The web, app, and database servers are hosted on Amazon EC2 instances across development, staging, and production environments. The EC2 instances average 25% CPU utilization during peak hours and 5% CPU utilization during off-peak hours. The production EC2 instances run 24/7, while development and staging EC2 instances operate for at least 10 hours daily. The company aims to automate stopping the development and staging EC2 instances when not in use.\n\nWhich EC2 instance purchasing option will be the MOST cost-effective for the company's needs?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development and staging EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Spot blocks for the production EC2 instances. Use Scheduled Reserved Instances for the development and staging EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Spot Instances for the production EC2 instances. Use Scheduled Reserved Instances for the development and staging EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use On-Demand Instances for the production EC2 instances. Use Spot Instances for the development and staging EC2 instances.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Option:\n\nUse Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development and staging EC2 instances.\n\nThe Reserved Instances offer significant discounts for long-term, steady-state workloads like production environments that run 24/7. On-Demand Instances provide flexibility for development and staging environments, allowing the company to pay only for the compute capacity they use, without any long-term commitment. This option is the most cost-effective solution for our case.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Spot Instances for the production EC2 instances. Use Scheduled Reserved Instances for the development and staging EC2 instances.\n\nSpot Instances are not suitable for production workloads that require constant uptime, as they can be terminated when the Spot price exceeds the maximum price.\n\n\n\n\nUse Spot blocks for the production EC2 instances. Use Scheduled Reserved Instances for the development and staging EC2 instances.\n\nSpot blocks still have the risk of being interrupted if the capacity is no longer available, making them unsuitable for production environments requiring constant uptime.\n\n\n\n\nUse On-Demand Instances for the production EC2 instances. Use Spot Instances for the development and staging EC2 instances.\n\nUsing On-Demand Instances for production workloads would not provide the cost savings that Reserved Instances offer for long-term, steady-state workloads.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/",
    "correctAnswerExplanations": [
      {
        "answer": "Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development and staging EC2 instances.",
        "explanation": "The Reserved Instances offer significant discounts for long-term, steady-state workloads like production environments that run 24/7. On-Demand Instances provide flexibility for development and staging environments, allowing the company to pay only for the compute capacity they use, without any long-term commitment. This option is the most cost-effective solution for our case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Spot Instances for the production EC2 instances. Use Scheduled Reserved Instances for the development and staging EC2 instances.",
        "explanation": "Spot Instances are not suitable for production workloads that require constant uptime, as they can be terminated when the Spot price exceeds the maximum price."
      },
      {
        "answer": "Use Spot blocks for the production EC2 instances. Use Scheduled Reserved Instances for the development and staging EC2 instances.",
        "explanation": "Spot blocks still have the risk of being interrupted if the capacity is no longer available, making them unsuitable for production environments requiring constant uptime."
      }
    ],
    "references": [
      "https://aws.amazon.com/ec2/pricing/reserved-instances/"
    ]
  },
  {
    "id": 62,
    "question": "An organization maintains a system that generates time-series data. The organization wants to leverage AWS to process the time-series data in the order it is produced. They also aim to keep operational management to a minimum. How should a solutions architect design this?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing data payloads. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a subscriber.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an Amazon Kinesis Data Stream to store the time-series data. Configure an AWS Lambda function to process data from the stream.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Set up an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing data payloads. Configure an AWS Lambda function as a subscriber.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up an Amazon Simple Queue Service (Amazon SQS) standard queue to hold the time-series data. Arrange an AWS Lambda function to process data from the queue.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nUse an Amazon Kinesis Data Stream to store the time-series data. Configure an AWS Lambda function to process data from the stream.\n\nThis solution maintains the order of the time-series data as it is processed. Amazon Kinesis Data Streams can capture, store, and process streaming data, including time-series data, and maintain the order of the data. AWS Lambda can be used to process the data in the stream, providing a serverless and low operational overhead solution.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing data payloads. Configure an AWS Lambda function as a subscriber.\n\nWhile SNS with Lambda can process messages, it does not guarantee order, which is a requirement in this case.\n\n\n\n\nSet up an Amazon Simple Queue Service (Amazon SQS) standard queue to hold the time-series data. Arrange an AWS Lambda function to process data from the queue.\n\nWhile SQS with Lambda can process messages, a standard SQS queue does not guarantee order of processing.\n\n\n\n\nSet up an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing data payloads. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a subscriber.\n\nThis option does not maintain the order of data processing, and also adds an unnecessary layer of complexity by involving both SNS and SQS.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use an Amazon Kinesis Data Stream to store the time-series data. Configure an AWS Lambda function to process data from the stream.",
        "explanation": "This solution maintains the order of the time-series data as it is processed. Amazon Kinesis Data Streams can capture, store, and process streaming data, including time-series data, and maintain the order of the data. AWS Lambda can be used to process the data in the stream, providing a serverless and low operational overhead solution."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing data payloads. Configure an AWS Lambda function as a subscriber.",
        "explanation": "While SNS with Lambda can process messages, it does not guarantee order, which is a requirement in this case."
      },
      {
        "answer": "Set up an Amazon Simple Queue Service (Amazon SQS) standard queue to hold the time-series data. Arrange an AWS Lambda function to process data from the queue.",
        "explanation": "While SQS with Lambda can process messages, a standard SQS queue does not guarantee order of processing."
      },
      {
        "answer": "Set up an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing data payloads. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a subscriber.",
        "explanation": "This option does not maintain the order of data processing, and also adds an unnecessary layer of complexity by involving both SNS and SQS."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html"
    ]
  },
  {
    "id": 63,
    "question": "A solutions architect is tasked with designing a solution that leverages Amazon CloudFront with Amazon S3 as the origin to host an e-commerce website. The company's security guidelines stipulate that all incoming traffic must be evaluated by AWS WAF.\n\nWhat should the solutions architect do to meet these requirements?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up a security group permitting only Amazon CloudFront IP addresses to access Amazon S3. Link AWS WAF with CloudFront.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up Amazon CloudFront and Amazon S3 to utilize an origin access identity (OAI) to limit access to the S3 bucket. Enable AWS WAF on the CloudFront distribution.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Configure Amazon CloudFront to redirect all incoming traffic to AWS WAF before accessing content from the S3 origin.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up an S3 bucket policy that only accepts requests originating from the AWS WAF Amazon Resource Name (ARN).",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct Option:\n\nSet up Amazon CloudFront and Amazon S3 to utilize an origin access identity (OAI) to limit access to the S3 bucket. Enable AWS WAF on the CloudFront distribution.\n\nOrigin access identity (OAI) is a special CloudFront user that you can associate with your distribution and then use to restrict access to your S3 content. If you're using S3 as your origin, AWS recommends using an OAI. This is the best way to prevent users from bypassing CloudFront and accessing the content directly in S3. Enabling AWS WAF directly on the CloudFront distribution ensures that all incoming traffic to the site is inspected, as per the company's security policy.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up an S3 bucket policy that only accepts requests originating from the AWS WAF Amazon Resource Name (ARN).\n\nAWS WAF does not make requests to the S3 bucket; it only inspects incoming traffic. Also, AWS WAF doesn't have an ARN that can be used for S3 bucket policies.\n\n\n\n\nConfigure Amazon CloudFront to redirect all incoming traffic to AWS WAF before accessing content from the S3 origin.\n\nThis is incorrect because AWS WAF is directly integrated with CloudFront and doesn't require redirecting traffic.\n\n\n\n\nSet up a security group permitting only Amazon CloudFront IP addresses to access Amazon S3. Link AWS WAF with CloudFront.\n\nSecurity groups are associated with EC2 instances, not with S3 buckets or CloudFront distributions, so this option is not applicable in this case.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html",
    "correctAnswerExplanations": [
      {
        "answer": "Set up Amazon CloudFront and Amazon S3 to utilize an origin access identity (OAI) to limit access to the S3 bucket. Enable AWS WAF on the CloudFront distribution.",
        "explanation": "Origin access identity (OAI) is a special CloudFront user that you can associate with your distribution and then use to restrict access to your S3 content. If you're using S3 as your origin, AWS recommends using an OAI. This is the best way to prevent users from bypassing CloudFront and accessing the content directly in S3. Enabling AWS WAF directly on the CloudFront distribution ensures that all incoming traffic to the site is inspected, as per the company's security policy."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up an S3 bucket policy that only accepts requests originating from the AWS WAF Amazon Resource Name (ARN).",
        "explanation": "AWS WAF does not make requests to the S3 bucket; it only inspects incoming traffic. Also, AWS WAF doesn't have an ARN that can be used for S3 bucket policies."
      },
      {
        "answer": "Configure Amazon CloudFront to redirect all incoming traffic to AWS WAF before accessing content from the S3 origin.",
        "explanation": "This is incorrect because AWS WAF is directly integrated with CloudFront and doesn't require redirecting traffic."
      },
      {
        "answer": "Set up a security group permitting only Amazon CloudFront IP addresses to access Amazon S3. Link AWS WAF with CloudFront.",
        "explanation": "Security groups are associated with EC2 instances, not with S3 buckets or CloudFront distributions, so this option is not applicable in this case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html",
      "https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html"
    ]
  },
  {
    "id": 64,
    "question": "A media company has millions of active customers and processes thousands of orders daily. Orders data are stored in Amazon S3 while additional customers data stored in Amazon RDS.\n\nThe company aims to make all data accessible to different teams for analytical purposes. The solution should enable detailed permissions management for data and reduce operational overhead as much as possible.\n\nWhich solution meets these requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a data lake using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation and use Lake Formation access controls to manage access.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use an AWS Lambda function to regularly transfer data from Amazon RDS to Amazon S3. Create an AWS Glue crawler and Use Amazon Athena to query the data. Use S3 policies to manage access.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon Redshift cluster. Use an AWS Lambda function to regularly copy data from both Amazon S3 and Amazon RDS to Amazon Redshift. Employ Amazon Redshift access controls to manage access.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Move the Orders data to be written directly to Amazon RDS. Use RDS access controls to manage access.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Option:\n\nCreate a data lake using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation and use Lake Formation access controls to manage access.\n\nAWS Lake Formation provides a comprehensive solution for building and managing data lakes on AWS. By creating a data lake using Lake Formation, the media company can consolidate and organize their data from different sources, such as Amazon S3 and Amazon RDS.\n\nCreating an AWS Glue JDBC connection to Amazon RDS allows for seamless extraction of customer data from RDS into the data lake. The data can then be registered in Lake Formation, enabling easy access and management.\n\nWith AWS Lake Formation, the media company can apply granular access controls to the data lake, defining permissions and policies based on teams and roles. This ensures that different teams have appropriate access to the data for their analytical purposes, while also maintaining data security and governance.\n\n\n\n\n\n\n\nIncorrect Options:\n\nMove the Orders data to be written directly to Amazon RDS. Use RDS access controls to manage access.\n\nMoving the orders data to be written directly to Amazon RDS would not be suitable for analytical purposes and may not align with the goal of reducing operational overhead. RDS is optimized for transactional workloads, and accessing the data for analytical purposes would require additional processing.\n\n\n\n\nUse an AWS Lambda function to regularly transfer data from Amazon RDS to Amazon S3. Create an AWS Glue crawler and Use Amazon Athena to query the data. Use S3 policies to manage access.\n\nUsing an AWS Lambda function to transfer data from Amazon RDS to Amazon S3 and querying it with Amazon Athena does not provide a unified and managed solution for data access and permissions management. It requires additional orchestration and maintenance.\n\n\n\n\nCreate an Amazon Redshift cluster. Use an AWS Lambda function to regularly copy data from both Amazon S3 and Amazon RDS to Amazon Redshift. Employ Amazon Redshift access controls to manage access.\n\nCreating an Amazon Redshift cluster and using AWS Lambda to copy data from S3 and RDS to Redshift introduces additional operational overhead and complexity. It may not align with the goal of reducing operational overhead.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html\n\nhttps://docs.aws.amazon.com/glue/latest/dg/connection-properties.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a data lake using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation and use Lake Formation access controls to manage access.",
        "explanation": "AWS Lake Formation provides a comprehensive solution for building and managing data lakes on AWS. By creating a data lake using Lake Formation, the media company can consolidate and organize their data from different sources, such as Amazon S3 and Amazon RDS."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Move the Orders data to be written directly to Amazon RDS. Use RDS access controls to manage access.",
        "explanation": "Moving the orders data to be written directly to Amazon RDS would not be suitable for analytical purposes and may not align with the goal of reducing operational overhead. RDS is optimized for transactional workloads, and accessing the data for analytical purposes would require additional processing."
      },
      {
        "answer": "Use an AWS Lambda function to regularly transfer data from Amazon RDS to Amazon S3. Create an AWS Glue crawler and Use Amazon Athena to query the data. Use S3 policies to manage access.",
        "explanation": "Using an AWS Lambda function to transfer data from Amazon RDS to Amazon S3 and querying it with Amazon Athena does not provide a unified and managed solution for data access and permissions management. It requires additional orchestration and maintenance."
      },
      {
        "answer": "Create an Amazon Redshift cluster. Use an AWS Lambda function to regularly copy data from both Amazon S3 and Amazon RDS to Amazon Redshift. Employ Amazon Redshift access controls to manage access.",
        "explanation": "Creating an Amazon Redshift cluster and using AWS Lambda to copy data from S3 and RDS to Redshift introduces additional operational overhead and complexity. It may not align with the goal of reducing operational overhead."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html",
      "https://docs.aws.amazon.com/glue/latest/dg/connection-properties.html"
    ]
  },
  {
    "id": 65,
    "question": "A company has a customer management application that stores customer data in Amazon RDS for PostgreSQL. During regular office hours, employees execute ad hoc queries for data analysis purposes. However, these data analysis queries are causing disruptions to the routine operations of the application. The company needs to prevent these disruptions without inhibiting the ability of employees to perform these queries.\n\nWhat action should a solutions architect take to meet these requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Schedule the data analysis queries to be executed during off-peak hours.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Migrate the customer management application to Amazon DynamoDB with provisioned capacity.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a read replica and balance the customer management application load between the primary DB instance and the read replica.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a read replica and redirect data analysis queries to the read replica.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Option:\n\nCreate a read replica and redirect data analysis queries to the read replica.\n\nCreating a read replica of the primary database would allow the data analysis queries to be offloaded from the primary DB instance to the read replica. This would effectively separate the operational load from the analytical load, preventing the analytical queries from interfering with the operational functionality of the application.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a read replica and balance the customer management application load between the primary DB instance and the read replica.\n\nWhile this option does distribute load, it does not address the issue at hand. Read replicas are meant for read-only operations and should not be used to handle write operations of the application.\n\n\n\n\nMigrate the customer management application to Amazon DynamoDB with provisioned capacity.\n\nThis is not an optimal solution, as it involves significant changes to the application and does not directly address the issue of separating operational and analytical queries.\n\n\n\n\nSchedule the data analysis queries to be executed during off-peak hours.\n\nThis option could limit the flexibility of the employees who need to run the queries during business hours and might not solve the issue if the off-peak time is not properly defined.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a read replica and redirect data analysis queries to the read replica.",
        "explanation": "Creating a read replica of the primary database would allow the data analysis queries to be offloaded from the primary DB instance to the read replica. This would effectively separate the operational load from the analytical load, preventing the analytical queries from interfering with the operational functionality of the application."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a read replica and balance the customer management application load between the primary DB instance and the read replica.",
        "explanation": "While this option does distribute load, it does not address the issue at hand. Read replicas are meant for read-only operations and should not be used to handle write operations of the application."
      },
      {
        "answer": "Migrate the customer management application to Amazon DynamoDB with provisioned capacity.",
        "explanation": "This is not an optimal solution, as it involves significant changes to the application and does not directly address the issue of separating operational and analytical queries."
      },
      {
        "answer": "Schedule the data analysis queries to be executed during off-peak hours.",
        "explanation": "This option could limit the flexibility of the employees who need to run the queries during business hours and might not solve the issue if the off-peak time is not properly defined."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"
    ]
  }
]