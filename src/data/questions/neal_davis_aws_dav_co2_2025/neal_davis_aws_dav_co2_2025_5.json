[
  {
    "id": 1,
    "question": "<p>An international research organization holds a wide range of data across several Amazon S3 buckets. They recently received an alert indicating potential exposure of sensitive financial data via a public-facing web portal. The developer's job is to trace all potential data leakage points across their AWS infrastructure.</p><p>What is the most effective strategy for this task?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Manually inspect each S3 bucket and the data tables contained within to identify any potential financial data exposures.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement Amazon Macie and apply the SensitiveData:S3Object/financial finding type across all S3 buckets to automatically identify potential exposure of sensitive financial data.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Implement Amazon Macie and apply the SensitiveData:S3Object/personal finding type across all S3 buckets. This approach would identify personal data but may not effectively identify all potential exposure of sensitive financial data.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Utilize AWS CloudTrail logs to track activity and find potential data exposures, specifically focusing on financial data transactions.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect sensitive data in AWS. Using the <strong>SensitiveData:S3Object/financial</strong> finding type within Macie, it is possible to automatically detect and identify sensitive financial data in your S3 buckets.</p><p><strong>CORRECT: </strong>\"Implement Amazon Macie and apply the SensitiveData:S3Object/financial finding type across all S3 buckets to automatically identify potential exposure of sensitive financial data\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement Amazon Macie and apply the SensitiveData:S3Object/personal finding type across all S3 buckets. This approach would identify personal data but may not effectively identify all potential exposure of sensitive financial data\" is incorrect.</p><p>Using Amazon Macie with the <strong>SensitiveData:S3Object/personal</strong> finding type will only help in identifying potential exposure of sensitive personal data but not specifically financial data. It is not an effective solution for this specific requirement.</p><p><strong>INCORRECT:</strong> \"Manually inspect each S3 bucket and the data tables contained within to identify any potential financial data exposures\" is incorrect.</p><p>Manual inspection of each S3 bucket and data table would be time-consuming and prone to human error, especially when dealing with a large amount of data and numerous S3 buckets.</p><p><strong>INCORRECT:</strong> \"Utilize AWS CloudTrail logs to track activity and find potential data exposures, specifically focusing on financial data transactions\" is incorrect.</p><p>AWS CloudTrail logs would provide data about AWS account activity, but they are not specifically designed to detect sensitive data exposures.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/macie/latest/user/findings-types.html\">https://docs.aws.amazon.com/macie/latest/user/findings-types.html</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/macie/latest/user/findings-types.html"
    ]
  },
  {
    "id": 2,
    "question": "<p>A company needs to encrypt a large quantity of data. The data encryption keys must be generated from a dedicated, tamper-resistant hardware device.</p><p>To deliver these requirements, which AWS service should the company use?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS CloudHSM</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>AWS KMS</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS IAM</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Certificate Manager</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud.</p><p>A Hardware Security Module (HSM) provides secure key storage and cryptographic operations within a tamper-resistant hardware device. CloudHSM allows you to securely generate, store, and manage cryptographic keys used for data encryption in a way that keys are accessible only by you.</p><p><strong>CORRECT: </strong>\"AWS CloudHSM\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS KMS\" is incorrect as it uses shared infrastructure (multi-tenant) and is therefore not a dedicated HSM.</p><p><strong>INCORRECT:</strong> \"AWS Certificate Manager\" is incorrect as this is used to generate and manage SSL/TLS certificates, it does not generate data encryption keys.</p><p><strong>INCORRECT:</strong> \"AWS IAM\" is incorrect as this service is not involved with generating encryption keys.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudhsm/faqs/\">https://aws.amazon.com/cloudhsm/faqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudhsm/\">https://digitalcloud.training/aws-cloudhsm/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/cloudhsm/faqs/",
      "https://digitalcloud.training/aws-cloudhsm/"
    ]
  },
  {
    "id": 3,
    "question": "<p>Every time an Amazon EC2 instance is launched, certain metadata about the instance should be recorded in an Amazon DynamoDB table. The data is gathered and written to the table by an AWS Lambda function.</p><p>What is the MOST efficient method of invoking the Lambda function?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a CloudTrail trail alarm that triggers the Lambda function based on the <code>RunInstances</code> API action</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a CloudWatch alarm that triggers the Lambda function based on log streams indicating an EC2 state change in CloudWatch logs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a CloudWatch Event with an event pattern looking for EC2 state changes and a target set to use the Lambda function</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure detailed monitoring on Amazon EC2 and create an alarm that triggers the Lambda function in initialization</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. CloudWatch Events becomes aware of operational changes as they occur. CloudWatch Events responds to these operational changes and takes corrective action as necessary, by sending messages to respond to the environment, activating functions, making changes, and capturing state information.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-05-05-4d71f304fa9f2b4d827c5a87a7108193.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-05-05-4d71f304fa9f2b4d827c5a87a7108193.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In this scenario the only workable solution is to create a CloudWatch Event with an event pattern looking for EC2 state changes and a target set to use the Lambda function.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Event with an event pattern looking for EC2 state changes and a target set to use the Lambda function\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that triggers the Lambda function based on log streams indicating an EC2 state change in CloudWatch logs\" is incorrect as Amazon EC2 does not create a log group or log stream by default.</p><p><strong>INCORRECT:</strong> \"Create a CloudTrail trail alarm that triggers the Lambda function based on the RunInstances API action\" is incorrect as you would need to create a CloudWatch alarm for CloudTrail events (CloudTrail does not have its own alarm feature).</p><p><strong>INCORRECT:</strong> \"Configure detailed monitoring on Amazon EC2 and create an alarm that triggers the Lambda function in initialization\" is incorrect as you cannot trigger a Lambda function on EC2 instances initialization using detailed monitoring (or the EC2 console).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  },
  {
    "id": 4,
    "question": "<p>A gaming application displays the results of games in a leaderboard. The leaderboard is updated by 4 KB messages that are retrieved from an Amazon SQS queue. The updates are received infrequently but the Developer needs to minimize the time between the messages arriving in the queue and the leaderboard being updated.</p><p>Which technique provides the shortest delay in updating the leaderboard?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Retrieve the messages from the queue using long polling every 15 seconds</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Reduce the size of the messages with compression before sending them</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Retrieve the messages from the queue using short polling every 10 seconds</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the message payload in Amazon S3 and use the SQS Extended Client Library for Java</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses <em>short polling</em>, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response.</p><p>You can use <em>long polling</em> to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue. When the wait time for the ReceiveMessage API action is greater than 0, <em>long polling</em> is in effect. The maximum long polling wait time is 20 seconds.</p><p>Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">ReceiveMessage</a> request) and false empty responses (when messages are available but aren't included in a response). It also returns messages as soon as they become available.</p><p><strong>CORRECT: </strong>\"Retrieve the messages from the queue using long polling every 15 seconds\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Retrieve the messages from the queue using short polling every 10 seconds\" is incorrect as short polling is configured when the WaitTimeSeconds parameter of a <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">ReceiveMessage</a> request is set to 0. Any number above zero indicates long polling is in effect.</p><p><strong>INCORRECT:</strong> \"Reduce the size of the messages with compression before sending them\" is incorrect as this will not mean messages are picked up earlier and there is no reason to compress messages that are 4 KB in size.</p><p><strong>INCORRECT:</strong> \"Store the message payload in Amazon S3 and use the SQS Extended Client Library for Java\" is incorrect as this is unnecessary for messages of this size and will also not result in the shortest delay when updating the leaderboard.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 5,
    "question": "<p>A developer is creating a serverless application that will use a DynamoDB table. The average item size is 9KB. The application will make 4 strongly consistent reads/sec, and 2 standard write/sec. How many RCUs/WCUs are required?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>12 RCU and 18 WCU</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>12 RCU and 36 WCU</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>24 RCU and 18 WCU</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>6 RCU and 18 WCU</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.</p><p><strong>Read capacity unit (RCU):</strong></p><p> • Each API call to read data from your table is a read request.</p><p> • Read requests can be strongly consistent, eventually consistent, or transactional.</p><p> • For items up to 4 KB in size, one RCU can perform one <em>strongly consistent</em> read request per&nbsp; second.</p><p> • Items larger than 4 KB require additional RCUs.</p><p>For items up to 4 KB in size, one RCU can perform two <em>eventually consistent</em> read requests per second.</p><p><em>Transactional</em> read requests require two RCUs to perform one read per second for items up to 4 KB.</p><p>For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.</p><p><strong>Write capacity unit (WCU):</strong></p><p> • Each API call to write data to your table is a write request.</p><p>For items up to 1 KB in size, one WCU can perform one<em> standard</em> write request per second.</p><p>Items larger than 1 KB require additional WCUs.</p><p><em> </em>• <em>Transactional</em> write requests require two WCUs to perform one write per second for items up to 1 KB.</p><p> • For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-24-22-68d5683a1fee43bfdba434ca4127edaf.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-24-22-68d5683a1fee43bfdba434ca4127edaf.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>To determine the number of RCUs required to handle 4 strongly consistent reads per/second with an average item size of 9KB, perform the following steps:</strong></p><p><strong>1. Determine the average item size by rounding up the next multiple of 4KB (9KB rounds up to 12KB).</strong></p><p><strong>2. Determine the RCU per item by dividing the item size by 4KB (12KB/4KB = 3).</strong></p><p><strong>3. Multiply the value from step 2 with the number of reads required per second (3x4 = 12).</strong></p><p><strong>To determine the number of WCUs required to handle 2 standard writes per/second with an average item size of 9KB, simply multiply the average item size by the number of writes required (9x2=18).</strong></p><p><strong>CORRECT: </strong>\"12 RCU and 18 WCU\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"24 RCU and 18 WCU\" is incorrect. This would be the correct answer for transactional reads and standard writes.</p><p><strong>INCORRECT:</strong> \"12 RCU and 36 WCU\" is incorrect. This would be the correct answer for strongly consistent reads and transactional writes.</p><p><strong>INCORRECT:</strong> \"6 RCU and 18 WCU\" is incorrect. This would be the correct answer for eventually consistent reads and standard writes</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/pricing/provisioned/\">https://aws.amazon.com/dynamodb/pricing/provisioned/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/dynamodb/pricing/provisioned/",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 6,
    "question": "<p>An IT automation architecture uses many AWS Lambda functions invoking one another as a large state machine. The coordination of this state machine is legacy custom code that breaks easily.<br>Which AWS Service can help refactor and manage the state machine?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS CodeBuild</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Step Functions</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p> AWS CloudFormation</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS CodePipeline</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>AWS Step Functions lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker, into feature-rich applications.</p><p>Workflows are made up of a series of steps, with the output of one step acting as input into the next. Application development is simpler and more intuitive using Step Functions, because it translates your workflow into a state machine diagram that is easy to understand, easy to explain to others, and easy to change.</p><p>Step Functions automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected. With Step Functions, you can craft long-running workflows such as machine learning model training, report generation, and IT automation.</p><p>Therefore, AWS Step Functions is the best AWS service to use when refactoring the application away from the legacy code.</p><p><strong>CORRECT: </strong>\"AWS Step Functions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CloudFormation\" is incorrect as CloudFormation is used for deploying resources no AWS but not for ongoing automation.</p><p><strong>INCORRECT:</strong> \"AWS CodePipeline\" is incorrect as this is used as part of a continuous integration and delivery (CI/CD) pipeline to deploy software updates to applications.</p><p><strong>INCORRECT:</strong> \"AWS CodeBuild\" is incorrect as this an AWS build/test service.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/step-functions/",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 7,
    "question": "<p>A Developer is looking for a way to use shorthand syntax to express functions, APIs, databases, and event source mappings. The Developer will test using AWS SAM to create a simple Lambda function using Nodejs.12x.</p><p>What is the SIMPLEST way for the Developer to get started with a Hello World Lambda function? </p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Install the AWS CLI, run <code>aws sam init</code> and use one of the AWS Quick Start Templates</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS CloudFormation to deploy a Hello World stack using AWS SAM</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the AWS Management Console to access AWS SAM and deploy a Hello World function</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Install the AWS SAM CLI, run <code>sam init</code> and use one of the AWS Quick Start Templates</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>The <code>sam init</code> command initializes a serverless application with an AWS SAM template. The template provides a folder structure for your Lambda functions and is connected to an event source such as APIs, S3 buckets, or DynamoDB tables. This application includes everything you need to get started and to eventually extend it into a production-scale application.</p><p>This is the simplest way for the Developer to quickly get started with testing AWS SAM. Before the Developer can use the “sam” commands it is necessary to install the AWS SAM CLI. This is separate to the AWS CLI.</p><p><strong>CORRECT: </strong>\"Install the AWS SAM CLI, run <code>sam init</code> and use one of the AWS Quick Start Templates\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Install the AWS CLI, run aws sam init and use one of the AWS Quick Start Templates\" is incorrect as “<code>sam init</code>” is not an AWS CLI command, therefore you cannot put “aws” in front of “sam”.</p><p><strong>INCORRECT:</strong> \"Use the AWS Management Console to access AWS SAM and deploy a Hello World function\" is incorrect as you cannot access AWS SAM through the console. You can, however, access the Serverless Application Repository through the console and deploy SAM templates.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to deploy a Hello World stack using AWS SAM\" is incorrect as though AWS SAM does use CloudFormation you cannot deploy SAM templates through the AWS CloudFormation console. You must use the SAM CLI or deploy using the Serverless Application Repository.</p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html\">https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html",
      "https://digitalcloud.training/aws-sam/"
    ]
  },
  {
    "id": 8,
    "question": "<p>An online retail platform uses the AWS SDK for Python (Boto3) on the frontend to handle user authentication through AWS Security Token Service (AWS STS). The platform stores its digital assets in an Amazon S3 bucket and delivers them using an Amazon CloudFront distribution, which uses the S3 bucket as its origin.</p><p>Currently, the application holds its role credentials in plaintext within a Python file in the application code. The platform developers are looking to improve security by creating a mechanism that enables the application to retrieve user credentials without embedding any credentials in the application code.</p><p>What solution would meet these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a CloudFront function on the distribution. Initiate this function for each viewer request. Shift the credentials from the Python file into this function and relocate all SDK calls from the frontend into the function.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Integrate a Lambda@Edge function with the CloudFront distribution. Trigger the function upon each viewer request. Give the execution role of the function the required permissions to interact with AWS STS. Shift all SDK calls from the frontend to this function.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure a CloudFront function within the distribution. Activate the function on viewer request. Grant the function's execution role the necessary permissions to access AWS STS. Relocate all SDK calls from the frontend to this function.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Attach a Lambda@Edge function to the CloudFront distribution. Invoke this function with each viewer request. Transfer the credentials from the Python file into this function and move all SDK calls from the frontend into this function.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>The question seeks a solution where the application retrieves user credentials without storing any credentials within the application code. It's a security risk to embed credentials in code; it's safer to employ AWS managed services, like Lambda@Edge, that can securely manage and provide temporary credentials through AWS STS. Lambda@Edge also enables you to run your code closer to your users, reducing latency.</p><p><strong>CORRECT: </strong>\"Integrate a Lambda@Edge function with the CloudFront distribution. Trigger the function upon each viewer request. Give the execution role of the function the required permissions to interact with AWS STS. Shift all SDK calls from the frontend to this function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure a CloudFront function within the distribution. Activate the function on viewer request. Grant the function's execution role the necessary permissions to access AWS STS. Relocate all SDK calls from the frontend to this function\" is incorrect.</p><p>CloudFront Functions are primarily used to perform lightweight HTTP(s) transformations and manipulations at AWS edge locations. These functions are designed to execute simple tasks like HTTP header manipulations, URL redirects, or cache key normalizations. They do not have permission to access AWS services like AWS STS, nor can they handle the more complex operations required for this use case. Therefore, this option is incorrect.</p><p><strong>INCORRECT:</strong> \"Attach a Lambda@Edge function to the CloudFront distribution. Invoke this function with each viewer request. Transfer the credentials from the Python file into this function and move all SDK calls from the frontend into this function\" is incorrect.</p><p>This option suggests transferring the plaintext credentials into a Lambda@Edge function, which is not a secure practice. It implies hardcoding the credentials into the Lambda@Edge function, which is discouraged in AWS (or any other environment). Credentials should not be hardcoded because they could be exposed through logs or version control systems, leading to potential security risks. This option does not comply with the security best practices and thus is incorrect.</p><p><strong>INCORRECT:</strong> \"Set up a CloudFront function on the distribution. Initiate this function for each viewer request. Shift the credentials from the Python file into this function and relocate all SDK calls from the frontend into the function\" is incorrect.</p><p>This option also proposes transferring the plaintext credentials into a CloudFront function, which is an insecure practice. Hardcoding credentials is considered risky as they could be unintentionally exposed, causing potential security vulnerabilities. Moreover, CloudFront Functions cannot access other AWS services and cannot perform complex tasks. Therefore, this option does not meet the security requirements and is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lambda/edge/\">https://aws.amazon.com/lambda/edge/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/lambda/edge/",
      "https://digitalcloud.training/amazon-cloudfront/"
    ]
  },
  {
    "id": 9,
    "question": "<p>A Developer needs to access AWS CodeCommit over SSH. The SSH keys configured to access AWS CodeCommit are tied to a user with the following permissions:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"version\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"2012-10-17\"</span></li><li class=\"L2\"><span class=\"str\">\"Statement\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"pun\">{</span></li><li class=\"L4\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L6\"><span class=\"str\">\"codecommit:BatchGetRepositories\"</span><span class=\"pun\">,</span></li><li class=\"L7\"><span class=\"str\">\"codecommit:Get*\"</span></li><li class=\"L8\"><span class=\"str\">\"codecommit:List*\"</span><span class=\"pun\">,</span></li><li class=\"L9\"><span class=\"str\">\"codecommit:GitPull\"</span></li><li class=\"L0\"><span class=\"pun\">],</span></li><li class=\"L1\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"*\"</span></li><li class=\"L2\"><span class=\"pun\">}</span></li><li class=\"L3\"><span class=\"pun\">]</span></li><li class=\"L4\"><span class=\"pun\">}</span></li></ol></pre></div></div><p>The Developer needs to create/delete branches.</p><p>Which specific IAM permissions need to be added based on the principle of least privilege?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>“<code>codecommit:Put*</code>:”</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>“<code>codecommit:*</code>”</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>“<code>codecommit:CreateBranch</code>” and “<code>codecommit:DeleteBranch</code>”</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>“<code>codecommit:Update*</code>”</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>The permissions assigned to the user account are missing the privileges to create and delete branches in AWS CodeCommit. The Developer needs to be assigned these permissions but according to the principal of least privilege it’s important to ensure no additional permissions are assigned.</p><p>The following API actions can be used to work with branches:</p><p> • CreateBranch , which creates a branch in a specified repository.</p><p> • DeleteBranch , which deletes the specified branch in a repository unless it is the default branch.</p><p> • GetBranch , which returns information about a specified branch.</p><p> • ListBranches , which lists all branches for a specified repository.</p><p> • UpdateDefaultBranch , which changes the default branch for a repository.</p><p>Therefore, the best answer is to add the “<code>codecommit:CreateBranch</code>” and “<code>codecommit:DeleteBranch</code>” permissions to the permissions policy.</p><p><strong>CORRECT: </strong>\"<code>codecommit:CreateBranch</code>” and “<code>codecommit:DeleteBranch</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"<code>codecommit:Put</code>*:\" is incorrect. The wildcard (*) will allow any API action starting with “Put”, however the only options are put-file and put-repository-triggers, neither of which is related to branches.</p><p><strong>INCORRECT:</strong> \"<code>codecommit:Update</code>*\" is incorrect. The wildcard (*) will allow any API action starting with “Update”, however none of the options available are suitable for working with branches.</p><p><strong>INCORRECT:</strong> \"<code>codecommit:*</code>\" is incorrect as this would allow any API action which does not follow the principal of least privilege.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/codecommit/index.html#cli-aws-codecommit\">https://docs.aws.amazon.com/cli/latest/reference/codecommit/index.html#cli-aws-codecommit</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/cli/latest/reference/codecommit/index.html#cli-aws-codecommit",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 10,
    "question": "<p>An application runs on Amazon EC2 and generates log files. A Developer needs to centralize the log files so they can be queried and retained. What is the EASIEST way for the Developer to centralize the log files?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a script that copies the log files to Amazon S3 and use a cron job to run the script on a recurring schedule</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a script that uses the AWS SDK to collect and send the log files to Amazon CloudWatch Logs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Setup a CloudWatch Events rule to trigger an SNS topic when an application log file is generated</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Install the Amazon CloudWatch Logs agent and collect the logs from the instances</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-37-19-bd6f9b9d88a836d9fbc2594bc0953dc5.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-37-19-bd6f9b9d88a836d9fbc2594bc0953dc5.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can then easily view them, search them for specific error codes or patterns, filter them based on specific fields, or archive them securely for future analysis.</p><p>To collect logs from Amazon EC2 and on-premises instances it is necessary to install an agent. There are two options: the unified CloudWatch Agent which collects logs and advanced metrics (such as memory usage), or the older CloudWatch Logs agent which only collects logs from Linux servers.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch Logs agent and collect the logs from the instances\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a script that copies the log files to Amazon S3 and use a cron job to run the script on a recurring schedule\" is incorrect as the best place to move the log files to for querying and long term retention would be CloudWatch Logs. It is also easier to use the agent than to create and maintain a script.</p><p><strong>INCORRECT:</strong> \"Create a script that uses the AWS SDK to collect and send the log files to Amazon CloudWatch Logs\" is incorrect as this is not the easiest way to achieve this outcome. It will be easier to use the CloudWatch Logs agent.</p><p><strong>INCORRECT:</strong> \"Setup a CloudWatch Events rule to trigger an SNS topic when an application log file is generated\" is incorrect as CloudWatch Events does not collect log files, it monitors state changes in resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  },
  {
    "id": 11,
    "question": "<p>An organization needs to add encryption in-transit to an existing website running behind an Elastic Load Balancer. The website’s Amazon EC2 instances are CPU-constrained and therefore load on their CPUs should not be increased. What should be done to secure the website? (Select TWO.)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure an Elastic Load Balancer with a KMS CMK</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure SSL certificates on an Elastic Load Balancer</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Install SSL certificates on the EC2 instances</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure an Elastic Load Balancer with SSL termination</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Configure an Elastic Load Balancer with SSL pass-through</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>The company need to add security to their website by encrypting traffic in-transit using HTTPS. This requires adding SSL/TLS certificates to enable the encryption. The process of encrypting and decrypting data is CPU intensive and therefore the company need to avoid adding certificates to the EC2 instances as that will place further load on their CPUs.</p><p>Therefore, the solution is to configure SSL certificates on the Elastic Load Balancer and then configure SSL termination. This can be done by adding a certificate to a HTTPS listener on the load balancer.</p><p><strong>CORRECT: </strong>\"Configure SSL certificates on an Elastic Load Balancer\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Configure an Elastic Load Balancer with SSL termination\" is a correct answer.</p><p><strong>INCORRECT:</strong> \"Configure an Elastic Load Balancer with SSL pass-through\" is incorrect as with pass-through the SSL session must be terminated on the EC2 instances which should be avoided as they are CPU-constrained.</p><p><strong>INCORRECT:</strong> \"Configure an Elastic Load Balancer with a KMS CMK\" is incorrect as a KMS CMK is used to encrypt data at rest, it is not used for in-transit encryption.</p><p><strong>INCORRECT:</strong> \"Install SSL certificates on the EC2 instances\" is incorrect as this would increase the load on the CPUs</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html",
      "https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/"
    ]
  },
  {
    "id": 12,
    "question": "<p>You run an ad-supported photo sharing website using Amazon S3 to serve photos to visitors of your site. At some point you find out that other sites have been linking to the photos on your site, causing loss to your business.<br>What is an effective method to mitigate this?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Block the IPs of the offending websites in Security Groups</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store photos on an EBS volume of the web server</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use CloudFront distributions for static content</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Remove public read access and use signed URLs with expiry dates</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>When Amazon S3 objects are private, only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant time-limited permission to download the objects.</p><p>When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The presigned URLs are valid only for the specified duration.</p><p>Anyone who receives the presigned URL can then access the object. In this scenario, the photos can be shared with the owner’s website but not with any other 3rd parties. This will stop other sites from linking to the photos as they will not display anywhere else.</p><p><strong>CORRECT: </strong>\"Remove public read access and use signed URLs with expiry dates\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store photos on an EBS volume of the web server\" is incorrect as this does not add any more control over content visibility in the website.</p><p><strong>INCORRECT:</strong> \"Use CloudFront distributions for static content\" is incorrect as this alone will not protect the content. You can also use pre-signed URLs with CloudFront, but this isn’t mentioned.</p><p><strong>INCORRECT:</strong> \"Block the IPs of the offending websites in Security Groups\" is incorrect as you can only configure allow rules in security groups so this would be hard to manage.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html",
      "https://digitalcloud.training/amazon-s3-and-glacier/"
    ]
  },
  {
    "id": 13,
    "question": "<p>An application is running on a cluster of Amazon EC2 instances. The application has received an error when trying to read objects stored within an Amazon S3 bucket. The bucket is encrypted with server-side encryption and AWS KMS managed keys (SSE-KMS). The error is as follows:</p><p><code>Service: AWSKMS; Status Code: 400, Error Code: ThrottlingException</code></p><p>Which combination of steps should be taken to prevent this failure? (Select TWO.)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Import a customer master key (CMK) with a larger key size</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Contact AWS support to request an AWS KMS rate limit increase</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use more than once customer master key (CMK) to encrypt S3 data</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Contact AWS support to request an S3 rate limit increase</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Perform error retries with exponential backoff in the application code</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>AWS KMS establishes quotas for the number of API operations requested in each second. When you exceed an API request quota, AWS KMS <em>throttles</em> the request, that is, it rejects an otherwise valid request and returns a ThrottlingException error like the following one.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-32-56-3ab1f386892eaf23c4dc3af057bf8b63.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-32-56-3ab1f386892eaf23c4dc3af057bf8b63.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>As the error indicates, one of the recommendations is to reduce the frequency of calls which can be implemented by using exponential backoff logic in the application code. It is also possible to contact AWS and request an increase in the quota.</p><p><strong>CORRECT: </strong>\"Contact AWS support to request an AWS KMS rate limit increase\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Perform error retries with exponential backoff in the application code\" is a correct answer.</p><p><strong>INCORRECT:</strong> \"Contact AWS support to request an S3 rate limit increase\" is incorrect as the error indicates throttling in AWS KMS.</p><p><strong>INCORRECT:</strong> \"Import a customer master key (CMK) with a larger key size\" is incorrect as the key size does not affect the quota for requests to AWS KMS.</p><p><strong>INCORRECT:</strong> \"Use more than once customer master key (CMK) to encrypt S3 data\" is incorrect as the issue is not the CMK it is the request quota on AWS KMS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/requests-per-second.html\">https://docs.aws.amazon.com/kms/latest/developerguide/requests-per-second.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/requests-per-second.html",
      "https://digitalcloud.training/aws-kms/"
    ]
  },
  {
    "id": 14,
    "question": "<p>A Developer is creating an AWS Lambda function that generates a new file each time it runs. Each new file must be checked into an AWS CodeCommit repository hosted in the same AWS account.</p><p>How should the Developer accomplish this?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Upload the new file to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. In the Step Function, add the new file to the repository</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>After the new file is created in Lambda, use cURL to invoke the CodeCommit API. Send the file to the repository</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>When the Lambda function starts, use the Git CLI to clone the repository. Check the new file into the cloned repository and push the change</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use an AWS SDK to instantiate a CodeCommit client. Invoke the put_file method to add the file to the repository</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>The Developer can use the AWS SDK to instantiate a CodeCommit client. For instance, the code might include the following:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-34-35-a431eb0e6bb8bb4717d5ccd60120a8a5.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-34-35-a431eb0e6bb8bb4717d5ccd60120a8a5.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The client can then be used with put_file which adds or updates a file in a branch in an AWS CodeCommit repository, and generates a commit for the addition in the specified branch.</p><p>The request syntax is as follows:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-34-48-59f1d0be8e99488c77f804470c7d2412.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-34-48-59f1d0be8e99488c77f804470c7d2412.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Use an AWS SDK to instantiate a CodeCommit client. Invoke the put_file method to add the file to the repository\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"When the Lambda function starts, use the Git CLI to clone the repository. Check the new file into the cloned repository and push the change\" is incorrect as there is no need to clone a repository, a file just needs to be added to an existing repository.</p><p><strong>INCORRECT:</strong> \"After the new file is created in Lambda, use cURL to invoke the CodeCommit API. Send the file to the repository\" is incorrect as a URL cannot be used to invoke a CodeCommit client and upload and check in the file.</p><p><strong>INCORRECT:</strong> \"Upload the new file to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. In the Step Function, add the new file to the repository\" is incorrect as Step Functions is not triggered by S3 events.</p><p><strong>References:</strong></p><p><a href=\"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/codecommit.html\">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/codecommit.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/codecommit.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 15,
    "question": "<p>An application writes items to an Amazon DynamoDB table. As the application scales to thousands of instances, calls to the DynamoDB API generate occasional ThrottlingException errors. The application is coded in a language that is incompatible with the AWS SDK.</p><p>What can be done to prevent the errors from occurring?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Pass API calls through Amazon API Gateway</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Send the items to DynamoDB through Amazon Kinesis Data Firehose</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add exponential backoff to the application logic</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon SQS as an API message bus</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Implementing error retries and exponential backoff is a good way to resolve this issue. Exponential backoff can improve an application's reliability by using progressively longer waits between retries. If you're using an AWS SDK, this logic is built‑in. If you're not using an AWS SDK, consider manually implementing exponential backoff.</p><p>Additional options for preventing throttling from occurring include:</p><p> • Distribute read and write operations as evenly as possible across your table. A hot partition can degrade the overall performance of your table. For more information, see Designing Partition Keys to Distribute Your Workload Evenly.</p><p> • Implement a caching solution. If your workload is mostly read access to static data, then query results can be delivered much faster if the data is in a well‑designed cache rather than in a database. DynamoDB Accelerator (DAX) is a caching service that offers fast in‑memory performance for your application. You can also use Amazon ElastiCache.</p><p><strong>CORRECT: </strong>\"Add exponential backoff to the application logic\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS as an API message bus\" is incorrect. SQS is used for decoupling (messages, nut not APIs), however for this scenario it would add extra cost and complexity.</p><p><strong>INCORRECT:</strong> \"Pass API calls through Amazon API Gateway\" is incorrect. For this scenario we don’t want to add an additional layer in when we can simply configure the application to back off and retry.</p><p><strong>INCORRECT:</strong> \"Send the items to DynamoDB through Amazon Kinesis Data Firehose\" is incorrect as DynamoDB is not a supported destination for Kinesis Data Firehose.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/general/latest/gr/api-retries.html\">https://docs.aws.amazon.com/general/latest/gr/api-retries.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-table-throttled/\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-table-throttled/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/general/latest/gr/api-retries.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-table-throttled/",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 16,
    "question": "<p>A Development team have moved their continuous integration and delivery (CI/CD) pipeline into the AWS Cloud. The team is leveraging AWS CodeCommit for management of source code. The team need to compile their source code, run tests, and produce software packages that are ready for deployment.</p><p>Which AWS service can deliver these outcomes?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS CodeBuild</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>AWS Cloud9</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS CodePipeline</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS CodeCommit</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>AWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more.</p><p>You can also customize build environments in CodeBuild to use your own build tools. CodeBuild scales automatically to meet peak build requests.</p><p>CodeBuild provides these benefits:</p><p><strong> </strong>• <strong>Fully managed</strong> – CodeBuild eliminates the need to set up, patch, update, and manage your own build servers.</p><p><strong> </strong>• <strong>On demand</strong> – CodeBuild scales on demand to meet your build needs. You pay only for the number of build minutes you consume.</p><p><strong> </strong>• <strong>Out of the box</strong> – CodeBuild provides preconfigured build environments for the most popular programming languages. All you need to do is point to your build script to start your first build.</p><p>Therefore, AWS CodeBuild is the best service to use to compile the Development team’s source code, run tests, and produce software packages that are ready for deployment.</p><p><strong>CORRECT: </strong>\"AWS CodeBuild\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodeCommit\" is incorrect. The team are already using CodeCommit for its correct purpose, which is to manage source code. CodeCommit cannot perform compiling of source code, testing, or package creation.</p><p><strong>INCORRECT:</strong> \"AWS CodePipeline\" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.</p><p><strong>INCORRECT:</strong> \"AWS Cloud9\" is incorrect. AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 17,
    "question": "<p>A development team require a fully-managed source control service that is compatible with Git.</p><p>Which service should they use?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS CodePipeline</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS CodeDeploy</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS CodeCommit</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>AWS Cloud9</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>AWS CodeCommit is a version control service hosted by Amazon Web Services that you can use to privately store and manage assets (such as documents, source code, and binary files) in the cloud. CodeCommit is a fully-managed service that hosts secure Git-based repositories.</p><p><strong>CORRECT: </strong>\"AWS CodeCommit\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CodeDeploy\" is incorrect. CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.</p><p><strong>INCORRECT:</strong> \"AWS CodePipeline\" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.</p><p><strong>INCORRECT:</strong> \"AWS Cloud9\" is incorrect. AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 18,
    "question": "<p>A Developer has created a serverless function that processes log files. The function should be invoked once every 15 minutes. How can the Developer automatically invoke the function using serverless services?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the Lambda scheduler to run based on recurring time value</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon SNS rule to send a notification to Lambda to instruct it to run</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Launch an EC2 Linux instance and add a command to periodically invoke the function to its /etc/crontab file</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon CloudWatch Events rule that is scheduled to run and invoke the function</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams.</p><p>You can use Amazon CloudWatch Events to invoke the Lambda function on a recurring schedule of 15 minutes. This solution is entirely automated and serverless.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudWatch Events rule that is scheduled to run and invoke the function\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Launch an EC2 Linux instance and add a command to periodically invoke the function to its /etc/crontab file \" is incorrect as this is automatic but it is not serverless.</p><p><strong>INCORRECT:</strong> \"Configure the Lambda scheduler to run based on recurring time value\" is incorrect as there is no Lambda scheduler that can be used.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS rule to send a notification to Lambda to instruct it to run\" is incorrect as you cannot invoke a function by sending a notification to it from Amazon SNS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html",
      "https://digitalcloud.training/amazon-cloudwatch/",
      "https://digitalcloud.training/aws-lambda/"
    ]
  },
  {
    "id": 19,
    "question": "<p>A Developer manages a website running behind an Elastic Load Balancer in the us-east-1 region. The Developer has recently deployed an identical copy of the website in us-west-1 and needs to send 20% of the traffic to the new site.</p><p>How can the Developer achieve this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a blue/green deployment with Amazon Elastic Beanstalk</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use an Amazon Route 53 Geolocation Routing Policy</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use a blue/green deployment with Amazon CodeDeploy</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use an Amazon Route 53 Weighted Routing Policy</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-10-25-ab6d0fe50495e5d954b16e81bd1154b5.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-10-25-ab6d0fe50495e5d954b16e81bd1154b5.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In this case the Developer can use a weighted routing policy to direct 20% of the incoming traffic to the new site as required.</p><p><strong>CORRECT: </strong>\"Use an Amazon Route 53 Weighted Routing Policy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an Amazon Route 53 Geolocation Routing Policy\" is incorrect as the Developer should use a weighted routing policy for this requirement as a specified percentage of traffic needs to be directed to the new website.</p><p><strong>INCORRECT:</strong> \"Use a blue/green deployment with Amazon Elastic Beanstalk\" is incorrect as the question does not state that Elastic Beanstalk is being used and the new website has already been deployed.</p><p><strong>INCORRECT:</strong> \"Use a blue/green deployment with Amazon CodeDeploy\" is incorrect as the question does not state that Amazon CodeDeploy is being used and the website has already been deployed.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted",
      "https://digitalcloud.training/amazon-route-53/"
    ]
  },
  {
    "id": 20,
    "question": "<p>An application is being instrumented to send trace data using AWS X-Ray. A Developer needs to upload segment documents using JSON-formatted strings to X-Ray using the API. Which API action should the developer use?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The <code>UpdateGroup</code> API action</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The <code>PutTelemetryRecords</code> API action</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The <code>PutTraceSegments</code> API action</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>The <code>GetTraceSummaries</code> API action</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>You can send trace data to X-Ray in the form of segment documents. A segment document is a JSON formatted string that contains information about the work that your application does in service of a request. Your application can record data about the work that it does itself in segments, or work that uses downstream services and resources in subsegments.</p><p>Segments record information about the work that your application does. A segment, at a minimum, records the time spent on a task, a name, and two IDs. The trace ID tracks the request as it travels between services. The segment ID tracks the work done for the request by a single service.</p><p>Example Minimal complete segment:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-14-03-7e305995aabee50345423568ced7525c.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-14-03-7e305995aabee50345423568ced7525c.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can upload segment documents with the <a href=\"https://docs.aws.amazon.com/xray/latest/api/API_PutTraceSegments.html\"><code>PutTraceSegments</code></a><code> </code>API. The API has a single parameter, <code>TraceSegmentDocuments</code>, that takes a list of JSON segment documents.</p><p>Therefore, the Developer should use the <code>PutTraceSegments</code> API action.</p><p><br></p><p><strong>CORRECT: </strong>\"The <code>PutTraceSegments</code> API action\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The <code>PutTelemetryRecords</code> API action\" is incorrect as this is used by the AWS X-Ray daemon to upload telemetry.</p><p><strong>INCORRECT:</strong> \"The <code>UpdateGroup</code> API action\" is incorrect as this updates a group resource.</p><p><strong>INCORRECT:</strong> \"The <code>GetTraceSummaries</code> API action\" is incorrect as this retrieves IDs and annotations for traces available for a specified time frame using an optional filter.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-api-sendingdata.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-api-sendingdata.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/xray/latest/api/API_PutTraceSegments.html",
      "https://docs.aws.amazon.com/xray/latest/devguide/xray-api-sendingdata.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 21,
    "question": "<p>A Developer needs to run some code using Lambda in response to an event and forward the execution result to another application using a pub/sub notification.</p><p>How can the Developer accomplish this?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure a Lambda “on success” destination and route the execution results to Amazon SNS</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SQS</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SNS</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a Lambda “on success” destination and route the execution results to Amazon SQS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>With Destinations, you can send asynchronous function execution results to a destination resource without writing code. A function execution result includes version, timestamp, request context, request payload, response context, and response payload. For each execution status (i.e. Success and Failure), you can choose one destination from four options: another <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/welcome.html\">Lambda function</a>, an <a href=\"https://docs.aws.amazon.com/sns/latest/dg/welcome.html\">SNS topic</a>, an <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html\">SQS standard queue</a>, or <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">EventBridge</a>.</p><p>For this scenario, the code will be run by Lambda and the execution result will then be sent to the SNS topic. The application that is subscribed to the SNS topics will then receive the notification.</p><p><strong>CORRECT: </strong>\"Configure a Lambda “on success” destination and route the execution results to Amazon SNS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SNS\" is incorrect as CloudWatch Events is used to track changes in the state of AWS resources. To forward execution results from Lambda a destination should be used.</p><p><strong>INCORRECT:</strong> \"Configure a Lambda “on success” destination and route the execution results to Amazon SQS\" is incorrect as SQS is a message queue not a pub/sub notification service.</p><p><strong>INCORRECT:</strong> \"Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SQS\" is incorrect as CloudWatch Events is used to track changes in the state of AWS resources. To forward execution results from Lambda a destination should be used (with an SNS topic).</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/11/aws-lambda-supports-destinations-for-asynchronous-invocations/\">https://aws.amazon.com/about-aws/whats-new/2019/11/aws-lambda-supports-destinations-for-asynchronous-invocations/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html",
      "https://docs.aws.amazon.com/sns/latest/dg/welcome.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html",
      "https://aws.amazon.com/about-aws/whats-new/2019/11/aws-lambda-supports-destinations-for-asynchronous-invocations/",
      "https://digitalcloud.training/aws-lambda/"
    ]
  },
  {
    "id": 22,
    "question": "<p>An Amazon Kinesis Data Stream has recently been configured to receive data from sensors in a manufacturing facility. A consumer EC2 instance is configured to process the data every 48 hours and save processing results to an Amazon RedShift data warehouse. Testing has identified a large amount of data is missing. A review of monitoring logs has identified that the sensors are sending data correctly and the EC2 instance is healthy.</p><p>What is the MOST likely explanation for this issue?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon RedShift is not suitable for storing streaming data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon Kinesis has too many shards provisioned</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The EC2 instance is failing intermittently</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Records are retained for 24 hours in the Kinesis Data Stream by default</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>Amazon Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence of data records meant to be written to and read from in real time. Data records are therefore stored in shards in your stream temporarily. The time period from when a record is added to when it is no longer accessible is called the <em>retention period</em>. A Kinesis data stream stores records from 24 hours by default, up to 8760 hours.</p><p>You can increase the retention period up to 8760 hours using the <a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_IncreaseStreamRetentionPeriod.html\">IncreaseStreamRetentionPeriod</a> operation. You can decrease the retention period down to a minimum of 24 hours using the <a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DecreaseStreamRetentionPeriod.html\">DecreaseStreamRetentionPeriod</a> operation. The request syntax for both operations includes the stream name and the retention period in hours. Finally, you can check the current retention period of a stream by calling the <a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStream.html\">DescribeStream</a> operation.</p><p>Both operations are easy to use. The following is an example of changing the retention period using the AWS CLI:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-58-43-abe0f94430b16f81fcaae80c865627f0.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-58-43-abe0f94430b16f81fcaae80c865627f0.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the most likely explanation is that the message retention period is set at the 24-hour default.</p><p><strong>CORRECT: </strong>\"Records are retained for 24 hours in the Kinesis Data Stream by default\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon RedShift is not suitable for storing streaming data\" is incorrect. In this architecture Amazon Kinesis is responsible for receiving streaming data and storing it in a stream. The EC2 instances can then process and store the data in a number of different destinations including Amazon RedShift.</p><p><strong>INCORRECT:</strong> \"The EC2 instance is failing intermittently\" is incorrect as the question states that a review of monitoring logs indicates that the EC2 instance is healthy. If it was failing intermittently this should be recorded in the logs.</p><p><strong>INCORRECT:</strong> \"Amazon Kinesis has too many shards provisioned\" is incorrect as this would just mean that the Kinesis Stream has more capacity, not less.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/kinesis/latest/APIReference/API_IncreaseStreamRetentionPeriod.html",
      "https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DecreaseStreamRetentionPeriod.html",
      "https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStream.html",
      "https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html",
      "https://digitalcloud.training/amazon-kinesis/"
    ]
  },
  {
    "id": 23,
    "question": "<p>A Developer needs to setup a new serverless application that includes AWS Lambda and Amazon API Gateway as part of a single stack. The Developer needs to be able to locally build and test the serverless applications before deployment on AWS.</p><p>Which service should the Developer use?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS CloudFormation</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Serverless Application Model (SAM)</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS Elastic Beanstalk</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS CodeBuild</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build <a href=\"https://aws.amazon.com/serverless/\">serverless applications</a> on AWS. A <strong>serverless application</strong> is a combination of Lambda functions, event sources, and other resources that work together to perform tasks.</p><p>AWS SAM provides you with a simple and clean syntax to describe the functions, APIs, permissions, configurations, and events that make up a serverless application.</p><p>The example AWS SAM template file below creates an AWS Lambda function and a simple Amazon API Gateway API with a Get method and a /greeting resource:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-08-03-6858777cb267b07ce091346528168780.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-08-03-6858777cb267b07ce091346528168780.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The AWS SAM CLI lets you locally build, test, and debug serverless applications that are defined by AWS SAM templates. The CLI provides a Lambda-like execution environment locally. It helps you catch issues upfront by providing parity with the actual Lambda execution environment.</p><p><strong>CORRECT: </strong>\"AWS Serverless Application Model (SAM)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS CloudFormation\" is incorrect as you cannot perform local build and test with AWS CloudFormation.</p><p><strong>INCORRECT:</strong> \"AWS Elastic Beanstalk\" is incorrect as you cannot deploy serverless applications or perform local build and test with Elastic Beanstalk.</p><p><strong>INCORRECT:</strong> \"AWS CodeBuild\" is incorrect as you cannot perform local build and test with AWS CodeBuild.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-sam/\">https://digitalcloud.training/aws-sam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/serverless/",
      "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html",
      "https://digitalcloud.training/aws-sam/"
    ]
  },
  {
    "id": 24,
    "question": "<p>A Developer has created a task definition that includes the following JSON code:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"placementConstraints\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L1\"><span class=\"pun\">{</span></li><li class=\"L2\"><span class=\"str\">\"expression\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"task:group == databases\"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"type\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"memberOf\"</span></li><li class=\"L4\"><span class=\"pun\">}</span></li><li class=\"L5\"><span class=\"pun\">]</span></li></ol></pre></div></div><p>What will be the effect for tasks using this task definition?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>They will be placed on container instances in the “databases” task group</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>They will become members of a task group called “databases”</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>They will not be placed on container instances in the “databases” task group</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>They will not be allowed to run unless they have the “databases” tag assigned</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>A <em>task placement constraint</em> is a rule that is considered during task placement. Task placement constraints can be specified when either running a task or creating a new service.</p><p> The memberOf task placement constraint places tasks on container instances that satisfy an expression.</p><p> The memberOf task placement constraint can be specified with the following actions:</p><p> • Running a task</p><p> • Creating a new service</p><p> • Creating a new task definition</p><p> • Creating a new revision of an existing task definition</p><p>The example JSON code uses the memberOf constraint to place tasks on instances in the databases task group. It can be specified with the following actions: <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateService.html\">CreateService</a>, <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_UpdateService.html\">UpdateService</a>, <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RegisterTaskDefinition.html\">RegisterTaskDefinition</a>, and <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RunTask.html\">RunTask</a>.</p><p><strong>CORRECT: </strong>\"They will be placed on container instances in the “databases” task group\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"They will become members of a task group called “databases”\" is incorrect. They will be placed on container instances in the “databases” task group.</p><p><strong>INCORRECT:</strong> \"They will not be placed on container instances in the “databases” task group\" is incorrect. This statement ensures the tasks ARE placed on the container instances in the “databases” task group.</p><p><strong>INCORRECT:</strong> \"They will not be allowed to run unless they have the “databases” tag assigned\" is incorrect. This JSON code is not related to tagging of the tasks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateService.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_UpdateService.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RegisterTaskDefinition.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RunTask.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html",
      "https://digitalcloud.training/amazon-ecs-and-eks/"
    ]
  },
  {
    "id": 25,
    "question": "<p>How can a Developer view a summary of proposed changes to an AWS CloudFormation stack without implementing the changes in production?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a Change Set</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use a direct update</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use drift detection</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a StackSet</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>When you need to update a stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running resources.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-40-43-a83262cbb0a2e50aae2536ca0c7399c4.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-40-43-a83262cbb0a2e50aae2536ca0c7399c4.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. You can create and manage change sets using the AWS CloudFormation console, AWS CLI, or AWS CloudFormation API.</p><p><strong>CORRECT: </strong>\"Create a Change Set\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a StackSet\" is incorrect as StackSets are used to create, update, or delete stacks across multiple accounts and regions with a single operation.</p><p><strong>INCORRECT:</strong> \"Use drift detection\" is incorrect as this is used to detect when a configuration deviates from the template configuration.</p><p><strong>INCORRECT:</strong> \"Use a direct update\" is incorrect as this will directly update the production resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html",
      "https://digitalcloud.training/aws-cloudformation/"
    ]
  },
  {
    "id": 26,
    "question": "<p>A Developer wants to find a list of items in a global secondary index from an Amazon DynamoDB table.</p><p>Which DynamoDB API call can the Developer use in order to consume the LEAST number of read capacity units?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Scan operation using strongly-consistent reads</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Query operation using eventually-consistent reads</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Scan operation using eventually-consistent reads</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Query operation using strongly-consistent reads</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table and issue Query or Scan requests against these indexes.</p><p>A <em>secondary index</em> is a data structure that contains a subset of attributes from a table, along with an alternate key to support Query operations. You can retrieve data from the index using a Query, in much the same way as you use Query with a table. A table can have multiple secondary indexes, which give your applications access to many different query patterns.</p><p>You can also issue scan operations on a global secondary index however it is less efficient as it will return all items in the index.</p><p><strong>CORRECT: </strong>\"Query operation using eventually-consistent reads\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Query operation using strongly-consistent reads\" is incorrect. Strongly consistent reads require more RCUs and also are not supported on a global secondary index (they are supported on local secondary indexes).</p><p><strong>INCORRECT:</strong> \"Scan operation using eventually-consistent reads\" is incorrect as a scan is less efficient than a query and will therefore use more RCUs.</p><p><strong>INCORRECT:</strong> \"Scan operation using strongly-consistent reads\" is incorrect as a scan is less efficient than a query and will therefore use more RCUs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/SecondaryIndexes.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 27,
    "question": "<p>A company manages an application that stores data in an Amazon DynamoDB table. The company need to keep a record of all new changes made to the DynamoDB table in another table within the same AWS region. What is the MOST suitable way to deliver this requirement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon DynamoDB streams</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon DynamoDB snapshots</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use CloudWatch events</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon CloudTrail</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>A <em>DynamoDB stream</em> is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.</p><p>Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A <em>stream record</em> contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the \"before\" and \"after\" images of modified items.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-40-07-0e8588f5b4f0c74121d9011e3edaa400.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-40-07-0e8588f5b4f0c74121d9011e3edaa400.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>This is the best way to capture a record of new changes made to the DynamoDB table. Another table can then be populated with this data so the data is stored persistently.</p><p><strong>CORRECT: </strong>\"Use Amazon DynamoDB streams\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use CloudWatch events\" is incorrect. CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. However, it does not capture the information that changes in a DynamoDB table so is unsuitable for this purpose.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudTrail\" is incorrect as CloudTrail records a history of API calls on your account. It is used for creating an audit trail of events.</p><p><strong>INCORRECT:</strong> \"Use Amazon DynamoDB snapshots\" is incorrect as snapshots only capture a point in time, they are not used for recording item-level changes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 28,
    "question": "<p>A three-tier application is being migrated from an on-premises data center. The application includes an Apache Tomcat web tier, an application tier running on Linux, and a MySQL back end. A Developer must refactor the application to run on the AWS cloud. The cloud-based application must be fault tolerant and elastic.</p><p>How can the Developer refactor the web tier and application tier? (Select TWO.)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement an Elastic Load Balancer for both the web tier and the application tier</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create an Auto Scaling group of EC2 instances for both the web tier and application tier</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon CloudFront distribution for the web tier</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a multi-AZ Amazon RDS database for the back end using the MySQL engine</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Implement an Elastic Load Balancer for the application tier</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Compute",
    "explanation": "<p>The key requirements in this scenario are to add fault tolerances and elasticity to the web tier and application tier. Note that no specific requirements for the back end have been included.</p><p>To add elasticity to the web and application tiers the Developer should create Auto Scaling groups of EC2 instances. We know that the application tier runs on Linux and the web tier runs on Apache Tomcat (which could be on Linux or Windows). Therefore, these workloads are suitable for an ASG and this will ensure the number of instances dynamically scales out and in based on actual usage.</p><p>To add fault tolerance to the web and application tiers the Developer should add an Elastic Load Balancer. This will ensure that if the number of EC2 instances are changed by the ASG, the load balancer is able to distribute traffic to them. This also assists with elasticity.</p><p><strong>CORRECT: </strong>\"Create an Auto Scaling group of EC2 instances for both the web tier and application tier\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Implement an Elastic Load Balancer for both the web tier and the application tier\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution for the web tier\" is incorrect as CloudFront is used for performance reasons, not elasticity or fault tolerance. You would use CloudFront to get content closer to end users around the world.</p><p><strong>INCORRECT:</strong> \"Use a multi-AZ Amazon RDS database for the back end using the MySQL engine\" is incorrect as the question does not ask for fault tolerance of the back end, only the web tier and the application tier.</p><p><strong>INCORRECT:</strong> \"Implement an Elastic Load Balancer for the application tier\" is incorrect. An Elastic Load Balancer should be implemented for both the web tier and the application tier as that is how we ensure fault tolerance and elasticity for both of those tiers.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html",
      "https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/",
      "https://digitalcloud.training/amazon-ec2-auto-scaling/"
    ]
  },
  {
    "id": 29,
    "question": "<p>A multimedia streaming service wants to migrate its user authentication system to AWS. The system keeps user session data during an active session, which is crucial for seamless user experience. The new system needs to be fault-tolerant, highly scalable natively, and any service disruption must not affect user experience.</p><p>What is the best option to store the user session data?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the user session data in Amazon CloudFront.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store the user session data in Amazon ElastiCache.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Enable session stickiness using elastic load balancers.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the user session data in Amazon S3.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>ElastiCache is a fully managed in-memory caching service that provides a high-performance, scalable, and cost-effective caching solution, while removing the complexity associated with deploying and managing a distributed cache environment. It is well-suited for storing user session data due to its high-speed and low-latency capabilities.</p><p><strong>CORRECT: </strong>\"Store the user session data in Amazon ElastiCache\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the user session data in Amazon CloudFront\" is incorrect.</p><p>CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. However, it is not designed for session state storage and thus, would not be the best choice for storing user session data.</p><p><strong>INCORRECT:</strong> \"Store the user session data in Amazon S3\" is incorrect.</p><p>Amazon S3 provides durable, scalable object storage but is not optimized for the type of high-speed, read/write operations typically associated with session data. While you could technically use it to store session data, it would likely not provide the same level of performance as an in-memory store like ElastiCache.</p><p><strong>INCORRECT:</strong> \"Enable session stickiness using elastic load balancers\" is incorrect.</p><p>Enabling session stickiness in load balancers can help ensure that a client is consistently directed to the same instance if that instance is healthy. However, this is more of a traffic routing mechanism rather than a storage solution for session data.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/elasticache/",
      "https://digitalcloud.training/amazon-elasticache/"
    ]
  },
  {
    "id": 30,
    "question": "<p>Data must be loaded into an application each week for analysis. The data is uploaded to an Amazon S3 bucket from several offices around the world. Latency is slowing the uploads and delaying the analytics job. What is the SIMPLEST way to improve upload times?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Upload via a managed AWS VPN connection</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Upload to a local Amazon S3 bucket within each region and enable Cross-Region Replication (CRR)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Upload using Amazon S3 Transfer Acceleration</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Upload to Amazon CloudFront and then download from the local cache to the S3 bucket</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p><p> You might want to use Transfer Acceleration on a bucket for various reasons, including the following:</p><p> • You have customers that upload to a centralized bucket from all over the world.</p><p> • You transfer gigabytes to terabytes of data on a regular basis across continents.</p><p> • You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.</p><p><strong>CORRECT: </strong>\"Upload using Amazon S3 Transfer Acceleration\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Upload to a local Amazon S3 bucket within each region and enable Cross-Region Replication (CRR)\" is incorrect as this would not speed up the upload as the process introduces more latency.</p><p><strong>INCORRECT:</strong> \"Upload via a managed AWS VPN connection\" is incorrect as this still uses the public Internet and there’s no real latency advantages here.</p><p><strong>INCORRECT:</strong> \"Upload to Amazon CloudFront and then download from the local cache to the S3 bucket\" is incorrect. This is going to require some time to propagate to the cache and requires some manual work in retrieving the data. The simplest solution is to use S3 Transfer Acceleration which basically does this for you.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html",
      "https://digitalcloud.training/amazon-s3-and-glacier/"
    ]
  },
  {
    "id": 31,
    "question": "<p>A Development team are deploying an AWS Lambda function that will be used by a production application. The function code will be updated regularly, and new versions will be published. The development team do not want to modify application code to point to each new version.</p><p>How can the Development team setup a static ARN that will point to the latest published version?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Setup a Route 53 Alias record that points to the published version</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Publish a mutable version and point it to the $LATEST version</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an unqualified ARN</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Setup an Alias that will point to the latest version</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-29-51-1f00a9304cc2f7e0318f540b1814ba83.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-29-51-1f00a9304cc2f7e0318f540b1814ba83.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>This is the best way to setup the Lambda function so you don’t need to modify the application code when a new version is published. Instead, the developer will simply need to update the Alias to point to the new version:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-12-31-637507fdd536a3b9809d552c1572c952.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-12-31-637507fdd536a3b9809d552c1572c952.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>As you can see above you can also point to multiple versions and send a percentage of traffic to each. This is great for testing new code.</p><p><strong>CORRECT: </strong>\"Setup an Alias that will point to the latest version\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Publish a mutable version and point it to the $LATEST version\" is incorrect as all published versions are immutable (cannot be modified) and you cannot modify a published version to point to the $LATEST version.</p><p><strong>INCORRECT:</strong> \"Use an unqualified ARN\" is incorrect as this is an ARN that does not have a version number which means it points to the $LATEST version, not to a published version (as published versions always have version numbers).</p><p><strong>INCORRECT:</strong> \"Setup a Route 53 Alias record that points to the published version\" is incorrect as you cannot point a Route 53 Alias record to an AWS Lambda function.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html",
      "https://digitalcloud.training/aws-lambda/"
    ]
  },
  {
    "id": 32,
    "question": "<p>What does an Amazon SQS delay queue accomplish?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The consumer can poll the queue for a configurable amount of time before retrieving a message</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Messages are hidden for a configurable amount of time after they are consumed from the queue</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Message cannot be deleted for a configurable amount of time after they are consumed from the queue</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Messages are hidden for a configurable amount of time when they are first added to the queue</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>Delay queues let you postpone the delivery of new messages to a queue for a number of seconds, for example, when your consumer application needs additional time to process messages.</p><p>If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-34-10-87835307943cf0d5f22a56da08d0a97f.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-34-10-87835307943cf0d5f22a56da08d0a97f.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct explanation is that with an Amazon SQS Delay Queue messages are hidden for a configurable amount of time when they are first added to the queue</p><p><strong>CORRECT: </strong>\"Messages are hidden for a configurable amount of time when they are first added to the queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Messages are hidden for a configurable amount of time after they are consumed from the queue\" is incorrect. They are hidden when they are added to the queue.</p><p><strong>INCORRECT:</strong> \"The consumer can poll the queue for a configurable amount of time before retrieving a message\" is incorrect. A delay queue simply delays visibility of the message, it does not affect polling behavior.</p><p><strong>INCORRECT:</strong> \"Message cannot be deleted for a configurable amount of time after they are consumed from the queue\" is incorrect. That is what a visibility timeout achieves.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 33,
    "question": "<p>A Developer is managing an application that includes an Amazon SQS queue. The consumers that process the data from the queue are connecting in short cycles and the queue often does not return messages. The cost for API calls is increasing. How can the Developer optimize the retrieval of messages and reduce cost?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Call the <code>ReceiveMessage</code> API with the <code>VisibilityTimeout</code> parameter set to 30</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Call the <code>SetQueueAttributes</code> API with the <code>maxReceiveCount</code> set to 20</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Call the <code>ReceiveMessage </code>API with the <code>WaitTimeSeconds </code>parameter set to 20</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Call the <code>SetQueueAttributes</code> API with the <code>DelaySeconds</code> parameter set to 900</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses <em>short polling</em>, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response. You can use <em>long polling</em> to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue.</p><p>When you consume messages from a queue using short polling, Amazon SQS samples a subset of its servers (based on a weighted random distribution) and returns messages from only those servers. Thus, a particular <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">ReceiveMessage</a> request might not return all of your messages. However, if you have fewer than 1,000 messages in your queue, a subsequent request will return your messages. If you keep consuming from your queues, Amazon SQS samples all of its servers, and you receive all of your messages.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-26-16-63a23f3e82c8aced28fea56a79fbead4.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-26-16-63a23f3e82c8aced28fea56a79fbead4.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>When the wait time for the ReceiveMessage API action is greater than 0, <em>long polling</em> is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">ReceiveMessage</a> request) and false empty responses (when messages are available but aren't included in a response)</p><p>Therefore, the Developer should call the <code>ReceiveMessage</code> API with the <code>WaitTimeSeconds</code> parameter set to 20 to enable long polling.</p><p><strong>CORRECT: </strong>\"Call the <code>ReceiveMessage</code> API with the <code>WaitTimeSecond</code>s parameter set to 20 \" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Call the <code>ReceiveMessage</code> API with the <code>VisibilityTimeout </code>parameter set to 30\" is incorrect</p><p><strong>INCORRECT:</strong> \"Call the <code>SetQueueAttributes</code> API with the <code>DelaySeconds</code> parameter set to 900\" is incorrect</p><p><strong>INCORRECT:</strong> \"Call the <code>SetQueueAttributes </code>API with the <code>maxReceiveCount</code> set to 20\" is incorrect</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 34,
    "question": "<p>A company currently runs a number of legacy automated batch processes for system update management and operational activities. The company are looking to refactor these processes and require a service that can coordinate multiple AWS services into serverless workflows.</p><p>What is the MOST suitable service for this requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Lambda</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Batch</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Step Functions</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon SWF</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or <em>task</em>, allowing you to scale and change applications quickly.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-27-06-992775d6a19ae40546963a3b42eab975.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-27-06-992775d6a19ae40546963a3b42eab975.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Step Functions provides a reliable way to coordinate components and step through the functions of your application. Step Functions offers a graphical console to visualize the components of your application as a series of steps. It automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected, every time. Step Functions logs the state of each step, so when things go wrong, you can diagnose and debug problems quickly.</p><p><strong>CORRECT: </strong>\"AWS Step Functions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon SWF\" is incorrect. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in <a href=\"https://aws.amazon.com/what-is-cloud-computing/\">the Cloud.</a> It does not coordinate serverless workflows.</p><p><strong>INCORRECT:</strong> \"AWS Batch\" is incorrect as this is used to run batch computing jobs on Amazon EC2 and is therefore not serverless.</p><p><strong>INCORRECT:</strong> \"AWS Lambda\" is incorrect as though it is serverless, it does not provide a native capability to coordinate multiple AWS services.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\">https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/what-is-cloud-computing/",
      "https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 35,
    "question": "<p>A Developer created an AWS Lambda function for a serverless application. The Lambda function has been executing for several minutes and the Developer cannot find any log data in CloudWatch Logs.</p><p>What is the MOST likely explanation for this issue?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The Lambda function is missing CloudWatch Logs as a source trigger to send log data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>The Lambda function is missing a target CloudWatch Logs group</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>AWS Lambda automatically monitors Lambda functions on your behalf, reporting metrics through Amazon CloudWatch. To help you troubleshoot failures in a function, Lambda logs all requests handled by your function and also automatically stores logs generated by your code through Amazon CloudWatch Logs.</p><p>Lambda automatically integrates with CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function, which is named /aws/lambda/<em>&lt;function name&gt;</em>.</p><p>An AWS Lambda function's execution role grants it permission to access AWS services and resources. You provide this role when you create a function, and Lambda assumes the role when your function is invoked. You can create an execution role for development that has permission to send logs to Amazon CloudWatch and upload trace data to AWS X-Ray.</p><p>For the lambda function to create log stream and publish logs to cloudwatch, the lambda execution role needs to have the following permissions:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-55-25-441990764d87d14c5376625017fb3928.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-55-25-441990764d87d14c5376625017fb3928.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The most likely cause of this issue is that the execution role assigned to the Lambda function does not have the permissions (shown above) to write to CloudWatch Logs.</p><p><strong>CORRECT: </strong>\"The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs\" is incorrect as this is not required, Lambda automatically logs data to CloudWatch logs and just needs the permissions to do so.</p><p><strong>INCORRECT:</strong> \"The Lambda function is missing a target CloudWatch Logs group\" is incorrect as the CloudWatch Logs group will be created automatically if the function has sufficient permissions.</p><p><strong>INCORRECT:</strong> \"The Lambda function is missing CloudWatch Logs as a source trigger to send log data\" is incorrect as CloudWatch Logs is a destination, not a source in this case. However, you do not need to configure CloudWatch Logs as a destination, it is automatic.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-monitoring.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-monitoring.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/lambda-monitoring.html",
      "https://digitalcloud.training/aws-lambda/",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  },
  {
    "id": 36,
    "question": "<p>The development team is experiencing issues with their application hosted on Amazon EC2 instances, as they are unable to connect to an Amazon S3 bucket during test runs.</p><p>What should be the appropriate measures to resolve this issue? (Select TWO.)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Verify the IAM roles attached to the EC2 instances and ensure they have the necessary permissions to access the S3 bucket.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Validate the VPC peering connections to ensure the S3 bucket is reachable.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Modify the Amazon S3 bucket to be public to allow EC2 instances to access it.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Check the security groups attached to the EC2 instances to ensure the required inbound rules are in place.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Check the bucket policies for the Amazon S3 bucket and confirm that they permit access from the EC2 instances.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Storage",
    "explanation": "<p>IAM roles attached to the EC2 instances allow applications running on these instances to use AWS services without the need to manage and store AWS credentials. If an EC2 instance is unable to access an S3 bucket, one possibility is that the IAM role associated with the EC2 instance does not have the necessary permissions to access the S3 bucket. Checking and updating the IAM roles can resolve this issue.</p><p>Amazon S3 bucket policies define who can access the contents of the bucket and what actions they can perform. If the EC2 instances are unable to access the S3 bucket, another possible cause is that the bucket policy does not permit access to the EC2 instances. Checking and modifying the bucket policies to grant access to the EC2 instances can fix this problem.</p><p><strong>CORRECT: </strong>\"Verify the IAM roles attached to the EC2 instances and ensure they have the necessary permissions to access the S3 bucket\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Check the bucket policies for the Amazon S3 bucket and confirm that they permit access from the EC2 instances\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check the security groups attached to the EC2 instances to ensure the required inbound rules are in place\" is incorrect.</p><p>Security groups are associated with EC2 instances and act as a firewall that controls the traffic for one or more instances. Inbound rules affect traffic coming towards the EC2 instance and this does not affect access to S3. For S3, the instance needs to have an outbound rule that allows the relevant protocols.</p><p><strong>INCORRECT:</strong> \"Validate the VPC peering connections to ensure the S3 bucket is reachable\" is incorrect.</p><p>Amazon VPC peering connection allows you to route traffic between two VPCs using private IPv4 or IPv6 addresses. However, S3 buckets are not accessed via VPC peering connections; they are accessed using the Amazon S3 service endpoint. Hence, validating VPC peering connections won't solve the issue here.</p><p><strong>INCORRECT:</strong> \"Modify the Amazon S3 bucket to be public to allow EC2 instances to access it\" is incorrect.</p><p>Making an Amazon S3 bucket public can pose serious security risks, as it will allow anyone to access the contents of the bucket. This action is generally not recommended, especially not to solve access issues for EC2 instances. The best practice is to manage access at a more granular level using IAM roles and bucket policies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html",
      "https://digitalcloud.training/aws-iam/"
    ]
  },
  {
    "id": 37,
    "question": "<p>An application needs to read up to 100 items at a time from an Amazon DynamoDB. Each item is up to 100 KB in size and all attributes must be retrieved.</p><p>What is the BEST way to minimize latency?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a Query operation with a <code>FilterExpression</code> </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use a Scan operation with pagination</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use <code>BatchGetItem</code> </p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use <code>GetItem</code> and use a projection expression</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>The BatchGetItem operation returns the attributes of one or more items from one or more tables. You identify requested items by primary key.</p><p>A single operation can retrieve up to 16 MB of data, which can contain as many as 100 items. In order to minimize response latency, BatchGetItem retrieves items in parallel.</p><p>By default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables.</p><p><strong>CORRECT: </strong>\"Use <code>BatchGetItem</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use <code>GetItem</code> and use a projection expression\" is incorrect as this will limit the attributes returned and will retrieve the items sequentially which results in more latency.</p><p><strong>INCORRECT:</strong> \"Use a Scan operation with pagination\" is incorrect as a Scan operation is the least efficient way to retrieve the data as all items in the table are returned and then filtered. Pagination just breaks the results into pages.</p><p><strong>INCORRECT:</strong> \"Use a Query operation with a <code>FilterExpression</code>\" is incorrect as this would limit the results that are returned.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html\">https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 38,
    "question": "<p>A healthcare service wants to exchange patient data securely with a partner organization through an HTTP API endpoint provided by the partner. The healthcare service has the requisite API key for accessing the HTTP API. The service needs a solution to manage the API key through code.</p><p>Which method will fulfill these requirements with maximum security?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon DynamoDB to store the API key and retrieve it when needed.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store the API key in an environment variable on the application server.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Secrets Manager to store and retrieve the API key.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Store the API key in the application code.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve API keys, database credentials, and other secrets throughout their lifecycle, making it the most secure option.</p><p><strong>CORRECT: </strong>\"Use AWS Secrets Manager to store and retrieve the API key\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the API key in the application code\" is incorrect.</p><p>Storing the API key directly in the application code is a security risk and could lead to unintended exposure of the key.</p><p><strong>INCORRECT:</strong> \"Use Amazon DynamoDB to store the API key and retrieve it when needed\" is incorrect.</p><p>While Amazon DynamoDB could be used to store the API key, it does not provide the same level of security management and key rotation capabilities that AWS Secrets Manager offers.</p><p><strong>INCORRECT:</strong> \"Store the API key in an environment variable on the application server\" is incorrect.</p><p>Storing the API key in an environment variable on the application server could lead to unintentional exposure of the key if the environment variables were logged or leaked in some way.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-secrets-manager/\">https://digitalcloud.training/aws-secrets-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/secrets-manager/",
      "https://digitalcloud.training/aws-secrets-manager/"
    ]
  },
  {
    "id": 39,
    "question": "<p>A Developer has joined a team and needs to connect to the AWS CodeCommit repository using SSH. What should the Developer do to configure access using Git?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>On the Developer’s IAM account, under security credentials, choose to create an access key and secret ID</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Generate an SSH public and private key. Upload the public key to the Developer’s IAM account</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create an account on Github and user those login credentials to login to AWS CodeCommit</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>On the Developer’s IAM account, under security credentials, choose to create HTTPS Git credentials for AWS CodeCommit</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>You need to configure your Git client to communicate with CodeCommit repositories. As part of this configuration, you provide IAM credentials that CodeCommit can use to authenticate you. IAM supports CodeCommit with three types of credentials:</p><p> • Git credentials, an IAM -generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.</p><p> • SSH keys, a locally generated public-private key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.</p><p> • <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html\">AWS access keys</a>, which you can use with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.</p><p>As the Developer is going to use SSH, he first needs to generate an SSH private and public key. These can then be used for authentication. The method of creating these depends on the operating system the Developer is using. Then, the Developer can upload the public key (by copying the contents of the file) into his IAM account under security credentials.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-17-00-02b21d08eb5bbcd140ffcb7e14ff00f3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_03-17-00-02b21d08eb5bbcd140ffcb7e14ff00f3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Generate an SSH public and private key. Upload the public key to the Developer’s IAM account\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"On the Developer’s IAM account, under security credentials, choose to create HTTPS Git credentials for AWS CodeCommit\" is incorrect as this method is used for creating credentials when you want to connect to CodeCommit using HTTPS.</p><p><strong>INCORRECT:</strong> \"Create an account on Github and user those login credentials to login to AWS CodeCommit\" is incorrect as you cannot login to AWS CodeCommit using credentials from Github.</p><p><strong>INCORRECT:</strong> \"On the Developer’s IAM account, under security credentials, choose to create an access key and secret ID\" is incorrect as though you can use access keys to authenticated to CodeCommit, this requires the credential helper, and enables access over HTTPS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html</a></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-unixes.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-unixes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html",
      "https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-unixes.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 40,
    "question": "<p>A Developer needs to restrict all users and roles from using a list of API actions within a member account in AWS Organizations. The Developer needs to deny access to a few specific API actions.</p><p>What is the MOST efficient way to do this?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an allow list and specify the API actions to deny</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an IAM policy that allows only the unrestricted API actions</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a deny list and specify the API actions to deny</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an IAM policy that denies the API actions for all users and roles</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>Service control policies (SCPs) are one type of policy that you can use to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines.</p><p>You can configure the SCPs in your organization to work as either of the following:</p><p> • A <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html#orgs_policies_denylist\">deny list</a> – actions are allowed by default, and you specify what services and actions are prohibited</p><p> • An <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html#orgs_policies_allowlist\">allow list</a> – actions are prohibited by default, and you specify what services and actions are allowed</p><p>As there are only a few API actions to restrict the most efficient strategy for this scenario is to create a deny list and specify the specific actions that are prohibited.</p><p><strong>CORRECT: </strong>\"Create a deny list and specify the API actions to deny\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an allow list and specify the API actions to deny\" is incorrect as with an allow list you specify the API actions to allow.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy that denies the API actions for all users and roles\" is incorrect as you cannot create deny policies in IAM. IAM policies implicitly deny access unless you explicitly allow permissions.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy that allows only the unrestricted API actions\" is incorrect. This will not work for administrative users such as the root account (as they have extra permissions) so an SCP must be used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html\">https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html#orgs_policies_denylist",
      "https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html#orgs_policies_allowlist",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html"
    ]
  },
  {
    "id": 41,
    "question": "<p>A Developer is creating a banking application that will be used to view financial transactions and statistics. The application requires multi-factor authentication to be added to the login protocol.</p><p>Which service should be used to meet this requirement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Cognito User Pool with MFA</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon Cognito Identity Pool with MFA</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Directory Service</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS IAM with MFA</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers.</p><p>User pools provide:</p><p> • Sign-up and sign-in services.</p><p> • A built-in, customizable web UI to sign in users.</p><p> • Social sign-in with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as sign-in with SAML identity providers from your user pool.</p><p> • User directory management and user profiles.</p><p> • Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.</p><p> • Customized workflows and user migration through AWS Lambda triggers.</p><p>Multi-factor authentication (MFA) increases security for your app by adding another authentication method, and not relying solely on username and password. You can choose to use SMS text messages, or time-based one-time (TOTP) passwords as second factors in signing in your users.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-11-11-29cb0afc44b30879151772e2713834e0.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-11-11-29cb0afc44b30879151772e2713834e0.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>For this scenario you would want to set the MFA setting to “Required” as the data is highly secure.</p><p><strong>CORRECT: </strong>\"Amazon Cognito User Pool with MFA\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon Cognito Identity Pool with MFA\" is incorrect</p><p><strong>INCORRECT:</strong> \"AWS IAM with MFA\" is incorrect. With IAM your user accounts are maintained in your AWS account rather than in a Cognito User Pool. For logging into a web or mobile app it is better to create and manage your users in a Cognito User Pool and add MFA to the User Pool for extra security.</p><p><strong>INCORRECT:</strong> \"AWS Directory Service\" is incorrect as this is a managed Active Directory service. For a web or mobile application using AWS Cognito User Pools is a better solution for storing your user accounts and authenticating to the application.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html",
      "https://digitalcloud.training/amazon-cognito/"
    ]
  },
  {
    "id": 42,
    "question": "<p>A company needs a fully-managed source control service that will work in AWS. The service must ensure that revision control synchronizes multiple distributed repositories by exchanging sets of changes peer-to-peer. All users need to work productively even when not connected to a network.</p><p>Which source control service should be used?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Subversion</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS CodeBuild</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS CodeCommit</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>AWS CodeStar</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>AWS CodeCommit is a version control service hosted by Amazon Web Services that you can use to privately store and manage assets (such as documents, source code, and binary files) in the cloud.</p><p>A repository is the fundamental version control object in CodeCommit. It's where you securely store code and files for your project. It also stores your project history, from the first commit through the latest changes. You can share your repository with other users so you can work together on a project. If you add AWS tags to repositories, you can set up notifications so that repository users receive email about events (for example, another user commenting on code).</p><p>You can also change the default settings for your repository, browse its contents, and more. You can create triggers for your repository so that code pushes or other events trigger actions, such as emails or code functions. You can even configure a repository on your local computer (a local repo) to push your changes to more than one repository.</p><p><strong>CORRECT: </strong>\"AWS CodeCommit\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Subversion\" is incorrect as this is not a fully managed source control system</p><p><strong>INCORRECT:</strong> \"AWS CodeBuild\" is incorrect as this is a service used for building and testing code.</p><p><strong>INCORRECT:</strong> \"AWS CodeStar\" is incorrect as this is not a source control system; it integrates with source control systems such as CodeCommit.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/codecommit/\">https://aws.amazon.com/codecommit/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/codecommit/",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 43,
    "question": "<p>A Developer needs to return a list of items in a global secondary index from an Amazon DynamoDB table.</p><p>Which DynamoDB API call can the Developer use in order to consume the LEAST number of read capacity units?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Query operation using strongly-consistent reads</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Scan operation using strongly-consistent reads</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Scan operation using eventually-consistent reads</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Query operation using eventually-consistent reads</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>The Query operation finds items based on primary key values. You can query any table or secondary index that has a composite primary key (a partition key and a sort key).</p><p>For items up to 4 KB in size, one RCU equals one strongly consistent read request per second or two eventually consistent read requests per second. Therefore, using eventually consistent reads uses fewer RCUs.</p><p><strong>CORRECT: </strong>\"Query operation using eventually-consistent reads\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Query operation using strongly-consistent reads\" is incorrect as strongly-consistent reads use more RCUs than eventually consistent reads.</p><p><strong>INCORRECT:</strong> \"Scan operation using eventually-consistent reads\" is incorrect. The Scan operation returns one or more items and item attributes by accessing every item in a table or a secondary index and therefore uses more RCUs than a query operation.</p><p><strong>INCORRECT:</strong> \"Scan operation using strongly-consistent reads\" is incorrect. The Scan operation returns one or more items and item attributes by accessing every item in a table or a secondary index and therefore uses more RCUs than a query operation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html\">https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 44,
    "question": "<p>Based on the following AWS CLI command the resulting output, what has happened here?</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pln\">$ aws </span><span class=\"kwd\">lambda</span><span class=\"pln\"> invoke </span><span class=\"pun\">--</span><span class=\"kwd\">function</span><span class=\"pun\">-</span><span class=\"pln\">name </span><span class=\"typ\">MyFunction</span><span class=\"pln\"> </span><span class=\"pun\">--</span><span class=\"pln\">payload ewogICJrZXkxIjogInZhbHVlMSIsCiAgImtleTIiOiAidmFsdWUyIiwKICAia2V5MyI6ICJ2YWx1ZTMiCn0</span><span class=\"pun\">=</span><span class=\"pln\"> response</span><span class=\"pun\">.</span><span class=\"pln\">json</span></li><li class=\"L1\"><span class=\"pun\">{</span></li><li class=\"L2\"><span class=\"str\">\"StatusCode\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"lit\">200</span></li><li class=\"L3\"><span class=\"pun\">}</span></li></ol></pre></div></div>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>An AWS Lambda function has been invoked asynchronously and has completed successfully</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>An AWS Lambda function has been invoked asynchronously and has not completed successfully</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>An AWS Lambda function has been invoked synchronously and has completed successfully</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>An AWS Lambda function has been invoked synchronously and has not completed successfully</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>When you invoke a function synchronously, Lambda runs the function and waits for a response. When the function execution ends, Lambda returns the response from the function's code with additional data, such as the version of the function that was executed. To invoke a function synchronously with the AWS CLI, use the invoke command.</p><p>The following diagram shows clients invoking a Lambda function synchronously. Lambda sends the events directly to the function and sends the function's response back to the invoker.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_00-54-43-14be40d280284280078fb2c39c93a4b4.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_00-54-43-14be40d280284280078fb2c39c93a4b4.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>We know the function has been run synchronously as the<code> --invocation-type Event</code> parameter has not been included. Also, the status code 200 indicates a successful execution of a synchronous execution.</p><p><strong>CORRECT: </strong>\"An AWS Lambda function has been invoked synchronously and has completed successfully\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"An AWS Lambda function has been invoked synchronously and has not completed successfully\" is incorrect as the status code 200 indicates a successful execution.</p><p><strong>INCORRECT:</strong> \"An AWS Lambda function has been invoked asynchronously and has completed successfully\" is incorrect as the <code>--invocation-type Event</code> has parameter is not included so this is not an asynchronous invocation.</p><p><strong>INCORRECT:</strong> \"An AWS Lambda function has been invoked asynchronously and has not completed successfully\" is incorrect as the<code> --invocation-type Event</code> has parameter is not included so this is not an asynchronous invocation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-sync.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-sync.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/invocation-sync.html",
      "https://digitalcloud.training/aws-lambda/"
    ]
  },
  {
    "id": 45,
    "question": "<p>A Developer has setup an Amazon Kinesis Data Stream with 6 shards to ingest a maximum of 2000 records per second. An AWS Lambda function has been configured to process these records. In which order will these records be processed?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Lambda will receive each record in the exact order it was placed into the shard. There is no guarantee of order across shards</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>The Developer can select exact order or reverse order using the <code>GetRecords</code> API</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Lambda will receive each record in the reverse order it was placed into the stream</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Lambda will receive each record in the exact order it was placed into the stream</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p><p>KDS receives data from producers, and the data is stored in shards. Consumers then take the data and process it. In this case the AWS Lambda function is consuming the records from the shards.</p><p>In this scenario an application will be producing records and placing them in the stream as in step 1 of the image below. The AWS Lambda function will then consume the records (step 2) and will then execute the function by assuming the execution role specified (step 3).</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-53-56-58214fabd5311dafd38861f672fb92d9.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-53-56-58214fabd5311dafd38861f672fb92d9.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>A shard is an append-only log and a unit of streaming capability. A shard contains an ordered sequence of records ordered by arrival time. The order is guaranteed within a shard but not across shards.</p><p>Therefore, the best answer to this question is that AWS Lambda will receive each record in the exact order it was placed into the shard but there is no guarantee of order across shards</p><p><strong>CORRECT: </strong>\"Lambda will receive each record in the exact order it was placed into the shard. There is no guarantee of order across shards\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Lambda will receive each record in the exact order it was placed into the stream \" is incorrect as there are multiple shards in the stream and the order of records is not guaranteed across shards.</p><p><strong>INCORRECT:</strong> \"Lambda will receive each record in the reverse order it was placed into the stream\" is incorrect as the order is guaranteed within a shard.</p><p><strong>INCORRECT:</strong> \"The Developer can select exact order or reverse order using the GetRecords API\" is incorrect as you cannot choose the order you receive records with the GetRecords API.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/getting-started/\">https://aws.amazon.com/kinesis/data-streams/getting-started/</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/getting-started/",
      "https://aws.amazon.com/kinesis/data-streams/faqs/",
      "https://digitalcloud.training/amazon-kinesis/"
    ]
  },
  {
    "id": 46,
    "question": "<p>A Developer must run a shell script on Amazon EC2 Linux instances each time they are launched by an Amazon EC2 Auto Scaling group. What is the SIMPLEST way to run the script?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add the script to the user data when creating the launch configuration</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure Amazon CloudWatch Events to trigger the AWS CLI when an instance is launched and run the script</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Run the script using the AWS Systems Manager Run Command</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Package the script in a zip file with some AWS Lambda source code. Upload to Lambda and run the function when instances are launched</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>The simplest option is to add the script to the user data when creating the launch configuration. User data is information that is parsed when the EC2 instances are launched. When you add a script to the user data in a launch configuration all instances that are launched by that Auto Scaling group will run the script.</p><p><strong>CORRECT: </strong>\"Add the script to the user data when creating the launch configuration\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure Amazon CloudWatch Events to trigger the AWS CLI when an instance is launched and run the script\" is incorrect as you cannot trigger the AWS CLI using CloudWatch Events and the script may not involve AWS CLI commands.</p><p><strong>INCORRECT:</strong> \"Package the script in a zip file with some AWS Lambda source code. Upload to Lambda and run the function when instances are launched\" is incorrect as Lambda does not run shell scripts. You could program the requirements into the function code however you still need a trigger which is not mentioned in this option.</p><p><strong>INCORRECT:</strong> \"Run the script using the AWS Systems Manager Run Command\" is incorrect as this is not the simplest method. For most Linux AMIs (except Amazon Linux) the developer’s would need to install the agent on the operating system. They would also then need to create a mechanism of triggering the run command.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html",
      "https://digitalcloud.training/amazon-ec2/"
    ]
  },
  {
    "id": 47,
    "question": "<p>An application uses Amazon Kinesis Data Streams to ingest and process large streams of data records in real time. Amazon EC2 instances consume and process the data using the Amazon Kinesis Client Library (KCL). The application handles the failure scenarios and does not require standby workers. The application reports that a specific shard is receiving more data than expected. To adapt to the changes in the rate of data flow, the “hot” shard is resharded.</p><p>Assuming that the initial number of shards in the Kinesis data stream is 6, and after resharding the number of shards increased to 8, what is the maximum number of EC2 instances that can be deployed to process data from all the shards?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>8</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>6</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>1</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>12</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-32-49-7f35405f45902946e8dbb0e5eefca485.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-25_01-32-49-7f35405f45902946e8dbb0e5eefca485.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In this scenario, the number of shards has been increased to 8. Therefore, the maximum number of instances that can be deployed is 8 as the number of instances cannot exceed the number of shards.</p><p><strong>CORRECT: </strong>\"8\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"6\" is incorrect as this is not the maximum number of instances that can be deployed to process 8 shards. The maximum number of instances should be the same as the number of shards.</p><p><strong>INCORRECT:</strong> \"12\" is incorrect as the number of instances exceeds the number of shards. You should ensure that the number of instances does not exceed the number of shards</p><p><strong>INCORRECT:</strong> \"1\" is incorrect as this is not the maximum number of instances that can be deployed to process 8 shards. The maximum number of instances should be the same as the number of shards.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html",
      "https://digitalcloud.training/amazon-kinesis/"
    ]
  },
  {
    "id": 48,
    "question": "<p>A company provides a large number of services on AWS to customers. The customers connect to one or more services directly and the architecture is becoming complex. How can the architecture be refactored to provide a single interface for the services?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS X-Ray</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Single Sign On (SSO)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon API Gateway</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>AWS Cognito</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services.</p><p>Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-32-09-13e8e2f70cfb9c8992122d15b7483be8.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-32-09-13e8e2f70cfb9c8992122d15b7483be8.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>API Gateway can be used as the single interface for consumers of the services provided by the organization in this scenario. This solution will simplify the architecture.</p><p><strong>CORRECT: </strong>\"Amazon API Gateway\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"AWS X-Ray\" is incorrect. AWS X-Ray is used for analyzing and debugging applications.</p><p><strong>INCORRECT:</strong> \"AWS Cognito\" is incorrect. AWS Cognito is used for adding sign-up, sign-in and access control to web and mobile apps.</p><p><strong>INCORRECT:</strong> \"AWS Single Sign On (SSO)\" is incorrect. AWS SSO is used to provide central management of multiple AWS accounts and business applications and to provide single sign-on to accounts.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/api-gateway/features/\">https://aws.amazon.com/api-gateway/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/api-gateway/features/",
      "https://digitalcloud.training/amazon-api-gateway/"
    ]
  },
  {
    "id": 49,
    "question": "<p>A Developer is migrating Docker containers to Amazon ECS. A large number of containers will be deployed across some newly deployed ECS containers instances using the same instance type. High availability is provided within the microservices architecture. Which task placement strategy requires the LEAST configuration for this scenario?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>spread</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>random</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>binpack</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Fargate</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>When a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified in the task definition, such as CPU and memory. Similarly, when you scale down the task count, Amazon ECS must determine which tasks to terminate. You can apply task placement strategies and constraints to customize how Amazon ECS places and terminates tasks. Task placement strategies and constraints are not supported for tasks using the Fargate launch type. By default, Fargate tasks are spread across Availability Zones.</p><p>A <em>task placement strategy</em> is an algorithm for selecting instances for task placement or tasks for termination. For example, Amazon ECS can select instances at random, or it can select instances such that tasks are distributed evenly across a group of instances.</p><p>Amazon ECS supports the following task placement strategies:</p><p> • binpack</p><p>Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.</p><p> • random</p><p>Place tasks randomly.</p><p> • spread</p><p>Place tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone. Service tasks are spread based on the tasks from that service. Standalone tasks are spread based on the tasks from the same task group.</p><p>Therefore, for this scenario the random task placement strategy is most suitable as it requires the least configuration.</p><p><strong>CORRECT: </strong>\"random\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"spread\" is incorrect. As high availability is taken care of within the containers there is no need to use a spread strategy to ensure HA.</p><p><strong>INCORRECT:</strong> \"binpack\" is incorrect as there is no need to pack the containers onto the fewest instances based on CPU or memory.</p><p><strong>INCORRECT:</strong> \"Fargate\" is incorrect as this is not a task placement strategy, it is a serverless service for running containers.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html",
      "https://digitalcloud.training/amazon-ecs-and-eks/"
    ]
  },
  {
    "id": 50,
    "question": "<p>A large quantity of sensitive data must be encrypted. A Developer will use a custom CMK to generate the encryption key. The key policy currently looks like this:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow Key Usage\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"Principal\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">{</span><span class=\"str\">\"AWS\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L4\"><span class=\"str\">\"arn:aws:iam::111122223333:user/CMKUser\"</span></li><li class=\"L5\"><span class=\"pun\">]},</span></li><li class=\"L6\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L7\"><span class=\"str\">\"kms:Encrypt\"</span><span class=\"pun\">,</span></li><li class=\"L8\"><span class=\"str\">\"kms:Decrypt\"</span><span class=\"pun\">,</span></li><li class=\"L9\"><span class=\"str\">\"kms:ReEncrypt*\"</span><span class=\"pun\">,</span></li><li class=\"L0\"><span class=\"str\">\"kms:DescribeKey\"</span></li><li class=\"L1\"><span class=\"pun\">],</span></li><li class=\"L2\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"*\"</span></li><li class=\"L3\"><span class=\"pun\">}</span></li></ol></pre></div></div><p>What API action must be added to the key policy?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p><code>kms:GetKeyPolicy</code> </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p><code>kms:EnableKey</code> </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p><code>kms:GenerateDataKey</code> </p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p><code>kms:CreateKey</code> </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>A key policy is a document that uses <a href=\"http://json.org/\">JSON (JavaScript Object Notation)</a> to specify permissions. You can work with these JSON documents directly, or you can use the AWS Management Console to work with them using a graphical interface called the <em>default view</em>.</p><p>The key policy supplied with this question is missing the GenerateDataKey API action which is a permission that is required to generate a data encryption key. A data encryption key is required to encrypt large amounts of data as a CMK can only encrypt up to 4 KB.</p><p>The GenerateDataKey API Generates a unique symmetric data key. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.</p><p><strong>CORRECT: </strong>\"<code>kms:GenerateDataKey</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"<code>kms:EnableKey</code>\" is incorrect as this sets the key state of a customer master key (CMK) to enabled. It allows you to use the CMK for <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#cryptographic-operations\">cryptographic operations</a>.</p><p><strong>INCORRECT:</strong> \"<code>kms:CreateKey</code>\" is incorrect as this creates a unique customer managed <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master-keys\">customer master key</a> (CMK) in your AWS account and Region. In this case the CMK already exists, the Developer needs to create a data encryption key.</p><p><strong>INCORRECT:</strong> \"<code>kms:GetKeyPolicy</code>\" is incorrect as this simply gets a key policy attached to the specified customer master key (CMK).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html\">https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "http://json.org/",
      "https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#cryptographic-operations",
      "https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master-keys",
      "https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html",
      "https://digitalcloud.training/aws-kms/"
    ]
  },
  {
    "id": 51,
    "question": "<p>A media company uses Amazon EC2 instances managed by AWS Elastic Beanstalk to run its high-traffic website. The engineering team needs to introduce a new feature, which requires upgrading the underlying platform to a newer version of Node.js. The deployment of the new code and the platform upgrade need to happen without causing any downtime.</p><p>Which strategy should the team adopt to fulfill these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a duplicate EC2 instance manually with the new version of Node.js and the updated code. Test the new instance and replace one of the instances behind the ELB once the testing is successful.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement Blue/Green (CNAME Swap) deployment using Elastic Beanstalk. Prepare a separate environment with the new version of Node.js and the new code, and once testing is complete, swap the CNAMEs of the two environments.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Upgrade the platform version of the current Elastic Beanstalk environment, then deploy the new application code. Monitor the application and quickly roll back changes if any issues occur.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS CodeDeploy to deploy the new application code first, then manually update Node.js on the Elastic Beanstalk environment.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>The Blue/Green deployment strategy involves creating a separate environment (the 'green' environment) with the new application version and platform updates. It allows full testing and validation. Once you're confident in the 'green' environment, you can swap the environment URLs to redirect traffic to the new environment. This results in zero downtime.</p><p><strong>CORRECT: </strong>\"Implement Blue/Green (CNAME Swap) deployment using Elastic Beanstalk. Prepare a separate environment with the new version of Node.js and the new code, and once testing is complete, swap the CNAMEs of the two environments\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Upgrade the platform version of the current Elastic Beanstalk environment, then deploy the new application code. Monitor the application and quickly roll back changes if any issues occur\" is incorrect.</p><p>This approach introduces risks that could result in downtime during the upgrade and deployment process. If any issues occur during the upgrade or after the deployment of the new application code, rolling back these changes could cause further downtime.</p><p><strong>INCORRECT:</strong> \"Create a duplicate EC2 instance manually with the new version of Node.js and the updated code. Test the new instance and replace one of the instances behind the ELB once the testing is successful\" is incorrect.</p><p>Managing the process manually and replacing instances behind the load balancer may result in inconsistencies and is not as streamlined or reliable as using the deployment and environment management capabilities of Elastic Beanstalk.</p><p><strong>INCORRECT:</strong> \"Use a rolling deployment for the new application code. Apply the code to a subset of EC2 instances until the tests pass. Redeploy the previous code if the tests fail\" is incorrect.</p><p>While AWS CodeDeploy is a robust deployment service, performing the platform upgrade manually after the code deployment doesn't guarantee zero downtime, as the platform upgrade might require a restart of the EC2 instances, causing service interruptions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html",
      "https://digitalcloud.training/aws-elastic-beanstalk/"
    ]
  },
  {
    "id": 52,
    "question": "<p>An application is running on a fleet of EC2 instances running behind an Elastic Load Balancer (ELB). The EC2 instances session data in a shared Amazon S3 bucket. Security policy mandates that data must be encrypted in transit.</p><p>How can the Developer ensure that all data that is sent to the S3 bucket is encrypted in transit?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an S3 bucket policy that denies traffic where SecureTransport is true</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an S3 bucket policy that denies traffic where SecureTransport is false</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure HTTP to HTTPS redirection on the Elastic Load Balancer</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>At the Amazon S3 bucket level, you can configure permissions through a bucket policy. For example, you can limit access to the objects in a bucket by IP address range or specific IP addresses. Alternatively, you can make the objects accessible only through HTTPS.</p><p>The following bucket policy allows access to Amazon S3 objects only through HTTPS (the policy was generated with the AWS Policy Generator).</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-15-33-c945d6087ebcc622bf8f8ec6d087332b.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-15-33-c945d6087ebcc622bf8f8ec6d087332b.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Here the bucket policy explicitly denies (\"Effect\": \"Deny\") all read access (\"Action\": \"s3:GetObject\") from anybody who browses (\"Principal\": \"*\") to Amazon S3 objects within an Amazon S3 bucket if they are not accessed through HTTPS (\"aws:SecureTransport\": \"false\").</p><p><strong>CORRECT: </strong>\"Create an S3 bucket policy that denies traffic where SecureTransport is false\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an S3 bucket policy that denies traffic where SecureTransport is true\" is incorrect. This will not work as it is denying traffic that IS encrypted in transit.</p><p><strong>INCORRECT:</strong> \"Create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption\" is incorrect. This will ensure that the data is encrypted at rest, but not in-transit.</p><p><strong>INCORRECT:</strong> \"Configure HTTP to HTTPS redirection on the Elastic Load Balancer\" is incorrect. This will ensure the client traffic reaching the ELB is encrypted however we need to ensure the traffic from the EC2 instances to S3 is encrypted and the ELB is not involved in this communication.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/\">https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/",
      "https://digitalcloud.training/amazon-s3-and-glacier/"
    ]
  },
  {
    "id": 53,
    "question": "<p>An AWS Lambda function must be connected to an Amazon VPC private subnet that does not have Internet access. The function also connects to an Amazon DynamoDB table. What MUST a Developer do to enable access to the DynamoDB table?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Attach an ENI to the DynamoDB table</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Attach an Internet Gateway</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a route table</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a VPC endpoint</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>To connect to AWS services from a private subnet with no internet access, use VPC endpoints. A <em>VPC endpoint</em> for DynamoDB enables resources in a VPC to use their private IP addresses to access DynamoDB with no exposure to the public internet.</p><p>When you create a VPC endpoint for DynamoDB, any requests to a DynamoDB endpoint within the Region (for example, <em>dynamodb.us-west-2.amazonaws.com</em>) are routed to a private DynamoDB endpoint within the Amazon network.</p><p><strong>CORRECT: </strong>\"Configure a VPC endpoint\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Attach an Internet Gateway\" is incorrect as you do not attach these to a private subnet.</p><p><strong>INCORRECT:</strong> \"Create a route table\" is incorrect as a route table will exist for all subnets and it does not help to route out from a private subnet via the Internet unless an entry for a NAT Gateway or Instance is added.</p><p><strong>INCORRECT:</strong> \"Attach an ENI to the DynamoDB table\" is incorrect as you do not attach Elastic Network Interfaces to DynamoDB tables.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/troubleshooting-networking.html\">https://docs.aws.amazon.com/lambda/latest/dg/troubleshooting-networking.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/troubleshooting-networking.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html",
      "https://digitalcloud.training/amazon-vpc/"
    ]
  },
  {
    "id": 54,
    "question": "<p>A Developer is writing an AWS Lambda function that processes records from an Amazon Kinesis Data Stream. The Developer must write the function so that it sends a notice to Administrators if it fails to process a batch of records.</p><p>How should the Developer write the function?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Separate the Lambda handler from the core logic</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Push the failed records to an Amazon SQS queue</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon CloudWatch Events to send the processed data</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure an Amazon SNS topic as an on-failure destination</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>With Destinations, you can route asynchronous function results as an execution record to a destination resource without writing additional code. An execution record contains details about the request and response in JSON format including version, timestamp, request context, request payload, response context, and response payload.</p><p>For each execution status such as <em>Success</em> or <em>Failure</em> you can choose one of four destinations: another Lambda function, SNS, SQS, or EventBridge. Lambda can also be configured to route different execution results to different destinations.</p><p>In this scenario the Developer can publish the processed data to an Amazon SNS topic by configuring an Amazon SNS topic as an on-failure destination.</p><p><strong>CORRECT: </strong>\"Configure an Amazon SNS topic as an on-failure destination\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Separate the Lambda handler from the core logic\" is incorrect as this will not assist with sending a notification to administrators.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events to send the processed data\" is incorrect as CloudWatch Events is used for tracking state changes, not forwarding execution results</p><p><strong>INCORRECT:</strong> \"Push the failed records to an Amazon SQS queue\" is incorrect as SQS will not notify the administrators, SNS should be used.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/\">https://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/",
      "https://digitalcloud.training/aws-lambda/"
    ]
  },
  {
    "id": 55,
    "question": "<p>A Developer is deploying an Amazon ECS update using AWS CodeDeploy. In the appspec.yaml file, which of the following is a valid structure for the order of hooks that should be specified?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>BeforeBlockTraffic &gt; AfterBlockTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p><br></p><p>BeforeInstall &gt; AfterInstall &gt; ApplicationStart &gt; ValidateService</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>BeforeInstall &gt; AfterInstall &gt; AfterAllowTestTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts.</p><p>The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.</p><p>The following code snippet shows a valid example of the structure of hooks for an Amazon ECS deployment:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-44-56-2017625f2c25c61736eb3231c9b6895f.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-44-56-2017625f2c25c61736eb3231c9b6895f.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, in this scenario a valid structure for the order of hooks that should be specified in the appspec.yml file is: BeforeInstall &gt; AfterInstall &gt; AfterAllowTestTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic</p><p><strong>CORRECT: </strong>\"BeforeInstall &gt; AfterInstall &gt; AfterAllowTestTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"BeforeInstall &gt; AfterInstall &gt; ApplicationStart &gt; ValidateService\" is incorrect as this would be valid for Amazon EC2.</p><p><strong>INCORRECT:</strong> \"BeforeAllowTraffic &gt; AfterAllowTraffic\" is incorrect as this would be valid for AWS Lambda.</p><p><strong>INCORRECT:</strong> \"BeforeBlockTraffic &gt; AfterBlockTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic\" is incorrect as this is a partial listing of hooks for Amazon EC2 but is incomplete.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 56,
    "question": "<p>A monitoring application that keeps track of a large eCommerce website uses Amazon Kinesis for data ingestion. During periods of peak data rates, the Kinesis stream cannot keep up with the incoming data.<br>What step will allow Kinesis data streams to accommodate the traffic during peak hours?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>1. Ingest multiple records into the stream in a single call using PutRecords</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Install the Kinesis Producer Library (KPL) for ingesting data into the stream</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an SQS queue and decouple the producers from the Kinesis data stream</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Increase the shard count of the stream using UpdateShardCount</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>The UpdateShardCount API action updates the shard count of the specified stream to the specified number of shards.</p><p>Updating the shard count is an asynchronous operation. Upon receiving the request, Kinesis Data Streams returns immediately and sets the status of the stream to UPDATING. After the update is complete, Kinesis Data Streams sets the status of the stream back to ACTIVE.</p><p>Depending on the size of the stream, the scaling action could take a few minutes to complete. You can continue to read and write data to your stream while its status is UPDATING.</p><p>To update the shard count, Kinesis Data Streams performs splits or merges on individual shards. This can cause short-lived shards to be created, in addition to the final shards. These short-lived shards count towards your total shard limit for your account in the Region.</p><p>When using this operation, we recommend that you specify a target shard count that is a multiple of 25% (25%, 50%, 75%, 100%). You can specify any target value within your shard limit. However, if you specify a target that isn't a multiple of 25%, the scaling action might take longer to complete.</p><p>This operation has the following default limits. By default, you cannot do the following:</p><p> • Scale more than ten times per rolling 24-hour period per stream</p><p> • Scale up to more than double your current shard count for a stream</p><p> • Scale down below half your current shard count for a stream</p><p> • Scale up to more than 500 shards in a stream</p><p> • Scale a stream with more than 500 shards down unless the result is less than 500 shards</p><p>Scale up to more than the shard limit for your account</p><p>Note that the question specifically states that the Kinesis data stream cannot keep up with incoming data. This indicates that the producers are attempting to add records to the stream but there are not enough shards to keep up with demand. Therefore, we need to add additional shards and can do this using the UpdateShardCount API action.</p><p><strong>CORRECT: </strong>\"Increase the shard count of the stream using UpdateShardCount\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Install the Kinesis Producer Library (KPL) for ingesting data into the stream\" is incorrect as that will help the producers to be more efficient and increase write throughput to a Kinesis data stream. However, this will not help as the Kinesis data stream already cannot keep up with the incoming demand.</p><p><strong>INCORRECT:</strong> \"Create an SQS queue and decouple the producers from the Kinesis data stream \" is incorrect. You cannot decouple a Kinesis producer from a Kinesis data stream using SQS. Kinesis is more than capable of keeping up with demand, it just needs more shards in this case.</p><p><strong>INCORRECT:</strong> \"Ingest multiple records into the stream in a single call using PutRecords\" is incorrect as the stream is already overloaded, we need more shards, not more data to be written.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_UpdateShardCount.html\">https://docs.aws.amazon.com/kinesis/latest/APIReference/API_UpdateShardCount.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/kinesis/latest/APIReference/API_UpdateShardCount.html",
      "https://digitalcloud.training/amazon-kinesis/"
    ]
  },
  {
    "id": 57,
    "question": "<p>An online multiplayer game employs Amazon API Gateway WebSocket APIs with an HTTP backend. The game developer needs to add a feature that identifies players with unstable connections who repeatedly join and leave the game. The developer also wants the ability to disconnect such players from the game.</p><p>What two modifications should the developer implement in the game to fulfill these requirements? (Select TWO.)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add logic to track the player's connection status using Amazon DynamoDB in the backend service.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Switch to AWS App Runner for the backend service.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Switch to REST APIs in the backend service.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Implement AWS Cognito for player authentication in the backend service.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Implement $connect and $disconnect routes in the backend service.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>By implementing $connect and $disconnect routes in the backend service, you can capture events when a player connects and disconnects from the WebSocket API. This enables tracking and managing of players' connection statuses.</p><p>Using Amazon DynamoDB (or another database service) allows you to persist and track the connection status of each player in real-time. This enables you to identify players who connect and disconnect frequently.</p><p><strong>CORRECT: </strong>\"Implement $connect and $disconnect routes in the backend service\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Add logic to track the player's connection status using Amazon DynamoDB in the backend service\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Switch to REST APIs in the backend service\" is incorrect.</p><p>Switching to REST APIs won't inherently provide the ability to manage or track unstable connections. WebSocket APIs are more suited for applications requiring real-time, two-way communication.</p><p><strong>INCORRECT:</strong> \"Implement AWS Cognito for player authentication in the backend service\" is incorrect.</p><p>While AWS Cognito provides authentication and user management, it doesn't offer features for tracking and managing unstable connections.</p><p><strong>INCORRECT:</strong> \"Switch to AWS App Runner for the backend service\" is incorrect.</p><p>AWS App Runner is a service that makes it easy to build, deploy, and scale containerized applications quickly, but it doesn't directly address the specific requirement of tracking and managing unstable connections.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api-route-keys-connect-disconnect.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api-route-keys-connect-disconnect.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api-route-keys-connect-disconnect.html",
      "https://digitalcloud.training/amazon-api-gateway/"
    ]
  },
  {
    "id": 58,
    "question": "<p>A software organization has developed a new feature in its serverless application hosted on AWS. This feature involves an AWS Lambda function that gets invoked by an Amazon API Gateway API. Currently, the API uses a specific Lambda alias to invoke the Lambda function. The organization wants to roll out this new feature to a select group of users for beta testing without affecting the application's existing users.</p><p>What would be the most efficient approach to meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AWS CLI to manually switch between the old and new versions of the Lambda function during testing periods.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement Amazon S3 bucket versioning on the Lambda function code. For testing purposes, link the API Gateway to the new version of the code stored in the S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS CodeDeploy to deploy the updated Lambda function. Split traffic between the new and old versions of the function for testing purposes.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a new version of the Lambda function. Build a new stage on API Gateway integrated with this new Lambda version. Utilize this new API Gateway stage for beta testing.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>In AWS, Lambda versions are an essential part of the deployment process. Once a version is published, the code within that version cannot be changed. It is ideal for ensuring stability and reliability between deployments.</p><p>API Gateway Stages can be used to route traffic to different Lambda versions. Hence, creating a new API Gateway stage for beta testing that is linked to the new version of the Lambda function will ensure that existing users (using the old stage) are not impacted by the new feature.</p><p><strong>CORRECT: </strong>\"Create a new version of the Lambda function. Build a new stage on API Gateway integrated with this new Lambda version. Utilize this new API Gateway stage for beta testing\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement Amazon S3 bucket versioning on the Lambda function code. For testing purposes, link the API Gateway to the new version of the code stored in the S3 bucket\" is incorrect.</p><p>While Amazon S3 versioning allows for keeping multiple variants of an object in the same bucket, it doesn't interact directly with AWS Lambda or API Gateway. Thus, you cannot control API Gateway to invoke different versions of Lambda function code stored in an S3 bucket.</p><p><strong>INCORRECT:</strong> \"Use AWS CodeDeploy to deploy the updated Lambda function. Split traffic between the new and old versions of the function for testing purposes\" is incorrect.</p><p>AWS CodeDeploy can deploy new versions of Lambda functions and shift traffic gradually but using it for testing purposes might risk exposing beta features to the regular users, potentially impacting their experience.</p><p><strong>INCORRECT:</strong> \"Use the AWS CLI to manually switch between the old and new versions of the Lambda function during testing periods\" is incorrect.</p><p>This approach is operationally intensive, prone to human error, and may impact regular users. AWS provides better ways of managing Lambda versions and aliases, which should be used instead of manual operations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-stages.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-stages.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-stages.html",
      "https://digitalcloud.training/aws-lambda/"
    ]
  },
  {
    "id": 59,
    "question": "<p>A developer has a user account in the Development AWS account. He has been asked to modify resources in a Production AWS account. What is the MOST secure way to provide temporary access to the developer?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Generate an access key on the second account using the root account and share the access keys with the developer for API access</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS KMS to generate cross-account customer master keys and use those get short-lived credentials</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add the user to a group in the second account that has a role attached granting the necessary permissions</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a cross-account access role, and use sts:AssumeRole API to get short-lived credentials</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>This should be implemented using a role in the Production account and a group in the Development account. The developer in the Development account would then be added to the group. The role in the Production account would provide the necessary access and would allow the group in the Development account to assume the role.</p><p>The following image depicts this setup:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-06-51-9ce802945c148c8c43663f25eec64296.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-06-51-9ce802945c148c8c43663f25eec64296.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the most secure way to achieve the required access is to use a role in the Production account that the user is able to assume and then the user can request short-lived credentials from the Security Token Service (STS).</p><p><strong>CORRECT: </strong>\"Create a cross-account access role, and use sts:AssumeRole API to get short-lived credentials\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Generate an access key on the second account using the root account and share the access keys with the developer for API access\" is incorrect as this is highly insecure. You should never share access keys across user accounts, and you should especially not use access keys associated with the root account.</p><p><strong>INCORRECT:</strong> \"Add the user to a group in the second account that has a role attached granting the necessary permissions\" is incorrect as you cannot add a user to a group in a different AWS account.</p><p><strong>INCORRECT:</strong> \"Use AWS KMS to generate cross-account customer master keys and use those get short-lived credentials\" is incorrect as you do not use AWS KMS CMKs for obtaining short-lived credentials from the STS service. CMKs are used for encrypting data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html",
      "https://digitalcloud.training/aws-iam/"
    ]
  },
  {
    "id": 60,
    "question": "<p>A Developer has created an AWS Lambda function in a new AWS account. The function is expected to be invoked 40 times per second and the execution duration will be around 100 seconds. What MUST the Developer do to ensure there are no errors?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Contact AWS Support to increase the concurrent execution limits</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Implement tracing with X-Ray</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Implement error handling within the function code</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Implement a Dead Letter Queue to capture invocation errors</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.</p><p>In this scenario the Lambda function will be invoked 40 times per second and run for 100 seconds. Therefore, there can be up to 4,000 executions running concurrently which is above the default per-region limit of 1,000 concurrent executions.</p><p>This can be easily rectified by contacting AWS support and requesting the concurrent execution limit to be increased.</p><p><strong>CORRECT: </strong>\"Contact AWS Support to increase the concurrent execution limits\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Implement error handling within the function code\" is incorrect. Though this could be useful it is not something that must be done based on what we know about this scenario.</p><p><strong>INCORRECT:</strong> \"Implement a Dead Letter Queue to capture invocation errors\" is incorrect as this would be implemented for message handling requirements.</p><p><strong>INCORRECT:</strong> \"Implement tracing with X-Ray\" is incorrect. X-Ray can be used to analyze and debug distributed applications. We don’t know of any specific issues with this function yet so this is not something that must be done.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html"
    ]
  },
  {
    "id": 61,
    "question": "<p>A media organization utilizes an Amazon API Gateway REST API endpoint to disseminate updates from an internal Content Management System (CMS) to Amazon EventBridge. An EventBridge rule is set up to monitor these updates and control content syndication in a primary AWS account. The organization now wants to extend the reach of these updates across several affiliate AWS accounts.</p><p>How can the developer accomplish this without altering the configuration of the CMS?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Lambda in the main account to clone updates and manually invoke it in affiliate accounts.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Establish multiple API Gateway REST API endpoints in affiliate accounts to directly receive updates from the CMS.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up multiple instances of the CMS, each linked to a different AWS account.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Implement an EventBridge event bus in the affiliate AWS accounts to create a rule that matches events and forwards them from the main account.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>EventBridge allows events to be sent to other AWS accounts, providing a straightforward solution to distribute the updates across multiple accounts. This wouldn't require any changes to the CMS configuration.</p><p><strong>CORRECT: </strong>\"Implement an EventBridge event bus in the affiliate AWS accounts to create a rule that matches events and forwards them from the main account\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Establish multiple API Gateway REST API endpoints in affiliate accounts to directly receive updates from the CMS\" is incorrect.</p><p>Setting up multiple API Gateway REST API endpoints in affiliate accounts to directly receive updates from the CMS would not only require changes to the CMS configuration but would also mean maintaining multiple endpoints.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda in the main account to clone updates and manually invoke it in affiliate accounts\" is incorrect.</p><p>Using AWS Lambda in the main account to clone updates and manually invoke it in affiliate accounts would be an inefficient solution, leading to potential latency and management overhead.</p><p><strong>INCORRECT:</strong> \"Set up multiple instances of the CMS, each linked to a different AWS account\" is incorrect.</p><p>Setting up multiple instances of the CMS, each linked to a different AWS account, would be a significantly complex and resource-intensive solution. It's neither practical nor necessary for the given requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  },
  {
    "id": 62,
    "question": "<p>An application scans an Amazon DynamoDB table once per day to produce a report. The scan is performed in non-peak hours when production usage uses around 50% of the provisioned throughput.</p><p>How can you MINIMIZE the time it takes to produce the report without affecting production workloads? (Select TWO.)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a Sequential Scan API operation</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use pagination to divide results into 1 MB pages</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Increase read capacity units during the scan operation</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a Parallel Scan API operation</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Use the Limit parameter</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Database",
    "explanation": "<p>By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data.</p><p>The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table's data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition.</p><p>To address these issues, the Scan operation can logically divide a table or secondary index into multiple <em>segments</em>, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own Scan request with the following parameters:</p><p> • Segment — A segment to be scanned by a particular worker. Each worker should use a different value for Segment.</p><p> • TotalSegments — The total number of segments for the parallel scan. This value must be the same as the number of workers that your application will use.</p><p>The following diagram shows how a multithreaded application performs a parallel Scan with three degrees of parallelism.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-09-16-b996301e40aa2d9a508f54946ea8c3bf.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-23_02-09-16-b996301e40aa2d9a508f54946ea8c3bf.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>To make the most of your table’s provisioned throughput, you’ll want to use the Parallel Scan API operation so that your scan is distributed across your table’s partitions. However, you also need to ensure the scan doesn’t consume your table’s provisioned throughput and cause the critical parts of your application to be throttled.</p><p>To control the amount of data returned per request, use the Limit parameter. This can help prevent situations where one worker consumes all of the provisioned throughput, at the expense of all other workers.</p><p>Therefore, the best solution to this problem is to use a parallel scan API operation with the Limit parameter.</p><p><strong>CORRECT: </strong>\"Use a Parallel Scan API operation \" is the correct answer.</p><p><strong>CORRECT:</strong> \"Use the Limit parameter\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Use a Sequential Scan API operation\" is incorrect as this would take more time and the question requests that we minimize the time it takes to complete the scan.</p><p><strong>INCORRECT:</strong> \"Increase read capacity units during the scan operation\" is incorrect as this would increase cost and we still need a solution to ensure we maximize usage of available throughput without affecting production workloads.</p><p><strong>INCORRECT:</strong> \"Use pagination to divide results into 1 MB pages\" is incorrect as this does only divides the results into pages, it does not segment and limit the amount of throughput used.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/\">https://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 63,
    "question": "<p>A Developer has noticed some suspicious activity in her AWS account and is concerned that the access keys associated with her IAM user account may have been compromised. What is the first thing the Developer do in should do in this situation?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Delete the compromised access keys</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Change her IAM User account password</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Report the incident to AWS Support</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Delete her IAM user account</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>In this case the Developer’s access keys may have been compromised so the first step would be to invalidate the access keys by deleting them.</p><p>The next step would then be to determine if any temporary security credentials have been issued an invalidating those too to prevent any further misuse.</p><p>The user account and user account password have not been compromised so they do not need to be deleted / changed as a first step. However, changing the account password would typically be recommended as a best practice in this situation.</p><p><strong>CORRECT: </strong>\"Delete the compromised access keys\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Delete her IAM user account\" is incorrect. This user account has not been compromised based on the available information, just the access keys. Deleting the access keys will prevent further misuse of the AWS account.</p><p><strong>INCORRECT:</strong> \"Report the incident to AWS Support\" is incorrect is a good practice but not the first step. The Developer should first attempt to mitigate any further misuse of the account by deleting the access keys.</p><p><strong>INCORRECT:</strong> \"Change her IAM User account password\" is incorrect as she does not have any evidence that the account has been compromised, just the access keys. However, it would be a good practice to change the password, just not the first thing to do.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/\">https://aws.amazon.com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/",
      "https://digitalcloud.training/aws-iam/"
    ]
  },
  {
    "id": 64,
    "question": "<p>An organization is developing a data processing application that is hosted on AWS Lambda and utilizes a PostgreSQL database on Amazon RDS. The security team mandates a policy that requires rotating database credentials every week.</p><p>What strategy should the developer adopt to manage the database credentials for the application?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy AWS Secrets Manager to store database credentials and set up automatic weekly rotation.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Store the credentials in environment variables of the Lambda function and manually update them every week.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable IAM database authentication and manage weekly rotation manually.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store database credentials in Amazon S3 buckets with versioning enabled, rotating, and uploading new credentials weekly.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>AWS Secrets Manager enables the creation, retrieval, and rotation of secrets like database credentials. Configuring automatic rotation weekly satisfies the security team's requirement, with the least manual effort.</p><p><strong>CORRECT: </strong>\"Deploy AWS Secrets Manager to store database credentials and set up automatic weekly rotation\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store database credentials in Amazon S3 buckets with versioning enabled, rotating and uploading new credentials weekly\" is incorrect.</p><p>Storing credentials in Amazon S3 is not a secure practice as it does not have built-in mechanisms for rotating secrets.</p><p><strong>INCORRECT:</strong> \"Enable IAM database authentication and manage weekly rotation manually\" is incorrect.</p><p>IAM database authentication allows you to use IAM users/roles to access your database, but it does not have an automatic mechanism for weekly rotation of IAM credentials.</p><p><strong>INCORRECT:</strong> \"Store the credentials in environment variables of the Lambda function and manually update them every week\" is incorrect.</p><p>Storing credentials as environment variables in AWS Lambda functions could expose them to potential security risks, and manual rotation weekly would be labor-intensive.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_managed.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_managed.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-secrets-manager/\">https://digitalcloud.training/aws-secrets-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_managed.html",
      "https://digitalcloud.training/aws-secrets-manager/"
    ]
  },
  {
    "id": 65,
    "question": "<p>A company is running a Docker application on Amazon ECS. The application must scale based on user load in the last 15 seconds.</p><p>How should the Developer instrument the code so that the requirement can be met?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p><p>User activity is not a standard CloudWatch metric and as stated above for the resolution we need in this scenario a custom CloudWatch metric is required anyway. Therefore, for this scenario the Developer should create a high-resolution custom Amazon CloudWatch metric for user activity data and publish the data every 5 seconds.</p><p><strong>CORRECT: </strong>\"Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds\" is incorrect as the resolution is lower than required which will not provide the granularity required.</p><p><strong>INCORRECT:</strong> \"Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds\" is incorrect as standard resolution metrics have a granularity of one minute.</p><p><strong>INCORRECT:</strong> \"Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds\" is incorrect as standard resolution metrics have a granularity of one minute.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  }
]