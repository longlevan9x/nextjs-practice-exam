[
  {
    "id": 1,
    "question": "<p>A company is deploying a static website hosted from an Amazon S3 bucket. The website must support encryption in-transit for website visitors.</p><p>Which combination of actions must the Developer take to meet this requirement? (Select TWO.)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon CloudFront distribution. Set the S3 bucket as an origin.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure the S3 bucket with an SSL/TLS certificate.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure an Amazon CloudFront distribution with an AWS WAF WebACL.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an AWS WAF WebACL with a secure listener.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Configure an Amazon CloudFront distribution with an SSL/TLS certificate.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>Amazon S3 static websites use the HTTP protocol only and you cannot enable HTTPS. To enable HTTPS connections to your S3 static website, use an Amazon CloudFront distribution that is configured with an SSL/TLS certificate. This will ensure that connections between clients and the CloudFront distribution are encrypted in-transit as per the requirements.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudFront distribution. Set the S3 bucket as an origin\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Configure an Amazon CloudFront distribution with an SSL/TLS certificate\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF WebACL with a secure listener\" is incorrect. You cannot configure a secure listener on a WebACL.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon CloudFront distribution with an AWS WAF WebACL\" is incorrect. This will not enable encrypted connections.</p><p><strong>INCORRECT:</strong> \"Configure the S3 bucket with an SSL/TLS certificate\" is incorrect. You cannot manually add SSL/TLS certificates to Amazon S3, and it is not possible to directly configure an S3 bucket that is configured as a static website to accept encrypted connections.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/",
      "https://digitalcloud.training/amazon-cloudfront/"
    ]
  },
  {
    "id": 2,
    "question": "<p>A company runs a legacy application that uses an XML-based SOAP interface. The company needs to expose the functionality of the service to external customers and plans to use Amazon API Gateway.</p><p>How can a Developer configure the integration?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a SOAP API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using AWS Lambda.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a RESTful API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through an Application Load Balancer.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a SOAP API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through a Network Load Balancer.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a RESTful API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>In API Gateway, an API's method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, the backend may return an integration response payload different from the method response payload, as expected by the frontend.</p><p>API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.</p><p><strong>CORRECT: </strong>\"Create a RESTful API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a RESTful API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through an Application Load Balancer\" is incorrect. The API Gateway cannot process the XML SOAP data and cannot pass it through an ALB.</p><p><strong>INCORRECT:</strong> \"Create a SOAP API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using AWS Lambda\" is incorrect. API Gateway does not support SOAP APIs.</p><p><strong>INCORRECT:</strong> \"Create a SOAP API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through a Network Load Balancer\" is incorrect. API Gateway does not support SOAP APIs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/request-response-data-mappings.html\">https://docs.aws.amazon.com/apigateway/latest/Developerguide/request-response-data-mappings.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/certification-training/aws-developer-associate/aws-networking-and-content-delivery/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/request-response-data-mappings.html",
      "https://digitalcloud.training/certification-training/aws-developer-associate/aws-networking-and-content-delivery/amazon-api-gateway/"
    ]
  },
  {
    "id": 3,
    "question": "<p>A Developer is creating a web application that will be used by employees working from home. The company uses a SAML directory on-premises for storing user information. The Developer must integrate with the SAML directory and authorize each employee to access only their own data when using the application.</p><p>Which approach should the Developer take?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create the application within an Amazon VPC and use a VPC endpoint with a trust policy to grant access to the employees.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use an Amazon Cognito identity pool, federate with the SAML provider, and use a trust policy with an IAM condition key to limit employee access.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a unique IAM role for each employee and have each employee assume the role to access the application so they can access their personal data only.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Cognito user pools, federate with the SAML provider, and use user pool groups with an IAM policy.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>Amazon Cognito leverages IAM roles to generate temporary credentials for your application's users. Access to permissions is controlled by a role's trust relationships.</p><p>In this example the Developer must limit access to specific identities in the SAML directory. The Developer can create a trust policy with an IAM condition key that limits access to a specific set of app users by checking the value of cognito-identity.amazonaws.com:sub:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-06-09_11-30-30-25305d386d0978ee6835d38458666dfe.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-06-09_11-30-30-25305d386d0978ee6835d38458666dfe.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Use an Amazon Cognito identity pool, federate with the SAML provider, and use a trust policy with an IAM condition key to limit employee access\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon Cognito user pools, federate with the SAML provider, and use user pool groups with an IAM policy\" is incorrect. A user pool can be used to authenticate but the identity pool is used to provide authorized access to AWS services.</p><p><strong>INCORRECT:</strong> \"Create the application within an Amazon VPC and use a VPC endpoint with a trust policy to grant access to the employees\" is incorrect. You cannot provide access to an on-premises SAML directory using a VPC endpoint.</p><p><strong>INCORRECT:</strong> \"Create a unique IAM role for each employee and have each employee assume the role to access the application so they can access their personal data only\" is incorrect. This is not an integration into the SAML directory and would be very difficult to manage.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/role-trust-and-permissions.html\">https://docs.aws.amazon.com/cognito/latest/Developerguide/role-trust-and-permissions.html</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/iam-roles.html\">https://docs.aws.amazon.com/cognito/latest/Developerguide/iam-roles.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cognito/\">https://digitalcloud.training/amazon-cognito/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/cognito/latest/developerguide/role-trust-and-permissions.html",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/iam-roles.html",
      "https://digitalcloud.training/amazon-cognito/"
    ]
  },
  {
    "id": 4,
    "question": "<p>A Developer is designing a fault-tolerant application that will use Amazon EC2 instances and an Elastic Load Balancer. The Developer needs to ensure that if an EC2 instance fails session data is not lost. How can this be achieved?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon SQS to save session data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use an EC2 Auto Scaling group to automatically launch new instances</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable Sticky Sessions on the Elastic Load Balancer</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon DynamoDB to perform scalable session handling</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>For this scenario the key requirement is to ensure the data is not lost. Therefore, the data must be stored in a durable data store outside of the EC2 instances. Amazon DynamoDB is a suitable solution for storing session data. DynamoDB has a session handling capability for multiple languages as in the below example for PHP:</p><p>“The <strong>DynamoDB Session Handler</strong> is a custom session handler for PHP that allows developers to use Amazon DynamoDB as a session store. Using DynamoDB for session storage alleviates issues that occur with session handling in a distributed web application by moving sessions off of the local file system and into a shared location. DynamoDB is fast, scalable, easy to setup, and handles replication of your data automatically.”</p><p>Therefore, the best answer is to use DynamoDB to store the session data.</p><p><strong>CORRECT: </strong>\"Use Amazon DynamoDB to perform scalable session handling\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable Sticky Sessions on the Elastic Load Balancer\" is incorrect. Sticky sessions attempts to direct a user that has reconnected to the application to the same EC2 instance that they connected to previously. However, this does not ensure that the session data is going to be available.</p><p><strong>INCORRECT:</strong> \"Use an EC2 Auto Scaling group to automatically launch new instances\" is incorrect as this does not provide a solution for storing the session data.</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS to save session data\" is incorrect as Amazon SQS is not suitable for storing session data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html\">https://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 5,
    "question": "<p>A company is deploying a microservices application on AWS Fargate using Amazon ECS. The application has environment variables that must be passed to a container for the application to initialize.</p><p>How should the environment variables be passed to the container?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use advanced container definition parameters and define environment variables under the environment parameter within the service definition.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use standard container definition parameters and define environment variables under the secrets parameter within the task definition.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use advanced container definition parameters and define environment variables under the environment parameter within the task definition.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use standard container definition parameters and define environment variables under the WorkingDirectory parameter within the service definition.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>When you register a task definition, you must specify a list of container definitions that are passed to the Docker daemon on a container instance.</p><p>The developer should use advanced container definition parameters and define environment variables to pass to the container.</p><p><strong>CORRECT: </strong>\"Use advanced container definition parameters and define environment variables under the environment parameter within the task definition\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use advanced container definition parameters and define environment variables under the environment parameter within the service definition\" is incorrect.</p><p>The task definition is the correct place to define the environment variables to pass to the container.</p><p><strong>INCORRECT:</strong> \"Use standard container definition parameters and define environment variables under the secrets parameter within the task definition\" is incorrect.</p><p>Advanced container definition parameters must be used to pass the environment variables to the container. The environment parameter should also be used.</p><p><strong>INCORRECT:</strong> \"Use standard container definition parameters and define environment variables under the WorkingDirectory parameter within the service definition\" is incorrect.</p><p>Advanced container definition parameters must be used to pass the environment variables to the container. The environment parameter should also be used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/userguide/task_definition_parameters.html#container_definition_environment\">https://docs.aws.amazon.com/AmazonECS/latest/userguide/task_definition_parameters.html#container_definition_environment</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/userguide/task_definition_parameters.html#container_definition_environment",
      "https://digitalcloud.training/amazon-ecs-and-eks/"
    ]
  },
  {
    "id": 6,
    "question": "<p>A Developer is setting up a code update to Amazon ECS using AWS CodeDeploy. The Developer needs to complete the code update quickly. Which of the following deployment types should the Developer use?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>In-place</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Canary</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Blue/green</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Linear</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>CodeDeploy provides two deployment type options – in-place and blue/green. Note that AWS Lambda and Amazon ECS deployments cannot use an in-place deployment type.</p><p>The Blue/green deployment type on an Amazon ECS compute platform works like this:</p><p>Traffic is shifted from the task set with the original version of an application in an Amazon ECS service to a replacement task set in the same service.</p><p>You can set the traffic shifting to linear or canary through the deployment configuration.</p><p>The protocol and port of a specified load balancer listener is used to reroute production traffic.</p><p>During a deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run.</p><p><strong>CORRECT: </strong>\"Blue/green\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Canary\" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments.</p><p><strong>INCORRECT:</strong> \"Linear\" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments.</p><p><strong>INCORRECT:</strong> \"In-place\" is incorrect as AWS Lambda and Amazon ECS deployments cannot use an in-place deployment type.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 7,
    "question": "<p>A developer is updating an Amazon Aurora MySQL database to allow more clients to connect. What database parameter needs to be updated to support a higher number of client connections?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>max_join_size</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>max_connections</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>max_allowed_packet</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>max_user_connections</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>The maximum number of connections allowed to an Aurora MySQL DB instance is determined by the max_connections parameter in the instance-level parameter group for the DB instance.</p><p>You can increase the maximum number of connections to your Aurora MySQL DB instance by scaling the instance up to a DB instance class with more memory, or by setting a larger value for the max_connections parameter in the DB parameter group for your instance, up to 16,000.</p><p><strong>CORRECT: </strong>\"max_connections\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"max_allowed_packet\" is incorrect. This parameter sets the maximum size of one packet or any generated or intermediate string.</p><p><strong>INCORRECT:</strong> \"max_join_size\" is incorrect. This option is used to set a limit on the maximum number of row accesses.</p><p><strong>INCORRECT:</strong> \"max_user_connections\" is incorrect. This option limits the number of simultaneous connections that the user can make.</p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-max-connections/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-max-connections/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-max-connections/",
      "https://digitalcloud.training/amazon-aurora/"
    ]
  },
  {
    "id": 8,
    "question": "<p>A Developer needs to scan a full DynamoDB 50GB table within non-peak hours. About half of the strongly consistent RCUs are typically used during non-peak hours and the scan duration must be minimized.</p><p>How can the Developer optimize the scan execution time without impacting production workloads?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use parallel scans while limiting the rate</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use sequential scans</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Increase the RCUs during the scan operation</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Change to eventually consistent RCUs during the scan operation</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Performing a scan on a table consumes a lot of RCUs. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set. To reduce the amount of RCUs used by the scan so it doesn’t affect production workloads whilst minimizing the execution time, there are a couple of recommendations the Developer can follow.</p><p>Firstly, the <em>Limit</em> parameter can be used to reduce the page size. The Scan operation provides a <em>Limit</em> parameter that you can use to set the page size for your request. Each Query or Scan request that has a smaller page size uses fewer read operations and creates a \"pause\" between each request.</p><p>Secondly, the Developer can configure parallel scans. With parallel scans the Developer can maximize usage of the available throughput and have the scans distributed across the table’s partitions.</p><p>A parallel scan can be the right choice if the following conditions are met:</p><p>The table size is 20 GB or larger.</p><p>The table's provisioned read throughput is not being fully used.</p><p>Sequential Scan operations are too slow.</p><p>Therefore, to optimize the scan operation the Developer should use parallel scans while limiting the rate as this will ensure that the scan operation does not affect the performance of production workloads and still have it complete in the minimum time.</p><p><strong>CORRECT: </strong>\"Use parallel scans while limiting the rate\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use sequential scans\" is incorrect as this is slower than parallel scans and the Developer needs to minimize scan execution time.</p><p><strong>INCORRECT:</strong> \"Increase the RCUs during the scan operation\" is incorrect as the table is only using half of the RCUs during non-peak hours so there are RCUs available. You could increase RCUs and perform the scan faster, but this would be more expensive. The better solution is to use parallel scans with the limit parameter.</p><p><strong>INCORRECT:</strong> \"Change to eventually consistent RCUs during the scan operation\" is incorrect as this does not provide a solution for preventing impact to the production workloads. The limit parameter should be used to ensure the tables RCUs are not fully used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html#QueryAndScanGuidelines.ParallelScan\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html#QueryAndScanGuidelines.ParallelScan</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/certification-training/aws-developer-associate/aws-database/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html#QueryAndScanGuidelines.ParallelScan",
      "https://digitalcloud.training/certification-training/aws-developer-associate/aws-database/amazon-dynamodb/"
    ]
  },
  {
    "id": 9,
    "question": "<p>A developer is using AWS CodeBuild to build an application into a Docker image. The buildspec file is used to run the application build. The developer needs to push the Docker image to an Amazon ECR repository only upon the successful completion of each build.</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add a post_build phase to the buildspec file that uses the commands block to push the Docker image.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Add a post_build phase to the buildspec file that uses the artifacts sequence to find the build artifacts and push to Amazon ECR.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add a post_build phase to the buildspec file that uses the finally block to push the Docker image.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Add an install phase to the buildspec file that uses the commands block to push the Docker image.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>The post_build phase is an optional sequence. It represents the commands, if any, that CodeBuild runs after the build. For example, you might use Maven to package the build artifacts into a JAR or WAR file, or you might push a Docker image into Amazon ECR. Then you might send a build notification through Amazon SNS.</p><p>Here is an example of a buildspec file with a post_build phase that pushes a Docker image to Amazon ECR:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-29_11-50-10-9f3a3ef04f3fab72a2d99c8c431ed141.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-29_11-50-10-9f3a3ef04f3fab72a2d99c8c431ed141.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Add a post_build phase to the buildspec file that uses the commands block to push the Docker image\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add a post_build phase to the buildspec file that uses the finally block to push the Docker image\" is incorrect.</p><p>Commands specified in a finally block are run after commands in the commands block. The commands in a finally block are run even if a command in the commands block fails. This would not be ideal as this would push the image to ECR even if commands in previous sequences failed.</p><p><strong>INCORRECT:</strong> \"Add an install phase to the buildspec file that uses the commands block to push the Docker image\" is incorrect.</p><p>These are commands that are run during installation. The develop would want to push the image only after all installations have succeeded. Therefore, the post_build phase should be used.</p><p><strong>INCORRECT:</strong> \"Add a post_build phase to the buildspec file that uses the artifacts sequence to find the build artifacts and push to Amazon ECR\" is incorrect.</p><p>The artifacts sequence is not required if you are building and pushing a Docker image to Amazon ECR, or you are running unit tests on your source code, but not building it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html",
      "https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 10,
    "question": "<p>A company has created a set of APIs using Amazon API Gateway and exposed them to partner companies. The APIs have caching enabled for all stages. The partners require a method of invalidating the cache that they can build into their applications.</p><p>What can the partners use to invalidate the API cache?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>They can invoke an AWS API endpoint which invalidates the cache</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>They must wait for the TTL to expire</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>They can use the query string parameter <code>INVALIDATE_CACHE</code> </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>They can pass the HTTP header <code>Cache-Control: max-age=0</code> </p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.</p><p>When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.</p><p>A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the <code>Cache-Control: max-age=0</code> header.</p><p>The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.</p><p>To grant permission for a client, attach a policy of the following format to an IAM execution role for the user.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-31-17-29b0e457a0843d462a0ea3fbe56e5bb8.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-31-17-29b0e457a0843d462a0ea3fbe56e5bb8.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>This policy allows the API Gateway execution service to invalidate the cache for requests on the specified resource (or resources).</p><p>Therefore, as described above the solution is to get the partners to pass the HTTP header <code>Cache-Control: max-age=0</code>.</p><p><strong>CORRECT: </strong>\"They can pass the HTTP header <code>Cache-Control: max-age=0</code>\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"They can use the query string parameter <code>INVALIDATE_CACHE</code>\" is incorrect. This is not a valid method of invalidating the cache with API Gateway.</p><p><strong>INCORRECT:</strong> \"They must wait for the TTL to expire\" is incorrect as this is not true, you do not need to wait as you can pass the HTTP header <code>Cache-Control: max-age=0</code> whenever you need to in order to invalidate the cache.</p><p><strong>INCORRECT:</strong> \"They can invoke an AWS API endpoint which invalidates the cache\" is incorrect. This is not a valid method of invalidating the cache with API Gateway.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html",
      "https://digitalcloud.training/amazon-api-gateway/"
    ]
  },
  {
    "id": 11,
    "question": "<p>A web application runs on a fleet of Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). A developer needs a store for session data so it can be reliably served across multiple requests.</p><p>Where is the best place to store the session data?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Write the data to an Amazon ElastiCache cluster.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Write the data to a shared Amazon EBS volume.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Write the data to the local instance store volumes.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Write the data to the root of the filesystem.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>ElastiCache is a good solution for storing session state data as it has very low latency and high performance. DynamoDB is often used for the same purpose. In this case the session data can be written to the ElastiCache cluster and can then be easily retrieved from subsequent sessions on the same or a different EC2 instance. This decouples the data from the individual instance so if an instance fails, the data is not lost.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-29_11-54-07-18affbc17863d347c7b505ea988c337d.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-29_11-54-07-18affbc17863d347c7b505ea988c337d.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Write the data to an Amazon ElastiCache cluster\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Write the data to a shared Amazon EBS volume\" is incorrect.</p><p>You cannot share an EBS volume except under specific circumstances and even then, you must share the volume from an EC2 instance which could fail.</p><p><strong>INCORRECT:</strong> \"Write the data to the root of the filesystem\" is incorrect.</p><p>This will result in data loss if the instance fails.</p><p><strong>INCORRECT:</strong> \"Write the data to the local instance store volumes\" is incorrect.</p><p>This will result in data loss if the instance fails.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/caching/session-management/\">https://aws.amazon.com/caching/session-management/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/caching/session-management/",
      "https://digitalcloud.training/amazon-elasticache/"
    ]
  },
  {
    "id": 12,
    "question": "<p>A customer requires a serverless application with an API which mobile clients will use. The API will have both and AWS Lambda function and an Amazon DynamoDB table as data sources. Responses that are sent to the mobile clients must contain data that is aggregated from both of these data sources.</p><p>The developer must minimize the number of API endpoints and must minimize the number of API calls that are required to retrieve the necessary data.</p><p>Which solution should the developer use to meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>REST API on Amazon API Gateway</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>REST API on AWS Elastic Beanstalk</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>GraphQL API on AWS AppSync</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>GraphQL API on an Amazon EC2 instance</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>GraphQL APIs built with AWS AppSync give front-end developers the ability to query multiple databases, microservices, and APIs from a single GraphQL endpoint. This would not be possible with a REST API running on API Gateway which would have a single target for each API endpoint.</p><p>The example diagram below depicts a solution that includes AWS AppSync in front of mobile clients. The services then connect to Lambda and DynamoDB via API Gateway and AppSync.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-29_11-26-10-97d7c66e5420d18322a48e08eb041f85.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-29_11-26-10-97d7c66e5420d18322a48e08eb041f85.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"GraphQL API on AWS AppSync\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"REST API on Amazon API Gateway\" is incorrect.</p><p>A REST API is not suitable as the question asks to reduce the number of API endpoints. With a REST API there is a single target such as a Lambda per API endpoint so more endpoints would be required.</p><p><strong>INCORRECT:</strong> \"GraphQL API on an Amazon EC2 instance\" is incorrect.</p><p>This would not be a serverless solution and the question states that the solution must be serverless.</p><p><strong>INCORRECT:</strong> \"REST API on AWS Elastic Beanstalk\" is incorrect.</p><p>As explained above.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mobile/appsync-microservices/\">https://aws.amazon.com/blogs/mobile/appsync-microservices/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/mobile/appsync-microservices/"
    ]
  },
  {
    "id": 13,
    "question": "<p>An application serves customers in several different geographical regions. Information about the location users connect from is written to logs stored in Amazon CloudWatch Logs. The company needs to publish an Amazon CloudWatch custom metric that tracks connections for each location.</p><p>Which approach will meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a CloudWatch metric filter to extract metrics from the log files with location as a dimension.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a CloudWatch Logs Insights query to extract the location information from the logs and to create a custom metric with location as a dimension.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Stream data to an Amazon Elasticsearch cluster in near-real time and export a custom metric.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a CloudWatch Events rule that creates a custom metric from the CloudWatch Logs group.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>You can search and filter the log data coming into CloudWatch Logs by creating one or more <em>metric filters</em>. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p><p>When you create a metric from a log filter, you can also choose to assign dimensions and a unit to the metric. In this case, the company can assign a dimension that uses the location information.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch metric filter to extract metrics from the log files with location as a dimension\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch Logs Insights query to extract the location information from the logs and to create a custom metric with location as a dimension\" is incorrect. You cannot create a custom metric through CloudWatch Logs Insights.</p><p><strong>INCORRECT:</strong> \"Configure a CloudWatch Events rule that creates a custom metric from the CloudWatch Logs group\" is incorrect. You cannot create a custom metric using a CloudWatch Events rule.</p><p><strong>INCORRECT:</strong> \"Stream data to an Amazon Elasticsearch cluster in near-real time and export a custom metric\" is incorrect. This is not a valid way of creating a custom metric in CloudWatch.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  },
  {
    "id": 14,
    "question": "<p>A company is migrating to the AWS Cloud and needs to build a managed Public Key Infrastructure (PKI) using AWS services. The solution must support the following features:</p><p>&nbsp; &nbsp; &nbsp;- IAM integration.</p><p>&nbsp; &nbsp; &nbsp;- Auditing with AWS CloudTrail.</p><p>&nbsp; &nbsp; &nbsp;- Private certificates.</p><p>&nbsp; &nbsp; &nbsp;- Subordinate certificate authorities (CAs).</p><p>Which solution should the company use to meet these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Certificate Manager.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Private Certificate Authority.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS Secrets Manager.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Key Management Service.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>An AWS Private CA hierarchy provides strong security and restrictive access controls for the most-trusted root CA at the top of the trust chain, while allowing more permissive access and bulk certificate issuance for subordinate CAs lower on the chain.</p><p>With AWS Private CA, you can create private certificates to identify resources and protect data. You can create versatile certificate and CA configurations to identify and protect your resources, including servers, applications, users, devices, and containers.</p><p>The service offers direct integration with AWS IAM, and you can control access to AWS Private CA with IAM policies.</p><p><strong>CORRECT: </strong>\"AWS Private Certificate Authority\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Certificate Manager\" is incorrect. This service is used for issuing SSL/TLS certificates and is not suitable as a private CA for building a PKI with subordinate CAs.</p><p><strong>INCORRECT:</strong> \"AWS Key Management Service\" is incorrect. This service is used to create and manage the encryption keys used for encrypting data at rest.</p><p><strong>INCORRECT:</strong> \"AWS Secrets Manager\" is incorrect. This service is used for storing secret information such as database connections strings and passwords with API access.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/private-ca/features/\">https://aws.amazon.com/private-ca/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certificate-manager/\">https://digitalcloud.training/aws-certificate-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/private-ca/features/",
      "https://digitalcloud.training/aws-certificate-manager/"
    ]
  },
  {
    "id": 15,
    "question": "<p>A review of Amazon CloudWatch metrics shows that there are a high number of reads taking place on a primary database built on Amazon Aurora with MySQL. What can a developer do to improve the read scaling of the database? (Select TWO.)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create Aurora Replicas in same cluster as the primary database instance.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create Aurora Replicas in a global S3 bucket as the primary read source.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a duplicate Aurora primary database to process read requests.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a separate Aurora MySQL cluster and configure binlog replication.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create a duplicate Aurora database cluster to process read requests.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Management & Governance",
    "explanation": "<p>Aurora Replicas can help improve read scaling because it synchronously updates data with the primary database (within 100 ms). Aurora Replicas are created in the same DB cluster within a Region. With Aurora MySQL you can also enable binlog replication to another Aurora DB cluster which can be in the same or a different Region.</p><p><strong>CORRECT: </strong>\"Create Aurora Replicas in same cluster as the primary database instance\" is the correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a separate Aurora MySQL cluster and configure binlog replication\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a duplicate Aurora database cluster to process read requests\" is incorrect. A duplicate Aurora database cluster would be a separate database with read and write capability and would not help with read scaling.</p><p><strong>INCORRECT:</strong> \"Create a duplicate Aurora primary database to process read requests\" is incorrect. A duplicate Aurora primary database would be for read and write requests and would not help with read scaling.</p><p><strong>INCORRECT:</strong> \"Creating read replicas of Aurora in a S3 global bucket as the primary read source\" is incorrect. S3 is an object storage service. It cannot be used to host databases.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html#Aurora.Managing.Performance.ReadScaling\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html#Aurora.Managing.Performance.ReadScaling</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html#Aurora.Managing.Performance.ReadScaling",
      "https://digitalcloud.training/amazon-aurora/"
    ]
  },
  {
    "id": 16,
    "question": "<p>A developer is partitioning data using Athena to improve performance when performing queries. What are two things the analyst can do that would counter any benefit of using partitions? (Select TWO.)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Segmenting data too finely.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Skewing data heavily to one partition value.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Storing the data in S3.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Using a Hive-style partition format.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Creating partitions directly from data source.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Analytics",
    "explanation": "<p>There is a cost associated with partitioning data. A higher number of partitions can also increase the overhead from retrieving and processing the partition metadata. Multiple smaller files can counter the benefit of using partitioning. If your data is heavily skewed to one partition value, and most queries use that value, then the overhead may wipe out the initial benefit.</p><p><strong>CORRECT: </strong>\"Segmenting data too finely\" is a correct answer (as explained above.)</p><p><strong>CORRECT:</strong> \"Skewing data heavily to one partition value\" is a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Storing the data in S3\" is incorrect. Data must be stored in S3 buckets.</p><p><strong>INCORRECT:</strong> \"Creating partitions directly from data source \" is incorrect. Athena can pull data directly from the S3 source.</p><p><strong>INCORRECT:</strong> \"Using a Hive-style partition format\" is incorrect. Athena is compatible with Hive-style partition formats.</p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
      "https://digitalcloud.training/amazon-athena/"
    ]
  },
  {
    "id": 17,
    "question": "<p>A Developer is deploying an AWS Lambda update using AWS CodeDeploy. In the appspec.yaml file, which of the following is a valid structure for the order of hooks that should be specified?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>BeforeInstall &gt; AfterInstall &gt; AfterAllowTestTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>BeforeInstall &gt; AfterInstall &gt; ApplicationStart &gt; ValidateService</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>BeforeBlockTraffic &gt; AfterBlockTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>BeforeAllowTraffic &gt; AfterAllowTraffic</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts.</p><p>The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.</p><p>The following code snippet shows a valid example of the structure of hooks for an AWS Lambda deployment:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-33-57-404e61aa9de4be6bc1dcd44c189c04c1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-33-57-404e61aa9de4be6bc1dcd44c189c04c1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, in this scenario a valid structure for the order of hooks that should be specified in the appspec.yml file is: BeforeAllowTraffic &gt; AfterAllowTraffic</p><p><strong>CORRECT: </strong>\"BeforeAllowTraffic &gt; AfterAllowTraffic\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"BeforeInstall &gt; AfterInstall &gt; ApplicationStart &gt; ValidateService\" is incorrect as this would be valid for Amazon EC2.</p><p><strong>INCORRECT:</strong> \"BeforeInstall &gt; AfterInstall &gt; AfterAllowTestTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic\" is incorrect as this would be valid for Amazon ECS.</p><p><strong>INCORRECT:</strong> \"BeforeBlockTraffic &gt; AfterBlockTraffic &gt; BeforeAllowTraffic &gt; AfterAllowTraffic\" is incorrect as this is a partial listing of hooks for Amazon EC2 but is incomplete.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 18,
    "question": "<p>A company runs many microservices applications that use Docker containers. The company are planning to migrate the containers to Amazon ECS. The workloads are highly variable and therefore the company prefers to be charged per running task.</p><p>Which solution is the BEST fit for the company’s requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon ECS with the EC2 launch type</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon ECS with the Fargate launch type</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>An Amazon ECS Cluster with Auto Scaling</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>An Amazon ECS Service with Auto Scaling</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>The key requirement is that the company should be charged per running task. Therefore, the best answer is to use Amazon ECS with the Fargate launch type as with this model AWS charge you for running tasks rather than running container instances.</p><p>The Fargate launch type allows you to run your containerized applications without the need to provision and manage the backend infrastructure. You just register your task definition and Fargate launches the container for you. The Fargate Launch Type is a serverless infrastructure managed by AWS.</p><p><strong>CORRECT: </strong>\"Amazon ECS with the Fargate launch type\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon ECS with the EC2 launch type\" is incorrect as with this launch type you pay for running container instances (EC2 instances).</p><p><strong>INCORRECT:</strong> \"An Amazon ECS Service with Auto Scaling\" is incorrect as this does not specify the launch type. You can run an ECS Service on the Fargate or EC2 launch types.</p><p><strong>INCORRECT:</strong> \"An Amazon ECS Cluster with Auto Scaling\" is incorrect as this does not specify the launch type. You can run an ECS Cluster on the Fargate or EC2 launch types.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html",
      "https://digitalcloud.training/amazon-ecs-and-eks/"
    ]
  },
  {
    "id": 19,
    "question": "<p>A developer has deployed an application on AWS Lambda. The application uses Python and must generate and then upload a file to an Amazon S3 bucket. The developer must implement the upload functionality with the least possible change to the application code.</p><p>Which solution BEST meets these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AWS CLI that is installed in the Lambda execution environment.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Make an HTTP request directly to the S3 API to upload the file.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the AWS SDK for Python that is installed in the Lambda execution environment.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Include the AWS SDK for Python in the Lambda function code.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>The best practice for Lambda development is to bundle all dependencies used by your Lambda function, including the AWS SDK. However, since this question specifically requests that the least possible changes are made to the application code, the developer can instead use the SDK for Python that is installed in the Lambda environment to upload the file to Amazon S3.</p><p><strong>CORRECT: </strong>\"Use the AWS SDK for Python that is installed in the Lambda execution environment\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Include the AWS SDK for Python in the Lambda function code\" is incorrect.</p><p>This is the best practice for deployment. However, in this case the developer must minimize changes to code and including the SDK as a dependency in the code would require potential updates to existing Python code.</p><p><strong>INCORRECT:</strong> \"Make an HTTP request directly to the S3 API to upload the file\" is incorrect.</p><p>AWS supports uploads to S3 using the console, AWS SDKs, REST API, and the AWS CLI.</p><p><strong>INCORRECT:</strong> \"Use the AWS CLI that is installed in the Lambda execution environment\" is incorrect.</p><p>The AWS CLI is not installed in the Lambda execution environment.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/upcoming-changes-to-the-python-sdk-in-aws-lambda/\">https://aws.amazon.com/blogs/compute/upcoming-changes-to-the-python-sdk-in-aws-lambda/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/compute/upcoming-changes-to-the-python-sdk-in-aws-lambda/",
      "https://digitalcloud.training/aws-lambda/"
    ]
  },
  {
    "id": 20,
    "question": "<p>A business is providing its clients read-only permissions to items within an Amazon S3 bucket, utilizing IAM permissions to limit access to this S3 bucket. Clients are only permitted to access their specific files. Regulatory compliance necessitates the enforcement of in-transit encryption during communication with Amazon S3.</p><p>What solution will fulfill these criteria?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable the Amazon S3 Transfer Acceleration feature to ensure encryption during transit.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Update the S3 bucket policy to include a condition that requires aws:SecureTransport for all actions.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Activate Amazon S3 server-side encryption to enforce encryption during transit.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Assign IAM roles enforcing SSL/TLS encryption to each customer.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>By adding a condition to the S3 bucket policy that requires aws:SecureTransport, you are mandating that all interactions with the bucket must be encrypted in transit using SSL/TLS.</p><p><strong>CORRECT: </strong>\"Update the S3 bucket policy to include a condition that requires aws:SecureTransport for all actions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Activate Amazon S3 server-side encryption to enforce encryption during transit\" is incorrect.</p><p>Server-side encryption for S3 secures the data at rest, not during transit.</p><p><strong>INCORRECT:</strong> \"Assign IAM roles enforcing SSL/TLS encryption to each customer\" is incorrect.</p><p>Although IAM roles are utilized to manage access to AWS resources, they do not inherently mandate SSL/TLS encryption for interactions with the resources.</p><p><strong>INCORRECT:</strong> \"Enable the Amazon S3 Transfer Acceleration feature to ensure encryption during transit\" is incorrect.</p><p>While Amazon S3 Transfer Acceleration does use SSL/TLS, it is designed to expedite the transfer of data over long distances between a user and an S3 bucket, not to mandate encryption in transit for all interactions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-2\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-2</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-2",
      "https://digitalcloud.training/amazon-s3-and-glacier/"
    ]
  },
  {
    "id": 21,
    "question": "<p>To reduce the cost of API actions performed on an Amazon SQS queue, a Developer has decided to implement long polling. Which of the following modifications should the Developer make to the API actions?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set the <code>SetQueueAttributes</code> with a <code>MessageRetentionPeriod</code> of 60</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set the <code>ReceiveMessage</code> API with a <code>WaitTimeSeconds</code> of 20</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set the <code>SetQueueAttributes</code> API with a <code>DelaySeconds</code> of 20</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set the <code>ReceiveMessage</code> API with a <code>VisibilityTimeout</code> of 30</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses short polling, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response. You can use long polling to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue. </p><p>When you consume messages from a queue using short polling, Amazon SQS samples a subset of its servers (based on a weighted random distribution) and returns messages from only those servers. Thus, a particular <code>ReceiveMessage</code> request might not return all of your messages. However, if you have fewer than 1,000 messages in your queue, a subsequent request will return your messages. If you keep consuming from your queues, Amazon SQS samples all of its servers, and you receive all of your messages.</p><p>The following diagram shows the short-polling behavior of messages returned from a standard queue after one of your system components makes a receive request. Amazon SQS samples several of its servers (in gray) and returns messages A, C, D, and B from these servers. Message E isn't returned for this request but is returned for a subsequent request.</p><p><br></p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-06-05_17-51-10-8ff4de7426893136b68645f9f1211ff8.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-06-05_17-51-10-8ff4de7426893136b68645f9f1211ff8.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><br></p><p>When the wait time for the <code>ReceiveMessage</code> API action is greater than 0, <em>long polling</em> is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a <code>ReceiveMessage</code> request) and false empty responses (when messages are available but aren't included in a response).</p><p>Long polling occurs when the <code>WaitTimeSeconds</code> parameter of a <code>ReceiveMessage</code> request is set to a value greater than 0 in one of two ways:</p><ul><li><p>The <code>ReceiveMessage</code> call sets <code>WaitTimeSeconds</code> to a value greater than 0.</p></li><li><p>The <code>ReceiveMessage</code> call doesn’t set <code>WaitTimeSeconds</code>, but the queue attribute <code>ReceiveMessageWaitTimeSeconds</code> is set to a value greater than 0.</p></li></ul><p><strong>Therefore, the Developer should </strong>set the <code>ReceiveMessage</code> API with a <code>WaitTimeSeconds</code> of 20.</p><p><strong>CORRECT: </strong>\"Set the <code>ReceiveMessage</code> API with a <code>WaitTimeSeconds</code> of 20\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Set the <code>SetQueueAttributes</code> API with a <code>DelaySeconds</code> of 20\" is incorrect as this would be used to configure a delay queue where the delivery of messages in the queue is delayed.</p><p><strong>INCORRECT:</strong> \"Set the <code>ReceiveMessage</code> API with a <code>VisibilityTimeout</code> of 30\" is incorrect as this would configure the visibility timeout which is the length of time a message that has been received is invisible.</p><p><strong>INCORRECT:</strong> \"Set the <code>SetQueueAttributes</code> with a <code>MessageRetentionPeriod</code> of 60\" is incorrect as this would configure how long messages are retained in the queue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html</a></p><p><br></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 22,
    "question": "<p>An organization is hosting a website on an Amazon EC2 instance in a public subnet. The website should allow public access for HTTPS traffic on TCP port 443 but should only accept SSH traffic on TCP port 22 from a corporate address range accessible over a VPN.</p><p>Which security group configuration will support both requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Allow traffic to both port 443 and port 22 from 0.0.0.0/0 and 192.168.0.0/16.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Allow traffic to port 443 from 0.0.0.0/0 and allow traffic to port 22 from 192.168.0.0/16.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Allow traffic to both port 443 and port 22 from the VPC CIDR block.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Allow traffic to port 22 from 0.0.0.0/0 and allow traffic to port 443 from 192.168.0.0/16.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>Allowing traffic from 0.0.0.0/0 to port 443 will allow any traffic from the internet to access the website. Limiting the IP address to 192.168.0.0/16 for port 22 will only allow local organizational traffic.</p><p><strong>CORRECT</strong>: \"Allow traffic to port 443 from 0.0.0.0/0 and allow traffic to port 22 from 192.168.0.0/16\" is the correct answer (as explained above.)</p><p><strong>INCORRECT</strong>: “Allow traffic to port 22 from 0.0.0.0/0 and allow traffic to port 443 from 192.168.0.0/16\" is incorrect. This will allow traffic from the Internet to port 22 and allow traffic to port 443 from the corporate address block only (192.168.0.0/16).</p><p><strong>INCORRECT</strong>: \"Allow traffic to both port 443 and port 22 from the VPC CIDR block\" is incorrect. This would not satisfy either requirement as internet-based users will not be able to access the website and corporate users will not be able to manage the instance via SSH.</p><p><strong>INCORRECT</strong>: \"Allow traffic to both port 443 and port 22 from 0.0.0.0/0 and 192.168.0.0/16\" is incorrect. This does not satisfy the requirement to restrict access to port 22 to only the corporate address block.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
      "https://digitalcloud.training/amazon-ec2/"
    ]
  },
  {
    "id": 23,
    "question": "<p>A Developer is building a three-tier web application that must be able to handle a minimum of 10,000 requests per minute. The requirements state that the web tier should be completely stateless while the application maintains session state data for users.</p><p>How can the session state data be maintained externally, whilst keeping latency at the LOWEST possible value?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement a shared Amazon EFS file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the EFS file system for session data storage</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon RedShift instance, then implement session handling at the application level to leverage a database inside the RedShift database instance for session data storage</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon ElastiCache Redis cluster, then implement session handling at the application level to leverage the cluster for session data storage</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>It is common to use key/value stores for storing session state data. The two options presented in the answers are Amazon DynamoDB and Amazon ElastiCache Redis. Of these two, ElastiCache will provide the lowest latency as it is an in-memory database.</p><p>Therefore, the best answer is to create an Amazon ElastiCache Redis cluster, then implement session handling at the application level to leverage the cluster for session data storage</p><p><strong>CORRECT: </strong>\"Create an Amazon ElastiCache Redis cluster, then implement session handling at the application level to leverage the cluster for session data storage\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage\" is incorrect as though this is a good solution for storing session state data, the latency will not be as low as with ElastiCache.</p><p><strong>INCORRECT:</strong> \"Create an Amazon RedShift instance, then implement session handling at the application level to leverage a database inside the RedShift database instance for session data storage\" is incorrect. RedShift is a data warehouse that is used for OLAP use cases, not for storing session state data.</p><p><strong>INCORRECT:</strong> \"Implement a shared Amazon EFS file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the EFS file system for session data storage\" is incorrect. For session state data a key/value store such as DynamoDB or ElastiCache will provide better performance.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/caching/session-management/\">https://aws.amazon.com/caching/session-management/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/caching/session-management/",
      "https://digitalcloud.training/amazon-elasticache/"
    ]
  },
  {
    "id": 24,
    "question": "<p>The Lambda function needs to write this data to an Amazon DynamoDB table. After deploying the function, the developer notices that the write operations to the DynamoDB table occasionally fail due to throttling.</p><p>What should the developer do to reduce the likelihood of these throttling issues without significantly over-provisioning the DynamoDB table's write capacity?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Convert the DynamoDB table to an on-demand capacity mode to automatically adjust its write capacity to match the workload.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon SQS to queue data before writing it to the DynamoDB table, ensuring a steady and controlled write rate.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Implement exponential backoff in the Lambda function's error handling code to retry failed write operations.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Increase the Lambda function's timeout setting to allow for retries in case of throttling.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Implementing exponential backoff is a best practice for handling retries in distributed systems, especially when dealing with AWS services like DynamoDB. Exponential backoff involves gradually increasing the delay between retry attempts to reduce the load on the service and the likelihood of subsequent throttling.</p><p>This approach allows the Lambda function to manage intermittent spikes more effectively in demand without requiring significant over-provisioning of the DynamoDB table's write capacity.</p><p><strong>CORRECT: </strong>\"Implement exponential backoff in the Lambda function's error handling code to retry failed write operations\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Increase the Lambda function's timeout setting to allow for retries in case of throttling\" is incorrect.</p><p>This issue is not directly related to handling throttling issues; increasing the timeout setting allows the function to run longer but does not address the root cause of the throttling.</p><p><strong>INCORRECT:</strong> \"Convert the DynamoDB table to an on-demand capacity mode to automatically adjust its write capacity to match the workload\" is incorrect.</p><p>This solution could potentially solve the issue by automatically scaling the table's capacity, but it might lead to higher costs due to uncontrolled scaling and does not represent a coding or architectural strategy to manage write throughput efficiently.</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS to queue data before writing it to the DynamoDB table, ensuring a steady and controlled write rate\" is incorrect.</p><p>This solution introduces additional complexity and potential latency without directly addressing the issue of handling write retries when throttling occurs. While SQS can help decouple components and manage load, the key to solving the immediate problem of write throttling is to implement a retry strategy that respects DynamoDB's adaptive capacity and avoids overwhelming the table with rapid, repeated write requests.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html",
      "https://digitalcloud.training/aws-lambda/"
    ]
  },
  {
    "id": 25,
    "question": "<p>An application needs to generate SMS text messages and emails for a large number of subscribers. Which AWS service can be used to send these messages to customers?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon SNS</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon SQS</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon SWF</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon SES</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>Amazon Simple Notification Service (Amazon SNS) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients—publishers and subscribers—also referred to as producers and consumers.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_05-48-02-778d257dde32ce0bc682c9b269c19d94.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-21_05-48-02-778d257dde32ce0bc682c9b269c19d94.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel.</p><p>Subscribers (that is, web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (that is, Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic.</p><p><strong>CORRECT: </strong>\"Amazon SNS\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon SES\" is incorrect as this service only sends email, not SMS text messages.</p><p><strong>INCORRECT:</strong> \"Amazon SQS\" is incorrect as this is a hosted message queue for decoupling application components.</p><p><strong>INCORRECT:</strong> \"Amazon SWF\" is incorrect as the Simple Workflow Service is used for orchestrating multi-step workflows.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/welcome.html\">https://docs.aws.amazon.com/sns/latest/dg/welcome.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/sns/latest/dg/welcome.html",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 26,
    "question": "<p>A Developer is writing code to run in a cron job on an Amazon EC2 instance that sends status information about the application to Amazon CloudWatch.</p><p>Which method should the Developer use?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AWS CLI put-metric-alarm command.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the unified CloudWatch agent to publish custom metrics.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the AWS CLI put-metric-data command.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use the CloudWatch console with detailed monitoring.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>The put-metric-data command publishes metric data points to Amazon CloudWatch. CloudWatch associates the data points with the specified metric. If the specified metric does not exist, CloudWatch creates the metric.</p><p><strong>CORRECT: </strong>\"Use the AWS CLI put-metric-data command\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use the AWS CLI put-metric-alarm command\" is incorrect. This command creates or updates an alarm and associates it with the specified metric, metric math expression, or anomaly detection model.</p><p><strong>INCORRECT:</strong> \"Use the unified CloudWatch agent to publish custom metrics\" is incorrect. It is not necessary to use the unified CloudWatch agent. In this case the Developer can use the AWS CLI with the cron job.</p><p><strong>INCORRECT:</strong> \"Use the CloudWatch console with detailed monitoring\" is incorrect. You cannot collect custom metric data using the CloudWatch console with detailed monitoring. Detailed monitoring sends data at 1-minute rather than 5-minute frequencies but will not collect custom data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/put-metric-data.html\">https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/put-metric-data.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/put-metric-data.html",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  },
  {
    "id": 27,
    "question": "<p>A developer is writing an application for a company. The program needs to access and read the file named \"secret-data.xlsx\" located in the root directory of an Amazon S3 bucket named \"DATA-BUCKET\". The company's security policies mandate the enforcement of the principle of least privilege for the IAM policy associated with the application.</p><p>Which IAM policy statement will comply with these security stipulations?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>{\"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET/*\"}</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>{\"Effect\": \"Allow\", \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET\"}</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>{\"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET/secret-data.xlsx\"}</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>{\"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET/*\"}</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>This statement provides the minimal permission necessary for the application to read the specific \"secret-data.xlsx\" file from the specified S3 bucket.</p><p><strong>CORRECT: </strong>\"{\"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET/secret-data.xlsx\"}\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"{\"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET/*\"}\" is incorrect.</p><p>This policy statement gives permission for all S3 actions on all objects in the bucket, which goes against the principle of least privilege.</p><p><strong>INCORRECT:</strong> \"{\"Effect\": \"Allow\", \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET\"}\" is incorrect.</p><p>This policy permits the application to list all objects in the bucket, but it doesn't grant read permission for \"secret-data.xlsx\".</p><p><strong>INCORRECT:</strong> \"{\"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::DATA-BUCKET/*\"}\" is incorrect.</p><p>This statement provides read access for all objects in the bucket, not just the \"secret-data.xlsx\" file, contradicting the principle of least privilege.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html",
      "https://digitalcloud.training/amazon-s3-and-glacier/"
    ]
  },
  {
    "id": 28,
    "question": "<p>An application asynchronously invokes an AWS Lambda function. The application has recently been experiencing occasional errors that result in failed invocations. A developer wants to store the messages that resulted in failed invocations such that the application can automatically retry processing them.</p><p>What should the developer do to accomplish this goal with the LEAST operational overhead?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure an Amazon S3 bucket as a destination for failed invocations. Configure event notifications to trigger the Lambda function to process the events.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure logging to an Amazon CloudWatch Logs group. Configure Lambda to read failed invocation events from the log group.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure Amazon EventBridge to send the messages to Amazon SNS to initiate the Lambda function again.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a redrive policy on an Amazon SQS queue. Set the dead-letter queue as an event source to the Lambda function.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>Amazon SQS supports <em>dead-letter queues</em> (DLQ), which other queues (<em>source queues</em>) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.</p><p>The <em>redrive policy</em> specifies the <em>source queue</em>, the <em>dead-letter queue</em>, and the conditions under which Amazon SQS moves messages from the former to the latter if the consumer of the source queue fails to process a message a specified number of times.</p><p>You can set your DLQ as an event source to the Lambda function to drain your DLQ. This will ensure that all failed invocations are automatically retried.</p><p><strong>CORRECT: </strong>\"Configure a redrive policy on an Amazon SQS queue. Set the dead-letter queue as an event source to the Lambda function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure logging to an Amazon CloudWatch Logs group. Configure Lambda to read failed invocation events from the log group\" is incorrect.</p><p>The information in the logs may not be sufficient for processing the event. This is not an automated or ideal solution.</p><p><strong>INCORRECT:</strong> \"Configure Amazon EventBridge to send the messages to Amazon SNS to initiate the Lambda function again\" is incorrect.</p><p>Amazon EventBridge can be configured as a failure destination and can send to SNS. SNS can also be configured with Lambda as a target. However, this solution requires more operational overhead compared to using a DLQ.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon S3 bucket as a destination for failed invocations. Configure event notifications to trigger the Lambda function to process the events\" is incorrect.</p><p>S3 is not a supported failure destination. Supported destinations are Amazon SNS, Amazon SQS, and Amazon EventBridge.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/\">https://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/</a></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rule-dlq.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rule-dlq.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/",
      "https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rule-dlq.html",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 29,
    "question": "<p>A business operates a web app on Amazon EC2 instances utilizing a bespoke Amazon Machine Image (AMI). They employ AWS CloudFormation for deploying their app, which is currently active in the us-east-1 Region. However, their goal is to extend the deployment to the us-west-1 Region.</p><p>During an initial attempt to create an AWS CloudFormation stack in us-west-1, the action fails, and an error message indicates that the AMI ID does not exist. A developer is tasked with addressing this error through a method that minimizes operational complexity.</p><p>Which action should the developer take?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Lambda to create an AMI in the us-west-1 Region during stack creation.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Modify the CloudFormation template to refer to the AMI in us-east-1 Region.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a new AMI in the us-west-1 Region and update the CloudFormation template with the new AMI ID.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Copy the AMI from the us-east-1 Region to the us-west-1 Region and use the new AMI ID in the CloudFormation template.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>This is the best option as it allows the developer to use the same AMI in a different region with minimal effort and maintenance.</p><p><strong>CORRECT: </strong>\"Copy the AMI from the us-east-1 Region to the us-west-1 Region and use the new AMI ID in the CloudFormation template\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a new AMI in the us-west-1 Region and update the CloudFormation template with the new AMI ID\" is incorrect.</p><p>This is incorrect as creating a new AMI would be operationally complex and time-consuming.</p><p><strong>INCORRECT:</strong> \"Modify the CloudFormation template to refer to the AMI in us-east-1 Region\" is incorrect.</p><p>AMIs are regional resources and cannot be used directly in other regions.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda to create an AMI in the us-west-1 Region during stack creation\" is incorrect.</p><p>This process would add unnecessary complexity and the new AMI would not be identical to the original one.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html",
      "https://digitalcloud.training/amazon-ec2/"
    ]
  },
  {
    "id": 30,
    "question": "<p>A Developer is creating a serverless application that uses an Amazon DynamoDB table. The application must make idempotent, all-or-nothing operations for multiple groups of write actions.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable DynamoDB streams and capture new images. Update the items in the table using the BatchWriteltem.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon SQS FIFO queue and use the SendMessageBatch operation to group the changes.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Update the items in the table using the TransactWriteltems operation to group the changes.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Update the items in the table using the BatchWriteltem operation and configure idempotency at the table level.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>TransactWriteItems is a synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that either all of them succeed or none of them succeeds.</p><p>A TransactWriteItems operation differs from a BatchWriteItem operation in that all the actions it contains must be completed successfully, or no changes are made at all. With a BatchWriteItem operation, it is possible that only some of the actions in the batch succeed while the others do not.</p><p><strong>CORRECT: </strong>\"Update the items in the table using the TransactWriteltems operation to group the changes\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Update the items in the table using the BatchWriteltem operation and configure idempotency at the table level\" is incorrect. As explained above, the TransactWriteItems operation must be used.</p><p><strong>INCORRECT:</strong> \"Enable DynamoDB streams and capture new images. Update the items in the table using the BatchWriteltem\" is incorrect. DynamoDB streams will not assist with making idempotent write operations.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SQS FIFO queue and use the SendMessageBatch operation to group the changes\" is incorrect. Amazon SQS should not be used as it does not assist and this solution is supposed to use a DynamoDB table</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_TransactWriteItems.html\">https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_TransactWriteItems.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_TransactWriteItems.html",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 31,
    "question": "<p>A developer must identify the public IP addresses of clients connecting to Amazon EC2 instances behind a public Application Load Balancer (ALB). The EC2 instances run an HTTP server that logs all requests to a log file.</p><p>How can the developer ensure the client public IP addresses are captured in the log files on the EC2 instances?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Install the AWS X-Ray daemon on the EC2 instances and configure request logging.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Install the Amazon CloudWatch Logs agent on the EC2 instances and configure logging.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure the HTTP server to add the x-forwarded-for request header to the logs.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure the HTTP server to add the x-forwarded-proto request header to the logs.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>The X-Forwarded-For request header is automatically added and helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer.</p><p>Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header.</p><p>The HTTP server may need to be configured to include the x-forwarded-for request header in the log files. Once this is done, the logs will contain the public IP addresses of the clients.</p><p><strong>CORRECT: </strong>\"Configure the HTTP server to add the x-forwarded-for request header to the logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the HTTP server to add the x-forwarded-proto request header to the logs\" is incorrect.</p><p>This request header identifies the protocol (HTTP or HTTPS).</p><p><strong>INCORRECT:</strong> \"Install the AWS X-Ray daemon on the EC2 instances and configure request logging\" is incorrect.</p><p>X-Ray is used for tracing applications; it will not help identify the public IP addresses of clients.</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch Logs agent on the EC2 instances and configure logging\" is incorrect.</p><p>The Amazon CloudWatch Logs agent will send application and system logs to CloudWatch Logs. This does not help to capture the client IP addresses of connections.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/x-forwarded-headers.html#x-forwarded-for\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/x-forwarded-headers.html#x-forwarded-for</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/x-forwarded-headers.html#x-forwarded-for",
      "https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/"
    ]
  },
  {
    "id": 32,
    "question": "<p>A Developer has created a task definition that includes the following JSON code:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"placementStrategy\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L1\"><span class=\"pun\">{</span></li><li class=\"L2\"><span class=\"str\">\"field\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"attribute:ecs.availability-zone\"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"type\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"spread\"</span></li><li class=\"L4\"><span class=\"pun\">},</span></li><li class=\"L5\"><span class=\"pun\">{</span></li><li class=\"L6\"><span class=\"str\">\"field\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"instanceId\"</span><span class=\"pun\">,</span></li><li class=\"L7\"><span class=\"str\">\"type\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"spread\"</span></li><li class=\"L8\"><span class=\"pun\">}</span></li><li class=\"L9\"><span class=\"pun\">]</span></li></ol></pre></div></div><p>What is the effect of this task placement strategy?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>It distributes tasks evenly across Availability Zones and then distributes tasks evenly across the instances within each Availability Zone</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>It distributes tasks evenly across Availability Zones and then distributes tasks randomly across instances within each Availability Zone</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>It distributes tasks evenly across Availability Zones and then bin packs tasks based on memory within each Availability Zone</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>It distributes tasks evenly across Availability Zones and then distributes tasks evenly across distinct instances within each Availability Zone</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>A <em>task placement strategy</em> is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.</p><p>Amazon ECS supports the following task placement strategies:</p><p><code>binpack</code></p><p>Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.</p><p><code>random</code></p><p>Place tasks randomly.</p><p><code>spread</code></p><p>Place tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone.</p><p>You can specify task placement strategies with the following actions: <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateService.html\">CreateService</a>, <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_UpdateService.html\">UpdateService</a>, and <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RunTask.html\">RunTask</a>. You can also use multiple strategies together as in the example JSON code provided with the question.</p><p><strong>CORRECT: </strong>\"It distributes tasks evenly across Availability Zones and then distributes tasks evenly across the instances within each Availability Zone\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"It distributes tasks evenly across Availability Zones and then bin packs tasks based on memory within each Availability Zone\" is incorrect as it does not use the binpack strategy.</p><p><strong>INCORRECT:</strong> \"It distributes tasks evenly across Availability Zones and then distributes tasks evenly across distinct instances within each Availability Zone\" is incorrect as it does not spread tasks across distinct instances (use a task placement constraint).</p><p><strong>INCORRECT:</strong> \"It distributes tasks evenly across Availability Zones and then distributes tasks randomly across instances within each Availability Zone\" is incorrect as it does not use the random strategy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateService.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_UpdateService.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RunTask.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html",
      "https://digitalcloud.training/amazon-ecs-and-eks/"
    ]
  },
  {
    "id": 33,
    "question": "<p>A Developer has recently created an application that uses an AWS Lambda function, an Amazon DynamoDB table, and also sends notifications using Amazon SNS. The application is not working as expected and the Developer needs to analyze what is happening across all components of the application.</p><p>What is the BEST way to analyze the issue?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable X-Ray tracing for the Lambda function</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Assess the application with Amazon Inspector</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Monitor the application with AWS Trusted Advisor</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon CloudWatch Events rule</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>AWS X-Ray makes it easy for developers to analyze the behavior of their production, distributed applications with end-to-end tracing capabilities. You can use X-Ray to identify performance bottlenecks, edge case errors, and other hard to detect issues.</p><p>AWS X-Ray provides an end-to-end, cross-service view of requests made to your application. It gives you an application-centric view of requests flowing through your application by aggregating the data gathered from individual services in your application into a single unit called a trace. You can use this trace to follow the path of an individual request as it passes through each service or tier in your application so that you can pinpoint where issues are occurring.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_13-05-35-b3f0f59b4ce5e42177cf09e46934c48e.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_13-05-35-b3f0f59b4ce5e42177cf09e46934c48e.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>AWS X-Ray will assist the developer with visually analyzing the end-to-end view of connectivity between the application components and how they are performing using a Service Map. X-Ray also provides aggregated data about the application.</p><p><strong>CORRECT: </strong>\"Enable X-Ray tracing for the Lambda function\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Events rule\" is incorrect as this feature of CloudWatch is used to trigger actions based on changes in the state of AWS services.</p><p><strong>INCORRECT:</strong> \"Assess the application with Amazon Inspector\" is incorrect. Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.</p><p><strong>INCORRECT:</strong> \"Monitor the application with AWS Trusted Advisor\" is incorrect. <strong>AWS Trusted Advisor</strong> is an online tool that provides you real time guidance to help you provision your resources following AWS best practices.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/xray/features/\">https://aws.amazon.com/xray/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/xray/features/",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 34,
    "question": "<p>A critical application is hosted on AWS exposed by an HTTP API through Amazon API Gateway. The API is integrated with an AWS Lambda function and the application data is housed in an Amazon RDS for PostgreSQL DB instance, featuring 2 vCPUs and 16 GB of RAM.</p><p>The company has been receiving customer complaints about occasional HTTP 500 Internal Server Error responses from some API calls during unpredictable peak usage times. Amazon CloudWatch Logs has recorded \"connection limit exceeded\" errors. The company wants to ensure resilience in the application, with no unscheduled downtime for the database.</p><p>Which solution would best fit these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon RDS Proxy and update the Lambda function to connect to the proxy.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Lambda to create a connection pool for the RDS instance.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Double the RAM and vCPUs of the RDS instance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Implement auto-scaling for the RDS instance based on connection count.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Amazon RDS Proxy is designed to improve application scalability and resilience by pooling and sharing database connections, reducing the CPU and memory overhead on the database. It handles the burst in connections seamlessly and improves the application's ability to scale.</p><p><strong>CORRECT: </strong>\"Use Amazon RDS Proxy and update the Lambda function to connect to the proxy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Double the RAM and vCPUs of the RDS instance\" is incorrect.</p><p>Merely augmenting the number of vCPUs and RAM for the RDS instance might not resolve the issue as it doesn't directly tackle the problem of too many connections.</p><p><strong>INCORRECT:</strong> \"Implement auto-scaling for the RDS instance based on connection count\" is incorrect.</p><p>Auto-scaling in response to connection count is not a feature provided by RDS.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda to create a connection pool for the RDS instance\" is incorrect.</p><p>The best solution is to use RDS proxy which is designed for creating a pool of connections. Lambda may not be the best solution for this problem as it could be costly and has a limitation in execution time.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/\">https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/",
      "https://digitalcloud.training/amazon-rds/"
    ]
  },
  {
    "id": 35,
    "question": "<p>A company has implemented AWS CodePipeline to automate its release pipelines. The Development team is writing an AWS Lambda function that will send notifications for state changes of each of the actions in the stages.</p><p>Which steps must be taken to associate the Lambda function with the event source?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an event trigger and specify the Lambda function from the CodePipeline console</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a trigger that invokes the Lambda function from the Lambda console by selecting CodePipeline as the event source</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon CloudWatch alarm that monitors status changes in CodePipeline and triggers the Lambda function</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/WhatIsCloudWatchEvents.html\">Amazon CloudWatch Events</a> help you to respond to state changes in your AWS resources. When your resources change state, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your AWS Lambda function to take action.</p><p>AWS CodePipeline can be configured as an event source in CloudWatch Events and can then send notifications using as service such as Amazon SNS.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_13-17-20-fee89385a439135b342353d0489097e2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_13-17-20-fee89385a439135b342353d0489097e2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the best answer is to create an Amazon CloudWatch Events rule that uses CodePipeline as an event source.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a trigger that invokes the Lambda function from the Lambda console by selecting CodePipeline as the event source\" is incorrect as CodePipeline cannot be configured as a trigger for Lambda.</p><p><strong>INCORRECT:</strong> \"Create an event trigger and specify the Lambda function from the CodePipeline console\" is incorrect as CodePipeline cannot be configured as a trigger for Lambda.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm that monitors status changes in CodePipeline and triggers the Lambda function\" is incorrect as CloudWatch Events is used for monitoring state changes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/WhatIsCloudWatchEvents.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  },
  {
    "id": 36,
    "question": "<p>An ecommerce company manages a storefront that uses an Amazon API Gateway API which exposes an AWS Lambda function. The Lambda functions processes orders and stores the orders in an Amazon RDS for MySQL database. The number of transactions increases sporadically during marketing campaigns, and then goes close to zero during quite times.</p><p>How can a developer increase the elasticity of the system MOST cost-effectively?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon SQS queue. Publish transactions to the queue and set the queue to invoke the Lambda function. Set the reserved concurrency of the Lambda function to be equal to the max number of database connections.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon SNS topic. Publish transactions to the topic configure an SQS queue as a destination. Configure Lambda to process transactions from the queue.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average connections of Aurora Replicas.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average CPU utilization.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>The most efficient solution would be to use Aurora Auto Scaling and configure the scaling events to happen based on target metric. The metric to use is <strong>Average connections of Aurora Replicas</strong> which will create a policy based on the average number of connections to Aurora Replicas.</p><p>This will ensure that the Aurora replicas scale based on actual numbers of connections to the replicas which will vary based on how busy the storefront is and how many transactions are being processed.</p><p><strong>CORRECT: </strong>\"Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average connections of Aurora Replicas\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average CPU utilization\" is incorrect.</p><p>The better metric to use for this situation would be the number of connections to Aurora Replicas as that is the metric that has the closest correlation to the number of transactions being executed.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic. Publish transactions to the topic configure an SQS queue as a destination. Configure Lambda to process transactions from the queue\" is incorrect.</p><p>This is highly inefficient. There is no need for an SNS topic in this situation.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SQS queue. Publish transactions to the queue and set the queue to invoke the Lambda function. Set the reserved concurrency of the Lambda function to be equal to the max number of database connections\" is incorrect.</p><p>This would be less cost effective as you would be paying for the reserved concurrency at all times.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html",
      "https://digitalcloud.training/amazon-aurora/"
    ]
  },
  {
    "id": 37,
    "question": "<p>A company has an application that logs all information to Amazon S3. Whenever there is a new log file, an AWS Lambda function is invoked to process the log files. The code works, gathering all of the necessary information. However, when checking the Lambda function logs, duplicate entries with the same request ID are found.</p><p>What is the BEST explanation for the duplicate entries?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The application stopped intermittently and then resumed</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The Lambda function failed, and the Lambda service retried the invocation with a delay</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>There was an S3 outage, which caused duplicate entries of the same log file</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The S3 bucket name was specified incorrectly</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>From the AWS documentation:</p><p>“When an error occurs, your function may be invoked multiple times. Retry behavior varies by error type, client, event source, and invocation type. For example, if you invoke a function asynchronously and it returns an error, Lambda executes the function up to two more times. For more information, see Retry Behavior.</p><p>For asynchronous invocation, Lambda adds events to a queue before sending them to your function. If your function does not have enough capacity to keep up with the queue, events may be lost. Occasionally, your function may receive the same event multiple times, even if no error occurs. To retain events that were not processed, configure your function with a dead-letter queue.”</p><p>Therefore, the most likely explanation is that the function failed, and Lambda retried the invocation.</p><p><strong>CORRECT: </strong>\"The Lambda function failed, and the Lambda service retried the invocation with a delay\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"The S3 bucket name was specified incorrectly\" is incorrect. If this was the case all attempts would fail but this is not the case.</p><p><strong>INCORRECT:</strong> \"There was an S3 outage, which caused duplicate entries of the same log file\" is incorrect. There cannot be duplicate log files in Amazon S3 as every object must be unique within a bucket. Therefore, if the same log file was uploaded twice it would just overwrite the previous version of the file. Also, if a separate request was made to Lambda it would have a different request ID.</p><p><strong>INCORRECT:</strong> \"The application stopped intermittently and then resumed\" is incorrect. The issue is duplicate entries of the same request ID.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html\">https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html",
      "https://digitalcloud.training/aws-lambda/"
    ]
  },
  {
    "id": 38,
    "question": "<p>A Development team would use a GitHub repository and would like to migrate their application code to AWS CodeCommit.<br>What needs to be created before they can migrate a cloned repository to CodeCommit over HTTPS?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A set of Git credentials generated with IAM</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>A public and private SSH key file</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>A GitHub secure authentication token</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>An Amazon EC2 IAM role with CodeCommit permissions</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>AWS CodeCommit is a managed version control service that hosts private Git repositories in the AWS cloud. To use CodeCommit, you configure your Git client to communicate with CodeCommit repositories. As part of this configuration, you provide IAM credentials that CodeCommit can use to authenticate you. IAM supports CodeCommit with three types of credentials:</p><p>Git credentials, an IAM -generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.</p><p>SSH keys, a locally generated public-private key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.</p><p>AWS access keys, which you can use with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.</p><p>In this scenario the Development team need to connect to CodeCommit using HTTPS so they need either AWS access keys to use the AWS CLI or Git credentials generated by IAM. Access keys are not offered as an answer choice so the best answer is that they need to create a set of Git credentials generated with IAM</p><p><strong>CORRECT: </strong>\"A set of Git credentials generated with IAM\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"A GitHub secure authentication token\" is incorrect as they need to authenticate to AWS CodeCommit, not GitHub (they have already accessed and cloned the repository).</p><p><strong>INCORRECT:</strong> \"A public and private SSH key file\" is incorrect as these are used to communicate with CodeCommit repositories using SSH, not HTTPS.</p><p><strong>INCORRECT:</strong> \"An Amazon EC2 IAM role with CodeCommit permissions\" is incorrect as you need the Git credentials generated through IAM to connect to CodeCommit.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 39,
    "question": "<p>An organization is selling memorabilia that is illegal in specific countries. How can a developer restrict access to the website to countries where the memorabilia are illegal?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a Web ACL in AWS WAF with a rule that matches the specified countries and blocks access.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a Web ACL in AWS WAF with a rule that matches the specified countries and triggers an SNS notification.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a Web ACL in AWS Shield with a rule that matches the specified countries and blocks access.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a Web ACL in AWS Shield with a rule that matches the specified countries and triggers an SNS notification.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>AWS WAF can be used to set up a WEB ACL that can be used to block statements that originate from a specific country.</p><p><strong>CORRECT: </strong>\"Create a Web ACL in AWS WAF with a rule that matches the specified countries and blocks access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a Web ACL in AWS Shield with a rule that matches the specified countries and blocks access\" is incorrect. AWS Shield is used to protect from DDoS attacks.</p><p><strong>INCORRECT:</strong> \"Create a Web ACL in AWS WAF with a rule that matches the specified countries and triggers an SNS notification\" is incorrect. This will not block access to specific countries.</p><p><strong>INCORRECT:</strong> \" Create a Web ACL in AWS Shield with a rule that matches the specified countries and triggers an SNS notification\" is incorrect. This will not block access to specific countries.</p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/waf-allow-block-country-geolocation/\">https://aws.amazon.com/premiumsupport/knowledge-center/waf-allow-block-country-geolocation/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/waf-allow-block-country-geolocation/",
      "https://digitalcloud.training/aws-waf-shield/"
    ]
  },
  {
    "id": 40,
    "question": "<p>A Developer has used a third-party tool to build, bundle, and package a software package on-premises. The software package is stored in a local file system and must be deployed to Amazon EC2 instances.</p><p>How can the application be deployed onto the EC2 instances?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS CodeBuild to commit the package and automatically deploy the software package.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS CodeDeploy and point it to the local file system to deploy the software package.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>AWS CodeDeploy can deploy software packages using an archive that has been uploaded to an Amazon S3 bucket. The archive file will typically be a .zip file containing the code and files required to deploy the software package.</p><p><strong>CORRECT: </strong>\"Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS CodeDeploy and point it to the local file system to deploy the software package\" is incorrect. You cannot point CodeDeploy to a local file system running on-premises.</p><p><strong>INCORRECT:</strong> \"Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances\" is incorrect. CodeCommit is a source control system. In this case the source code has already been package using a third-party tool.</p><p><strong>INCORRECT:</strong> \"Use AWS CodeBuild to commit the package and automatically deploy the software package\" is incorrect. CodeBuild does not commit packages (CodeCommit does) or deploy the software. It is a build service.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorials-windows-upload-application.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorials-windows-upload-application.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorials-windows-upload-application.html",
      "https://digitalcloud.training/amazon-s3-and-glacier/"
    ]
  },
  {
    "id": 41,
    "question": "<p>A critical application runs on an Amazon EC2 instance. A Developer has configured a custom Amazon CloudWatch metric that monitors application availability with a data granularity of 1 second. The Developer must be notified within 30 seconds if the application experiences any issues.</p><p>What should the Developer do to meet this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Specify an Amazon SNS topic for alarms when issuing the put-metric-data AWS CLI command.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use a default CloudWatch metric, configure an alarm, and use Amazon SNS to send the alert.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon CloudWatch Logs Insights and trigger an Amazon Eventbridge rule to send a notification.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a high-resolution CloudWatch alarm and use Amazon SNS to send the alert.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>If you set an alarm on a high-resolution metric, you can specify a high-resolution alarm with a period of 10 seconds or 30 seconds, or you can set a regular alarm with a period of any multiple of 60 seconds. There is a higher charge for high-resolution alarms.</p><p>Amazon SNS can then be used to send notifications based on the CloudWatch alarm.</p><p><strong>CORRECT: </strong>\"Configure a high-resolution CloudWatch alarm and use Amazon SNS to send the alert\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Specify an Amazon SNS topic for alarms when issuing the put-metric-data AWS CLI command\" is incorrect. You cannot specify an SNS topic with this CLI command.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Logs Insights and trigger an Amazon Eventbridge rule to send a notification\" is incorrect. Logs Insights cannot be used for alarms or alerting based on custom CloudWatch metrics.</p><p><strong>INCORRECT:</strong> \"Use a default CloudWatch metric, configure an alarm, and use Amazon SNS to send the alert\" is incorrect. There is no default metric that would monitor the application uptime and the resolution would be lower.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#high-resolution-alarms\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#high-resolution-alarms</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#high-resolution-alarms",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  },
  {
    "id": 42,
    "question": "<p>A startup is developing a prototype for a news aggregator application. This application will display the latest news for a specific industry and provide a RESTful API endpoint that clients can invoke. Where feasible, the application should leverage AWS's caching features to reduce the load on the backend service. The backend of the application is expected to handle a modest amount of traffic, primarily during testing periods.</p><p>Which method would be the most cost-effective for the developer to implement this REST endpoint?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Construct an Amazon ECS service for the application, using Amazon ElastiCache for caching.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy an AWS Elastic Beanstalk application and use Amazon DynamoDB for caching services.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Launch an Amazon EC2 instance, host the application on it, and employ Amazon ElastiCache for caching purposes.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Utilize AWS API Gateway with an AWS Lambda function as the backend and enable caching in API Gateway.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>AWS API Gateway, combined with AWS Lambda, offers a serverless solution where costs are directly related to usage. API Gateway also provides a caching feature which can help reduce the load on the backend.</p><p><strong>CORRECT: </strong>\"Utilize AWS API Gateway with an AWS Lambda function as the backend and enable caching in API Gateway\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Launch an Amazon EC2 instance, host the application on it, and employ Amazon ElastiCache for caching purposes\" is incorrect.</p><p>Using an Amazon EC2 instance would incur constant costs, regardless of whether the instance is in use or idle. Additionally, employing AWS ElastiCache could increase the costs unnecessarily, especially for a prototype application.</p><p><strong>INCORRECT:</strong> \"Construct an Amazon ECS service for the application, using Amazon ElastiCache for caching\" is incorrect.</p><p>Amazon ECS is overkill for a simple prototype application like this. It requires more resources and management than a serverless solution.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Elastic Beanstalk application and use Amazon DynamoDB for caching services\" is incorrect.</p><p>While AWS Elastic Beanstalk simplifies application deployment, it does not inherently provide caching capabilities. Amazon DynamoDB is a database service, not a caching service, making it unsuitable for this requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html",
      "https://digitalcloud.training/amazon-api-gateway/"
    ]
  },
  {
    "id": 43,
    "question": "<p>A Developer is writing an imaging microservice on AWS Lambda. The service is dependent on several libraries that are not available in the Lambda runtime environment.</p><p>Which strategy should the Developer follow to create the Lambda deployment package?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a ZIP file with the source code and an appspec.yml file. Add the libraries to the appspec.yml file and upload to Amazon S3. Deploy using CloudFormation</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a ZIP file with the source code and all dependent libraries</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a ZIP file with the source code and a script that installs the dependent libraries at runtime</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a ZIP file with the source code and a buildspec.yml file that installs the dependent libraries on AWS Lambda</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>A deployment package is a ZIP archive that contains your function code and dependencies. You need to create a deployment package if you use the Lambda API to manage functions, or if you need to include libraries and dependencies other than the AWS SDK.</p><p>You can upload the package directly to Lambda, or you can use an Amazon S3 bucket, and then upload it to Lambda. If the deployment package is larger than 50 MB, you must use Amazon S3.</p><p><strong>CORRECT: </strong>\"Create a ZIP file with the source code and all dependent libraries\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a ZIP file with the source code and a script that installs the dependent libraries at runtime\" is incorrect as the Developer should not run a script at runtime as this will cause latency. Instead, the Developer should include the dependent libraries in the ZIP package.</p><p><strong>INCORRECT:</strong> \"Create a ZIP file with the source code and an appspec.yml file. Add the libraries to the appspec.yml file and upload to Amazon S3. Deploy using CloudFormation\" is incorrect. The appspec.yml file is used with CodeDeploy, you cannot add libraries into it, and it is not deployed using CloudFormation.</p><p><strong>INCORRECT:</strong> \"Create a ZIP file with the source code and a buildspec.yml file that installs the dependent libraries on AWS Lambda\" is incorrect as the buildspec.yml file is used with CodeBuild for compiling source code and running tests. It cannot be used to install dependent libraries within Lambda.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/python-package.html\">https://docs.aws.amazon.com/lambda/latest/dg/python-package.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/python-package.html",
      "https://digitalcloud.training/aws-lambda/"
    ]
  },
  {
    "id": 44,
    "question": "<p>A developer is setting up the primary key for an Amazon DynamoDB table that logs a company's purchases from various suppliers. Each transaction is recorded with these attributes: supplierId, transactionTime, item, and unitCost.</p><p>Which primary key configuration will be valid in this case?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A composite primary key with supplierId as the partition key and unitCost as the sort key.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A simple primary key with supplierId serving as the partition key.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>A composite primary key with supplierId as the partition key and item as the sort key.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A composite primary key with supplierId as the partition key and transactionTime as the sort key.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>A composite primary key with supplierId as the partition key and transactionTime as the sort key ensures each entry is unique. It is extremely unlikely for the same supplier to make two separate deliveries at the exact same millisecond.</p><p><strong>CORRECT: </strong>\"A composite primary key with supplierId as the partition key and transactionTime as the sort key\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"A simple primary key with supplierId serving as the partition key\" is incorrect.</p><p>This simple primary key (only partition key) will not ensure uniqueness because multiple purchases can be made from the same supplier.</p><p><strong>INCORRECT:</strong> \"A composite primary key with supplierId as the partition key and item as the sort key\" is incorrect.</p><p>Using supplierId and item as a composite key would not guarantee uniqueness, as a supplier can deliver the same item multiple times.</p><p><strong>INCORRECT:</strong> \"A composite primary key with supplierId as the partition key and unitCost as the sort key\" is incorrect.</p><p>A composite key using supplierId and unitCost would not ensure uniqueness either, as the same supplier can supply multiple items with the same unit cost.</p><p><strong>References:</strong></p><p><a href=\"https://repost.aws/knowledge-center/dynamodb-create-composite-key\">https://repost.aws/knowledge-center/dynamodb-create-composite-key</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://repost.aws/knowledge-center/dynamodb-create-composite-key",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 45,
    "question": "<p>A team of Developers require read-only access to an Amazon DynamoDB table. The Developers have been added to a group. What should an administrator do to provide the team with access whilst following the principal of least privilege? </p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a customer managed policy with read/write access to DynamoDB for all resources. Attach the policy to the group</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Assign the <code>AWSLambdaDynamoDBExecutionRole</code> AWS managed policy to the group</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a customer managed policy with read only access to DynamoDB and specify the ARN of the table for the “Resource” element. Attach the policy to the group</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Assign the AmazonDynamoDBReadOnlyAccess AWS managed policy to the group</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>The key requirement is to provide read-only access to the team for a specific DynamoDB table. Therefore, the AWS managed policy cannot be used as it will provide access to all DynamoDB tables in the account which does not follow the principal of least privilege.</p><p>Therefore, a customer managed policy should be created that provides read-only access and specifies the ARN of the table. For instance, the resource element might include the following ARN:</p><p><code>arn:aws:dynamodb:us-west-1:515148227241:table/exampletable</code></p><p>This will lock down access to the specific DynamoDB table, following the principal of least privilege.</p><p><strong>CORRECT: </strong>\"Create a customer managed policy with read only access to DynamoDB and specify the ARN of the table for the “Resource” element. Attach the policy to the group\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Assign the <code>AmazonDynamoDBReadOnlyAccess</code> AWS managed policy to the group\" is incorrect as this will provide read-only access to all DynamoDB tables in the account.</p><p><strong>INCORRECT:</strong> \"Assign the <code>AWSLambdaDynamoDBExecutionRole</code> AWS managed policy to the group\" is incorrect as this is a role used with AWS Lambda.</p><p><strong>INCORRECT:</strong> \"Create a customer managed policy with read/write access to DynamoDB for all resources. Attach the policy to the group\" is incorrect as read-only access should be provided, not read/write.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 46,
    "question": "<p>A developer is updating an Amazon ECS app that uses an ALB with two target groups and a single listener. The developer has an AppSpec file in an S3 bucket and an AWS CodeDeploy deployment group tied to the ALB and AppSpec file. The developer needs to use an AWS Lambda function for update validation before deployment.</p><p>Which solution meets these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add a listener to the ALB. Update the AppSpec file to link the Lambda function to the AfterAllowTraffic lifecycle hook.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add a listener to the ALB. Update the AppSpec file to link the Lambda function to the BeforeAllowTraffic lifecycle hook.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Attach a listener to the deployment group. Update the AppSpec file to link the Lambda function to the AfterAllowTraffic lifecycle hook.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Attach a listener to the deployment group. Update the AppSpec file to link the Lambda function to the BeforeAllowTraffic lifecycle hook.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>The AppSpec file allows a developer to specify scripts to be run at different lifecycle hooks. The BeforeAllowTraffic lifecycle event occurs before the updated task set is moved to the target group that is receiving live traffic. So, any validation before production deployment should be configured at this lifecycle event. The listener is configured at the ALB level, not at the deployment group.</p><p><strong>CORRECT: </strong>\"Add a listener to the ALB. Update the AppSpec file to link the Lambda function to the BeforeAllowTraffic lifecycle hook\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add a listener to the ALB. Update the AppSpec file to link the Lambda function to the AfterAllowTraffic lifecycle hook\" is incorrect.</p><p>This is incorrect because AfterAllowTraffic lifecycle event occurs after the updated task set is moved to the target group that is receiving live traffic. Validation should be performed before the updated task set receives live traffic, not after.</p><p><strong>INCORRECT:</strong> \"Attach a listener to the deployment group. Update the AppSpec file to link the Lambda function to the BeforeAllowTraffic lifecycle hook\" is incorrect.</p><p>This is incorrect because the listener is not attached to the deployment group. It is configured at the ALB level.</p><p><strong>INCORRECT:</strong> \"Attach a listener to the deployment group. Update the AppSpec file to link the Lambda function to the AfterAllowTraffic lifecycle hook\" is incorrect.</p><p>Listeners are added to the ALB, not the deployment group, and validation should occur before the updated task set receives live traffic.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 47,
    "question": "<p>An application uses an Amazon RDS database. The company requires that the performance of database reads is improved, and they want to add a caching layer in front of the database. The cached data must be encrypted, and the solution must be highly available.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon CloudFront with multiple origins.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon ElastiCache for Redis in cluster mode.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon DynamoDB Accelerator (DAX).</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon ElastiCache for Memcached.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Amazon ElastiCache is an in-memory database cache that can be used in front of Amazon RDS. The key to answering this question is to know the differences between ElastiCache Memcached and ElastiCache Redis. To support both encryption and high availability we must use ElastiCache Redis with cluster mode enabled.</p><p>You can see the differences between the different engines and configuration options for ElastiCache in the table below:</p><p><br></p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-29_11-24-04-6f7ba820e70f1c88c4d37dbf4a94517b.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-29_11-24-04-6f7ba820e70f1c88c4d37dbf4a94517b.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Amazon ElastiCache for Redis in cluster mode\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache for Memcached\" is incorrect.</p><p>The Memcached engine does not support encryption or high availability.</p><p><strong>INCORRECT:</strong> \"Amazon CloudFront with multiple origins\" is incorrect.</p><p>You cannot configure an Amazon RDS as an origin for Amazon RDS. Also, what would the second origin be anyway? There’s only one database!</p><p><strong>INCORRECT:</strong> \"Amazon DynamoDB Accelerator (DAX)\" is incorrect.</p><p>DynamoDB DAX can be used to increase the performance of DynamoDB tables and offload read requests. It cannot be used in front of an Amazon RDS database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis-RedisCluster.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis-RedisCluster.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis-RedisCluster.html",
      "https://digitalcloud.training/amazon-elasticache/"
    ]
  },
  {
    "id": 48,
    "question": "<p>A developer is running queries on Hive-compatible partitions in Athena using DDL but is facing time out issues. What is the most effective and efficient way to prevent this from continuing to happen?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the ALTER TABLE ADD PARTITION command to update the column names.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the MSCK REPAIR TABLE command to update the metadata in the catalog.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Export the data into DynamoDB to perform queries in a more flexible schema.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Export the data into a JSON document to clean any errors and upload the cleaned data into S3.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Analytics",
    "explanation": "<p>The MSCK REPAIR TABLE command scans Amazon S3 for Hive compatible partitions that were added to the file system after the table was created. It compares the partitions in the table metadata and the partitions in S3. If new partitions are present in the S3 location that you specified when you created the table, it adds those partitions to the metadata and to the Athena table. MSK REPAIR TABLE can work better than DDL if have more than a few thousand partitions and DDL is facing timeout issues.</p><p><strong>CORRECT: </strong>\"Use the MSCK REPAIR TABLE command to update the metadata in the catalog\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the ALTER TABLE ADD PARTITION command to update the column names\" is incorrect. This DDL command is used to add one or more partition columns.</p><p><strong>INCORRECT:</strong> \"Export the data into DynamoDB to perform queries in a more flexible schema\" is incorrect. DynamoDB is a NoSQL table.</p><p><strong>INCORRECT:</strong> \"Export the data into a JSON document to clean any errors and upload the cleaned data into S3\" is incorrect. This is not an efficient or effective way to reduce DDL time out issues.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/msck-repair-table.html\">https://docs.aws.amazon.com/athena/latest/ug/msck-repair-table.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/msck-repair-table.html",
      "https://digitalcloud.training/amazon-athena/"
    ]
  },
  {
    "id": 49,
    "question": "<p>A Developer is deploying an application in a microservices architecture on Amazon ECS. The Developer needs to choose the best task placement strategy to MINIMIZE the number of instances that are used. Which task placement strategy should be used?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>random</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>spread</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>binpack</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>weighted</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>A <em>task placement strategy</em> is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.</p><p>Amazon ECS supports the following task placement strategies:</p><p><strong>binpack</strong> - Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.</p><p><strong>random</strong> - Place tasks randomly.</p><p><strong>spread</strong> - Place tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone. Service tasks are spread based on the tasks from that service. Standalone tasks are spread based on the tasks from the same task group.</p><p>The <strong>binpack</strong> task placement strategy is the most suitable for this scenario as it minimizes the number of instances used which is a requirement for this solution.</p><p><strong>CORRECT: </strong>\"binpack\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"random\" is incorrect as this would assign tasks randomly to EC2 instances which would not result in minimizing the number of instances used.</p><p><strong>INCORRECT:</strong> \"spread\" is incorrect as this would spread the tasks based on a specified value. This is not used for minimizing the number of instances used.</p><p><strong>INCORRECT:</strong> \"weighted\" is incorrect as this is not an ECS task placement strategy. Weighted is associated with Amazon Route 53 routing policies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html",
      "https://digitalcloud.training/amazon-ecs-and-eks/"
    ]
  },
  {
    "id": 50,
    "question": "<p>A Development team is involved with migrating an on-premises MySQL database to Amazon RDS. The database usage is very read-heavy. The Development team wants re-factor the application code to achieve optimum read performance for queries.</p><p>How can this objective be met?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add database retries to the code and vertically scale the Amazon RDS database</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add a connection string to use an Amazon RDS read replica for read queries</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Add a connection string to use a read replica on an Amazon EC2 instance</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon RDS with a multi-AZ deployment</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Amazon RDS uses the MariaDB, MySQL, Oracle, and PostgreSQL DB engines' built-in replication functionality to create a special type of DB instance called a Read Replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the Read Replica.</p><p>You can reduce the load on your source DB instance by routing read queries from your applications to the Read Replica. Using Read Replicas, you can elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.</p><p>In the image below a primary Amazon RDS database server allows reads and writes while a Read Replica can be used for running read-only workloads such as BI/reporting. This reduces the load on the primary database.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-36-41-a8756f8d4c1a46eefffad5b79b56ed16.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-36-41-a8756f8d4c1a46eefffad5b79b56ed16.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>It is necessary to add logic to your code to direct read traffic to the Read Replica and write traffic to the primary database. Therefore, in this scenario the Development team will need to “Add a connection string to use an Amazon RDS read replica for read queries”.</p><p><strong>CORRECT: </strong>\"Add a connection string to use an Amazon RDS read replica for read queries\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add database retries to the code and vertically scale the Amazon RDS database\" is incorrect as this is not a good way to scale reads as you will likely hit a ceiling at some point in terms of cost or instance type. Scaling reads can be better implemented with horizontal scaling using a Read Replica.</p><p><strong>INCORRECT:</strong> \"Use Amazon RDS with a multi-AZ deployment\" is incorrect as this creates a standby copy of the database in another AZ that can be failed over to in a failure scenario. This is used for DR not (at least not primarily) used for scaling performance. It is possible for certain RDS engines to use a multi-AZ standby as a read replica however the requirements in this solution do not warrant this configuration.</p><p><strong>INCORRECT:</strong> \"Add a connection string to use a read replica on an Amazon EC2 instance\" is incorrect as Read Replicas are something you create on Amazon RDS, not on an EC2 instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
      "https://digitalcloud.training/amazon-rds/"
    ]
  },
  {
    "id": 51,
    "question": "<p>A Developer is creating a new web application that will be deployed using AWS Elastic Beanstalk from the AWS Management Console. The Developer is about to create a source bundle which will be uploaded using the console.</p><p>Which of the following are valid requirements for creating the source bundle? (Select TWO.)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Must not include a parent folder or top-level directory.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Must include the cron.yaml file.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Must not exceed 512 MB.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Must include a parent folder or top-level directory.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Must consist of one or more ZIP files.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Compute",
    "explanation": "<p>When you use the AWS Elastic Beanstalk console to deploy a new application or an application version, you'll need to upload a source bundle. Your source bundle must meet the following requirements:</p><p>&nbsp; •&nbsp; Consist of a single ZIP file or WAR file (you can include multiple WAR files inside your ZIP file)</p><p>&nbsp; •&nbsp; Not exceed 512 MB</p><p>&nbsp; •&nbsp; Not include a parent folder or top-level directory (subdirectories are fine)</p><p>If you want to deploy a worker application that processes periodic background tasks, your application source bundle must also include a cron.yaml file, but in other cases it is not required.</p><p><strong>CORRECT: </strong>\"Must not include a parent folder or top-level directory\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Must not exceed 512 MB\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Must include the cron.yaml file\" is incorrect. As mentioned above, this is not required in all cases.</p><p><strong>INCORRECT:</strong> \"Must include a parent folder or top-level directory\" is incorrect. A parent folder or top-level directory must NOT be included.</p><p><strong>INCORRECT:</strong> \"Must consist of one or more ZIP files\" is incorrect. You bundle into a single ZIP or WAR file.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html",
      "https://digitalcloud.training/aws-elastic-beanstalk/"
    ]
  },
  {
    "id": 52,
    "question": "<p>A Development team wants to run their container workloads on Amazon ECS. Each application container needs to share data with another container to collect logs and metrics.</p><p>What should the Development team do to meet these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a single pod specification. Include both containers in the specification. Mount a persistent volume to both containers</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create one task definition. Specify both containers in the definition. Mount a shared volume between those two containers</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create two pod specifications. Make one to include the application container and the other to include the other container. Link the two pods together</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create two task definitions. Make one to include the application container and the other to include the other container. Mount a shared volume between the two tasks</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>Amazon ECS tasks support Docker volumes. To use data volumes, you must specify the volume and mount point configurations in your task definition. Docker volumes are supported for the EC2 launch type only.</p><p>To configure a Docker volume, in the task definition volumes section, define a data volume with name and DockerVolumeConfiguration values. In the containerDefinitions section, define multiple containers with mountPoints values that reference the name of the defined volume and the containerPath value to mount the volume at on the container.</p><p>The containers should both be specified in the same task definition. Therefore, the Development team should create one task definition, specify both containers in the definition and then mount a shared volume between those two containers</p><p><strong>CORRECT: </strong>\"Create one task definition. Specify both containers in the definition. Mount a shared volume between those two containers\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create two pod specifications. Make one to include the application container and the other to include the other container. Link the two pods together\" is incorrect as pods are a concept associated with the Elastic Kubernetes Service (EKS).</p><p><strong>INCORRECT:</strong> \"Create two task definitions. Make one to include the application container and the other to include the other container. Mount a shared volume between the two tasks\" is incorrect as a single task definition should be created with both containers.</p><p><strong>INCORRECT:</strong> \"Create a single pod specification. Include both containers in the specification. Mount a persistent volume to both containers\" is incorrect as pods are a concept associated with the Elastic Kubernetes Service (EKS).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-volumes.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-volumes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-volumes.html",
      "https://digitalcloud.training/amazon-ecs-and-eks/"
    ]
  },
  {
    "id": 53,
    "question": "<p>A company is using an AWS Step Functions state machine. When testing the state machine errors were experienced in the Step Functions task state machine. To troubleshoot the issue a developer requires that the state input be included along with the error message in the state output.</p><p>Which coding practice can preserve both the original input and the error for the state?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use ErrorEquals in a Retry statement to include the original input with the error.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use OutputPath in a Retry statement to include the original input with the error.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use InputPath in a Catch statement to include the original input with the error.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use ResultPath in a Catch statement to include the original input with the error.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>A Step Functions execution receives a JSON text as input and passes that input to the first state in the workflow. Individual states receive JSON as input and usually pass JSON as output to the next state.</p><p>In the Amazon States Language, these fields filter and control the flow of JSON from state to state:</p><p> • InputPath</p><p> • OutputPath</p><p> • ResultPath</p><p> • Parameters</p><p> • ResultSelector</p><p>Use ResultPath to combine a task result with task input, or to select one of these. The path you provide to ResultPath controls what information passes to the output. Use ResultPath in a Catch to include the error with the original input, instead of replacing it. The following code is an example of this tactic:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-30_04-15-24-31b57043970155ff3ce455619308a672.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2022-04-30_04-15-24-31b57043970155ff3ce455619308a672.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Use ResultPath in a Catch statement to include the original input with the error\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use InputPath in a Catch statement to include the original input with the error\" is incorrect.</p><p>You can use InputPath to select a portion of the state input.</p><p><strong>INCORRECT:</strong> \"Use ErrorEquals in a Retry statement to include the original input with the error\" is incorrect.</p><p>A retry is used to attempt to retry the process that caused the error based on the retry policy described by ErrorEquals.</p><p><strong>INCORRECT:</strong> \"Use OutputPath in a Retry statement to include the original input with the error\" is incorrect.</p><p>OutputPath enables you to select a portion of the state output to pass to the next state. This enables you to filter out unwanted information and pass only the portion of JSON that you care about.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/input-output-resultpath.html\">https://docs.aws.amazon.com/step-functions/latest/dg/input-output-resultpath.html</a></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-handling-error-conditions.html\">https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-handling-error-conditions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/step-functions/latest/dg/input-output-resultpath.html",
      "https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-handling-error-conditions.html",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 54,
    "question": "<p>An application uses an Amazon DynamoDB table that is 50 GB in size and provisioned with 10,000 read capacity units (RCUs) per second. The table must be scanned during non-peak hours when normal traffic consumes around 5,000 RCUs. The Developer must scan the whole table in the shortest possible time whilst ensuring the normal workload is not affected.</p><p>How would the Developer optimize this scan cost-effectively?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use sequential scans and set the ConsistentRead parameter to false.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the Parallel Scan API operation and limit the rate.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Increase read capacity units during the scan operation.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use sequential scans and apply a FilterExpression.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>To make the most of the table’s provisioned throughput, the Developer can use the Parallel Scan API operation so that the scan is distributed across the table’s partitions. This will help to optimize the scan to complete in the fastest possible time. However, the Developer will also need to apply rate limiting to ensure that the scan does not affect normal workloads.</p><p><strong>CORRECT: </strong>\"Use the Parallel Scan API operation and limit the rate\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use sequential scans and apply a FilterExpression\" is incorrect. A FilterExpression is a string that contains conditions that DynamoDB applies after the Scan operation, but before the data is returned to you. This will not assist with speeding up the scan or preventing it from affecting normal workloads.</p><p><strong>INCORRECT:</strong> \"Increase read capacity units during the scan operation\" is incorrect. There are already more RCUs provisioned than are needed during the non-peak hours. The key here is to use what is available for cost-effectiveness whilst ensuing normal workloads are not affected.</p><p><strong>INCORRECT:</strong> \"Use sequential scans and set the ConsistentRead parameter to false\" is incorrect. This setting would turn off consistent reads making the scan eventually consistent. This will not satisfy the requirements of the question.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/\">https://aws.amazon.com/blogs/Developer/rate-limited-scans-in-amazon-dynamodb/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 55,
    "question": "<p>A developer is working on an application that must save hundreds of sensitive files. The application needs to encrypt each file using a unique key before storing it.</p><p>What should the developer do to implement this in the application?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a crypto library to generate a unique encryption key for the application, employ the encryption key to secure the data, and store the encrypted data.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the AWS KMS GenerateDataKey API to acquire a data key, use the data key to encrypt the data, and store both the encrypted data key and the data.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Utilize the AWS Key Management Service (KMS) Encrypt API to secure the data, storing both the encrypted data key and the actual data.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Upload the data to an Amazon S3 bucket employing server-side encryption with a key from AWS KMS.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>The AWS KMS GenerateDataKey API returns a plaintext version of the key and a copy of the key encrypted under a KMS key. The application can use the plaintext key to encrypt data, and then discard it from memory as soon as possible to reduce potential exposure.</p><p><strong>CORRECT: </strong>\"Use the AWS KMS GenerateDataKey API to acquire a data key, use the data key to encrypt the data, and store both the encrypted data key and the data\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Utilize the AWS Key Management Service (KMS) Encrypt API to secure the data, storing both the encrypted data key and the actual data\" is incorrect.</p><p>The AWS KMS Encrypt API could be used, but it doesn't generate a unique key for each file. It encrypts data under a specified KMS key, which isn't the requirement here.</p><p><strong>INCORRECT:</strong> \"Use a crypto library to generate a unique encryption key for the application, employ the encryption key to secure the data, and store the encrypted data\" is incorrect.</p><p>Although a cryptography library can be used to generate a key and encrypt the data, it puts the onus of secure key management on the application. It's a less secure and more complex solution compared to using KMS.</p><p><strong>INCORRECT:</strong> \"Upload the data to an Amazon S3 bucket employing server-side encryption with a key from AWS KMS\" is incorrect.</p><p>The requirement is to encrypt data within the application prior to storage. Uploading to an S3 bucket using server-side encryption doesn't fulfill this requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html",
      "https://digitalcloud.training/aws-kms/"
    ]
  },
  {
    "id": 56,
    "question": "<p>An application deployed on AWS Elastic Beanstalk experienced increased error rates during deployments of new application versions, resulting in service degradation for users. The Development team believes that this is because of the reduction in capacity during the deployment steps. The team would like to change the deployment policy configuration of the environment to an option that maintains full capacity during deployment while using the existing instances.</p><p>Which deployment policy will meet these requirements while using the existing instances?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>All at once</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Immutable</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Rolling with additional batch</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Rolling</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.</p><p>All at once:</p><p>· Deploys the new version to all instances simultaneously.</p><p>Rolling:</p><p>· Update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy (downtime for 1 bucket at a time).</p><p>Rolling with additional batch:</p><p>· Like Rolling but launches new instances in a batch ensuring that there is full availability.</p><p>Immutable:</p><p>· Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic to these instances once healthy.</p><p>· Zero downtime.</p><p>Blue / Green deployment:</p><p>· Zero downtime and release facility.</p><p>· Create a new “stage” environment and deploy updates there.</p><p>The rolling with additional batch launches a new batch to ensure capacity is not reduced and then updates the existing instances. Therefore, this is the best option to use for these requirements.</p><p><strong>CORRECT: </strong>“Rolling with additional batch” is the correct answer.</p><p><strong>INCORRECT:</strong> “Rolling” is incorrect as this will only use the existing instances without introducing an extra batch and therefore this will reduce the capacity of the application while the updates are taking place.</p><p><strong>INCORRECT:</strong> “All at once” is incorrect as this will run the updates on all instances at the same time causing a total outage.</p><p><strong>INCORRECT:</strong> “Immutable” is incorrect as this installs the updates on new instances, not existing instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html",
      "https://digitalcloud.training/aws-elastic-beanstalk/"
    ]
  },
  {
    "id": 57,
    "question": "<p>A developer is responsible for a business critical application that uses Amazon DynamoDB as its main data repository. This DynamoDB table holds millions of records and handles high volumes of requests. The developer must implement near-real time processing on the records as soon as they are inserted or modified in the DynamoDB table.</p><p>What's the most efficient way to introduce this capability with MINIMUM modification to the existing application code?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon SQS to queue incoming data and process it using Amazon EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Lambda triggered by DynamoDB Streams to process the documents.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up an Amazon Kinesis Data Stream to process updates from the DynamoDB table.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Modify the application code to add processing logic after each DynamoDB write operation.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>AWS Lambda can be triggered by DynamoDB Streams to automatically process changes to the DynamoDB table, ensuring near-real-time processing without substantial changes to the application code.</p><p><strong>CORRECT: </strong>\"Use AWS Lambda triggered by DynamoDB Streams to process the documents\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon SQS to queue incoming data and process it using Amazon EC2 instances\" is incorrect.</p><p>Amazon SQS would require significant changes to the application code to integrate the queuing and processing mechanism. Moreover, it would not guarantee near-real-time processing.</p><p><strong>INCORRECT:</strong> \"Modify the application code to add processing logic after each DynamoDB write operation\" is incorrect.</p><p>Modifying the application code to include processing logic after each write operation would entail significant changes to the code and may also slow down write operations.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon Kinesis Data Stream to process updates from the DynamoDB table\" is incorrect.</p><p>Amazon Kinesis Data Streams can be used to capture the changes, but consumers are required to perform the processing, and this is not mentioned in the solution. The simplest solution is to use DynamoDB streams with Lambda.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 58,
    "question": "<p>A Developer will be launching several Docker containers on a new Amazon ECS cluster using the EC2 Launch Type. The containers will all run a web service on port 80.</p><p>What is the EASIEST way the Developer can configure the task definition to ensure the web services run correctly and there are no port conflicts on the host instances?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Specify port 80 for the container port and port 0 for the host port</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Leave both the container port and host port configuration blank</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Specify port 80 for the container port and a unique port number for the host port</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Specify a unique port number for the container port and port 80 for the host port</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>Port mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition. The container port is the port number on the container that is bound to the user-specified or automatically assigned host port. The host port is the port number on the container instance to reserve for your container.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-14-21-2e05425ee95cbc437d856fb9b53d6aa2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-14-21-2e05425ee95cbc437d856fb9b53d6aa2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>As we cannot have multiple services bound to the same host port, we need to ensure that each container port mapping uses a different host port. The easiest way to do this is to set the host port number to 0 and ECS will automatically assign an available port. We also need to assign port 80 to the container port so that the web service is able to run.</p><p><strong>CORRECT: </strong>\"Specify port 80 for the container port and port 0 for the host port\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Specify port 80 for the container port and a unique port number for the host port\" is incorrect as this is more difficult to manage as you have to manually assign the port number.</p><p><strong>INCORRECT:</strong> \"Specify a unique port number for the container port and port 80 for the host port\" is incorrect as the web service on the container needs to run on pot 80 and you can only bind one container to port 80 on the host so this would not allow more than one container to work.</p><p><strong>INCORRECT:</strong> \"Leave both the container port and host port configuration blank\" is incorrect as this would mean that ECS would dynamically assign both the container and host port. As the web service must run on port 80 this would not work correctly.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html\">https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html",
      "https://digitalcloud.training/amazon-ecs-and-eks/"
    ]
  },
  {
    "id": 59,
    "question": "<p>A serverless application uses an AWS Lambda function to process Amazon S3 events. The Lambda function executes 20 times per second and takes 20 seconds to complete each execution.</p><p>How many concurrent executions will the Lambda function require?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>20</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>40</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>400</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>5</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.</p><p>To calculate the concurrency requirements for the Lambda function simply multiply the number of executions per second (20) by the time it takes to complete the execution (20).</p><p>Therefore, for this scenario, the calculation is 20 x 20 = 400.</p><p><strong>CORRECT: </strong>\"400\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"5\" is incorrect. Please use the formula above to calculate concurrency requirements.</p><p><strong>INCORRECT:</strong> \"40\" is incorrect. Please use the formula above to calculate concurrency requirements.</p><p><strong>INCORRECT:</strong> \"20\" is incorrect. Please use the formula above to calculate concurrency requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html",
      "https://digitalcloud.training/aws-lambda/"
    ]
  },
  {
    "id": 60,
    "question": "<p>An organization handles data that requires high availability in its relational database. The main headquarters for the organization is in Virginia with smaller offices located in California. The main headquarters uses the data more frequently than the smaller offices. How should the developer configure their databases to meet high availability standards?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in Virginia.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a DynamoDB database with the primary database in Virginia and specify the failover to the DynamoDB replica in another AZ in Virginia.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Athena database with the primary database in Virginia and specify the failover to the Athena replica in another AZ in Virginia.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in California.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Aurora is a relational database that provides high availability by allowing customers to create up to 15 database replications in different Availability Zones. It also allows you to specify which Aurora replica can be promoted to the primary database should the primary database become unavailable. Selecting the AZ that is closest to the main headquarters should not negatively impact the smaller offices but changing the primary database to California could negatively impact the main headquarters.</p><p><strong>CORRECT: </strong>\"Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in Virginia\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in California\" is incorrect. It could create some latency issues for the main headquarters in Virginia.</p><p><strong>INCORRECT:</strong> \"Create a DynamoDB database with the primary database in Virginia and specify the failover to the DynamoDB replica in another AZ in Virginia\" is incorrect. DynamoDB is not a relational database.</p><p><strong>INCORRECT:</strong> \"Create an Athena database with the primary database in Virginia and specify the failover to the Athena replica in another AZ in Virginia\" is incorrect. Athena analyzes data but is not a database service.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html",
      "https://digitalcloud.training/amazon-aurora/"
    ]
  },
  {
    "id": 61,
    "question": "<p>A developer is looking to verify that redirects are performing as expected. What is the most efficient way that the developer can access the web logs and perform an analysis on them?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the logs in EBS and use Athena to run SQL queries.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store the logs in a S3 bucket and use Athena to run SQL queries.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Store the logs in an Instance Store and use Athena to run SQL queries.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the logs in EFS and use Athena to run SQL queries.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Analytics",
    "explanation": "<p>Logs can be stored in an S3 bucket to be retained for review and inspection. Athena is an interactive query service that can work directly with S3 and run ad-hoc SQL queries.</p><p><strong>CORRECT: </strong>\"Store the logs in a S3 bucket and use Athena to run SQL queries\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the logs in EBS and use Athena to run SQL queries\" is incorrect. As explained above log records are stored in S3 and Athena is compatible to perform queries directly in S3 buckets.</p><p><strong>INCORRECT:</strong> \"Store the logs in an Instance Store and use Athena to run SQL queries\" is incorrect. Instance stores are ephemeral and temporarily store data for EC2 instances. They cannot be used with Athena.</p><p><strong>INCORRECT:</strong> \"Store the logs in EFS and use Athena to run SQL queries\" is incorrect. As explained above log records are stored in S3 and Athena is compatible to perform queries directly in S3 buckets.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/what-is.html\">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/what-is.html",
      "https://digitalcloud.training/amazon-athena/"
    ]
  },
  {
    "id": 62,
    "question": "<p>An application on-premises uses Linux servers and a relational database using PostgreSQL. The company will be migrating the application to AWS and require a managed service that will take care of capacity provisioning, load balancing, and auto-scaling.</p><p>Which combination of services should the Developer use? (Select TWO.)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Lambda with CloudWatch Events</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon RDS with PostrgreSQL</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon EC2 with PostgreSQL</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Elastic Beanstalk</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Amazon EC2 with Auto Scaling</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Compute",
    "explanation": "<p>The company require a managed service therefore the Developer should choose to use Elastic Beanstalk for the compute layer and Amazon RDS with the PostgreSQL engine for the database layer.</p><p>AWS Elastic Beanstalk will handle all capacity provisioning, load balancing, and auto-scaling for the web front-end and Amazon RDS provides push-button scaling for the backend.</p><p><strong>CORRECT: </strong>\"AWS Elastic Beanstalk\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Amazon RDS with PostrgreSQL\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 with Auto Scaling\" is incorrect as though these services will be used to provide the automatic scalability required for the solution, they still need to be managed. The questions asks for a managed solution and Elastic Beanstalk will manage this for you. Also, there is no mention of a load balancer so connections cannot be distributed to instances.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 with PostgreSQL\" is incorrect as the question asks for a managed service and therefore the database should be run on Amazon RDS.</p><p><strong>INCORRECT:</strong> \"AWS Lambda with CloudWatch Events\" is incorrect as there is no mention of refactoring application code to run on AWS Lambda.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p><p><a href=\"https://aws.amazon.com/rds/postgresql/\">https://aws.amazon.com/rds/postgresql/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/elasticbeanstalk/",
      "https://aws.amazon.com/rds/postgresql/",
      "https://digitalcloud.training/aws-elastic-beanstalk/",
      "https://digitalcloud.training/amazon-rds/"
    ]
  },
  {
    "id": 63,
    "question": "<p>An e-commerce web application that shares session state on-premises is being migrated to AWS. The application must be fault tolerant, natively highly scalable, and any service interruption should not affect the user experience.</p><p>What is the best option to store the session state?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable session stickiness using elastic load balancers</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store the session state in Amazon ElastiCache</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Store the session state in Amazon S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the session state in Amazon CloudFront</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>There are various ways to manage user sessions including storing those sessions locally to the node responding to the HTTP request or designating a layer in your architecture which can store those sessions in a scalable and robust manner. Common approaches used include utilizing Sticky sessions or using a Distributed Cache for your session management.</p><p>In this scenario, a distributed cache is suitable for storing session state data. ElastiCache can perform this role and with the Redis engine replication is also supported. Therefore, the solution is fault-tolerant and natively highly scalable.</p><p><strong>CORRECT: </strong>\"Store the session state in Amazon ElastiCache\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Store the session state in Amazon CloudFront\" is incorrect as CloudFront is not suitable for storing session state data, it is used for caching content for better global performance.</p><p><strong>INCORRECT:</strong> \"Store the session state in Amazon S3\" is incorrect as though you can store session data in Amazon S3 and replicate the data to another bucket, this would result in a service interruption if the S3 bucket was not accessible.</p><p><strong>INCORRECT:</strong> \"Enable session stickiness using elastic load balancers\" is incorrect as this feature directs sessions from a specific client to a specific EC2 instances. Therefore, if the instance fails the user must be redirected to another EC2 instance and the session state data would be lost.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/caching/session-management/\">https://aws.amazon.com/caching/session-management/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/caching/session-management/",
      "https://digitalcloud.training/amazon-elasticache/"
    ]
  },
  {
    "id": 64,
    "question": "<p>An organization is launching a new service that will use an IoT device. How can secure communication protocols be established over the internet to ensure the security of the IoT devices during the launch?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Certificate Manager (ACM) to provide TLS secured communications to IoT devices and deploy X.509 certificates in the IoT environment.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use IoT Core to provide TLS secured communications to AWS from the IoT devices by issuing X.509 certificates.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use IoT Greengrass to enable TLS secured communications to AWS from the IoT devices by issuing X.509 certificates.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Private Certificate Authority (CA) to provide TLS secured communications to the IoT devices and deploy X.509 certificates in the IoT environment.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>AWS Certificate Manager (ACM) is used to provision X.509 certificates for TLS/SSL secured communications. It can be used to create certificates for use with many AWS services and applications. It is compatible with IoT devices and applications such as IoT Core and IoT Greengrass.</p><p><strong>CORRECT: </strong>\"Use AWS Certificate Manager (ACM) to provide TLS secured communications to IoT devices and deploy X.509 certificates in the IoT environment\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Private Certificate Authority (CA) to provide TLS secured communications to the IoT devices and deploy X.509 certificates in the IoT environment\" is incorrect. AWS Private Certificate Authority cannot be used over the internet.</p><p><strong>INCORRECT:</strong> \"Use IoT Greengrass to enable TLS secured communications to AWS from the IoT devices by issuing X.509 certificates\" is incorrect. AWS IoT Greengrass is not a certificate authority.</p><p><strong>INCORRECT:</strong> \"Use IoT Core to provide TLS secured communications to AWS from the IoT devices by issuing X.509 certificates\" is incorrect. AWS IoT Core is not a certificate authority.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/iot/latest/developerguide/device-certs-your-own.html\">https://docs.aws.amazon.com/iot/latest/developerguide/device-certs-your-own.html</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/iot/latest/developerguide/device-certs-your-own.html"
    ]
  },
  {
    "id": 65,
    "question": "<p>A company has a large Amazon DynamoDB table which they scan periodically so they can analyze several attributes. The scans are consuming a lot of provisioned throughput. What technique can a Developer use to minimize the impact of the scan on the table's provisioned throughput?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set a smaller page size for the scan</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use parallel scans</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Prewarm the table by updating all items</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Define a range key on the table</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>In general, Scan operations are less efficient than other operations in DynamoDB. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set.</p><p>If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation.</p><p>The following diagram illustrates the impact of a sudden spike of capacity unit usage by Query and Scan operations, and its impact on your other requests against the same table.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-28-17-ac3a660ebbeeb2f3ede6dce4e9b8aa76.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-04-19_12-28-17-ac3a660ebbeeb2f3ede6dce4e9b8aa76.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Instead of using a large Scan operation, you can use the following techniques to minimize the impact of a scan on a table's provisioned throughput.</p><p><strong>Reduce page size</strong></p><p>Because a Scan operation reads an entire page (by default, 1 MB), you can reduce the impact of the scan operation by setting a smaller page size. The Scan operation provides a <em>Limit</em> parameter that you can use to set the page size for your request. Each Query or Scan request that has a smaller page size uses fewer read operations and creates a \"pause\" between each request.</p><p><strong>Isolate scan operations</strong></p><p>DynamoDB is designed for easy scalability. As a result, an application can create tables for distinct purposes, possibly even duplicating content across several tables. You want to perform scans on a table that is not taking \"mission-critical\" traffic. Some applications handle this load by rotating traffic hourly between two tables—one for critical traffic, and one for bookkeeping. Other applications can do this by performing every write on two tables: a \"mission-critical\" table, and a \"shadow\" table.</p><p>Therefore, the best option to reduce the impact of the scan on the table's provisioned throughput is to set a smaller page size for the scan.</p><p><strong>CORRECT: </strong>\"Set a smaller page size for the scan\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use parallel scans\" is incorrect as this will return results faster but place more burden on the table’s provisioned throughput.</p><p><strong>INCORRECT:</strong> \"Define a range key on the table\" is incorrect. A range key is a composite key that includes the hash key and another attribute. This is of limited use in this scenario as the table is being scanned to analyze multiple attributes.</p><p><strong>INCORRECT:</strong> \"Prewarm the table by updating all items\" is incorrect as updating all items would incur significant costs in terms of provisioned throughput and would not be advantageous.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  }
]