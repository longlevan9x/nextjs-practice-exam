[
  {
    "id": 1,
    "question": "A company wants to migrate an e-commerce application to AWS Cloud. They need a solution to deploy the application on Amazon EC2 instances with high availability. As a Solutions Architect, what action would you suggest for this requirement?",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use an Auto Scaling Group to distribute user traffic to EC2 instances across multiple Availability Zones. Allocate an Elastic IP address to recover instance failure.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use an Application Load Balancer to distribute incoming traffic to EC2 instances across multiple Availability Zones. Configure an Auto Scaling group to recover instance failure.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Network Load Balancer to distribute incoming traffic to EC2 instances across multiple Availability Zones. Allocate an Elastic IP address to recover instance failure.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use an Auto Saling Group to distribute incoming traffic to EC2 instances across multiple Availability Zones. Use an Application Load Balancer to recover instance failure.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nUse an Application Load Balancer to distribute incoming traffic to EC2 instances across multiple Availability Zones. Configure an Auto Scaling group to recover instance failure.\n\nElastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing scales your load balancer as your incoming traffic changes over time. It can automatically scale to the vast majority of workloads.\n\nAn Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. An Auto Scaling group starts by launching enough instances to meet its desired capacity. It maintains this number of instances by performing periodic health checks on the instances in the group. The Auto Scaling group continues to maintain a fixed number of instances even if an instance becomes unhealthy. If an instance becomes unhealthy, the group terminates the unhealthy instance and launches another instance to replace it.\n\n\n\n\nIncorrect Options:\n\nUse an Auto Scaling Group to distribute user traffic to EC2 instances across multiple Availability Zones. Allocate an Elastic IP address to recover instance failure.\n\nUse Network Load Balancer to distribute incoming traffic to EC2 instances across multiple Availability Zones. Allocate an Elastic IP address to recover instance failure.\n\nUse an Auto Saling Group to distribute incoming traffic to EC2 instances across multiple Availability Zones. Use an Application Load Balancer to recover instance failure.\n\nAll of the options are incorrect.\n\n\n\n\nAn Elastic IP address is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is allocated to your AWS account, and is yours until you release it. By using an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. Alternatively, you can specify the Elastic IP address in a DNS record for your domain, so that your domain points to your instance.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html",
    "correctAnswerExplanation": {
      "answer": "Use an Application Load Balancer to distribute incoming traffic to EC2 instances across multiple Availability Zones. Configure an Auto Scaling group to recover instance failure.",
      "explanation": "Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing scales your load balancer as your incoming traffic changes over time. It can automatically scale to the vast majority of workloads."
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Use an Auto Scaling Group to distribute user traffic to EC2 instances across multiple Availability Zones. Allocate an Elastic IP address to recover instance failure.",
        "explanation": "<strong>Use Network Load Balancer to distribute incoming traffic to EC2 instances across multiple Availability Zones. Allocate an Elastic IP address to recover instance failure.</strong>"
      },
      {
        "answer": "Use an Auto Saling Group to distribute incoming traffic to EC2 instances across multiple Availability Zones. Use an Application Load Balancer to recover instance failure.",
        "explanation": "All of the options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html"
    ]
  },
  {
    "id": 2,
    "question": "A financial company has some confidential documents and wants to store them in AWS Cloud. They assigned you to do it, and security is their main concern.\n\nWhich of the following features ensures the data security to store these documents securely? (Select two)",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Public Data Set Encryption",
        "correct": false
      },
      {
        "id": 2,
        "answer": "KMS On-Premises Data Encryption",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Active Directory On-Premises Encryption",
        "correct": false
      },
      {
        "id": 4,
        "answer": "S3 Client-Side Encryption",
        "correct": true
      },
      {
        "id": 5,
        "answer": "S3 Server-Side Encryption",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nS3 Server-Side Encryption & S3 Client-Side Encryption\n\nData protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers). You can protect data in transit using Secure Socket Layer/Transport Layer Security (SSL/TLS) or client-side encryption. You have the following options for protecting data at rest in Amazon S3:\n\nServer-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.\n\nClient-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.\n\nFor security purposes, you can protect your data in AWS; At rest and in transit. If you want to store data to EBS volume, you need to enable EBS encryption and for Amazon S3, you need to enable saver-side and client-side encryption.\n\n\n\n\nIncorrect Options:\n\nActive Directory On-Premises Encryption\n\nKMS On-Premises Data Encryption\n\nPublic Data Set Encryption\n\nAll above are incorrect. All options have been used to confuse you.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html",
    "correctAnswerExplanation": {
      "answer": "S3 Server-Side Encryption</strong> &amp; <strong>S3 Client-Side Encryption",
      "explanation": "Data protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers). You can protect data in transit using Secure Socket Layer/Transport Layer Security (SSL/TLS) or client-side encryption. You have the following options for protecting data at rest in Amazon S3:"
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Server-Side Encryption</strong> – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p></li><li><p><strong>Client-Side Encryption</strong> – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p></li></ul><p>For security purposes, you can protect your data in AWS; At rest and in transit. If you want to store data to EBS volume, you need to enable EBS encryption and for Amazon S3, you need to enable saver-side and client-side encryption.</p><p><br></p><p>Incorrect Options:</p><p><strong>Active Directory On-Premises Encryption",
        "explanation": "<strong>KMS On-Premises Data Encryption</strong>"
      },
      {
        "answer": "Public Data Set Encryption",
        "explanation": "All above are incorrect. All options have been used to confuse you."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html"
    ]
  },
  {
    "id": 3,
    "question": "A startup wants to deploy a dockerized application to AWS Cloud. The CTO requests you to provide a pricing model for Elastic Container Service (ECS) with EC2 launch type and Fargate launch type.\n\nWhich of the following statements are correct regarding the pricing model for these launch types? (Select two)",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "ECS with EC2 launch type is charged based on EC2 instance and EBS volume used.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Both ECS launch types are charged based on the vCPU and memory requested by the application.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "ECS with the Fargate launch type is charged based on the vCPU and memory resources that the application requests.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Both ECS launch types are charged based on the EC2 instance and the EBS volume used.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "ECS with Fargate launch type is charged based on how many hours the application has been running.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Options:\n\nECS with EC2 launch type is charged based on EC2 instance and EBS volume used.\n\nECS with the Fargate launch type is charged based on the vCPU and memory resources that the application requests.\n\nAmazon Elastic Container Service (ECS) is a highly scalable, high performance container management service that supports Docker containers and allows you to easily run applications on a managed cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances. Amazon ECS eliminates the need for you to install, operate, and scale your own cluster management infrastructure.\n\nECS With AWS Fargate, you pay for the amount of vCPU and memory resources that your containerized application requests. vCPU and memory resources are calculated from the time your container images are pulled until the Amazon ECS Task terminates, rounded up to the nearest second. A minimum charge of one minute applies.\n\nECS With EC2 launch type, there is no additional charge for running application. You pay for AWS resources (such as Amazon EC2 instances or Amazon EBS volumes) you create to store and run your application. You only pay for what you use, as you use it; there are no minimum fees and no upfront commitments.\n\n\n\n\nIncorrect Options:\n\nBoth ECS launch types are charged based on the vCPU and memory requested by the application.\n\nBoth ECS launch types are charged based on the EC2 instance and the EBS volume used.\n\nECS with Fargate launch type is charged based on how many hours the application has been running.\n\nAll of the above statements are incorrect.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/ecs/pricing",
    "correctAnswerExplanation": {
      "answer": "ECS with EC2 launch type is charged based on EC2 instance and EBS volume used.",
      "explanation": "<strong>ECS with the Fargate launch type is charged based on the vCPU and memory resources that the application requests.</strong>"
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "ECS With AWS Fargate</strong>, you pay for the amount of vCPU and memory resources that your containerized application requests. vCPU and memory resources are calculated from the time your container images are pulled until the Amazon ECS Task terminates, rounded up to the nearest second. A minimum charge of one minute applies.</p><p><strong>ECS With EC2 launch type</strong>, there is no additional charge for running application. You pay for AWS resources (such as Amazon EC2 instances or Amazon EBS volumes) you create to store and run your application. You only pay for what you use, as you use it; there are no minimum fees and no upfront commitments.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-06-14_19-05-36-22925f0a7571e206ab0ecbcc70e763eb.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-06-14_19-05-36-22925f0a7571e206ab0ecbcc70e763eb.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span></p><p><br></p><p>Incorrect Options:</p><p><strong>Both ECS launch types are charged based on the vCPU and memory requested by the application.",
        "explanation": "<strong>Both ECS launch types are charged based on the EC2 instance and the EBS volume used.</strong>"
      },
      {
        "answer": "ECS with Fargate launch type is charged based on how many hours the application has been running.",
        "explanation": "All of the above statements are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/ecs/pricing"
    ]
  },
  {
    "id": 4,
    "question": "A WordPress application is hosted on multiple EC2 instances with an Auto Scaling group and uses Amazon Aurora for its database. Users' uploaded document files are stored on one of the attached EBS volumes. Currently, they noticed that the system's performance is relatively slow. The CTO wants to improve the architecture of the system.\n\nWhat will you do to implement a scalable, high-availability shared file system?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Elastic File System (Amazon EFS)",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use ElastiCache to provide high performance with low latency",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an S3 bucket that can share with all EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Upgrading EBS volume to higher IOPS SSD volume",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nUse Amazon Elastic File System (Amazon EFS)\n\nAmazon Elastic File System is a cloud storage service provided by Amazon Web Services designed to provide scalable, elastic and encrypted file storage for use with both AWS cloud services and on-premises resources. An Amazon EFS file system is excellent as a managed network file system that can be shared across different Amazon EC2 instances. Amazon EFS works like NAS devices and performs well for big data analytics, media processing workflows, and content management.\n\nYou can access your Amazon EFS file system concurrently from multiple NFS clients, so applications that scale beyond a single connection can access a file system. Amazon EC2 and other AWS compute instances running in multiple Availability Zones within the same AWS Region can access the file system, so that many users can access and share a common data source.\n\n\n\n\nIncorrect Options:\n\nUse ElastiCache to provide high performance with low latency - ElastiCache is an in-memory data store that should be used for data caching. So in our case, this option is incorrect.\n\nUpgrading EBS volume to higher IOPS SSD volume - A single EBS volume can attach to an EC2 instance. Therefore if you increase the IOPS, it can't be improved the latency to communicate to/from another instance. So this is incorrect.\n\nCreate an S3 bucket that can share with all EC2 instances - You can probably use Amazon S3 with WordPress. To do this, you need some configuration and install an additional plugin in WordPress. So this is not the best option in our case.\n\nhttps://aws.amazon.com/blogs/compute/deploying-a-highly-available-wordpress-site-on-amazon-lightsail-part-2-using-amazon-s3-with-wordpress-to-securely-deliver-media-files\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/efs\n\nhttps://docs.aws.amazon.com/efs/latest/ug/how-it-works.html",
    "correctAnswerExplanation": {
      "answer": "Use Amazon Elastic File System (Amazon EFS)",
      "explanation": "Amazon Elastic File System is a cloud storage service provided by Amazon Web Services designed to provide scalable, elastic and encrypted file storage for use with both AWS cloud services and on-premises resources. An Amazon EFS file system is excellent as a managed network file system that can be shared across different Amazon EC2 instances. Amazon EFS works like NAS devices and performs well for big data analytics, media processing workflows, and content management."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/compute/deploying-a-highly-available-wordpress-site-on-amazon-lightsail-part-2-using-amazon-s3-with-wordpress-to-securely-deliver-media-files",
      "https://aws.amazon.com/efs",
      "https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html"
    ]
  },
  {
    "id": 5,
    "question": "An organization has a payroll management system that is running on an Amazon EC2 instance. The application is intended to be used by internal users only, and no one will be able to access it from the public internet. The organization uses a static IP that the application must connect to with a secure encrypted connection\n\nWhich configuration will allow only this IP to access the application?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an Inbound Rule in Security Group with Protocol - TCP, Port range - 443, Source - 0.0.0.0/0",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Inbound Rule in Security Group with Protocol - TCP, Port range - 443, Source - x.x.x.x/32",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an Inbound Rule in Security Group with Protocol - TCP, Port range - 443, Source - x.x.x.x/0",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Inbound Rule in Security Group with Protocol - UDP, Port range - 443, Source - x.x.x.x/32",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nCreate an Inbound Rule in Security Group with Protocol - TCP, Port range - 443, Source - x.x.x.x/32\n\nA security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. When you launch an instance, you can specify one or more security groups. If you don't specify a security group, Amazon EC2 uses the default security group. You can add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time. New and modified rules are automatically applied to all instances that are associated with the security group. When Amazon EC2 decides whether to allow traffic to reach an instance, it evaluates all of the rules from all of the security groups that are associated with the instance.\n\nIn our case, we need to only allow a specific IP address, therefore we should use x.x.x.x/32. the /32 provides the ability to apply the rule to only one IP and /0 denotes the whole network. The HTTPS protocol uses TCP and 443 port.\n\n\n\n\nIncorrect Options:\n\nCreate an Inbound Rule in Security Group with Protocol - UDP, Port range - 443, Source - x.x.x.x/32\n\nCreate an Inbound Rule in Security Group with Protocol - TCP, Port range - 443, Source - 0.0.0.0/0\n\nCreate an Inbound Rule in Security Group with Protocol - TCP, Port range - 443, Source - x.x.x.x/0\n\nAll above are incorrect.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html",
    "correctAnswerExplanation": {
      "answer": "Create an Inbound Rule in Security Group with Protocol - TCP, Port range - 443, Source - x.x.x.x/32",
      "explanation": "A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. When you launch an instance, you can specify one or more security groups. If you don't specify a security group, Amazon EC2 uses the default security group. You can add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time. New and modified rules are automatically applied to all instances that are associated with the security group. When Amazon EC2 decides whether to allow traffic to reach an instance, it evaluates all of the rules from all of the security groups that are associated with the instance."
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Create an Inbound Rule in Security Group with Protocol - UDP, Port range - 443, Source - x.x.x.x/32",
        "explanation": "<strong>Create an Inbound Rule in Security Group with Protocol - TCP, Port range - 443, Source - 0.0.0.0/0</strong>"
      },
      {
        "answer": "Create an Inbound Rule in Security Group with Protocol - TCP, Port range - 443, Source - x.x.x.x/0",
        "explanation": "All above are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html"
    ]
  },
  {
    "id": 6,
    "question": "A large data analytical company has an application that processes thousands of data sets to transform its machine learning model. The application requires a high-performance parallel file system to process datasets concurrently. It also needs a cost-effective storage solution to store infrequently used datasets.\n\nWhich of the following Amazon storage services should be used for this requirement? (Select two)",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon FSx For Lustre",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Amazon S3",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Amazon EBS Provisioned IOPS SSD",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon Elastic File System",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Amazon FSx For Windows File Server",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nAmazon S3 - Amazon S3 is object storage built to store and retrieve any amount of data from anywhere. It’s a simple storage service that offers industry leading durability, availability, performance, security, and virtually unlimited scalability at very low costs.\n\nAmazon S3 offers a range of storage classes designed for different use cases. For example, you can store mission-critical production data in S3 Standard for frequent access, save costs by storing infrequently accessed data in S3 Standard-IA or S3 One Zone-IA, and archive data at the lowest costs in S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive.\n\nAmazon FSx For Lustre - Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance, scalable storage for compute workloads. Amazon FSx for Lustre delivers the performance to satisfy a wide variety of high-performance workloads. The Lustre file system is optimized for data processing, with sub-millisecond latencies and throughput that scales to hundreds of gigabytes per second.\n\nUse Amazon FSx for Lustre for workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, financial modeling, genome sequencing, and electronic design automation (EDA). Amazon FSx for Lustre provides a parallel file system. In parallel file systems, data is stored across multiple network file servers to maximize performance and reduce bottlenecks, and each server has multiple disks.\n\nIn our case, we need to use Amazon FSx For Lustre for a high-performance parallel file system and Amazon S3 for storing rarely used datasets\n\n\n\n\n\n\n\nIncorrect Options:\n\nAmazon FSx For Windows File Server - Amazon FSx For Windows File Server does not support a parallel file system like Lustre.\n\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.\n\nAmazon Elastic File System - Although EFS supports parallel access to data, it cannot provide the high-performance capabilities required for machine learning workloads.\n\nAmazon Elastic File System (Amazon EFS) is a simple, serverless, set-and-forget elastic file system that makes it easy to set up, scale, and cost-optimize file storage in AWS. Amazon EFS file systems can automatically scale from gigabytes to petabytes of data without needing to provision storage.\n\nAmazon EBS Provisioned IOPS SSD - Amazon Elastic Block Store (Amazon EBS) provides block level storage volumes for use with EC2 instances. EBS's SSD provision provides a high-performance storage service that requires frequent use. Also provides HDD provisions for less frequent data access and it is not ideal for data archiving. Compared to FSx and S3, Amazon EBS is more expensive. So this option is incorrect in our case.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/fsx\n\nhttps://aws.amazon.com/s3/storage-classes\n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-overview/storage-services.html",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/fsx",
      "https://aws.amazon.com/s3/storage-classes",
      "https://docs.aws.amazon.com/whitepapers/latest/aws-overview/storage-services.html"
    ]
  },
  {
    "id": 7,
    "question": "A company has some sensitive data stored in an Amazon S3 bucket. They want to follow security guidelines for objects stored in Amazon S3 and protect data from accidental deletion. Which of the following should be recommended for this requirement? (Select two)",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Enable MFA delete on the bucket",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use IAM policy that has no delete permission",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure S3 bucket policy to block all unauthorized access",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Enable versioning on the bucket",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Create an event trigger on deletion that invokes an SNS notification",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nEnable versioning on the bucket - Versioning allows the ability to keep multiple variants of an object in the Amazon S3 bucket. when an object is modified or deleted, the bucket will keep the previous version of the modified object so you can retrieve or restore it when you need them.\n\nEnable MFA delete on the bucket - To enable MFA deletion, you can add an extra layer of protection to confirm the deletion of an object. When a user attempts to delete an object, the bucket requires secondary authentication with MFA code, which will help prevent accidental bucket deletion.\n\n\n\n\nIncorrect Options:\n\nCreate an event trigger on deletion that invokes an SNS notification - An event trigger on deletion will notify you when an object is deleted. It does not meet the goal of protection object deletion by mistake. The object cannot be restored once an object is deleted.\n\nUse IAM policy that has no delete permission - You can use the IAM policy to prevent modification of the S3 bucket. which means the user can not delete any objects. This is not the objective. We should only prevent accidental deletion. So this is an incorrect option.\n\nConfigure S3 bucket policy to block all unauthorized access - The bucket policy does not help protect you from the accidental deletion of objects. This allows you to manage the permissions of Amazon S3 resources, which allow or deny the action requested by a principal (a user or role).\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html"
    ]
  },
  {
    "id": 8,
    "question": "An organization operates a distributed system and is placed on multiple Amazon EC2 instances that process large financial data 24x7. A solution architect is responsible for ensuring the high availability of applications and provides a solution for monitoring memory and disk usage metrics for all instances.\n\nWhich solution should be provided to meet the criteria?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS CloudWatch agent for all Amazon EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon CloudTrail event log for all Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use default CloudWatch configuration for all Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Inspector agent for all Amazon EC2 instances.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nUse AWS CloudWatch agent for all Amazon EC2 instances\n\nCloudWatch Agent is a software package that autonomously and continuously runs on your servers. Using CloudWatch Agent, we can collect metrics and logs from Amazon Elastic Compute Cloud (Amazon EC2), hybrid, and on-premises servers running both Linux and Windows. CloudWatch Agent provides access to more system level and in-guest metrics, in addition to host metrics already provided by Amazon EC2. it collects matric from Amazon EC2 for monitoring CPU utilization, network utilization, disk performance, and disk reads/writes. The agent also lets us collect, aggregate, and summarize metrics and logs from containerized applications and microservices.\n\n\n\n\nIncorrect Options:\n\nUse default CloudWatch configuration for all Amazon EC2 instances - The default configuration of CloudWatch does not collect metrics for memory and disk usage. So this option is incorrect.\n\nUse Amazon Inspector agent for all Amazon EC2 instances - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure. It doesn’t collect custom data for memory and disk usage metrics. So this option is incorrect.\n\n\n\n\nUse Amazon CloudTrail event log for all Amazon EC2 instances. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you track changes made to your AWS resources and troubleshoot operational issues. It doesn’t collect custom data for memory and disk usage metrics. So this option is incorrect.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html",
    "correctAnswerExplanation": {
      "answer": "Use AWS CloudWatch agent for all Amazon EC2 instances",
      "explanation": "CloudWatch Agent is a software package that autonomously and continuously runs on your servers. Using CloudWatch Agent, we can collect metrics and logs from Amazon Elastic Compute Cloud (Amazon EC2), hybrid, and on-premises servers running both Linux and Windows. CloudWatch Agent provides access to more system level and in-guest metrics, in addition to host metrics already provided by Amazon EC2. it collects matric from Amazon EC2 for monitoring CPU utilization, network utilization, disk performance, and disk reads/writes. The agent also lets us collect, aggregate, and summarize metrics and logs from containerized applications and microservices."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html"
    ]
  },
  {
    "id": 9,
    "question": "A company wants to publish a static website on Amazon S3 that will provide a product listing to its customers. Most audiences are located in the United States, Canada, and Europe. The company needs a cost-effectively way to reduce latency in these regions.\n\nAs a Solution Architect, which solution would you suggest that will be most affordable for this need?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon CloudFront distribution that uses origins in targeted regions.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS WAF to restrict all countries except targeted regions.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon CloudFront distribution and set price class to use only targeted regions.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an Amazon CloudFront distribution with the price class for all Edge Locations.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Options:\n\nCreate an Amazon CloudFront distribution and set price class to use only targeted regions\n\nCloudFront edge locations are grouped into geographic regions, and we’ve grouped regions into price classes as shows in the following table. You choose a price class when you create or update a CloudFront distribution.\n\nBy default, CloudFront responds to requests based only on performance. Objects are served from the edge location that has the lowest latency for the viewer. If you’re willing to accept potentially higher latency for viewers in some geographic regions in return for lower cost, you can choose a price class that doesn’t include all geographic regions. Some viewers, especially those in geographic regions that are not in your price class, might see higher latency than if your content was served from all CloudFront edge locations. For example, if you choose Price Class 100, viewers in India might experience higher latency than if you choose Price Class 200.\n\n\n\n\nIncorrect Options:\n\nCreate an Amazon CloudFront distribution with the price class for all Edge Locations - It will be more expensive because it will cache content in all Edge locations around the world.\n\nCreate an Amazon CloudFront distribution that uses origins in targeted regions - This is incorrect. You should use a price class to limit content cache to reduce costs.\n\nUse AWS WAF to restrict all countries except targeted regions - The AWS WAF can be used for geo-restriction but it paid service. It cannot help you to reduce latency.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html",
    "correctAnswerExplanation": {
      "answer": "Create an Amazon CloudFront distribution and set price class to use only targeted regions",
      "explanation": "CloudFront edge locations are grouped into geographic regions, and we’ve grouped regions into price classes as shows in the following table. You choose a price class when you create or update a CloudFront distribution."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html"
    ]
  },
  {
    "id": 10,
    "question": "An analytics company wants to transfer 70 TB of data to AWS. This data is stored in a client-specific format in the client's on-premises data center. Before sending data, they need to transform these data into an industry-standard format. Which of the following is the most affordable option with high performance?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Order 1 Snowball Edge Storage Optimized device",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Order 2 Snowball Edge Compute Optimized devices",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Setup AWS direct connect to transfer data and use Amazon EMR for data processing",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Order 1 Snowmobile to transfer data and use Amazon EMR for data processing",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Options:\n\nOrder 2 Snowball Edge Compute Optimized devices - Snowball Edge Compute Optimized provides powerful computing resources for use cases such as machine learning, full motion video analysis, analytics, and local computing stacks. These capabilities include 52 vCPUs, 208 GiB of memory, and an optional NVIDIA Tesla V100 GPU. For storage, the device provides 42 TB usable HDD capacity for Amazon S3 compatible object storage or EBS-compatible block volumes, as well as 7.68 TB of usable NVMe SSD capacity for EBS-compatible block volumes.\n\nSince they have 70 TB of data, so they need to order 2 Snowball Edge Compute Optimized devices\n\n\n\n\nIncorrect Options:\n\nOrder 1 Snowball Edge Storage Optimized device - Snowball Edge Storage Optimized devices are well suited for large-scale data migrations and recurring transfer workflows, as well as local computing with higher capacity needs. Snowball Edge Storage Optimized provides 80 TB of HDD capacity for block volumes and Amazon S3-compatible object storage, and 1 TB of SATA SSD for block volumes. For computing resources, the device provides 40 vCPUs, and 80 GiB of memory to support Amazon EC2 sbe1 instances (equivalent to C5). So this is not suitable for data processing with high-performance walkthrough.\n\nOrder 1 Snowmobile to transfer data and use Amazon EMR for data processing - Each Snowmobile comes with up to 100PB of storage capacity. To migrate large datasets of 10PB or more in a single location, you should use Snowmobile. For datasets less than 10PB or distributed in multiple locations, you should use Snowball.\n\nSetup AWS direct connect to transfer data and use Amazon EMR for data processing - AWS Direct Connect links your internal network to an AWS Direct Connect location over a standard Ethernet fiber-optic cable. One end of the cable is connected to your router, the other to an AWS Direct Connect router. You need significant investment and it takes at least a month to set up the AWS Direct Connect. So it is not suitable for our case.\n\nAmazon EMR is the industry-leading cloud big data platform for data processing, interactive analysis, and machine learning using open source frameworks such as Apache Spark, Apache Hive, and Presto. With EMR you can run petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 1.7x faster than standard Apache Spark.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/snowball/features\n\nhttps://docs.aws.amazon.com/snowball/latest/developer-guide/specifications.html#specs-compute-optimized",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/snowball/features",
      "https://docs.aws.amazon.com/snowball/latest/developer-guide/specifications.html#specs-compute-optimized"
    ]
  },
  {
    "id": 11,
    "question": "An e-commerce company is planning to launch its application in AWS Cloud. The system requires a database that can scale globally without any touch. The product model is unorganized, so the database should handle frequent schema changes and have zero tolerance for downtime or performance issues when changing schemas. It must be provided high performance with low latency response time.\n\nWhich solution should be used to meet the requirement?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Redshift with RA3 instances",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon Aurora with Read Replicas",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon RDS in Multi-AZ Deployments",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon DynamoDB with DAX",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nUse Amazon DynamoDB with DAX\n\nAmazon DynamoDB is a fully managed, serverless, key-value NoSQL database designed to run high-performance applications at any scale. DynamoDB is schemeless, so it is the best choice when the schema changes frequently. It supports high-traffic, extreme-scaled events and can handle millions of queries per second.\n\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second.\n\nDynamoDB + DAX takes performance to the next level with response times in microseconds for millions of requests per second for read-heavy workloads. With DAX, your applications remain fast and responsive, even when a popular event or news story drives unprecedented request volumes your way.\n\n\n\n\nIncorrect Options:\n\nUse Amazon RDS in Multi-AZ Deployments & Use Amazon Aurora with Read Replicas - Both are incorrect because both of them are a type of relational database. these are not suitable for unstructured data whose schema changes frequently.\n\nUse Amazon Redshift with RA3 instances - Amazon Redshift is a fully managed, scalable cloud data warehouse that accelerates your time to insights with fast, easy, and secure analytics at scale. It is not suitable for unstructured data whose schema changes frequently.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/dynamodb\n\nhttps://aws.amazon.com/dynamodb/dax",
    "correctAnswerExplanation": {
      "answer": "Use Amazon DynamoDB with DAX",
      "explanation": "Amazon DynamoDB is a fully managed, serverless, key-value NoSQL database designed to run high-performance applications at any scale. DynamoDB is schemeless, so it is the best choice when the schema changes frequently. It supports high-traffic, extreme-scaled events and can handle millions of queries per second."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/dynamodb",
      "https://aws.amazon.com/dynamodb/dax"
    ]
  },
  {
    "id": 12,
    "question": "Some essential documents of a business are stored in an Amazon S3 bucket. The owner wants to protect the data from accidental deletion. Which combination of actions should be used to ensure data availability? (Select two)",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Enable versioning on the S3 bucket.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Enable MFA Delete on the S3 bucket.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a lifecycle policy for all objects.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Enable client-side encryption on the S3 bucket.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Create a bucket policy on the S3 bucket.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nEnable versioning on the S3 bucket\n\nEnable MFA Delete on the S3 bucket\n\nVersioning in Amazon S3 is a means of keeping multiple variants of an object in the same bucket. You can use the S3 Versioning feature to preserve, retrieve, and restore every version of every object stored in your buckets. With versioning, you can recover more easily from both unintended user actions and application failures. After versioning is enabled for a bucket, if Amazon S3 receives multiple write requests for the same object simultaneously, it stores all of those objects.\n\nVersioning-enabled buckets can help you recover objects from accidental deletion or overwrite. For example, if you delete an object, Amazon S3 inserts a delete marker instead of removing the object permanently. The delete marker becomes the current object version. If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version.\n\nWhen working with S3 Versioning in Amazon S3 buckets, you can optionally add another layer of security by configuring a bucket to enable MFA (multi-factor authentication) delete. When you do this, the bucket owner must include two forms of authentication in any request to delete a version or change the versioning state of the bucket.\n\nMFA delete thus provides added security if, for example, your security credentials are compromised. MFA delete can help prevent accidental bucket deletions by requiring the user who initiates the delete action to prove physical possession of an MFA device with an MFA code and adding an extra layer of friction and security to the delete action.\n\n\n\n\nIncorrect Options:\n\nCreate a bucket policy on the S3 bucket.\n\nCreate a lifecycle policy for all objects.\n\nEnable client-side encryption on the S3 bucket.\n\nAll the above options are incorrect.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html",
    "correctAnswerExplanation": {
      "answer": "Enable versioning on the S3 bucket",
      "explanation": "<strong>Enable MFA Delete on the S3 bucket</strong>"
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a bucket policy on the S3 bucket.",
        "explanation": "<strong>Create a lifecycle policy for all objects.</strong>"
      },
      {
        "answer": "Enable client-side encryption on the S3 bucket.",
        "explanation": "All the above options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html"
    ]
  },
  {
    "id": 13,
    "question": "A business uses Amazon DynamoDB to store its data. When a new developer joins, the team assigns full access to the developer for DynamoDB. While developing a new feature, he accidentally deleted a few tables from the production environment. After this incident, the CTO does not want such incidents to happen in the future.\n\nAs a Solutions Architect, what effective ways should you recommend for this requirement?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Only Grant full permission to root user to access database.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Remove full database access for all IAM users.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set a permission boundary to control the maximum permissions to the IAM principals.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Only head of the IT department should have full permission to access the database.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nSet a permission boundary to control the maximum permissions to the IAM principals\n\nA permissions boundary is an advanced feature that allows you to limit the maximum permissions that a principal can have. As the IAM administrator, you can define one or more permissions boundaries using managed policies and allow your employee to create a principal with this boundary. The employee can then attach a permissions policy to this principal. However, the effective permissions of the principal are the intersection of the permissions boundary and permissions policy. As a result, the new principal cannot exceed the boundary that you defined.\n\n\n\n\nIncorrect Options:\n\nRemove full database access for all IAM users - In practical life, removing full permission from all users can not be a solution. Some users will need full permission to perform administrative tasks.\n\nOnly Grant full permission to root user to access database - As a best practice, you should not use root user to perform administrative procedures regularly.\n\nOnly head of the IT department should have full permission to access the database - This option can be good for small teams or startups. But it can be difficult for a large team to manage all the administrative tasks.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries",
    "correctAnswerExplanation": {
      "answer": "Set a permission boundary to control the maximum permissions to the IAM principals",
      "explanation": "A permissions boundary is an advanced feature that allows you to limit the maximum permissions that a principal can have. As the IAM administrator, you can define one or more permissions boundaries using managed policies and allow your employee to create a principal with this boundary. The employee can then attach a permissions policy to this principal. However, the effective permissions of the principal are the intersection of the permissions boundary and permissions policy. As a result, the new principal cannot exceed the boundary that you defined."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries"
    ]
  },
  {
    "id": 14,
    "question": "A company runs an image-based application on an Amazon EC2 instance that uses Amazon S3 to store images updated by its users. At users' request, some of these images are encrypted in S3 using AWS-KMS. The application manages its own Customer Master Key (CMK) for these encryptions. Incidentally, a junior DevOps engineer deleted the CMK an hour ago, and all the images became unrecoverable. They need solutions to overcome this situation.\n\nWhat steps should you recommend to solve this problem?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Contact AWS Support to recover the CMK key, which may take up to 48 hours.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "The key is now in \"Pending Delete\" state. To recover the KMS key, you can cancel the key deletion.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Once deleted, you will not be able to recover the key.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "You can retrieve the key from the AWS-KMS service by logging in using the AWS root user.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nThe key is now in \"Pending Delete\" state. To recover the KMS key, you can cancel the key deletion.\n\nDeleting an AWS KMS key (KMS key) from AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. It deletes the key material and all metadata associated with the KMS key and is irreversible. After a KMS key is deleted, you can no longer decrypt the data that was encrypted under that KMS key, which means that data becomes unrecoverable.\n\nAWS offers a waiting period to delete permanently a key. You can set the waiting period to 7–30 days. The default waiting period is 30 days. In this period, AWS marks the deletions key as \"Pending Deletion\". To recover the KMS key, you can cancel key deletion before the waiting period ends. After the waiting period ends you cannot cancel key deletion, and AWS KMS deletes the KMS key.\n\n\n\n\nIncorrect Options:\n\nOnce deleted, you will not be able to recover the key - When you delete a key, AWS marks the temporary delete key as \"Pending Delete\". So you can get an opportunity to recover the key during the waiting period. After the waiting period ends, you cannot cancel key deletion, and AWS KMS permanently deletes the KMS key.\n\nContact AWS Support to recover the CMK key, which may take up to 48 hours - This is incorrect. AWS support doesn’t store any backup for KMS Keys.\n\nYou can retrieve the key from the AWS-KMS service by logging in using the AWS root user - This is incorrect. Once permanently deleted, root users will not be able to recover the key.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html",
    "correctAnswerExplanation": {
      "answer": "The key is now in \"Pending Delete\" state. To recover the KMS key, you can cancel the key deletion.",
      "explanation": "Deleting an AWS KMS key (KMS key) from AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. It deletes the key material and all metadata associated with the KMS key and is irreversible. After a KMS key is deleted, you can no longer decrypt the data that was encrypted under that KMS key, which means that data becomes unrecoverable."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html"
    ]
  },
  {
    "id": 15,
    "question": "A financial company operates an application hosted in an on-premises data center. The application frequently processes financial data every second and stores the data in an Oracle database. Due to database failure, they want to move their business to the AWS cloud to improve the performance of their applications. A Solution Architect is requested to migrate, and the database must remain available in the case of server failure.\n\nWhich is the most suitable solution for this requirement?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Launch an Oracle RAC in Amazon RDS with Recovery Manager (RMAN) enabled.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Oracle database on RDS with a multi-AZ deployment.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Migrate Oracle database to Amazon Aurora with Multi-AZ deployments.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Launch an Oracle RAC in Amazon RDS with Multi-AZ deployments.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nCreate an Oracle database on RDS with a multi-AZ deployment\n\nAmazon RDS for Oracle is a fully managed commercial database that makes it easy to set up, operate, and scale Oracle deployments in the cloud.\n\nAmazon RDS for Oracle makes it easy to use replication to enhance availability and reliability for production workloads. Using the Multi-AZ deployment option you can run mission critical workloads with high availability and built-in automated fail-over from your primary database to a synchronously replicated secondary database in case of a failure. In an Amazon RDS Multi-AZ deployment, Amazon RDS automatically creates a primary database (DB) instance and synchronously replicates the data to an instance in a different AZ. When it detects a failure, Amazon RDS automatically fails over to a standby instance without manual intervention.\n\n\n\n\nIncorrect Options:\n\nLaunch an Oracle RAC in Amazon RDS with Multi-AZ deployments - Oracle Real Application Clusters (RAC) is not currently supported by Amazon RDS.\n\nLaunch an Oracle RAC in Amazon RDS with Recovery Manager (RMAN) enabled - Amazon RDS for Oracle supports Oracle native backup tools like Oracle Recovery Manager (RMAN) to backup you database. You can restore the RMAN database backups in the same or different Region on Amazon Elastic Compute Cloud (Amazon EC2) or on premises for disaster recovery purposes, or to demonstrate backup compliance to auditors and regulators to meet business and regulatory requirements.\n\nMigrate Oracle database to Amazon Aurora with Multi-AZ deployments - Amazon Aurora does not support Oracle database. It designed for unparalleled high performance and availability at global scale with full MySQL and PostgreSQL compatibility.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/oracle\n\nhttps://aws.amazon.com/rds/features/multi-az",
    "correctAnswerExplanation": {
      "answer": "Create an Oracle database on RDS with a multi-AZ deployment",
      "explanation": "Amazon RDS for Oracle is a fully managed commercial database that makes it easy to set up, operate, and scale Oracle deployments in the cloud."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/rds/oracle",
      "https://aws.amazon.com/rds/features/multi-az"
    ]
  },
  {
    "id": 16,
    "question": "Over a petabyte of data is stored in an on-premises data center and managed by Microsoft's Distributed File System (DFS). The organization wants to transform into a hybrid cloud solution and process data analytics workloads on AWS.\n\nAs a Cloud Solution Architect, what AWS service can you recommend for this migration process?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon S3 for Windows File Server",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon FSx for Lustre",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon FSx for Windows File Server",
        "correct": true
      },
      {
        "id": 4,
        "answer": "AWS Managed Microsoft AD",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nAmazon FSx for Windows File Server - Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.\n\nYou can run up to thousands of Amazon FSx for Windows File Server file systems in your account, with each file system having up to 64 TB of data. To unify your data from multiple file systems into one common folder structure, Amazon FSx supports the use of Microsoft’s Distributed File System (DFS) to organize shares into a single folder structure up to hundreds of PB in size.\n\n\n\n\nIncorrect Options:\n\nAmazon FSx for Lustre - Amazon FSx for Lustre makes it easy and cost effective to launch, run, and scale the world’s most popular high-performance file system. You can use Amazon FSx for Lustre for workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, financial modeling, genome sequencing, and electronic design automation (EDA). it doesn't support Microsoft’s Distributed File System (DFS).\n\nAWS Managed Microsoft AD - AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft Active Directory (AD), enables your directory-aware workloads and AWS resources to use managed Active Directory (AD) in AWS. AWS Managed Microsoft AD is built on actual Microsoft AD and does not require you to synchronize or replicate data from your existing Active Directory to the cloud.\n\nAmazon S3 for Windows File Server - Amazon S3 is object storage built to store and retrieve any amount of data from anywhere. It’s a simple storage service that offers industry leading durability, availability, performance, security, and virtually unlimited scalability at very low costs. Amazon does not support Windows File Server.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/fsx/windows",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/fsx/windows"
    ]
  },
  {
    "id": 17,
    "question": "An application uses an Amazon EC2 Linux instance and stores data on an Amazon EBS volume. The owner requested a solution architect to provide a solution that would increase the resiliency of the application in case of failure.\n\nWhat should the Solution Architect do to meet the requirements?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an Application Load Balancer with Auto Scaling groups across multiple AZs. Store data on Amazon EFS and mount a target on each instance.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create Auto Scaling groups for multiple EC2 instances and store data in Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Launch the application on multiple EC2 instances in multiple AZs and attach an EBS volume to each EC2 instance.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Application Load Balancer across multiple AZs and Mount an instance store on each EC2 instance.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nCreate an Application Load Balancer with Auto Scaling groups across multiple AZs. Store data on Amazon EFS and mount a target on each instance.\n\nApplication Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Application Load Balancing scales your load balancer as your incoming traffic changes over time.\n\nAWS Auto Scaling group monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. AWS Auto Scaling can help you optimize your utilization and cost efficiencies when consuming AWS services so you only pay for the resources you actually need. When demand drops, AWS Auto Scaling will automatically remove any excess resource capacity so you avoid overspending. AWS Auto Scaling is free to use and allows you to optimize the costs of your AWS environment.\n\nYou can access your Amazon EFS file system concurrently from multiple NFS clients, so applications that scale beyond a single connection can access a file system. Amazon EC2 and other AWS compute instances running in multiple Availability Zones within the same AWS Region can access the file system at the same time, so that many users can access and share a common data source.\n\n\n\n\nIncorrect Options:\n\nLaunch the application on multiple EC2 instances in multiple AZs and attach an EBS volume to each EC2 instance\n\nCreate an Application Load Balancer across multiple AZs and Mount an instance store on each EC2 instance.\n\nCreate Auto Scaling groups for multiple EC2 instances and store data in Amazon S3 bucket.\n\nAll the above are incorrect.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/elasticloadbalancing/application-load-balancer\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html\n\nhttps://docs.aws.amazon.com/efs/latest/ug/whatisefs.html",
    "correctAnswerExplanation": {
      "answer": "Create an Application Load Balancer with Auto Scaling groups across multiple AZs. Store data on Amazon EFS and mount a target on each instance.",
      "explanation": "Application Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Application Load Balancing scales your load balancer as your incoming traffic changes over time."
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Launch the application on multiple EC2 instances in multiple AZs and attach an EBS volume to each EC2 instance",
        "explanation": "<strong>Create an Application Load Balancer across multiple AZs and Mount an instance store on each EC2 instance.</strong>"
      },
      {
        "answer": "Create Auto Scaling groups for multiple EC2 instances and store data in Amazon S3 bucket.",
        "explanation": "All the above are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html",
      "https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html"
    ]
  },
  {
    "id": 18,
    "question": "To design a hybrid cloud architecture, a business wants to establish a connection between their on-premises data center and AWS infrastructure. They are looking for durable storage to back up their data and documents stored on-premises and a local cache that provides low latency access. These backup data should be stored for the next decade at affordable prices. This data server must be accessible via the Message Block (SMB) protocol and retrieved within minutes.\n\nWhat is the most cost-effective way to meet the requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Order AWS Snowball to transfer backup data to S3 and set up a lifecycle policy to move data into Glacier storage class.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS Direct Connect to connect with AWS and upload data to EBS volume, and set up a lifecycle policy to move data into Glacier storage class.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use File Gateway to connect AWS and set up a lifecycle policy to move data into Glacier storage class.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use tape gateway to connect AWS and set up a lifecycle policy to move data into Glacier storage class.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nUse File Gateway to connect AWS and set up a lifecycle policy to move data into Glacier storage class\n\nFile Gateway supports a file interface into Amazon Simple Storage Service (Amazon S3) and combines a service and a virtual software appliance. By using this combination, you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB). The software appliance, or gateway, is deployed into your on-premises environment as a virtual machine (VM) running on VMware ESXi, Microsoft Hyper-V, or Linux Kernel-based Virtual Machine (KVM) hypervisor.\n\nThe gateway provides access to objects in S3 as files or file share mount points. With a S3 File Gateway, you can do the following:\n\nYou can store and retrieve files directly using the NFS version 3 or 4.1 protocol.\n\nYou can store and retrieve files directly using the SMB file system version, 2 and 3 protocol.\n\nYou can access your data directly in Amazon S3 from any AWS Cloud application or service.\n\nYou can manage your S3 data using lifecycle policies, cross-region replication, and versioning. You can think of a S3 File Gateway as a file system mount on Amazon S3.\n\nYou can configure lifecycle policy of your objects so that they are stored cost-effectively. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:\n\nTransition actions – These actions define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after creating them, or archive objects to the S3 Glacier Flexible Retrieval storage class one year after creating them.\n\nExpiration actions – These actions define when objects expire. Amazon S3 deletes expired objects on your behalf.\n\nAmazon S3 supports a waterfall model for transitioning between storage classes, as shown in the following diagram.\n\n\n\n\nIncorrect Options:\n\nUse tape gateway to connect AWS and set up a lifecycle policy to move data into Glacier storage class - Although tape gateways can be used at an affordable price to back up data to the Amazon S3 Glacier, but it does not support instantly retrieving data in minutes and maintains a local cache with low latency access. So this option is incorrect in our case.\n\nUse AWS Direct Connect to connect with AWS and upload data to EBS volume, and set up a lifecycle policy to move data into Glacier storage class - AWS Direct Connect with AWS Storage Gateway can create a dedicated network connection between your on-premises file gateway and the AWS for high-throughput workload needs. In our case, we need a storage solution as low as possible, but the EBS volume is more expensive and not as durable as the S3. So this option is incorrect.\n\nOrder AWS Snowball to transfer backup data to S3 and set up a lifecycle policy to move data into Glacier storage class - AWS Snowball is suitable for migrating data from an on-premises data center to AWS. It is not a good approach for hybrid cloud architecture.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/storagegateway/index.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html",
    "correctAnswerExplanation": {
      "answer": "Use File Gateway to connect AWS and set up a lifecycle policy to move data into Glacier storage class",
      "explanation": "File Gateway supports a file interface into Amazon Simple Storage Service (Amazon S3) and combines a service and a virtual software appliance. By using this combination, you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB). The software appliance, or gateway, is deployed into your on-premises environment as a virtual machine (VM) running on VMware ESXi, Microsoft Hyper-V, or Linux Kernel-based Virtual Machine (KVM) hypervisor."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/storagegateway/index.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html"
    ]
  },
  {
    "id": 19,
    "question": "A financial company has some sensitive data stored in an on-premises data center and wants to use some AWS services to manipulate these data. So they need a dedicated, highly secure connection with low latency and high throughput performance.\n\nWhich architectural solution would you recommend for this requirement?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Direct Connect with VPN",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use site-to-site VPN",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS Direct Connect",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Transit Gateway With VPN",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nUse AWS Direct Connect with VPN - With AWS Direct Connect + VPN, you can combine AWS Direct Connect dedicated network connections with the Amazon VPC VPN. AWS Direct Connect public VIF establishes a dedicated network connection between your network to public AWS resources, such as an Amazon virtual private gateway IPsec endpoint. The following figure illustrates this option.\n\nThis solution combines the benefits of the end-to-end secure IPSec connection with low latency and increased bandwidth of the AWS Direct Connect to provide a more consistent network experience than internet-based VPN connections.\n\n\n\n\nIncorrect Options:\n\nUse site-to-site VPN - An AWS Site-to-Site VPN connection connects your VPC to your datacenter. Amazon supports Internet Protocol security (IPsec) VPN connections. Data transferred between your VPC and datacenter routes over an encrypted VPN connection to help maintain the confidentiality and integrity of data in transit. However, Site-to-site VPN doesn't provide dedicated connection with low latency and high throughput\n\nUse AWS Direct Connect - AWS Direct Connect provides a private network connection between your facilities and AWS. But it does not provide an encrypted connection.\n\nUse AWS Transit Gateway With VPN - AWS Transit Gateway + VPN provides the option of creating an IPsec VPN connection between your remote network and the Transit Gateway over the internet. But it does not provide low latency and high throughput connection between a data center and AWS Cloud.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html"
    ]
  },
  {
    "id": 20,
    "question": "A gaming application running on AWS uses ECS, Cloudfront, Lambda, and DynamodB services. DynamoDB is used to store players' data, and CloudFront is used to distribute static resources to users worldwide. Many users complain that data retrieval is very slow during peak hours. As a Solution Architect, you are asked to provide a solution to improve game performance that reduces database response times from milliseconds to microseconds.\n\nWhich solution should you suggest that will solve the problem?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Migrate data to Amazon RDS",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use DynamoDB Auto Scaling",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon DynamoDB Accelerator (DAX)",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nUse Amazon DynamoDB Accelerator (DAX) - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second.\n\nDAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.\n\nDynamoDB + DAX takes performance to the next level with response times in microseconds for millions of requests per second for read-heavy workloads. With DAX, your applications remain fast and responsive, even when a popular event or news story drives unprecedented request volumes your way. No tuning required.\n\n\n\n\nIncorrect Options:\n\nUse Amazon ElastiCache - Although you can use Amazon ElastiCache to cache data, you need to do more work to do this and you are responsible for managing the cache data. On other hand, DynamoDB Accelerator (DAX) is a fully managed service, so AWS will take care of everything for you and DAX is more beneficial to improving DynamoDB performance from milliseconds to microseconds.\n\nMigrate data to Amazon RDS - Amazon RDS is a good choice for storing structured data. But in our case, RDS is not the best option for storing data like gaming applications where data is updated in real-time.\n\nUse DynamoDB Auto Scaling - DynamoDB Auto Scaling can automatically scale the capabilities of the database. But it doesn't help you improve your performance.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/dynamodb/dax",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/dynamodb/dax"
    ]
  },
  {
    "id": 21,
    "question": "Which of the following statement is true about the RDS Read replicas?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Supports asynchronous replication and spans at least two Availability Zones within a single region.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Supports synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Supports asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Supports synchronous replication and spans at least two Availability Zones within a single region.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nSupports asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region.\n\nRead Replica is a feature of Amazon RDS that increases the performance and durability of an RDS database instance. It elastically scales beyond the capacity constraints of read-heavy database workloads. You can create multiple replicas for a DB instance and serve high-volume application traffic from multiple copies of your data, thus increasing read-throughput. It can improve low latency to replicate cross-origin near users.\n\nAmazon RDS creates a separate DB instance for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines by creating a snapshot of the source DB instance. Then it uses the engines' native asynchronous replication to update the read replica when the source DB changes.\n\n\n\n\nIncorrect Options:\n\nSupports asynchronous replication and spans at least two Availability Zones within a single region.\n\nSupports synchronous replication and spans at least two Availability Zones within a single region.\n\nSupports synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region.\n\nAll of the above options are incorrect.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/read-replicas",
    "correctAnswerExplanation": {
      "answer": "Supports asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region.",
      "explanation": "Read Replica is a feature of Amazon RDS that increases the performance and durability of an RDS database instance. It elastically scales beyond the capacity constraints of read-heavy database workloads. You can create multiple replicas for a DB instance and serve high-volume application traffic from multiple copies of your data, thus increasing read-throughput. It can improve low latency to replicate cross-origin near users."
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Supports asynchronous replication and spans at least two Availability Zones within a single region.",
        "explanation": "<strong>Supports synchronous replication and spans at least two Availability Zones within a single region.</strong>"
      },
      {
        "answer": "Supports synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region.",
        "explanation": "All of the above options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/features/read-replicas"
    ]
  },
  {
    "id": 22,
    "question": "A business is hosted in an on-premises data center and the transactions workload is increasing day by day, which is becoming unpredictable. So the owner wants to migrate to AWS and needs a relational database that automatically scales up during the maximum load of the application and scales down when the peak is over.\n\nWhich of the following is the most cost-effective solution for this requirement?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Launch an Amazon Aurora DB cluster with minimum and maximum capacity.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Launch Amazon Aurora provisioned DB cluster with memory optimized instance.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Launch Amazon ElastiCache for Redis to cache data.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Launch a DynamoDB with DynamoDB Accelerator (DAX).",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nLaunch an Amazon Aurora DB cluster with minimum and maximum capacity\n\nYou can use Aurora Serverless, an on-demand, autoscaling configuration for Amazon Aurora to scale database compute resources based on application demand. It enables you to run your database in the cloud without worrying about database capacity management. You can specify the desired database capacity range and your database will scale based on your application’s needs.\n\nAn Amazon Aurora DB cluster consists of one or more DB instances and a cluster volume that manages the data for those DB instances. An Aurora cluster volume is a virtual database storage volume that spans multiple Availability Zones, with each Availability Zone having a copy of the DB cluster data. Two types of DB instances make up an Aurora DB cluster:\n\nPrimary DB instance – Supports read and write operations, and performs all of the data modifications to the cluster volume. Each Aurora DB cluster has one primary DB instance.\n\nAurora Replica – Connects to the same storage volume as the primary DB instance and supports only read operations. Each Aurora DB cluster can have up to 15 Aurora Replicas in addition to the primary DB instance. Maintain high availability by locating Aurora Replicas in separate Availability Zones. Aurora automatically fails over to an Aurora Replica in case the primary DB instance becomes unavailable. You can specify the failover priority for Aurora Replicas. Aurora Replicas can also offload read workloads from the primary DB instance.\n\n\n\n\n\n\n\nIncorrect Options:\n\nLaunch Amazon Aurora provisioned DB cluster with memory optimized instance - Aurora provisioned DB cluster is a non-Serverless DB cluster for Aurora, which you manually manage capacity. Aurora Serverless clusters and provisioned clusters both have the same high-capacity, distributed, and highly available storage volume. You can adjust capacity manually based on the expected workload. So it is not suitable for our case.\n\nLaunch Amazon ElastiCache for Redis to cache data - Amazon ElastiCache for Redis is a fast in-memory data store that provides sub-millisecond latency. You can use Amazon ElastiCache for Redis to cache application data. But it is not a good choice to store long-term data. So it is incorrect.\n\nLaunch a DynamoDB with DynamoDB Accelerator (DAX) - Amazon DynamoDB is a fully managed proprietary NoSQL database service that supports key–value and document data structures. So it is incorrect because we need a relational database.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html",
    "correctAnswerExplanation": {
      "answer": "Launch an Amazon Aurora DB cluster with minimum and maximum capacity",
      "explanation": "You can use Aurora Serverless, an on-demand, autoscaling configuration for Amazon Aurora to scale database compute resources based on application demand. It enables you to run your database in the cloud without worrying about database capacity management. You can specify the desired database capacity range and your database will scale based on your application’s needs."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html"
    ]
  },
  {
    "id": 23,
    "question": "A company is developing an application that uses a self-managed database deployed on an EC2 instance. The application must support a high-performance database workload. The company wants an EBS volume with a maximum Provisioned IOPS (PIOPS) of 256,000, which can be attached with multiple EC2 instances in the same AZ.\n\nWhich of the following options is correct for this criterion? (Select two)",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Multi-Attach enabled io1/io2 volumes with Nitro-based EC2 instances",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Multi-Attach enabled gp2 volumes with Nitro-based EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "answer": "io2 volumes on EC2 instances without Nitro System",
        "correct": false
      },
      {
        "id": 4,
        "answer": "io2 Block Express volumes with Nitro-based EC2 instances",
        "correct": true
      },
      {
        "id": 5,
        "answer": "gp3 volumes with Nitro-based EC2 instances",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nio2 Block Express volumes with Nitro-based EC2 instances\n\nio2 Block Express volume is the next generation of Amazon EBS storage server architecture. It has been built for the purpose of meeting the performance requirements of the most demanding I/O intensive applications that run on Nitro-based Amazon EC2 instances. You can provision volume with Provisioned IOPS up to 256,000, with an IOPS:GiB ratio of 1,000:1. Maximum IOPS can be provisioned with volumes 256 GiB in size and larger (1,000 IOPS × 256 GiB = 256,000 IOPS)\n\nMulti-Attach enabled io1/io2 volumes with Nitro-based EC2 instances - Amazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD (io1 or io2) volume to multiple instances that are in the same Availability Zone. You can attach multiple Multi-Attach enabled volumes to an instance or set of instances. Each instance to which the volume is attached has full read and write permission to the shared volume. Multi-Attach makes it easier for you to achieve higher application availability in clustered Linux applications that manage concurrent write operations.\n\n\n\n\nIncorrect Options:\n\nio2 volumes on EC2 instances without Nitro System - Provisioned IOPS SSD volumes can range in size from 4 GiB to 16 TiB and you can provision from 100 IOPS up to 64,000 IOPS per volume. You can achieve up to 64,000 IOPS only on Instances built on the Nitro System. On other instance families you can achieve performance up to 32,000 IOPS. The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1 for io1 volumes, and 500:1 for io2 volumes.\n\ngp3 volumes with Nitro-based EC2 instances - gp3 volume doesn't support Nitro-based EC2 instances. These volumes deliver a consistent baseline rate of 3,000 IOPS and 125 MiB/s, included with the price of storage.\n\nMulti-Attach enabled gp2 volumes with Nitro-based EC2 instances - gp3 does not support multi-attachments with multiple EC2 instances.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html",
    "correctAnswerExplanation": {
      "answer": "io2 Block Express volumes with Nitro-based EC2 instances",
      "explanation": "io2 Block Express volume is the next generation of Amazon EBS storage server architecture. It has been built for the purpose of meeting the performance requirements of the most demanding I/O intensive applications that run on Nitro-based Amazon EC2 instances. You can provision volume with Provisioned IOPS up to 256,000, with an IOPS:GiB ratio of 1,000:1. Maximum IOPS can be provisioned with volumes 256 GiB in size and larger (1,000 IOPS × 256 GiB = 256,000 IOPS)"
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html"
    ]
  },
  {
    "id": 24,
    "question": "Which of the following AWS services should be used to federate a company's workforce in AWS accounts and business applications? (Select two)",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "AWS Single Sign-On (SSO)",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Multi-Factor Authentication",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Access keys",
        "correct": false
      },
      {
        "id": 4,
        "answer": "AWS Identity and Access Management (IAM)",
        "correct": true
      },
      {
        "id": 5,
        "answer": "AWS Organizations",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nAWS Single Sign-On (SSO)\n\nAWS Identity and Access Management (IAM)\n\nIdentity federation is a system of trust between two parties for the purpose of authenticating users and conveying information needed to authorize their access to resources. In this system, an identity provider (IdP) is responsible for user authentication, and a service provider (SP), such as a service or an application, controls access to resources. By administrative agreement and configuration, the SP trusts the IdP to authenticate users and relies on the information provided by the IdP about them. After authenticating a user, the IdP sends the SP a message, called an assertion, containing the user's sign-in name and other attributes that the SP needs to establish a session with the user and to determine the scope of resource access that the SP should grant. Federation is a common approach to building access control systems which manage users centrally within a central IdP and govern their access to multiple applications and services acting as SPs.\n\nYou can use two AWS services to federate your workforce into AWS accounts and business applications: AWS Single Sign-On (SSO) or AWS Identity and Access Management (IAM). AWS SSO is a great choice to help you define federated access permissions for your users based on their group memberships in a single centralized directory. If you use multiple directories, or want to manage the permissions based on user attributes, consider AWS IAM as your design alternative.\n\nAWS SSO makes it easy to centrally manage federated access to multiple AWS accounts and business applications and provide users with single sign-on access to all their assigned accounts and applications from one place. You can use AWS SSO for identities in the AWS SSO’s user directory, your existing corporate directory, or external IdP.\n\nYou can enable federated access to AWS accounts using AWS Identity and Access Management (IAM). The flexibility of the AWS IAM allows you to enable a separate SAML 2.0 or an Open ID Connect (OIDC) IdP for each AWS account and use federated user attributes for access control. With AWS IAM, you can pass user attributes, such as cost center, title, or locale, from your IdPs to AWS, and implement fine-grained access permissions based on these attributes. AWS IAM helps you define permissions once, and then grant, revoke or modify AWS access by simply changing the attributes in the IdP. You can apply the same federated access policy to multiple AWS accounts by implementing reusable custom managed IAM policies.\n\n\n\n\nIncorrect Options:\n\nAWS Access keys - Access keys are long-term credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK).\n\nAWS Organizations - AWS Organizations helps you centrally govern your environment as you scale your workloads on AWS. Whether you are a growing startup or a large enterprise, Organizations helps you to programmatically create new accounts and allocate resources, simplify billing by setting up a single payment method for all of your accounts, create groups of accounts to organize your workflows, and apply policies to these groups for governance. In addition, AWS Organizations is integrated with other AWS services so you can define central configurations, security mechanisms, and resource sharing across accounts in your organization.\n\nMulti-Factor Authentication - Multi-factor authentication (MFA) in AWS is a simple best practice that adds an extra layer of protection on top of your user name and password. With MFA enabled, when a user signs in to an AWS Management Console, they will be prompted for their user name and password (the first factor—what they know), as well as for an authentication code from their AWS MFA device (the second factor—what they have).\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/identity/federation",
    "correctAnswerExplanation": {
      "answer": "AWS Single Sign-On (SSO)",
      "explanation": "<strong>AWS Identity and Access Management (IAM)</strong>"
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/identity/federation"
    ]
  },
  {
    "id": 25,
    "question": "A media-based web application is placed on multiple EC2 instances with Auto Scaling Groups behind an application load balancer. The CTO noticed that the application received a lot of requests from various illegal external sources, and the requested IP addresses were constantly changing. He requests a Solution Architect to solve performance issues and needs to implement a solution that would block illegal requests with minimal impact on traffic.\n\nWhat steps should the Solution Architect take to meet this requirement?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Add a regular rule to a web ACL and attach it to the application load balancer.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a custom rule for security groups to block illegitimate requests.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Add a rate-based rule to a web ACL and attach it to the application load balancer.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create a custom network ACL and connect to the subnet of the application load balancer.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nAdd a rate-based rule to a web ACL and attach it to the application load balancer.\n\nA rate-based rule tracks the rate of requests for each originating IP address, and triggers the rule action on IPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span. You can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests. By default, AWS WAF aggregates requests based on the IP address from the web request origin, but you can configure the rule to use an IP address from an HTTP header, like X-Forwarded-For, instead.\n\nAWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting.\n\nAWS WAF is tightly integrated with Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync – services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end users. This means security doesn’t come at the expense of performance. Blocked requests are stopped before they reach your web servers. When you use AWS WAF on regional services, such as Application Load Balancer, Amazon API Gateway, and AWS AppSync, your rules run in region and can be used to protect internet-facing resources as well as internal resources.\n\n\n\n\nIncorrect Options:\n\nAdd a regular rule to a web ACL and attach it to the application load balancer - Regular rules use only conditions to target specific requests. You can use this exactly matches the statement defined in the rule. For example, the requests come from 192.x.x.xx.\n\nCreate a custom rule for security groups to block illegitimate requests - The security group can be used for only allowing incoming traffic and you cannot deny any traffic using this.\n\nCreate a custom network ACL and connect to the subnet of the application load balancer - You can use network ACL to block incoming traffic, but you can't define the request limit. So for the dynamic IP, you can't use this option.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\n\nhttps://aws.amazon.com/waf",
    "correctAnswerExplanation": {
      "answer": "Add a rate-based rule to a web ACL and attach it to the application load balancer.",
      "explanation": "A rate-based rule tracks the rate of requests for each originating IP address, and triggers the rule action on IPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span. You can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests. By default, AWS WAF aggregates requests based on the IP address from the web request origin, but you can configure the rule to use an IP address from an HTTP header, like X-Forwarded-For, instead."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html",
      "https://aws.amazon.com/waf"
    ]
  },
  {
    "id": 26,
    "question": "A sports company has developed a notification system for its users using the Amazon SNS service, powered by lambda functions. Typically, the system can handle 100 requests per second, and at peak times, the rate increases to 1500 requests per second. Suddenly, they noticed that some notifications were not being delivered to users.\n\nWhat solution should you provide for the best possible solution to this problem?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "The development team needs to provide an Auto Scaling group to scale the Amazon SNS service.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "The Lambda function has exceeded the account concurrency quota. So the team needs to contact AWS support to increase the account limit.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "The development team needs to provide more servers to run the Lambda function.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "The Amazon SNS service has exceeded the concurrency quota limit So the team needs to contact AWS support to increase the limit.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nThe Lambda function has exceeded the account concurrency quota. So the team needs to contact AWS support to increase the account limit.\n\nAWS Lambda currently supports 1000 concurrent executions per AWS account per region. If your Amazon SNS message deliveries to AWS Lambda contribute to crossing these concurrency quotas, your Amazon SNS message deliveries will be throttled. If you need more quota limits, you can request AWS Support to increase quota limits.\n\nLambda sets quotas for the amount of compute and storage resources that you can use to run and store functions. The following quotas apply per AWS Region and can be increased.\n\n\n\n\nIncorrect Options:\n\nThe development team needs to provide more servers to run the Lambda function - AWS Lambda is a serverless service. You don’t need to provision a server for Lambda and it will automatically scale when needed.\n\nThe Amazon SNS service has exceeded the concurrency quota limit So the team needs to contact AWS support to increase the limit - Amazon SNS is a fully managed service and it scales dynamically when needed.\n\nThe development team needs to provide an Auto Scaling group to scale the Amazon SNS service - Amazon SNS is a serverless service. You don’t need to provision a server.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html\n\nhttps://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html",
    "correctAnswerExplanation": {
      "answer": "The Lambda function has exceeded the account concurrency quota. So the team needs to contact AWS support to increase the account limit.",
      "explanation": "AWS Lambda currently supports 1000 concurrent executions per AWS account per region. If your Amazon SNS message deliveries to AWS Lambda contribute to crossing these concurrency quotas, your Amazon SNS message deliveries will be throttled. If you need more quota limits, you can request AWS Support to increase quota limits."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html",
      "https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html"
    ]
  },
  {
    "id": 27,
    "question": "A company has an application that requires multiple EC2 instances and can be used to perform specialized random I/O tasks. All instances must have access to a dataset that is replicated across instances. According to the application architecture, a replacement instance must continue the dataset process if one instance goes down.\n\nWhich storage option is the most cost-effective and best resource-productive solution for this architecture?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use EFS mount points",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use EBS volume",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use S3 bucket",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Instance Store",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nUse Instance Store - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.\n\nAn instance store consists of one or more instance store volumes exposed as block devices. The size of an instance store as well as the number of devices available varies by instance type.\n\nThe virtual devices for instance store volumes are ephemeral[0-23]. Instance types that support one instance store volume have ephemeral0. Instance types that support two instance store volumes have ephemeral0 and ephemeral1, and so on.\n\nThe instance type determines the size of the instance store available and the type of hardware used for the instance store volumes. Instance store volumes are included as part of the instance's usage cost. You must specify the instance store volumes that you'd like to use when you launch the instance (except for NVMe instance store volumes, which are available by default). Then format and mount the instance store volumes before using them. You can't make an instance store volume available after you launch the instance.\n\n\n\n\nIncorrect Options:\n\nUse EBS volume - EBS volume needs General Purpose or IOPS based storage type for provisioning. That will increase costs.\n\nUse EFS mount points - This means the additional resources need to be arranged and is not the best resource productive solution as we are looking for.\n\nUse S3 bucket - This doesn't perform random I/O operations. It's just used to confuse you.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html"
    ]
  },
  {
    "id": 28,
    "question": "A photo-sharing application allows users to upload photos and edit them online. The application offers free & paid subscriptions to users. The paying user's photos should be processed first. All photos are uploaded to an Amazon S3 bucket which uses an event notification to send notifications to Amazon SQS.\n\nAs a solution architect, which method should you use to meet this need?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use on SQS FIFO queue and assign a higher priority for the paid subscribers to process first.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use a separate SQS FIFO queue for paid and free. Set paid queue to use long polling and free queue for short polling.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use one SQS standard queue and use a batch job for the paid subscribers and short polling for the free users.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use a separate SQS Standard queue for the paid subscriptions and configure instances to prioritize polling for paid queue first.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nUse a separate SQS Standard queue for the paid subscriptions and configure instances to prioritize polling for paid queue first.\n\nYou should use a separate queue to define prioritize of work. The application must handle the prioritized queue over others.\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.\n\nSQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.\n\n\n\n\nIncorrect Options:\n\nUse on SQS FIFO queue and assign a higher priority for the paid subscribers to process first - FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and it does not prioritize messages within the queue.\n\nUse one SQS standard queue and use a batch job for the paid subscribers and short polling for the free users - A batch job can increase performance but it does not order based on priority.\n\nUse a separate SQS FIFO queue for paid and free. Set paid queue to use long polling and free queue for short polling - Short polling and long polling are not provided the prioritization of a queue. These are used to control the waiting time for the process to complete.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html",
    "correctAnswerExplanation": {
      "answer": "Use a separate SQS Standard queue for the paid subscriptions and configure instances to prioritize polling for paid queue first.",
      "explanation": "You should use a separate queue to define prioritize of work. The application must handle the prioritized queue over others."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html"
    ]
  },
  {
    "id": 29,
    "question": "A company wants to develop a microservices application based on serverless architecture. The backend services may be constructed with a mix of programming languages. The company intends to deploy its application on AWS Lambda, and they want to explore the supported languages ​​offered by the AWS Lambda runtime.\n\nWhich of the following programming languages ​​is supported by Lambda runtime? (Select three)",
    "corrects": [
      4,
      5,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "PHP",
        "correct": false
      },
      {
        "id": 2,
        "answer": "C/C++",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Dart",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Ruby",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Go",
        "correct": true
      },
      {
        "id": 6,
        "answer": "Python",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nPython, Go & Ruby\n\nLambda runtime supports various languages or frameworks. Which are the following below:\n\nNode.js\n\nPython\n\nRuby\n\nJava\n\nGo\n\n.NET Core\n\n\n\n\nIncorrect Options:\n\nPHP\n\nC/C++\n\nDart\n\nAll above options are incorrect\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html",
    "correctAnswerExplanation": {
      "answer": "Python, Go &amp; Ruby",
      "explanation": "Lambda runtime supports various languages or frameworks. Which are the following below:"
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "PHP",
        "explanation": "<strong>C/C++</strong>"
      },
      {
        "answer": "Dart",
        "explanation": "All above options are incorrect"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html"
    ]
  },
  {
    "id": 30,
    "question": "A company has an application that needs to be able to send messages to another company's Amazon SQS queues. How can you provide the least privileged access to send messages?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a cross-account role with access to all SQS queues and set second account in trust document for the role.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Update permission policy of SQS queue to grant the sqs:SendMessage to second AWS account.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Update permission policy of SQS queue to grant all permission to second AWS account.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a new account and grant sqs:SendMessage permission of SQS queue. share the credentials with second account.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nUpdate permission policy of SQS queue to grant the sqs:SendMessage to second AWS account.\n\nYou should give the least privileges that you actually need to perform tasks, and AWS recommends this. In our case, you need to attach an SQS queue policy with sqs: SendMessage privileges that grant the second AWS account.\n\nThe following example policy grants AWS account number 111122223333 the SendMessage permission for the queue named 444455556666/queue1 in the US East (Ohio) region.\n\n\n\n\nIncorrect Options:\n\nUpdate permission policy of SQS queue to grant all permission to second AWS account - This would provide the permissions for all SQS queues, not just send permission.\n\nCreate a new account and grant sqs:SendMessage permission of SQS queue. share the credentials with second account - This is just a distraction. You no need to create a new account. You should only grant sqs:SendMessage to second account.\n\nCreate a cross-account role with access to all SQS queues and set second account in trust document for the role - This will allow for all SQS queues for all but the second account should only be able to access the SQS queue. So it is an incorrect option.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-examples-of-sqs-policies.html",
    "correctAnswerExplanation": {
      "answer": "Update permission policy of SQS queue to grant the sqs:SendMessage to second AWS account.",
      "explanation": "You should give the least privileges that you actually need to perform tasks, and AWS recommends this. In our case, you need to attach an SQS queue policy with sqs: SendMessage privileges that grant the second AWS account."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-examples-of-sqs-policies.html"
    ]
  },
  {
    "id": 31,
    "question": "A business is hosted in multiple EC2 instances behind an Auto Scaling Group. The CTO does not want to incur high operating costs for over-provisioning resources. Since you are a Solutions Architect, you are requested to provide a cost-effective solution without dropping the performance.\n\nWhich Dynamic Scaling policy should be used to meet the requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Step scaling",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use simple scaling",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use scheduled scaling",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use target tracking scaling",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Options:\n\nUse target tracking scaling\n\nAn Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.\n\nYou can use scaling policies to increase or decrease the number of instances in your group dynamically to meet changing conditions. When the scaling policy is in effect, the Auto Scaling group adjusts the desired capacity of the group, between the minimum and maximum capacity values that you specify, and launches or terminates the instances as needed. You can also scale on a schedule.\n\nYou can configure your Auto Scaling group to scale dynamically to meet this need by creating a target tracking, step, or simple scaling policy. Amazon EC2 Auto Scaling can then scale out your group (add more instances) to deal with high demand at peak times, and scale in your group (run fewer instances) to reduce costs during periods of low utilization.\n\nAmazon EC2 Auto Scaling supports the following types of dynamic scaling policies:\n\nTarget tracking scaling—Increase or decrease the current capacity of the group based on a target value for a specific metric. This is similar to the way that your thermostat maintains the temperature of your home—you select a temperature and the thermostat does the rest.\n\nStep scaling—Increase or decrease the current capacity of the group based on a set of scaling adjustments, known as step adjustments, that vary based on the size of the alarm breach.\n\nSimple scaling—Increase or decrease the current capacity of the group based on a single scaling adjustment.\n\nIf you are scaling based on a utilization metric that increases or decreases proportionally to the number of instances in an Auto Scaling group, you should use target tracking scaling policies.\n\n\n\n\nIncorrect Options:\n\nUse Step scaling & Use simple scaling - These policies have been discussed above.\n\nUse scheduled scaling - Scheduled scaling helps you to set up your own scaling schedule according to predictable load changes. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can configure a schedule for Amazon EC2 Auto Scaling to increase capacity on Wednesday and decrease capacity on Friday.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html",
    "correctAnswerExplanation": {
      "answer": "Use target tracking scaling",
      "explanation": "An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html"
    ]
  },
  {
    "id": 32,
    "question": "An organization has a financial application for internal employee use only and wants to deploy an Amazon EC2 instance on a private subnet. For security reasons, the application must communicate privately with Amazon DynamoDB and Amazon S3, which must not pass through the public Internet.\n\nWhat step should be taken to address this requirement?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Direct Connect and connect via private endpoints",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS Transit Gateway and connect via public endpoints",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS VPN CloudHub and connect via private endpoints",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use VPC endpoints and connect via private endpoints",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nUse VPC endpoints and connect via private endpoints\n\nWith Amazon VPC, you can launch Amazon EC2 instances into a virtual private cloud (VPC), which is logically isolated from other networks—including the public internet. With an Amazon VPC, you have control over its IP address range, subnets, routing tables, network gateways, and security settings.\n\nWhen you create a VPC endpoint for DynamoDB and Amazon S3, any requests to a DynamoDB and Amazon S3 endpoint within the Region (for example, dynamodb.us-west-2.amazonaws.com) are routed to a private DynamoDB and Amazon S3 endpoint within the Amazon network. You don't need to modify your applications running on EC2 instances in your VPC. The endpoint name remains the same, but the route to DynamoDB and Amazon S3 stays entirely within the Amazon network, and does not access the public internet.\n\nThe following diagram shows how an EC2 instance in a VPC can use a VPC endpoint to access DynamoDB and Amazon S3.\n\n\n\n\nIncorrect Options:\n\nUse AWS Transit Gateway and connect via public endpoints - The transit gateway is a way to connect your VPC and on-premises networks and acts as a cloud router that allows you to integrate multiple networks.\n\nUse AWS Direct Connect and connect via private endpoints - AWS Direct Connect is used to establish a private network connection from on-premises to AWS.\n\nUse AWS VPN CloudHub and connect via private endpoints - AWS VPN CloudHub is an internal VPN software platform that lets users manage their secure VPN connections in the cloud.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\n\nhttps://docs.aws.amazon.com/glue/latest/dg/vpc-endpoints-s3.html",
    "correctAnswerExplanation": {
      "answer": "Use VPC endpoints and connect via private endpoints",
      "explanation": "With Amazon VPC, you can launch Amazon EC2 instances into a virtual private cloud (VPC), which is logically isolated from other networks—including the public internet. With an Amazon VPC, you have control over its IP address range, subnets, routing tables, network gateways, and security settings."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html",
      "https://docs.aws.amazon.com/glue/latest/dg/vpc-endpoints-s3.html"
    ]
  },
  {
    "id": 33,
    "question": "A large IT company needs to review tasks about AWS identity and access management. The IT department has been requested to prepare a checklist for this task.\n\nWhich of the following best practices would a DevOps engineer recommend? (Select two)",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use access key to grant permissions on Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Enable MFA for all IAM users",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Configure CloudTrail to save all event logs",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Grant maximum privileges to access all resources",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Create less IAM user and share with all employees",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nEnable MFA for all IAM users - For increased security, you should enable multi-factor authentication (MFA) to protect your AWS resources. MFA adds an extra layer of protection on top of your user name and password. With MFA enabled, when a user signs in to an AWS Management Console, they will be prompted for their user name and password (the first factor—what they know), as well as for an authentication code from their AWS MFA device (the second factor—what they have).\n\nConfigure CloudTrail to save all event logs - CloudTrail helps you prove compliance, improve security posture, and consolidate activity records across regions and accounts. CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you track changes made to your AWS resources and troubleshoot operational issues.\n\n\n\n\nIncorrect Options:\n\nGrant maximum privileges to access all resources - AWS recommends to granting least privilege or granting only the permissions required to perform a task.\n\nCreate less IAM user and share with all employees - AWS does not recommended to share IAM users to multiple employees.\n\nUse access key to grant permissions on Amazon EC2 instances - You should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials such as access keys to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html"
    ]
  },
  {
    "id": 34,
    "question": "An IT team at a company uses an Amazon EC2 Linux instance to develop an application and an Amazon EBS volume to store the application code. The CTO wants the instance to be unavailable outside business hours to save costs. The requirement is that the EC2 instance must store data in memory when EC2 is unavailable.\n\nHow can you meet these requirements?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use an On-Demand instance. Stop the instance outside business hours. Start it again when required.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an On-Demand instance. Hibernate outside business hours and start it again when required.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use a Reserved instance. Stop the instance outside business hours. Start it again when required.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use a Spot instance. Terminate the instance outside business hours. recreate it again when required.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Options:\n\nUse an On-Demand instance. Hibernate outside business hours and start it again when required.\n\nWith On-Demand instances, you pay for compute capacity by the hour or the second depending on which instances you run. No longer-term commitments or upfront payments are needed. You can increase or decrease your compute capacity depending on the demands of your application and only pay the specified per hourly rates for the instance you use.\n\nOn-Demand instances are recommended for:\n\nUsers that prefer the low cost and flexibility of Amazon EC2 without any up-front payment or long-term commitment\n\nApplications with short-term, spiky, or unpredictable workloads that cannot be interrupted\n\nApplications being developed or tested on Amazon EC2 for the first time\n\nWhen you hibernate an instance, Amazon EC2 signals the operating system to perform hibernation (suspend-to-disk). Hibernation saves the contents from the instance memory (RAM) to your Amazon Elastic Block Store (Amazon EBS) root volume. Amazon EC2 persists the instance's EBS root volume and any attached EBS data volumes. When you start your instance:\n\nThe EBS root volume is restored to its previous state\n\nThe RAM contents are reloaded\n\nThe processes that were previously running on the instance are resumed\n\nPreviously attached data volumes are reattached and the instance retains its instance ID\n\n\n\n\nIncorrect Options:\n\nUse a Reserved instance. Stop the instance outside business hours. Start it again when required - The Reserved instance is not cost-effective for a short-term commitment. When an instance is stopped, the OS also goes shut down and the contents of memory will be lost.\n\nUse an On-Demand instance. Stop the instance outside business hours. Start it again when required - When an instance is stopped, the OS also goes shut down and the contents of memory will be lost.\n\nUse a Spot instance. Terminate the instance outside business hours. recreate it again when required - Spot instances can be interrupted at any time. So it is not useful for the uninterrupted workload.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html\n\nhttps://aws.amazon.com/ec2/pricing",
    "correctAnswerExplanation": {
      "answer": "Use an On-Demand instance. Hibernate outside business hours and start it again when required.",
      "explanation": "With On-Demand instances, you pay for compute capacity by the hour or the second depending on which instances you run. No longer-term commitments or upfront payments are needed. You can increase or decrease your compute capacity depending on the demands of your application and only pay the specified per hourly rates for the instance you use."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html",
      "https://aws.amazon.com/ec2/pricing"
    ]
  },
  {
    "id": 35,
    "question": "An organization runs an application hosted in AWS Cloud using multiple Amazon EC2 instances with an Auto Scaling Group behind an Elastic Load Balancer and the database using Amazon RDS in Multi-AZ deployments. Application widely processes read and write Database operations. To improve system availability and performance, they want to closely monitor how processes or threads use the DB CPU, the percentage of bandwidth, and the memory used by each process.\n\nAs a Solution Architect, which solutions should you use to monitor the database correctly?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Enable Enhanced Monitoring on Amazon RDS.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Check the RDS console for CPU% and MEM% metrics, CPU bandwidth, and memory consumed by each process.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon CloudTrail for all Amazon RDS instances to monitor metrics.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Cloudwatch to monitor Amazon RDS's CPU usage.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nEnable Enhanced Monitoring on Amazon RDS\n\nAmazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view all the system metrics and process information for your RDS DB instances on the console. You can manage which metrics you want to monitor for each instance and customize the dashboard according to your requirements.\n\nRDS delivers the metrics from Enhanced Monitoring into your Amazon CloudWatch Logs account. You can create metrics filters in CloudWatch from CloudWatch Logs and display the graphs on the CloudWatch dashboard. You can consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice.\n\n\n\n\nThe Enhanced Monitoring metrics shown in the Process list view are organized as follows:\n\nRDS child processes – Shows a summary of the RDS processes that support the DB instance, for example mysqld for MySQL DB instances. Process threads appear nested beneath the parent process. Process threads show CPU utilization only as other metrics are the same for all threads for the process. The console displays a maximum of 100 processes and threads. The results are a combination of the top CPU consuming and memory consuming processes and threads. If there are more than 50 processes and more than 50 threads, the console displays the top 50 consumers in each category. This display helps you identify which processes are having the greatest impact on performance.\n\nRDS processes – Shows a summary of the resources used by the RDS management agent, diagnostics monitoring processes, and other AWS processes that are required to support RDS DB instances.\n\nOS processes – Shows a summary of the kernel and system processes, which generally have minimal impact on performance.\n\nThe items listed for each process are:\n\nVIRT – Displays the virtual size of the process.\n\nRES – Displays the actual physical memory being used by the process.\n\nCPU% – Displays the percentage of the total CPU bandwidth being used by the process.\n\nMEM% – Displays the percentage of the total memory being used by the process.\n\n\n\n\nIncorrect Options:\n\nUse Amazon CloudWatch to monitor Amazon RDS's CPU usage - You can monitor Amazon RDS with CloudWatch. But It can’t provide much details information such as Enhanced Monitoring provides.\n\nCheck the RDS console for CPU% and MEM% metrics, CPU bandwidth, and memory consumed by each process - The Amazon RDS console doesn’t provide CPU% and MEM% metrics. This option has been used for creating confusion.\n\nUse Amazon CloudTrail for all Amazon RDS instances to monitor metrics - Amazon CloudTrail doesn’t collect metrics for memory and disk usage. It records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you track changes made to your AWS resources and troubleshoot operational issues.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.overview.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.Viewing.html",
    "correctAnswerExplanation": {
      "answer": "Enable Enhanced Monitoring on Amazon RDS",
      "explanation": "Amazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view all the system metrics and process information for your RDS DB instances on the console. You can manage which metrics you want to monitor for each instance and customize the dashboard according to your requirements."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.overview.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.Viewing.html"
    ]
  },
  {
    "id": 36,
    "question": "A large company has a hybrid cloud architecture and hosts resources in the AWS Cloud and on-premises data center. They stored credentials in an active directory on on-premises and want to access all resources in both environments using these credentials.\n\nAs a Solution Architect, which of the following should you use to meet the requirements?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon VPC",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use SAML 2.0-Based Federation",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use IAM users' credentials",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use OpenID Connect",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nUse SAML 2.0-Based Federation\n\nAWS supports identity federation with SAML 2.0 (Security Assertion Markup Language 2.0), an open standard that many identity providers (IdPs) use. This feature enables federated single sign-on (SSO), so users can log into the AWS Management Console or call the AWS API operations without you having to create an IAM user for everyone in your organization. By using SAML, you can simplify the process of configuring federation with AWS, because you can use the IdP's service instead of writing custom identity proxy code.\n\nBefore you can use SAML 2.0-based federation as described in the preceding scenario and diagram, you must configure your organization's IdP and your AWS account to trust each other. The general process for configuring this trust is described in the following steps. Inside your organization, you must have an IdP that supports SAML 2.0, like Microsoft Active Directory Federation Service (AD FS, part of Windows Server), Shibboleth, or another compatible SAML 2.0 provider.\n\n\n\n\nIncorrect Options:\n\nUse IAM users' credentials - In this case, we need to use on-premises credentials which are stored in Active Directory. So this option is incorrect.\n\nUse Amazon VPC - This option is incorrect because Amazon VPC enables you to launch AWS resources into a virtual network that you've defined. It doesn't provide any authentication.\n\nUse OpenID Connect - You can use the OpenId connect through Amazon Cognito which provides third-party authentication provider. But we need to use on-premises certificates which are stored in the active directory. So this option is wrong.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html",
    "correctAnswerExplanation": {
      "answer": "Use SAML 2.0-Based Federation",
      "explanation": "AWS supports identity federation with SAML 2.0 (Security Assertion Markup Language 2.0), an open standard that many identity providers (IdPs) use. This feature enables federated single sign-on (SSO), so users can log into the AWS Management Console or call the AWS API operations without you having to create an IAM user for everyone in your organization. By using SAML, you can simplify the process of configuring federation with AWS, because you can use the IdP's service instead of writing custom identity proxy code."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html"
    ]
  },
  {
    "id": 37,
    "question": "An intern has configured a target group for an Application Load Balancer with IP address as the target type. Which IP address type is valid for this target type?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Private IP address",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Elastic IP address",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Adjustable IP address",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Public IP address",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nPrivate IP address - When you create a target group, you specify its target type, which determines the type of target you specify when registering targets with this target group. After you create a target group, you cannot change its target type.\n\nThe following are the possible target types:\n\ninstance - the targets are specified by instance ID.\n\nip - the targets are IP addresses.\n\nlambda - the target is a Lambda function.\n\nIf you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces.\n\n\n\n\nIncorrect Options:\n\nPublic IP address & Elastic IP address - You cannot specify publicly routable IP addresses as a target type for a target group.\n\nAdjustable IP address - There is no such thing. This option has been used to confuse you\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#target-type",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#target-type"
    ]
  },
  {
    "id": 38,
    "question": "A large company uses an OU of AWS Organization to manage multiple member accounts. They must limit the ability only to launch specific Amazon EC2 instance types. How can the policy be applied across accounts with minimal effort?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an IAM policy to deny all except a specific instance type.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS resource Access Manager to control who can launch the specific instance type.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an SCP with a deny rule that denies all except a specific instance type.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an SCP with an allow rule that enables launching the specific instance types.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nCreate an SCP with a deny rule that denies all except a specific instance type.\n\nService control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines.\n\nSCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.\n\nExample: with this SCP, any instance launches not using the t2.micro instance type are denied.\n\n\n\n\nIncorrect Options:\n\nCreate an SCP with an allow rule that enables launching the specific instance types - For SCP policy you need to add deny rule.\n\nCreate an IAM policy to deny all except a specific instance type - For IAM policy, you need to attach all IAM users and it cannot attract with the AWS account.\n\nUse AWS resource Access Manager to control who can launch the specific instance type - AWS Resource Access Manager (RAM) helps you to share securely AWS resources within your AWS organization. It doesn't offer any permission-based service.\n\nAll above are incorrect.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
    "correctAnswerExplanation": {
      "answer": "Create an SCP with a deny rule that denies all except a specific instance type.",
      "explanation": "Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
    ]
  },
  {
    "id": 39,
    "question": "A business needs a storage solution to store data at the lowest possible cost. A DevOps engineer has been requested to research the pricing model. The engineer created 1GB of test data and copied it to Amazon S3 standard storage class, EBS volume (General Purpose SSD (gp2)) with provisioned storage of 50GB, and EFS Standard storage class. At the end of the month, he analyzed the bill for cost expenses.\n\nWhat is the correct order of these storage charges for test results?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "S3 Standard < EFS Standard < EBS Volume",
        "correct": true
      },
      {
        "id": 2,
        "answer": "EBS Volume < S3 Standard < EFS Standard",
        "correct": false
      },
      {
        "id": 3,
        "answer": "S3 Standard < EBS Volume < EFS Standard",
        "correct": false
      },
      {
        "id": 4,
        "answer": "EFS Standard < S3 Standard < EBS Volume",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Options:\n\nS3 Standard < EFS Standard < EBS Volume\n\nThe S3 Standard class charges $0.023 per GB per month in the US East (Ohio) region.\n\nThe EFS Standard Storage charges $0.30 per GB per month in the US East (Ohio) region.\n\nEBS volumes General Purpose SSD (gp2) charges is $0.10 per GB and for provisioned storage of 50GB is $0.10*50= $5 monthly.\n\nTherefore, S3 Standard < EFS Standard < EBS Volume is the correct order for this test.\n\n\n\n\nIncorrect Options:\n\nS3 Standard < EBS Volume < EFS Standard\n\nEFS Standard < S3 Standard < EBS Volume\n\nEBS Volume < S3 Standard < EFS Standard\n\nAll of the above options are incorrect.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/pricing\n\nhttps://aws.amazon.com/efs/pricing\n\nhttps://aws.amazon.com/ebs/pricing",
    "correctAnswerExplanation": {
      "answer": "S3 Standard &lt; EFS Standard &lt; EBS Volume",
      "explanation": "The S3 Standard class charges $0.023 per GB per month in the US East (Ohio) region."
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "S3 Standard &lt; EBS Volume &lt; EFS Standard",
        "explanation": "<strong>EFS Standard &lt; S3 Standard &lt; EBS Volume</strong>"
      },
      {
        "answer": "EBS Volume &lt; S3 Standard &lt; EFS Standard",
        "explanation": "All of the above options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/pricing",
      "https://aws.amazon.com/efs/pricing",
      "https://aws.amazon.com/ebs/pricing"
    ]
  },
  {
    "id": 40,
    "question": "A company has an application that needs to be deployed on an Amazon EC2 instance. This application will be used for internal use by company employees. For data storing, they cannot use Amazon S3 and want to use a separate EBS volume to store data at the lowest possible cost. So CTO wants to explore all EBS volume types which cannot use as a boot volume.\n\nWhich storage volume type cannot be used as a boot volume for the Amazon EC2 instance?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Instance Store",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Throughput Optimized HDD",
        "correct": true
      },
      {
        "id": 3,
        "answer": "General Purpose SSD",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Provisioned IOPS SSD",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nThroughput Optimized HDD - ST1 is backed by hard disk drives (HDDs) and is ideal for frequently accessed, throughput intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads. These volumes deliver performance in terms of throughput, measured in MB/s, and include the ability to burst up to 250 MB/s per TB, with a baseline throughput of 40 MB/s per TB and a maximum throughput of 500 MB/s per volume. ST1 is designed to deliver the expected throughput performance 99% of the time and has enough I/O credits to support a full-volume scan at the burst rate.\n\nThe HDD-backed volumes provided by Amazon EBS fall into these categories:\n\nThroughput Optimized HDD — A low-cost HDD designed for frequently accessed, throughput-intensive workloads.\n\nCold HDD — The lowest-cost HDD design for less frequently accessed workloads.\n\nThe following is a summary of the use cases and characteristics of HDD-backed volumes.\n\n\n\n\nIncorrect Options:\n\nGeneral Purpose SSD\n\nProvisioned IOPS SSD\n\nInstance Store\n\nAll above storage volumes ‌can be used as a root volume.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
    "correctAnswerExplanation": {
      "answer": "Throughput Optimized HDD</strong> - ST1 is backed by hard disk drives (HDDs) and is ideal for frequently accessed, throughput intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads. These volumes deliver performance in terms of throughput, measured in MB/s, and include the ability to burst up to 250 MB/s per TB, with a baseline throughput of 40 MB/s per TB and a maximum throughput of 500 MB/s per volume. ST1 is designed to deliver the expected throughput performance 99% of the time and has enough I/O credits to support a full-volume scan at the burst rate.</p><p>The HDD-backed volumes provided by Amazon EBS fall into these categories:</p><ul><li><p>Throughput Optimized HDD — A low-cost HDD designed for frequently accessed, throughput-intensive workloads.</p></li><li><p>Cold HDD — The lowest-cost HDD design for less frequently accessed workloads.</p></li></ul><p>The following is a summary of the use cases and characteristics of HDD-backed volumes.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-06-14_17-16-48-7c281a7745e92cced8dee9e96c465c19.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-06-14_17-16-48-7c281a7745e92cced8dee9e96c465c19.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><br></p><p>Incorrect Options:</p><p><strong>General Purpose SSD",
      "explanation": "<strong>Provisioned IOPS SSD</strong>"
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Instance Store",
        "explanation": "All above storage volumes ‌can be used as a root volume."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
    ]
  },
  {
    "id": 41,
    "question": "A business has a lot of big data stored in Amazon S3 and serves it for analytics solutions. Amazon S3 also acts as a data lake where temporary query results are reserved only for 24 hours. Another analytics pipeline will use these query results. After pipeline jobs are done, the result will be removed from the data lake.\n\nWhat is the most cost-effective storage class for storing the temporary query result?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon S3 Standard-Infrequent Access storage class",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon S3 One Zone-Infrequent Access storage class",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon S3 Glacier Instant Retrieval storage class",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon S3 Standard storage class",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Options:\n\nAmazon S3 Standard storage class - S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\n\nS3 Standard is ideal for your most frequently accessed or modified data that requires access in milliseconds and high throughput performance. S3 Standard is ideal for data that is read or written very often, as there are no retrieval charges. S3 Standard is optimized for a wide variety of use cases, including data lakes, cloud native applications, dynamic websites, content distribution, mobile and gaming applications, and analytics.\n\n\n\n\nIncorrect Options:\n\nAmazon S3 Standard-Infrequent Access storage class - Amazon S3 Standard-Infrequent Access (S3 Standard-IA) is an Amazon S3 storage class for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA is designed for long-lived, infrequently accessed data that is retained for months or years. Data that is deleted from S3 Standard-IA within 30 days will be charged for a full 30 days.\n\nAmazon S3 One Zone-Infrequent Access storage class - S3 One Zone-IA storage class is an Amazon S3 storage class that customers can choose to store objects in a single availability zone. S3 One Zone-IA storage redundantly stores data within that single Availability Zone to deliver storage at 20% less cost than geographically redundant S3 Standard-IA storage, which stores data redundantly across multiple geographically separate Availability Zones. The minimum storage duration charge is 30 days, so this option is NOT cost-effective\n\nAmazon S3 Glacier Instant Retrieval storage class - Amazon S3 Glacier Instant Retrieval is an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds. With S3 Glacier Instant Retrieval, you can save up to 68% on storage costs compared to using the S3 Standard-Infrequent Access (S3 Standard-IA) storage class, when your data is accessed once per quarter. Objects that are archived to S3 Glacier Instant Retrieval have a minimum of 90 days of storage, and objects deleted, overwritten, or transitioned before 90 days incur a pro-rated charge equal to the storage charge for the remaining days.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/s3/storage-classes"
    ]
  },
  {
    "id": 42,
    "question": "A business has a data warehouse that uses redshift clusters to analyze its data. CTO wants to track all API calls in Redshift. Which AWS service allows you to monitor API calls and provide data for auditing and compliance purposes?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "AWS X-Ray",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon Inspector",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS CloudTrail",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Amazon CloudWatch",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nAWS CloudTrail - AWS CloudTrail enables auditing, security monitoring, and operational troubleshooting by tracking user activity and API usage. CloudTrail logs, continuously monitors, and retains account activity related to actions across your AWS infrastructure, giving you control over storage, analysis, and remediation actions.\n\nCloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you track changes made to your AWS resources and troubleshoot operational issues. CloudTrail makes it easier to ensure compliance with internal policies and regulatory standards.\n\nHow it works\n\n\n\n\nIncorrect Options:\n\nAmazon CloudWatch - The primary purpose of Amazon CloudWatch that to monitor AWS resource usage reports. It does not track API calls or event history.\n\nAWS X-Ray - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. So this option is incorrect.\n\nAmazon Inspector - Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure. It does not track any API calls.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/cloudtrail\n\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/cloudtrail",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html"
    ]
  },
  {
    "id": 43,
    "question": "A business runs a large batch processing job every month. The job uses 10 Amazon EC2 instances and runs 5 hours daily for a week without interruption. The company needs a way to reduce costs for this batch processing job.\n\nWhich EC2 instances type should the company use to reduce costs?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Spot Instances",
        "correct": false
      },
      {
        "id": 2,
        "answer": "On-Demand Instances",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Dedicated Instances",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Reserved Instances",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Options:\n\nOn-Demand Instances - With On-Demand instances, you pay for compute capacity by the hour or the second depending on which instances you run. No longer-term commitments or upfront payments are needed. You can increase or decrease your compute capacity depending on the demands of your application and only pay the specified per hourly rates for the instance you use.\n\nOn-Demand instances are recommended for:\n\nUsers that prefer the low cost and flexibility of Amazon EC2 without any up-front payment or long-term commitment\n\nApplications with short-term, spiky, or unpredictable workloads that cannot be interrupted\n\nApplications being developed or tested on Amazon EC2 for the first time\n\nThe company needs 5 hours daily for a week of every month or 420 hours yearly which is uninterruptible. Based on price comparison On-Demand and Reserved Instances, The On-Demand offers better value.\n\n\n\n\nIncorrect Options:\n\nReserved Instances - Reserved Instances offer low prices on EC2 in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a 1 or 3 year term. It is better for loan term use.\n\nSpot Instances - Spot instances can be interrupted at any time. So this option is incorrect.\n\nDedicated Instances -This instance is more pricey compared to other instances.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/ec2/pricing",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/ec2/pricing"
    ]
  },
  {
    "id": 44,
    "question": "A video-based company is planning to launch an application in AWS Cloud. The company is looking for low-cost storage solutions. They require maximum I/O performance storage for video processing, highly durable storage for video storage, and long-term storage to back up other files.\n\nWhich of the following services should you advise for the solution? (Select three)",
    "corrects": [
      2,
      4,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon EFS for durable data storage",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon S3 Glacier for archival storage",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Amazon S3 Standard for archival storage",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon EC2 instance store for maximum performance",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Amazon EBS for maximum performance",
        "correct": false
      },
      {
        "id": 6,
        "answer": "Amazon S3 standard for durable data storage",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nAmazon S3 standard for durable data storage - S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\n\nAmazon S3 Glacier for archival storage - The Amazon S3 Glacier storage classes are purpose-built for data archiving, and are designed to provide you with the highest performance, the most retrieval flexibility, and the lowest cost archive storage in the cloud. You can choose from three archive storage classes optimized for different access patterns and storage duration. For archive data that needs immediate access, such as medical images, news media assets, or genomics data, choose the S3 Glacier Instant Retrieval storage class, an archive storage class that delivers the lowest cost storage with milliseconds retrieval. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, choose S3 Glacier Flexible Retrieval (formerly S3 Glacier), with retrieval in minutes or free bulk retrievals in 5—12 hours. To save even more on long-lived archive storage such as compliance archives and digital media preservation, choose S3 Glacier Deep Archive, the lowest cost storage in the cloud with data retrieval from 12—48 hours.\n\nInstance store for maximum performance - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Some instance types use NVMe or SATA-based solid state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures.\n\n\n\n\nIncorrect Options:\n\nAmazon EBS for maximum performance - Amazon EBS does not provide the maximum I/O performance like an Instance store can.\n\nAmazon EFS for durable data storage - Amazon EFS does not offer as much durability as the Amazon S3 offer.\n\nAmazon S3 Standard for archival storage - Amazon S3 Standard is more costly than S3 Glacier. You should use Amazon S3 Glacier for storing archival data.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\n\nhttps://aws.amazon.com/s3/storage-classes",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
      "https://aws.amazon.com/s3/storage-classes"
    ]
  },
  {
    "id": 45,
    "question": "A company wants to develop a multi-tier movie-based application that will run on multiple EC2 instances behind an Auto Scaling group with an Application Load Balancer across multiple Availability Zones. All contents will store in the Amazon S3 bucket and will use an Amazon Aurora database for its database. As a Solution Architect, you must make the application more resilient to improve the request rate.\n\nWhich of the following solutions should be provided for this requirement? (Select Two)",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon S3 ‎Intelligent-Tiering storage class",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon CloudFront in front of the ALB",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use AWS Global Accelerator",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Aurora Replica",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Use AWS VPN",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nUse Aurora Replica - You can use Aurora Replicas to scale read-heavy operations for your applications. You can do this by connecting a reader endpoint to a cluster. This way, Aurora can spread the load for read-only traffic across as many Aurora replicas in your cluster. Aurora Replicas also helps you improve the availability of your applications. If a writer instance in a cluster is unavailable, Aurora automatically promotes one of the reader instances to take over as the new writer.\n\nAn Aurora DB cluster supports up to 15 replicas. Aurora Replicas distribute incoming traffic across multiple AZs that span a DB cluster within an AWS Region.\n\nUse Amazon CloudFront in front of the ALB - You can use Amazon CloudFront in front of an application load balancer to improve latency. For a web application or other static content that distributes data with an application load balancer in Elastic Application Balancing, CloudFront caches objects and serves them directly to users, reducing the load on your application load balancer.\n\n\n\n\nIncorrect Options:\n\nUse AWS Global Accelerator - AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. AWS Global Accelerator is easy to set up, configure, and manage. It provides static IP addresses that provide a fixed entry point to your applications and eliminate the complexity of managing specific IP addresses for different AWS Regions and Availability Zones. AWS Global Accelerator always routes user traffic to the optimal endpoint based on performance, reacting instantly to changes in application health, your user’s location, and policies that you configure.\n\nUse AWS VPN - AWS Virtual Private Network solutions establish secure connections between your on-premises networks, remote offices, client devices, and the AWS global network. AWS VPN is comprised of two services: AWS Site-to-Site VPN and AWS Client VPN. Each service provides a highly-available, managed, and elastic cloud VPN solution to protect your network traffic.\n\nUse Amazon S3 ‎Intelligent-Tiering storage class - The class is designed to optimize storage costs by automatically moving data to the most cost-effective access tier when access patterns change. This can reduce cost but it does impact performance.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html"
    ]
  },
  {
    "id": 46,
    "question": "An online social content-based organization runs an application on AWS that delivers content globally. The application is deployed on multiple Amazon EC2 instances in a private subnet behind an Application Load Balancer (ALB). The company must block access to a specific country for some country restrictions.\n\nWhat steps should take to address this need?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure security group for EC2 instances to deny incoming traffic for specific countries.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use A Network ACL to block the IP ranges for specific countries.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon CloudFront to serve content and geographic restrictions feature to deny access to blocked countries.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure ALB security group to deny incoming traffic for specific countries.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nUse Amazon CloudFront to serve content and geographic restrictions feature to deny access to blocked countries.\n\nYou can use geographic restrictions, sometimes known as geo blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront distribution. To use geographic restrictions, you have two options:\n\nUse the CloudFront geographic restrictions feature. Use this option to restrict access to all of the files that are associated with a distribution and to restrict access at the country level.\n\nUse a third-party geolocation service. Use this option to restrict access to a subset of the files that are associated with a distribution or to restrict access at a finer granularity than the country level.\n\nWhen a user requests your content, CloudFront typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geographic restrictions feature to do one of the following:\n\nAllow your users to access your content only if they’re in one of the approved countries on your allow list.\n\nPrevent your users from accessing your content if they’re in one of the banned countries on your block list.\n\n\n\n\nIncorrect Options:\n\nUse A Network ACL to block the IP ranges for specific countries - This is incorrect. the IP ranges of a countries mange are extremely difficult.\n\nConfigure ALB security group to deny incoming traffic for specific countries -The security group does not provide blocks feature for specific countries.\n\nConfigure security group for EC2 instances to deny incoming traffic for specific countries - The security group does not provide blocks feature for specific countries.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html",
    "correctAnswerExplanation": {
      "answer": "Use Amazon CloudFront to serve content and geographic restrictions feature to deny access to blocked countries.",
      "explanation": "You can use geographic restrictions, sometimes known as geo blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront distribution. To use geographic restrictions, you have two options:"
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html"
    ]
  },
  {
    "id": 47,
    "question": "A microservices-based e-commerce application running on AWS and uses Amazon RDS to store its data. Recently they faced a problem, the application received a burst of traffic within seconds when a new product was launched, and the performance dropped. A solution architect requested to re-architect the service that will improve performance and allow users to access data using the Amazon API Gateway globally.\n\nWhat solution should be used to meet the criteria?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Elastic Beanstalk with Auto Scaling to handle the bursts of traffic.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use the Amazon ECS cluster with Service Auto Scaling to handle the bursts of traffic.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use an Auto Scaling group of Amazon EC2 instances to handle the bursts of traffic.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Lambda function to handle the bursts of traffic.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nUse AWS Lambda function to handle the bursts of traffic.\n\nYou can invoke a Lambda function over HTTPS by defining a custom RESTful API using Amazon API Gateway. This gives you an endpoint for your function which can respond to REST calls like GET, PUT, and POST.\n\nThe first time you invoke your function, AWS Lambda creates an instance of the function and runs its handler method to process the event. When the function returns a response, it stays active and waits to process additional events. If you invoke the function again while the first event is being processed, Lambda initializes another instance, and the function processes the two events concurrently. As more events come in, Lambda routes them to available instances and creates new instances as needed. When the number of requests decreases, Lambda stops unused instances to free up scaling capacity for other functions.\n\nYour functions' concurrency is the number of instances that serve requests at a given time. For an initial burst of traffic, your functions' cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region. Note that the burst concurrency quota is not per-function; it applies to all your functions in the Region.\n\nBurst concurrency quotas\n\n3000 – US West (Oregon), US East (N. Virginia), Europe (Ireland)\n\n1000 – Asia Pacific (Tokyo), Europe (Frankfurt), US East (Ohio)\n\n500 – Other Regions\n\nAfter the initial burst, your functions' concurrency can scale by an additional 500 instances each minute. This continues until there are enough instances to serve all requests, or until a concurrency limit is reached. When requests come in faster than your function can scale, or when your function is at maximum concurrency, additional requests fail with a throttling error (429 status code).\n\n\n\n\nIncorrect Options:\n\nUse the Amazon ECS cluster with Service Auto Scaling to handle the bursts of traffic.\n\nUse Amazon Elastic Beanstalk with Auto Scaling to handle the bursts of traffic.\n\nUse an Auto Scaling group of Amazon EC2 instances to handle the bursts of traffic.\n\nAuto Scaling needs a delay of a few minutes to provision new resources. In our case, we need to handle bursts of traffic within seconds. So These options are incorrect in our case.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/startups/from-0-to-100-k-in-seconds-instant-scale-with-aws-lambda\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.htm",
    "correctAnswerExplanation": {
      "answer": "Use AWS Lambda function to handle the bursts of traffic.",
      "explanation": "You can invoke a Lambda function over HTTPS by defining a custom RESTful API using Amazon API Gateway. This gives you an endpoint for your function which can respond to REST calls like GET, PUT, and POST."
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Use the Amazon ECS cluster with Service Auto Scaling to handle the bursts of traffic.",
        "explanation": "<strong>Use Amazon Elastic Beanstalk with Auto Scaling to handle the bursts of traffic.</strong>"
      },
      {
        "answer": "Use an Auto Scaling group of Amazon EC2 instances to handle the bursts of traffic.",
        "explanation": "Auto Scaling needs a delay of a few minutes to provision new resources. In our case, we need to handle bursts of traffic within seconds. So These options are incorrect in our case."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/startups/from-0-to-100-k-in-seconds-instant-scale-with-aws-lambda",
      "https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.htm"
    ]
  },
  {
    "id": 48,
    "question": "A company has two AWS accounts containing a single VPC each (e.g. VPC-APP and VPC-AUTH). Each VPC contains one Amazon EC2 instance, one for an application and another for an authentication server. They require a mechanism to communicate securely between these VPCs. There should be no single point of failure or bandwidth limitation.\n\nWhat should be the solution to meet the need?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Attach a virtual private gateway to VPCs (ex. VPC-APP and VPC-AUTH) and enable routing.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a VPC gateway endpoint for each EC2 instance and update route tables.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a VPC peering connection between two VPCs (ex. VPC-APP and VPC-AUTH).",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Attach a Direct Connect gateway to VPCs (ex. VPC-APP and VPC-AUTH) and enable routing.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nCreate a VPC peering connection between two VPCs (ex. VPC-APP and VPC-AUTH).\n\nA VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection).\n\nAWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection, and does not rely on a separate piece of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck.\n\nA VPC peering connection helps you to facilitate the transfer of data. For example, if you have more than one AWS account, you can peer the VPCs across those accounts to create a file sharing network. You can also use a VPC peering connection to allow other VPCs to access resources you have in one of your VPCs.\n\n\n\n\nIncorrect Options:\n\nCreate a VPC gateway endpoint for each EC2 instance and update route tables.\n\nAttach a virtual private gateway to VPCs (ex. VPC-APP and VPC-AUTH) and enable routing.\n\nAttach a Direct Connect gateway to VPCs (ex. VPC-APP and VPC-AUTH) and enable routing.\n\nAll above options are incorrect\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/working-with-vpc-peering.html",
    "correctAnswerExplanation": {
      "answer": "Create a VPC peering connection between two VPCs (ex. VPC-APP and VPC-AUTH).",
      "explanation": "A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection)."
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a VPC gateway endpoint for each EC2 instance and update route tables.",
        "explanation": "<strong>Attach a virtual private gateway to VPCs (ex. VPC-APP and VPC-AUTH) and enable routing.</strong>"
      },
      {
        "answer": "Attach a Direct Connect gateway to VPCs (ex. VPC-APP and VPC-AUTH) and enable routing.",
        "explanation": "All above options are incorrect"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
      "https://docs.aws.amazon.com/vpc/latest/peering/working-with-vpc-peering.html"
    ]
  },
  {
    "id": 49,
    "question": "An audit team of a business produces an audit report every month which needs to be accessed twice a year. The team uses a lambda function to process these reports. These reports need to be stored in an S3 bucket, run in terabytes, and should be available in milliseconds of latency.\n\nWhich storage class should be used for this requirement at the most affordable price?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Amazon S3 Glacier Deep Archive",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon S3 Standard",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Options:\n\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA)\n\nSince the audit team needs to access these reports twice in a year, but requires rapid access when needed, the most affordable storage class would be the S3 Standard-IA.\n\nS3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval charge. This combination of low cost and high performance make S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files.\n\nKey Features:\n\nSame low latency and high throughput performance of S3 Standard\n\nDesigned for durability of 99.999999999% of objects across multiple Availability Zones\n\nResilient against events that impact an entire Availability Zone\n\nData is resilient in the event of one entire Availability Zone destruction\n\nDesigned for 99.9% availability over a given year\n\nBacked with the Amazon S3 Service Level Agreement for availability\n\nSupports SSL for data in transit and encryption of data at rest\n\nS3 Lifecycle management for automatic migration of objects to other S3 Storage Classes\n\n\n\n\nIncorrect Options:\n\nAmazon S3 Standard - S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics. The S3 Standard is a bit more expensive than the S3 Standard-IA. So it is not suitable for cost-effective price.\n\nAmazon S3 Glacier Deep Archive - S3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice in a year. It should be used for backup or disaster recovery use cases. It does not support rapid access, the retrieval time is 12 hours.\n\nAmazon S3 Intelligent-Tiering (S3 Intelligent-Tiering) - S3 Intelligent-Tiering is an Amazon S3 storage class designed to optimize storage costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. S3 Intelligent-Tiering charges you a small monitoring and automation fee. However, Standard-IA has the same availability as S3 Intelligent-Tearing. So, in our case, S3 Standard-IA is cheaper to use instead of S3 Intelligent-Tiering.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes",
    "correctAnswerExplanation": {
      "answer": "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)",
      "explanation": "Since the audit team needs to access these reports twice in a year, but requires rapid access when needed, the most affordable storage class would be the S3 Standard-IA."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/s3/storage-classes"
    ]
  },
  {
    "id": 50,
    "question": "A company operates an application using Amazon EC2 instance, Amazon RDS database, elastic load balancer, Amazon Route 53, and CloudFront service. They want to improve the security of these services. An AWS expert advised them to verify the possibility of using the Amazon GuardDuty service.\n\nWhich of the following entities are supported by GuardDuty?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "S3 access logs, VPC Flow Logs, API Gateway logs",
        "correct": false
      },
      {
        "id": 2,
        "answer": "CloudFront logs, ELB logs, CloudTrail events",
        "correct": false
      },
      {
        "id": 3,
        "answer": "CloudTrail events, VPC Flow Logs, DNS logs",
        "correct": true
      },
      {
        "id": 4,
        "answer": "API Gateway logs, DNS logs, CloudTrail events",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nCloudTrail events, VPC Flow Logs, DNS logs\n\nAmazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. Amazon GuardDuty offers threat detection enabling you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon Simple Storage Service (Amazon S3). GuardDuty analyzes continuous metadata streams generated from your account and network activity found in AWS CloudTrail Events, Amazon Virtual Private Cloud (VPC) Flow Logs, and domain name system (DNS) Logs. GuardDuty also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning (ML) to more accurately identify threats.\n\nAmazon GuardDuty can be set up and deployed with a few clicks in the AWS Management console. Once enabled, GuardDuty immediately starts analyzing continuous streams of account and network activity in near real-time and at scale.\n\nHow GuardDuty works\n\n\n\n\nIncorrect Options:\n\nS3 access logs, VPC Flow Logs, API Gateway logs\n\nCloudFront logs, ELB logs, CloudTrail events\n\nAPI Gateway logs, DNS logs, CloudTrail events\n\nAll above are incorrect.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/guardduty/faqs",
    "correctAnswerExplanation": {
      "answer": "CloudTrail events, VPC Flow Logs, DNS logs",
      "explanation": "Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. Amazon GuardDuty offers threat detection enabling you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon Simple Storage Service (Amazon S3). GuardDuty analyzes continuous metadata streams generated from your account and network activity found in AWS CloudTrail Events, Amazon Virtual Private Cloud (VPC) Flow Logs, and domain name system (DNS) Logs. GuardDuty also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning (ML) to more accurately identify threats."
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "S3 access logs, VPC Flow Logs, API Gateway logs",
        "explanation": "<strong>CloudFront logs, ELB logs, CloudTrail events</strong>"
      },
      {
        "answer": "API Gateway logs, DNS logs, CloudTrail events",
        "explanation": "All above are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/guardduty/faqs"
    ]
  },
  {
    "id": 51,
    "question": "In the role of Solutions Architect, you are requested to create an IAM policy for the S3 bucket and assign to a newly joined employee.\n\n{ \n \"Version\": \"2012-10-17\", \n \"Statement\": [ \n  { \n   \"Effect\": \"Allow\", \n   \"Action\": [ \n    \"s3:Get*\", \n    \"s3:List*\" \n   ], \n   \"Resource\": \"*\" \n  }, \n  { \n   \"Effect\": \"Allow\", \n   \"Action\": \"s3:PutObject\", \n   \"Resource\": \"arn:aws:s3:::test/*\" \n  } \n ] \n}\n\nWhat does the above IAM policy allow? (Select two)",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Allowed to read objects in the test S3 bucket but not allowed to list the objects in the bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Allowed to change access rights for the test S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Allowed to read and delete objects from the test S3 bucket.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Allowed to write objects into the test S3 bucket.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Allowed to read objects from all S3 buckets owned by the account.",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nAllowed to write objects into the test S3 bucket.\n\nAllowed to read objects from all S3 buckets owned by the account.\n\nYou manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, ACLs, and session policies.\n\nIAM policies define permissions for an action regardless of the method that you use to perform the operation. For example, if a policy allows the GetUser action, then a user with that policy can get user information from the AWS Management Console, the AWS CLI, or the AWS API. When you create an IAM user, you can choose to allow console or programmatic access. If console access is allowed, the IAM user can sign in to the console using a user name and password. Or if programmatic access is allowed, the user can use access keys to work with the CLI or API.\n\nFor this IAM policy, users can only be allowed to get, write and get the list of objects in the test S3 bucket.\n\n\n\n\nIncorrect Options:\n\nAllowed to change access rights for the test S3 bucket.\n\nAllowed to read objects in the test S3 bucket but not allowed to list the objects in the bucket.\n\nAllowed to read and delete objects from the test S3 bucket.\n\nAll above options are incorrect.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\n\nhttps://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket",
    "correctAnswerExplanation": {
      "answer": "Allowed to write objects into the test S3 bucket.",
      "explanation": "<strong>Allowed to read objects from all S3 buckets owned by the account.</strong>"
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Allowed to change access rights for the test S3 bucket.",
        "explanation": "<strong>Allowed to read objects in the test S3 bucket but not allowed to list the objects in the bucket.</strong>"
      },
      {
        "answer": "Allowed to read and delete objects from the test S3 bucket.",
        "explanation": "All above options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
      "https://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket"
    ]
  },
  {
    "id": 52,
    "question": "An online gaming company hosts an application on multiple Amazon EC2 instances behind an Application Load Balancer in multiple regions. For regional law and distribution policy, they must store content based on users' region.\n\nAs a Solution Architect, what solution should you provide to meet the criteria?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Application Load Balancers with multi-Region routing.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon Route 53 with geolocation routing policy.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Amazon Route 53 with geoproximity routing policy.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon CloudFront with multiple origins and AWS WAF.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nUse Amazon Route 53 with geolocation routing policy - Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from.\n\nWhen you use geolocation routing, you can localize your content and present some or all of your website in the language of your users. You can also use geolocation routing to restrict distribution of content to only the locations in which you have distribution rights. Another possible use is for balancing load across endpoints in a predictable, easy-to-manage way, so that each user location is consistently routed to the same endpoint.\n\nWhen you create a record, you choose a routing policy, which determines how Amazon Route 53 responds to queries:\n\nSimple routing policy – Use for a single resource that performs a given function for your domain, for example, a web server that serves content for the example.com website. You can use simple routing to create records in a private hosted zone.\n\nFailover routing policy – Use when you want to configure active-passive failover. You can use failover routing to create records in a private hosted zone.\n\nGeolocation routing policy – Use when you want to route traffic based on the location of your users. You can use geolocation routing to create records in a private hosted zone.\n\nGeoproximity routing policy – Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.\n\nLatency routing policy – Use when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the best latency. You can use latency routing to create records in a private hosted zone.\n\nIP-based routing policy – Use when you want to route traffic based on the location of your users, and have the IP addresses that the traffic originates from.\n\nMultivalue answer routing policy – Use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random. You can use multivalue answer routing to create records in a private hosted zone.\n\nWeighted routing policy – Use to route traffic to multiple resources in proportions that you specify. You can use weighted routing to create records in a private hosted zone.\n\n\n\n\nIncorrect Options:\n\nUse Application Load Balancers with multi-region routing - There is no such as thing a multi-Region routing for Application Load balancer. This is used to distract you.\n\nUse Amazon Route 53 with geoproximity routing policy - This policy discus above.\n\nUse Amazon CloudFront with multiple origins and AWS WAF - AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots. It does not provide route service.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html"
    ]
  },
  {
    "id": 53,
    "question": "An organization has an application of highly available architecture that uses an Elastic Load Balancer attached to an Auto Scaling Group. The application has been deployed in multiple Availability Zones using multiple EC2 instances. The analysis team wants to monitor these instances based on a specific metric, which is not available in CloudWatch.\n\nWhich custom metric do you have to set manually?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Network packets out",
        "correct": false
      },
      {
        "id": 2,
        "answer": "CPU Utilization",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Disk Read Operations",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Memory Utilization",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nMemory Utilization\n\nYou can monitor your instances using Amazon CloudWatch, which collects and processes raw data from Amazon EC2 into readable, near real-time metrics. These statistics are recorded for a period of 15 months, so that you can access historical information and gain a better perspective on how your web application or service is performing.\n\nYou can collect metrics from servers by installing the CloudWatch agent on the server. You can install the agent on both Amazon EC2 instances and on-premises servers, and on computers running either Linux, Windows Server, or macOS. If you install the agent on an Amazon EC2 instance, the metrics it collects are in addition to the metrics enabled by default on Amazon EC2 instances.\n\nThe following lists the metrics that you can collect with the CloudWatch agent.\n\nAmount of CPU time\n\nCPU utilization\n\nDisk utilization\n\nNetwork interface\n\nMemory Utilization\n\nProcesses Utilization\n\nAmount of Swap Space\n\n\n\n\nHow it works\n\n\n\n\nIncorrect Options:\n\nDisk Read Operations\n\nNetwork Packets Out\n\nCPU Utilization\n\nAll above are incorrect. By default, the following graphs are available:\n\nAverage CPU Utilization (Percent)\n\nAverage Disk Reads (Bytes)\n\nAverage Disk Writes (Bytes)\n\nMaximum Network In (Bytes)\n\nMaximum Network Out (Bytes)\n\nSummary Disk Read Operations (Count)\n\nSummary Disk Write Operations (Count)\n\nSummary Status (Any)\n\nSummary Status Instance (Count)\n\nSummary Status System (Count)\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/graphs-in-the-aws-management-console.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html",
    "correctAnswerExplanation": {
      "answer": "Memory Utilization",
      "explanation": "You can monitor your instances using Amazon CloudWatch, which collects and processes raw data from Amazon EC2 into readable, near real-time metrics. These statistics are recorded for a period of 15 months, so that you can access historical information and gain a better perspective on how your web application or service is performing."
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Disk Read Operations",
        "explanation": "<strong>Network Packets Out</strong>"
      },
      {
        "answer": "CPU Utilization",
        "explanation": "All above are incorrect. By default, the following graphs are available:"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/graphs-in-the-aws-management-console.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html"
    ]
  },
  {
    "id": 54,
    "question": "A company has an application deployed in 30 EC2 instances to support its maximum workloads. After reviewing the work pressure, the analysis team found that most of the time, around 15 EC2 instances are in the ideal position, and at minor times in the month, 5 instances cover all workload. So the owner requested you to optimize the workload, which must be highly available and reduce costs as possible.\n\nAs a Solution Architect, which solution should you provide for this case?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an Auto Scaling group with a minimum of 5 and a maximum of 30 instances. deployed 30 instances in multiple Regions.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Auto Scaling group with a minimum of 5 and a maximum of 15 instances. deployed all instances in multiple Availability Zones.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Auto Scaling group with a minimum of 5 and a maximum of 30 instances. deployed 15 instances in multiple Availability Zones.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an Auto Scaling group with a minimum of 15 and a maximum of 30 instances. deployed 15 instances in multiple Availability Zones.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nCreate an Auto Scaling group with a minimum of 5 and a maximum of 30 instances. deployed 15 instances in multiple Availability Zones.\n\nAmazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. You can specify the maximum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes above this size. If you specify the desired capacity, either when you create the group or at any time thereafter, Amazon EC2 Auto Scaling ensures that your group has this many instances. If you specify scaling policies, then Amazon EC2 Auto Scaling can launch or terminate instances as demand on your application increases or decreases.\n\nFor example, the following Auto Scaling group has a minimum size of one instance, a desired capacity of two instances, and a maximum size of four instances. The scaling policies that you define adjust the number of instances, within your minimum and maximum number of instances, based on the criteria that you specify.\n\nYou can achieve high availability by deploying your applications to span across multiple Availability Zones. Using Elastic Load Balancing (ELB), you get improved fault tolerance as the ELB service automatically balances traffic across multiple instances in multiple Availability Zones, ensuring that only healthy instances receive traffic. The desired goal is to have an independent copy of each application stack in two or more AZs, with automated traffic routing to healthy resources.\n\nIn our scenario, 30 EC2 instances can handle the maximum workload, 15 instances can handle regular traffic and the minimum workload can handle by 5 instances.\n\n\n\n\nIncorrect Options:\n\nCreate an Auto Scaling group with a minimum of 5 and a maximum of 15 instances. deployed 5 instances in multiple Availability Zones.\n\nCreate an Auto Scaling group with a minimum of 5 and a maximum of 30 instances. deployed 30 instances in multiple Regions.\n\nCreate an Auto Scaling group with a minimum of 15 and a maximum of 30 instances. deployed 15 instances in multiple Availability Zones.\n\nAll above are incorrect.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\n\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/regions-and-azs.html",
    "correctAnswerExplanation": {
      "answer": "Create an Auto Scaling group with a minimum of 5 and a maximum of 30 instances. deployed 15 instances in multiple Availability Zones.",
      "explanation": "Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. You can specify the maximum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes above this size. If you specify the desired capacity, either when you create the group or at any time thereafter, Amazon EC2 Auto Scaling ensures that your group has this many instances. If you specify scaling policies, then Amazon EC2 Auto Scaling can launch or terminate instances as demand on your application increases or decreases."
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Create an Auto Scaling group with a minimum of 5 and a maximum of 15 instances. deployed 5 instances in multiple Availability Zones.",
        "explanation": "<strong>Create an Auto Scaling group with a minimum of 5 and a maximum of 30 instances. deployed 30 instances in multiple Regions.</strong>"
      },
      {
        "answer": "Create an Auto Scaling group with a minimum of 15 and a maximum of 30 instances. deployed 15 instances in multiple Availability Zones.",
        "explanation": "All above are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html",
      "https://docs.aws.amazon.com/documentdb/latest/developerguide/regions-and-azs.html"
    ]
  },
  {
    "id": 55,
    "question": "A company wants to deploy its business infrastructure on Amazon EC2 instances. Businesses require high availability and automatic scaling based on demand which must be cost-effective.\n\nAs a solution architect, what solution do you take to meet the requirements?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use an Application Load Balancer in front of an Auto Scaling group and deploy instances in multiple Regions.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an Amazon CloudFront in front of an Auto Scaling group and deploy instances in multiple Regions.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use an Amazon API Gateway in front of an Auto Scaling group and deploy instances in multiple Availability Zones.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use an Application Load Balancer in front of an Auto Scaling group and deploy instances in multiple Availability Zones.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nUse an Application Load Balancer in front of an Auto Scaling group and deploy instances in multiple Availability Zones.\n\nSo in our case, we should use an Application Load Balancer to distribute user traffic to multiple AZs and it must be placed in front of an Auto Scaling group. Automated scaling groups constantly monitor and automatically create instances when traffic increases and terminate instances when traffic decreases.\n\nApplication Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Application Load Balancing scales your load balancer as your incoming traffic changes over time.\n\nAmazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. You can specify the maximum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes above this size. If you specify the desired capacity, either when you create the group or at any time thereafter, Amazon EC2 Auto Scaling ensures that your group has this many instances. If you specify scaling policies, then Amazon EC2 Auto Scaling can launch or terminate instances as demand on your application increases or decreases.\n\nFor example, the following Auto Scaling group has a minimum size of one instance, a desired capacity of two instances, and a maximum size of four instances. The scaling policies that you define adjust the number of instances, within your minimum and maximum number of instances, based on the criteria that you specify.\n\nSo in our case, we should use an Application Load Balancer to distribute user traffic to multiple AZs and it must be placed in front of an Auto Scaling group. Automated scaling groups constantly monitor and automatically create instances when traffic increases and terminate instances when traffic decreases.\n\n\n\n\nIncorrect Options:\n\nUse an Amazon API Gateway in front of an Auto Scaling group and deploy instances in multiple Availability Zones - Amazon API gateway doesn’t offer load balancing for EC2 instances.\n\nUse an Application Load Balancer in front of an Auto Scaling group and deploy instances in multiple Regions - Auto Scaling group can be used for multiple Availability Zones in a single region. It cannot work with multiple Regions.\n\nUse an Amazon CloudFront in front of an Auto Scaling group and deploy instances in multiple Regions - CloudFront is a CDN based service and it cannot use for load balancing.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/elasticloadbalancing\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html",
    "correctAnswerExplanation": {
      "answer": "Use an Application Load Balancer in front of an Auto Scaling group and deploy instances in multiple Availability Zones.",
      "explanation": "So in our case, we should use an Application Load Balancer to distribute user traffic to multiple AZs and it must be placed in front of an Auto Scaling group. Automated scaling groups constantly monitor and automatically create instances when traffic increases and terminate instances when traffic decreases."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/elasticloadbalancing",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html"
    ]
  },
  {
    "id": 56,
    "question": "An organization operates an application in an on-premises data center that collects medical data. This data is stored on Network Attached Storage (NAS) which collects data around 10 TB each day. The company intends to upload this data to Amazon S3, where the data will be processed using an analytics application. The Data must be transferred securely.\n\nWhich solution should be used so that data transfer will be reliable and time-efficient?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS DataSync via AWS Direct Connect.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon S3 Transfer Acceleration on the Internet.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Order AWS Snowball devices.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Database Migration Service on the Internet.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nUse AWS DataSync via AWS Direct Connect\n\nAWS DataSync is a secure, online service that automates and accelerates moving data between on premises and AWS Storage services. DataSync can copy data between Network File System (NFS) shares, Server Message Block (SMB) shares, Hadoop Distributed File Systems (HDFS), self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, Amazon FSx for Windows File Server file systems, Amazon FSx for Lustre file systems, and Amazon FSz for OpenZFS file systems.\n\nYou can use AWS DataSync with your Direct Connect link to access public service endpoints or private VPC endpoints. When using VPC endpoints, data transferred between the DataSync agent and AWS services does not traverse the public internet or need public IP addresses, increasing the security of data as it is copied over the network.\n\nHow it works\n\n\n\n\nIncorrect Options:\n\nOrder AWS Snowball devices - About 10 TB of data is generated every day and we need to send data to S3 when these data are collected. AWS Snowball is not time efficient, it takes time to ship these devices. So this is incorrect.\n\nUse AWS Database Migration Service on the Internet - AWS Database Migration Service is used for migrating database. So it is incorrect.\n\nUse Amazon S3 Transfer Acceleration on the Internet - Data transfer through the public Internet is not provided in a fast or secure way. This option is incorrect.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/datasync\n\nhttps://aws.amazon.com/datasync/faqs",
    "correctAnswerExplanation": {
      "answer": "Use AWS DataSync via AWS Direct Connect",
      "explanation": "AWS DataSync is a secure, online service that automates and accelerates moving data between on premises and AWS Storage services. DataSync can copy data between Network File System (NFS) shares, Server Message Block (SMB) shares, Hadoop Distributed File Systems (HDFS), self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, Amazon FSx for Windows File Server file systems, Amazon FSx for Lustre file systems, and Amazon FSz for OpenZFS file systems."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/datasync",
      "https://aws.amazon.com/datasync/faqs"
    ]
  },
  {
    "id": 57,
    "question": "An IT team created an application and deployed in an Amazon EC2 instance. The application stores its data in an Amazon S3 bucket. A developer store the S3 credential in a configuration file in the root EBS volume. A solution architect has been asked to redesign for S3 certification to follow security compliance.\n\nWhat solution will the Solution Architect do to achieve security compliance?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an IAM role to grant permission to access the S3 bucket and attach it with the EC2 instance.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Install an Amazon-trusted root certificate on the instance and enabled SSL/TLS encrypted connections to the S3 bucket policy.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Store the credential to an Amazon RDS. Create an IAM role with permission to access database and attach it to the EC2 instance.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Attach an additional EBS volume to the EC2 instance with encryption enabled and move the configuration file to the encrypted volume.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nCreate an IAM role to grant permission to access the S3 bucket and attach it with the EC2 instance.\n\nYou should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials such as access keys to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to Amazon S3 Bucket. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can use the role-supplied temporary credentials to make requests.\n\nIn the above figure, a developer runs an application on an EC2 instance that requires access to the S3 bucket named photos. An administrator creates the Get-pics service role and attaches the role to the EC2 instance. The role includes a permissions policy that grants read-only access to the specified S3 bucket. It also includes a trust policy that allows the EC2 instance to assume the role and retrieve the temporary credentials. When the application runs on the instance, it can use the role's temporary credentials to access the photos bucket. The administrator doesn't have to grant the developer permission to access the photos bucket, and the developer never has to share or manage credentials.\n\n\n\n\nIncorrect Options:\n\nStore the credential to an Amazon RDS. Create an IAM role with permission to access database and attach it to the EC2 instance - It does not increase the security of credential. It just relocates the credential, the contents are still unsecured. So it is an insecure process.\n\nAttach an additional EBS volume to the EC2 instance with encryption enabled and move the configuration file to the encrypted volume - This only works with encryption at rest, it will still be read, and the credential is insecure.\n\nInstall an Amazon-trusted root certificate on the instance and enabled SSL/TLS encrypted connections to the S3 bucket policy - This method is still insecure and not good practice as AWS recommended. You should use the IAM role instead of direct credential.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html",
    "correctAnswerExplanation": {
      "answer": "Create an IAM role to grant permission to access the S3 bucket and attach it with the EC2 instance.",
      "explanation": "You should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials such as access keys to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to Amazon S3 Bucket. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can use the role-supplied temporary credentials to make requests."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html"
    ]
  },
  {
    "id": 58,
    "question": "A company is running an application on Amazon EC2 in the US region (us-east-1). Now they want to increase the business audience in Europe, so they need to deploy another EC2 instance in eu-west-2. What steps should a solution architect take to meet the requirements? (Select two)",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "launch a new EC2 instance from the Amazon Machine Image (AMI) in the second region.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Copy the Amazon EBS volume from the Amazon S3 and launch an EC2 instance in the second region using the volume.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Launch a new EC2 instance in the second region and copy the volume from Amazon S3 to the new instance.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon Machine Image (AMI) from an EC2 instance and copy it to the second region.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Detach the EBS volume from the EC2 instance and copy it to an Amazon S3 bucket in the second region.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nCreate an Amazon Machine Image (AMI) from an EC2 instance and copy it to the second region\n\nlaunch a new EC2 instance from the Amazon Machine Image (AMI) in the second region\n\nYou can copy an Amazon Machine Image (AMI) within or across AWS Regions using the AWS Management Console, the AWS Command Line Interface or SDKs, or the Amazon EC2 API. After that, you can launch an instance from the AMI.\n\nWith an Amazon EBS-backed AMI, each of its backing snapshots is copied to an identical but distinct target snapshot. If you copy an AMI to a new Region, the snapshots are complete (non-incremental) copies. If you encrypt unencrypted backing snapshots or encrypt them to a new KMS key, the snapshots are complete (non-incremental) copies. Subsequent copy operations of an AMI result in incremental copies of the backing snapshots.\n\nCopying an AMI across geographically diverse Regions provides the following benefits:\n\nConsistent global deployment: Copying an AMI from one Region to another enables you to launch consistent instances in different Regions based on the same AMI.\n\nScalability: You can more easily design and build global applications that meet the needs of your users, regardless of their location.\n\nPerformance: You can increase performance by distributing your application, as well as locating critical components of your application in closer proximity to your users. You can also take advantage of Region-specific features, such as instance types or other AWS services.\n\nHigh availability: You can design and deploy applications across AWS Regions, to increase availability.\n\n\n\n\nIncorrect Options:\n\nDetach the EBS volume from the EC2 instance and copy it to an Amazon S3 bucket in the second region - AWS does not offer to directly copy an EBS volume to S3 bucket.\n\nLaunch a new EC2 instance in the second region and copy the volume from Amazon S3 to the new instance - You cannot directly create an EBS volume from Amazon S3 bucket.\n\nCopy the Amazon EBS volume from the Amazon S3 and launch an EC2 instance in the second region using the volume - You cannot directly copy an EBS volume.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html",
    "correctAnswerExplanation": {
      "answer": "Create an Amazon Machine Image (AMI) from an EC2 instance and copy it to the second region",
      "explanation": "<strong>launch a new EC2 instance from the Amazon Machine Image (AMI) in the second region</strong>"
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html"
    ]
  },
  {
    "id": 59,
    "question": "An organization decided to migrate its business to AWS Cloud from its on-premises data center. An AWS expert has been requested to provide security best practices to set up a new AWS account.\n\nWhich of the following recommendation should be followed while creating the AWS account? (Select two)",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create one IAM user for each department so that they can be managed easily.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Encrypt the access keys and store them in external sources.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Enable Multi-Factor Authentication (MFA) for all AWS accounts.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Share root user access keys to the IT department for accessing all AWS resources.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Configure account password policy for complexity requirements and mandatory rotation.",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nEnable Multi-Factor Authentication (MFA) for all AWS accounts - For increased security, you should enable multi-factor authentication (MFA) to protect your AWS resources. MFA adds an extra layer of protection on top of your user name and password.\n\nConfigure account password policy for complexity requirements and mandatory rotation - If you allow users to change their own passwords, create a custom password policy that requires them to create strong passwords and rotate their passwords periodically.\n\n\n\n\nIncorrect Options:\n\nEncrypt the access keys and store them in external sources - You should not store access keys to an external source. You should rotate the key regularly.\n\nShare root user access keys to the IT department for accessing all AWS resources - AWS does not recommend sharing root user access key with others. The root user has full administrative permission by default, so it could be risked. To allow your users individual programmatic access, create an IAM user with personal access keys.\n\nCreate one IAM user for each department so that they can be managed easily - It is not good practice to create department-wise IAM users. You should create individual IAM users and group them with the IAM Group.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html"
    ]
  },
  {
    "id": 60,
    "question": "A development team has developed an application that is deployed in multiple Amazon EC2 instances with the Auto Scaling group behind an application load balancer. All static contents are stored in the Amazon S3 bucket, and application data is stored in the Amazon DynamoDB. After six months, the usage report found that over 90% of the read requests were coming from worldwide users, and they faced performance issues.\n\nWhich following action would you recommend for the most effective solution to increase performance?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use CloudFront for S3 and DynamoDB Accelerator (DAX) for DynamoDB",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use ElastiCache Memcached for S3 and DAX for DynamoDB",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use ElastiCache Redis for DynamoDB and CloudFront for S3",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct Options:\n\nUse CloudFront for S3 and DynamoDB Accelerator (DAX) for DynamoDB\n\nAmazon CloudFront speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.\n\nWhen a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content. Then, for the next local request for the same content, it’s already cached nearby and can be served immediately.\n\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second.\n\nDAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.\n\nThough DynamoDB offers consistent single-digit-millisecond latency, DynamoDB + DAX takes performance to the next level with response times in microseconds for millions of requests per second for read-heavy workloads. With DAX, your applications remain fast and responsive, even when a popular event or news story drives unprecedented request volumes your way. No tuning required.\n\n\n\n\nIncorrect Options:\n\nUse ElastiCache Redis for DynamoDB and CloudFront for S3\n\nUse ElastiCache Memcached for S3 and DAX for DynamoDB\n\nUse ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3\n\nAll above options are incorrect.\n\n\n\n\nAmazon ElastiCache for Redis - Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. It combines the speed, simplicity, and versatility of open-source Redis with manageability, security, and scalability from Amazon to power the most demanding real-time applications in Gaming, Ad-Tech, E-Commerce, Healthcare, Financial Services, and IoT. Although, you can use Amazon ElastiCache for Redis to cache application data. but DAX is much better for caching DynamoDB because it is fully managed service.\n\nAmazon ElastiCache for Memcached - Amazon ElastiCache for Memcached is a Memcached-compatible, in-memory, key-value store service that can be used as a cache or a data store. It delivers the performance, ease-of-use, and simplicity of Memcached. ElastiCache for Memcached is fully managed, scalable, and secure - making it an ideal candidate for use cases where frequently accessed data must be in-memory. Amazon ElastiCache for Memcached cannot be used to cache any static objects like the Amazon S3 objects.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/cloudfront\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud\n\nhttps://aws.amazon.com/dynamodb/dax",
    "correctAnswerExplanation": {
      "answer": "Use CloudFront for S3 and DynamoDB Accelerator (DAX) for DynamoDB",
      "explanation": "Amazon CloudFront speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance."
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Use ElastiCache Redis for DynamoDB and CloudFront for S3",
        "explanation": "<strong>Use ElastiCache Memcached for S3 and DAX for DynamoDB</strong>"
      },
      {
        "answer": "Use ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3",
        "explanation": "All above options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/cloudfront",
      "https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud",
      "https://aws.amazon.com/dynamodb/dax"
    ]
  },
  {
    "id": 61,
    "question": "A multinational company has many branches all over the world. They use a centralized distribution system to manage and analyze data on a daily basis. Every day, each branch uploads a large dataset to an Amazon S3 bucket. The managing application is deployed in the US-East-1 region. Data uploading from other regions is experiencing low performance, and they want to improve data uploading in the fastest way for all branches.\n\nWhich option can meet the requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS direct connect to establish a connection between AWS and all branches",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Upload data to a closest S3 bucket, then merge all data to the central bucket",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Enable Transfer Acceleration to S3 bucket",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use Site-to-Site VPN connection to upload data",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nEnable Transfer Acceleration to S3 bucket - Amazon S3 is object storage built to store and retrieve any amount of data from anywhere. It’s a simple storage service that offers industry leading durability, availability, performance, security, and virtually unlimited scalability at very low costs.\n\nAmazon S3 supports Transfer Acceleration that can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Customers who have either web or mobile applications with widespread users or applications hosted far away from their S3 bucket can experience long and variable upload and download speeds over the Internet. S3 Transfer Acceleration (S3TA) reduces the variability in Internet routing, congestion and speeds that can affect transfers, and logically shortens the distance to S3 for remote applications.\n\nAmazon S3 Transfer Acceleration is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration is designed to optimize transfer speeds from across the world into S3 buckets. Transfer Acceleration takes advantage of the globally distributed edge locations in Amazon CloudFront. As the data arrives at an edge location, the data is routed to Amazon S3 over an optimized network path.\n\nYou can use S3 Accelerate Speed Comparison tools to test speed.\n\nhttps://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html\n\n\n\n\nIncorrect Options:\n\nUpload data to a closest S3 bucket, then merge all data to the central bucket - It can be more expensive and complicated to manage data. You need a separate application or lambda function to merge data from the local bucket to the central bucket.\n\nuse Site-to-Site VPN connection to upload data - Site-to-site VPN can only establish a secure connection between an on-premises network and Amazon VPC. But it does not improve performance.\n\nUse AWS direct connect to establish a connection between AWS and all branches - You can use AWS Direct Connection to establish a connection between AWS and all branches. But it can be more expensive than Amazon S3 Transfer Acceleration.\n\nAll above are incorrect.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/transfer-acceleration\n\nhttps://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html",
    "correctAnswerExplanation": null,
    "incorrectAnswerExplanations": [],
    "references": [
      "https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html",
      "https://aws.amazon.com/s3/transfer-acceleration",
      "https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html"
    ]
  },
  {
    "id": 62,
    "question": "A business is looking for a process for on-premises backup of its infrastructure. An architect has been requested to provide a solution to reduce the cost of backups on AWS. Existing backup applications and workflows must continue to function.\n\nWhich solution should be provided for this requirement?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use the AWS storage gateway with an iSCSI-Virtual Tape Library (VTL) and connect to the backup application.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use the Amazon EFS file system and connect the application using the NFS protocol.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use the AWS storage gateway and connect to the backup application using the NFS protocol.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use the Amazon EFS file system and connect the application using the iSCSI protocol.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nUse the AWS storage gateway with an iSCSI-Virtual Tape Library (VTL) and connect to the backup application\n\nTape Gateway enables you to replace using physical tapes on premises with virtual tapes in AWS without changing existing backup workflows. Tape Gateway supports all leading backup applications and caches virtual tapes on premises for low-latency data access. Tape Gateway encrypts data between the gateway and AWS for secure data transfer, and compresses data and transitions virtual tapes between Amazon S3 and Amazon S3 Glacier Flexible Retrieval, or Amazon S3 Glacier Deep Archive, to minimize storage costs.\n\nTape Gateway emulates physical tape libraries, removes the cost and complexity of managing physical tape infrastructure, and provides more durability than physical tapes. Tape Gateway presents cloud-backed storage through an iSCSI-based virtual tape library to your on-premises backup application. You can backup data directly from your backup application to Tape Gateway. Tape Gateway asynchronously copies your backups to AWS and enables you to scale backup storage needs on-premises with a pay-as-you-go pricing.\n\nHow it works\n\n\n\n\nIncorrect Options:\n\nUse the Amazon EFS file system and connect the application using the NFS protocol.\n\nUse the Amazon EFS file system and connect the application using the iSCSI protocol.\n\nUse the AWS storage gateway and connect to the backup application using the NFS protocol.\n\nAll above options are incorrect.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/storagegateway/vtl",
    "correctAnswerExplanation": {
      "answer": "Use the AWS storage gateway with an iSCSI-Virtual Tape Library (VTL) and connect to the backup application",
      "explanation": "Tape Gateway enables you to replace using physical tapes on premises with virtual tapes in AWS without changing existing backup workflows. Tape Gateway supports all leading backup applications and caches virtual tapes on premises for low-latency data access. Tape Gateway encrypts data between the gateway and AWS for secure data transfer, and compresses data and transitions virtual tapes between Amazon S3 and Amazon S3 Glacier Flexible Retrieval, or Amazon S3 Glacier Deep Archive, to minimize storage costs."
    },
    "incorrectAnswerExplanations": [
      {
        "answer": "Use the Amazon EFS file system and connect the application using the NFS protocol.",
        "explanation": "<strong>Use the Amazon EFS file system and connect the application using the iSCSI protocol.</strong>"
      },
      {
        "answer": "Use the AWS storage gateway and connect to the backup application using the NFS protocol.",
        "explanation": "All above options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/storagegateway/vtl"
    ]
  },
  {
    "id": 63,
    "question": "A business is running in the AWS cloud using Amazon EC2 instances, Auto Scaling Group, Application Load Balancer, Amazon S3, Amazon CloudFront, and Route 53. Recently, the security team found some issues with poor deployment and configuration practices within the VPC. As a Solution Architect, you are requested to ensure that the application maintains a secure configuration.\n\nWhich solution do you offer that is the most effective method?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Manually check all applications and service configurations before deployment.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Remove the permission of staff to deploy applications.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS Inspector for applying secure configurations.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use CloudFormation with securely configured templates.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Applications and Architectures",
    "explanation": "Correct Options:\n\nUse CloudFormation with securely configured templates\n\nAWS CloudFormation is a service that helps you model and set up your AWS resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and CloudFormation takes care of provisioning and configuring those resources for you. You don't need to individually create and configure AWS resources and figure out what's dependent on what; CloudFormation handles that.\n\nHow it works\n\nThe CloudFormation templates help you create and manage the appropriate security configuration for your resource. You can repeatedly deploy resources with the same settings which can reduce the risk of human error.\n\n\n\n\nIncorrect Options:\n\nRemove the permission of staff to deploy applications - Removing the permission of staff to deploy applications does not help you to deploy applications securely.\n\nManually check all applications and service configurations before deployment - Manual checking is not an effective way to check all applications and resources for security vulnerabilities.\n\nUse AWS Inspector for applying secure configurations - Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities. But it does not actually secure your application and resources.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/cloudformation/resources/templates\n\nhttps://aws.amazon.com/cloudformation",
    "correctAnswerExplanation": {
      "answer": "Use CloudFormation with securely configured templates",
      "explanation": "AWS CloudFormation is a service that helps you model and set up your AWS resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and CloudFormation takes care of provisioning and configuring those resources for you. You don't need to individually create and configure AWS resources and figure out what's dependent on what; CloudFormation handles that."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/cloudformation/resources/templates",
      "https://aws.amazon.com/cloudformation"
    ]
  },
  {
    "id": 64,
    "question": "An e-commerce application is hosted on AWS. The application experienced a performance issue last weekend due to heavy traffic. The CTO has decided to double capacity every weekend so that the application does not face any performance issues.\n\nWhat is the most effective way to confirm requirements?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Add a Step Scaling policy",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Add a Scheduled Scaling action",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Add a Simple Scaling policy",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Add double Amazon EC2 instances",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct Options:\n\nAdd a Scheduled Scaling action\n\nScheduled scaling helps you to set up your own scaling schedule according to predictable load changes. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can configure a schedule for Amazon EC2 Auto Scaling to increase capacity on Wednesday and decrease capacity on Friday.\n\nTo use scheduled scaling, you create scheduled actions. Scheduled actions are performed automatically as a function of date and time. When you create a scheduled action, you specify when the scaling activity should occur and the new desired, minimum, and maximum sizes for the scaling action. You can create scheduled actions that scale one time only or that scale on a recurring schedule.\n\n\n\n\nIncorrect Options:\n\nAdd a Step Scaling policy - The Step Scaling policy increase or decrease the current capacity of the group based on a set of scaling adjustments, known as step adjustments, that vary based on the size of the alarm breach. It is suitable when traffic is unpredictable.\n\nAdd a Simple Scaling policy - Increase or decrease the current capacity of the group based on a single scaling adjustment. Amazon EC2 Auto Scaling originally supported only simple scaling policies. If you created your scaling policy before target tracking and step policies were introduced, your policy is treated as a simple scaling policy.\n\nAdd double Amazon EC2 instances - Adding double Amazon EC2 instances can improve performance. This additional instance will continue all the time and it will increase the cost. In our case, we only need double instance size on weekends. So this option is incorrect.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html",
    "correctAnswerExplanation": {
      "answer": "Add a Scheduled Scaling action",
      "explanation": "Scheduled scaling helps you to set up your own scaling schedule according to predictable load changes. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can configure a schedule for Amazon EC2 Auto Scaling to increase capacity on Wednesday and decrease capacity on Friday."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html"
    ]
  },
  {
    "id": 65,
    "question": "A startup uses an Amazon RDS database for an application. Sometimes they face many disruptions in database availability. What is a possible way to prevent losing database availability?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a read replica",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Increase database instance size",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a snapshot of the database",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Enable multi-AZ deployment",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct Options:\n\nEnable multi-AZ deployment\n\nAmazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments with a single standby DB instance. Amazon RDS uses several different technologies to provide this failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use the Amazon failover technology. Microsoft SQL Server DB instances use SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs).\n\nIn a Multi-AZ DB instance deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance. It can also help protect your databases against DB instance failure and Availability Zone disruption.\n\n\n\n\nIncorrect Options:\n\nCreate a snapshot of the database - Creating a snapshot is a process of manually backing up a database. It does not offer the advantage of high availability features.\n\nIncrease database instance size - This ability offers to increase the DB instance capacity size. So doing this, the database can store more data. But it doesn't provide high availability and failover support\n\nCreate a read replica - Reed Replica provides enhanced performance for read-heavy database workloads. This may work for data read operation, but it does not work when you try to write data when the availability zone is down.\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/multi-az\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html",
    "correctAnswerExplanation": {
      "answer": "Enable multi-AZ deployment",
      "explanation": "Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments with a single standby DB instance. Amazon RDS uses several different technologies to provide this failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use the Amazon failover technology. Microsoft SQL Server DB instances use SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs)."
    },
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/rds/features/multi-az",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html"
    ]
  }
]