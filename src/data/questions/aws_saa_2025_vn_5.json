[
  {
    "id": 261,
    "question": "A company has an online gaming application that has TCP and UDP multiplayer gaming capabilities. The company uses Amazon Route 53 to point the application traffic to multiple Network Load Balancers (NLBs) in different AWS Regions. The company needs to improve application performance and decrease latency for the online game in preparation for user growth.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Add an Amazon CloudFront distribution in front of the NLBs. Increase the Cache-Control max-age parameter.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Replace the NLBs with Application Load Balancers (ALBs). Configure Route 53 to use latency-based routing.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Add AWS Global Accelerator in front of the NLBs. Configure a Global Accelerator endpoint to use the correct listener ports.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Add an Amazon API Gateway endpoint behind the NLBs. Enable API caching. Override method caching for the different stages.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Add AWS Global Accelerator in front of the NLBs. Configure a Global Accelerator endpoint to use the correct listener ports.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 262,
    "question": "A company needs to integrate with a third-party data feed. The data feed sends a webhook to notify an external service when new data is ready for consumption. A developer wrote an AWS Lambda function to retrieve data when the company receives a webhook callback. The developer must make the Lambda function available for the third party to call.\n\nWhich solution will meet these requirements with the MOST operational efficiency? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Create a function URL for the Lambda function. Provide the Lambda function URL to the third party for the webhook.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Deploy an Application Load Balancer (ALB) in front of the Lambda function. Provide the ALB URL to the third party for the webhook.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Attach the topic to the Lambda function. Provide the public hostname of the SNS topic to the third party for the webhook.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Attach the queue to the Lambda function. Provide the public hostname of the SQS queue to the third party for the webhook.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Create a function URL for the Lambda function. Provide the Lambda function URL to the third party for the webhook.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 263,
    "question": "A company has a workload in an AWS Region. Customers connect to and access the workload by using an Amazon API Gateway REST API. The company uses Amazon Route 53 as its DNS provider. The company wants to provide individual and secure URLs for all customers.\n\nWhich combination of steps will meet these requirements with the MOST operational efficiency? (Choose three.) 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Register the required domain in a registrar. Create a wildcard custom domain name in a Route 53 hosted zone and record in the zone that points to the API Gateway endpoint.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Request a wildcard certificate that matches the domains in AWS Certificate Manager (ACM) in a different Region.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Create hosted zones for each customer as required in Route 53. Create zone records that point to the API Gateway endpoint.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM) in the same Region.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 5,
        "answer": "Create multiple API endpoints for each customer in API Gateway.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 6,
        "answer": "Create a custom domain name in API Gateway for the REST API. Import the certificate from AWS Certificate Manager (ACM).",
        "correct": true,
        "explanation": ""
      }
    ],
    "corrects": [
      1,
      4,
      6
    ],
    "multiple": true,
    "correctAnswerExplanations": [
      {
        "answer": "Register the required domain in a registrar. Create a wildcard custom domain name in a Route 53 hosted zone and record in the zone that points to the API Gateway endpoint.",
        "explanation": ""
      },
      {
        "answer": "Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM) in the same Region.",
        "explanation": ""
      },
      {
        "answer": "Create a custom domain name in API Gateway for the REST API. Import the certificate from AWS Certificate Manager (ACM).",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 264,
    "question": "A company stores data in Amazon S3. According to regulations, the data must not contain personally identifiable information (PII). The company recently discovered that S3 buckets have some objects that contain PII. The company needs to automatically detect PII in S3 buckets and to notify the company’s security team.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Macie. Create an Amazon EventBridge rule to filter the SensitiveData event type from Macie findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use Amazon GuardDuty. Create an Amazon EventBridge rule to filter the CRITICAL event type from GuardDuty findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use Amazon Macie. Create an Amazon EventBridge rule to filter the SensitiveData:S3Object/Personal event type from Macie findings and to send an Amazon Simple Queue Service (Amazon SQS) notification to the security team.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use Amazon GuardDuty. Create an Amazon EventBridge rule to filter the CRITICAL event type from GuardDuty findings and to send an Amazon Simple Queue Service (Amazon SQS) notification to the security team.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon Macie. Create an Amazon EventBridge rule to filter the SensitiveData event type from Macie findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 265,
    "question": "A company wants to build a logging solution for its multiple AWS accounts. The company currently stores logs from all accounts in a centralized account. An Amazon S3 bucket has been created in the centralized account to store VPC flow logs and AWS CloudTrail logs. All logs must be immediately accessible for 30 days for frequent analysis, with reduced access afterward but still requiring high availability, and deleted 90 days after creation. Which solution will most cost-effectively meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) classs 30 days after creation. Write an expiration action that directs Amazon S3 to delete objects after 90 days.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class 30 days after creation. Move all objects to the S3 Glacier Flexible Retrieval storage class after 90 days. Write an expiration action that directs Amazon S3 to delete objects after 90 days.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Transition objects to the S3 Glacier Flexible Retrieval storage class 30 days after creation. Write an expiration action that directs Amazon S3 to delete objects after 90 days.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Transition objects to the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class 30 days after creation. Move all objects to the S3 Glacier Flexible Retrieval storage class after 90 days. Write an expiration action that directs Amazon S3 to delete objects after 90 days.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) classs 30 days after creation. Write an expiration action that directs Amazon S3 to delete objects after 90 days.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 266,
    "question": "A company is building an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for its workloads. All secrets that are stored in Amazon EKS must be encrypted in the Kubernetes etcd key-value store.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Create a new AWS Key Management Service (AWS KMS) key. Use AWS Secrets Manager to manage, rotate, and store all secrets in Amazon EKS.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Create a new AWS Key Management Service (AWS KMS) key. Enable Amazon EKS KMS secrets encryption on the Amazon EKS cluster.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Create the Amazon EKS cluster with default options. Use the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver as an add-on.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Create a new AWS Key Management Service (AWS KMS) key with the alias/aws/ebs alias. Enable default Amazon Elastic Block Store (Amazon EBS) volume encryption for the account.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Create a new AWS Key Management Service (AWS KMS) key. Enable Amazon EKS KMS secrets encryption on the Amazon EKS cluster.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 267,
    "question": "A company wants to provide data scientists with near real-time read-only access to the company's production Amazon RDS for PostgreSQL database. The database is currently configured as a Single-AZ database. The data scientists use complex queries that will not affect the production database. The company needs a solution that is highly available.\n\nWhich solution will meet these requirements MOST cost-effectively? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Scale the existing production database in a maintenance window to provide enough power for the data scientists.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Change the setup from a Single-AZ to a Multi-AZ instance deployment with a larger secondary standby instance. Provide the data scientists access to the secondary instance.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Change the setup from a Single-AZ to a Multi-AZ instance deployment. Provide two additional read replicas for the data scientists.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Change the setup from a Single-AZ to a Multi-AZ cluster deployment with two readable standby instances. Provide read endpoints to the data scientists.",
        "correct": true,
        "explanation": ""
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Change the setup from a Single-AZ to a Multi-AZ cluster deployment with two readable standby instances. Provide read endpoints to the data scientists.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 268,
    "question": "A company runs a three-tier web application in the AWS Cloud that operates across three Availability Zones. The application architecture has an Application Load Balancer, an Amazon EC2 web server that hosts user session states, and a MySQL database that runs on an EC2 instance. The company expects sudden increases in application traffic. The company wants to be able to scale to meet future application capacity demands and to ensure high availability across all three Availability Zones.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment. Use Amazon ElastiCache for Redis with high availability to store session data and to cache reads. Migrate the web server to an Auto Scaling group that is in three Availability Zones.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment. Use Amazon ElastiCache for Memcached with high availability to store session data and to cache reads. Migrate the web server to an Auto Scaling group that is in three Availability Zones.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Migrate the MySQL database to Amazon DynamoDB Use DynamoDB Accelerator (DAX) to cache reads. Store the session data in DynamoDB. Migrate the web server to an Auto Scaling group that is in three Availability Zones.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Migrate the MySQL database to Amazon RDS for MySQL in a single Availability Zone. Use Amazon ElastiCache for Redis with high availability to store session data and to cache reads. Migrate the web server to an Auto Scaling group that is in three Availability Zones.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment. Use Amazon ElastiCache for Redis with high availability to store session data and to cache reads. Migrate the web server to an Auto Scaling group that is in three Availability Zones.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 269,
    "question": "A global video streaming company uses Amazon CloudFront as a content distribution network (CDN). The company wants to roll out content in a phased manner across multiple countries. The company needs to ensure that viewers who are outside the countries to which the company rolls out content are not able to view the content.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Add geographic restrictions to the content in CloudFront by using an allow list. Set up a custom error message.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Set up a new URL tor restricted content. Authorize access by using a signed URL and cookies. Set up a custom error message.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Encrypt the data for the content that the company distributes. Set up a custom error message.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Create a new URL for restricted content. Set up a time-restricted access policy for signed URLs.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Add geographic restrictions to the content in CloudFront by using an allow list. Set up a custom error message.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 270,
    "question": "A company wants to use the AWS Cloud to improve its on-premises disaster recovery (DR) configuration. The company's core production business application uses Microsoft SQL Server Standard, which runs on a virtual machine (VM). The application has a recovery point objective (RPO) of 30 seconds or fewer and a recovery time objective (RTO) of 60 minutes. The DR solution needs to minimize costs wherever possible.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Configure a multi-site active/active setup between the on-premises server and AWS by using Microsoft SQL Server Enterprise with Always On availability groups.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Configure a warm standby Amazon RDS for SQL Server database on AWS. Configure AWS Database Migration Service (AWS DMS) to use change data capture (CDC).",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use AWS Elastic Disaster Recovery configured to replicate disk changes to AWS as a pilot light.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use third-party backup software to capture backups every night. Store a secondary set of backups in Amazon S3.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Elastic Disaster Recovery configured to replicate disk changes to AWS as a pilot light.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 271,
    "question": "A company has an on-premises server that uses an Oracle database to process and store customer information. The company wants to use an AWS database service to achieve higher availability and to improve application performance. The company also wants to offload reporting from its primary database system.cần replicate\n\nWhich solution will meet these requirements in the MOST operationally efficient way? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Database Migration Service (AWS DMS) to create an Amazon RDS DB instance in multiple AWS Regions. Point the reporting functions toward a separate DB instance from the primary DB instance.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use Amazon RDS in a Multi-AZ deployment (One standby) to create an Oracle database. Create a read replica in the same zone as the primary DB instance. Direct the reporting functions to the read replica.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use Amazon RDS deployed in a Multi-AZ cluster deployment to create an Oracle database. Direct the reporting functions to use the reader instance in the cluster deployment.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use Amazon RDS deployed in a Multi-AZ instance deployment to create an Amazon Aurora database. Direct the reporting functions to the reader instances.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon RDS in a Multi-AZ deployment (One standby) to create an Oracle database. Create a read replica in the same zone as the primary DB instance. Direct the reporting functions to the read replica.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 272,
    "question": "A company wants to build a web application on AWS. Client access requests to the website are not predictable and can be idle for a long time. Only customers who have paid a subscription fee can have the ability to sign in and use the web application.\n\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose three.) 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS Lambda function to retrieve user information from Amazon DynamoDB. Create an Amazon API Gateway endpoint to accept RESTful APIs. Send the API calls to the Lambda function.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Create an Amazon Elastic Container Service (Amazon ECS) service behind an Application Load Balancer to retrieve user information from Amazon RDS. Create an Amazon API Gateway endpoint to accept RESTful APIs. Send the API calls to the Lambda function.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Create an Amazon Cognito user pool to authenticate users.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Create an Amazon Cognito identity pool to authenticate users.",
        "correct": null,
        "explanation": ""
      },
      {
        "id": 5,
        "answer": "Use AWS Amplify to serve the frontend web content with HTML, CSS, and JS. Use an integrated Amazon CloudFront configuration.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 6,
        "answer": "Use Amazon S3 static web hosting with PHP, CSS, and JS. Use Amazon CloudFront to serve the frontend web content.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1,
      3,
      5
    ],
    "multiple": true,
    "correctAnswerExplanations": [
      {
        "answer": "Create an AWS Lambda function to retrieve user information from Amazon DynamoDB. Create an Amazon API Gateway endpoint to accept RESTful APIs. Send the API calls to the Lambda function.",
        "explanation": ""
      },
      {
        "answer": "Create an Amazon Cognito user pool to authenticate users.",
        "explanation": ""
      },
      {
        "answer": "Use AWS Amplify to serve the frontend web content with HTML, CSS, and JS. Use an integrated Amazon CloudFront configuration.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 273,
    "question": "A media company uses an Amazon CloudFront distribution to deliver content over the internet. The company wants only premium customers to have access to the media streams and file content. The company stores all content in an Amazon S3 bucket. The company also delivers content on demand to customers for a specific purpose, such as movie rentals or music downloads.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Generate and provide S3 signed cookies to premium customers.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Generate and provide CloudFront signed URLs to premium customers.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use origin access control (OAC) to limit the access of non-premium customers.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Generate and activate field-level encryption to block non-premium customers.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Generate and provide CloudFront signed URLs to premium customers.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 274,
    "question": "A company runs Amazon EC2 instances in multiple AWS accounts that are individually bled. The company recently purchased a Savings Plan. Because of changes in the company’s business requirements, the company has decommissioned a large number of EC2 instances. The company wants to use its Savings Plan discounts on its other AWS accounts.\n\nWhich combination of steps will meet these requirements? (Choose two.) 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "From the AWS Account Management Console of the management account, turn on discount sharing from the billing preferences section.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "From the AWS Account Management Console of the account that purchased the existing Savings Plan, turn on discount sharing from the billing preferences section. Include all accounts.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "From the AWS Organizations management account, use AWS Resource Access Manager (AWS RAM) to share the Savings Plan with other accounts.",
        "correct": null,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Create an organization in AWS Organizations in a new payer account. Invite the other AWS accounts to join the organization from the management account.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 5,
        "answer": "Create an organization in AWS Organizations in the existing AWS account with the existing EC2 instances and Savings Plan. Invite the other AWS accounts to join the organization from the management account.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1,
      4
    ],
    "multiple": true,
    "correctAnswerExplanations": [
      {
        "answer": "From the AWS Account Management Console of the management account, turn on discount sharing from the billing preferences section.",
        "explanation": ""
      },
      {
        "answer": "Create an organization in AWS Organizations in a new payer account. Invite the other AWS accounts to join the organization from the management account.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 275,
    "question": "A retail company uses a regional Amazon API Gateway API for its public REST APIs. The API Gateway endpoint is a custom domain name that points to an Amazon Route 53 alias record. A solutions architect needs to create a solution that has minimal effects on customers and minimal data loss to release the new version of APIs.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Create a canary release deployment stage for API Gateway. Deploy the latest API version. Point an appropriate percentage of traffic to the canary stage. After API verification, promote the canary stage to the production stage.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Create a new API Gateway endpoint with a new version of the API in OpenAPI YAML file format. Use the import-to-update operation in merge mode into the API in API Gateway. Deploy the new version of the API to the production stage.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Create a new API Gateway endpoint with a new version of the API in OpenAPI JSON file format. Use the import-to-update operation in overwrite mode into the API in API Gateway. Deploy the new version of the API to the production stage.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Create a new API Gateway endpoint with new versions of the API definitions. Create a custom domain name for the new API Gateway API. Point the Route 53 alias record to the new API Gateway API custom domain name.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Create a canary release deployment stage for API Gateway. Deploy the latest API version. Point an appropriate percentage of traffic to the canary stage. After API verification, promote the canary stage to the production stage.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 276,
    "question": "A company wants to direct its users to a backup static error page if the company's primary website is unavailable. The primary website's DNS records are hosted in Amazon Route 53. The domain is pointing to an Application Load Balancer (ALB). The company needs a solution that minimizes changes and infrastructure overhead.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Update the Route 53 records to use a latency routing policy. Add a static error page that is hosted in an Amazon S3 bucket to the records so that the traffic is sent to the most responsive endpoints.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Set up a Route 53 active-passive failover configuration. Direct traffic to a static error page that is hosted in an Amazon S3 bucket when Route 53 health checks determine that the ALB endpoint is unhealthy.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance that hosts a static error page as endpoints. Configure Route 53 to send requests to the instance only if the health checks fail for the ALB.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Update the Route 53 records to use a multivalue answer routing policy. Create a health check. Direct traffic to the website if the health check passes. Direct traffic to a static error page that is hosted in Amazon S3 if the health check does not pass.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Set up a Route 53 active-passive failover configuration. Direct traffic to a static error page that is hosted in an Amazon S3 bucket when Route 53 health checks determine that the ALB endpoint is unhealthy.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 277,
    "question": "A recent analysis of a company's IT expenses highlights the need to reduce backup costs. The company's chief information officer wants to simplify the on-premises backup infrastructure and reduce costs by eliminating the use of physical backup tapes. The company must preserve the existing investment in the on-premises backup applications and workflows.\n\nWhat should a solutions architect recommend? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Set up AWS Storage Gateway to connect with the backup applications using the NFS interface.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Set up an Amazon EFS file system that connects with the backup applications using the NFS interface.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Set up an Amazon EFS file system that connects with the backup applications using the iSCSI interface.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Set up AWS Storage Gateway to connect with the backup applications using the iSCSI-virtual tape library (VTL) interface.",
        "correct": true,
        "explanation": ""
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Set up AWS Storage Gateway to connect with the backup applications using the iSCSI-virtual tape library (VTL) interface.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 278,
    "question": "A company has data collection sensors at different locations. The data collection sensors stream a high volume of data to the company. The company wants to design a platform on AWS to ingest and process high-volume streaming data. The solution must be scalable and support data collection in near real time. The company must store the data in Amazon S3 for future reporting.\n\nWhich solution will meet these requirements with the LEAST operational overhead? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Kinesis Data Firehose to deliver streaming data to Amazon S3.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use AWS Glue to deliver streaming data to Amazon S3.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use AWS Lambda to deliver streaming data and store the data to Amazon S3.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use AWS Database Migration Service (AWS DMS) to deliver streaming data to Amazon S3.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon Kinesis Data Firehose to deliver streaming data to Amazon S3.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 279,
    "question": "A company has separate AWS accounts for its finance, data analytics, and development departments. Because of costs and security concerns, the company wants to control which services each AWS account can use.\n\nWhich solution will meet these requirements with the LEAST operational overhead? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Systems Manager templates to control which AWS services each department can use.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Create organization units (OUs) for each department in AWS Organizations. Attach service control policies (SCPs) to the OUs.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use AWS CloudFormation to automatically provision only the AWS services that each department can use.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Set up a list of products in AWS Service Catalog in the AWS accounts to manage and control the usage of specific AWS services.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Create organization units (OUs) for each department in AWS Organizations. Attach service control policies (SCPs) to the OUs.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 280,
    "question": "A company has created a multi-tier application for its ecommerce website. The website uses an Application Load Balancer that resides in the public subnets, a web tier in the public subnets, and a MySQL cluster hosted on Amazon EC2 instances in the private subnets. The MySQL database needs to retrieve product catalog and pricing information that is hosted on the internet by a third-party provider. A solutions architect must devise a strategy that maximizes security without increasing operational overhead.\n\nWhat should the solutions architect do to meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy a NAT instance in the VPC. Route all the internet-based traffic through the NAT instance.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Deploy a NAT gateway in the public subnets. Modify the private subnet route table to direct all internet-bound traffic to the NAT gateway.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Configure an internet gateway and attach it to the VPModify the private subnet route table to direct internet-bound traffic to the internet gateway.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Configure a virtual private gateway and attach it to the VPC. Modify the private subnet route table to direct internet-bound traffic to the virtual private gateway.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Deploy a NAT gateway in the public subnets. Modify the private subnet route table to direct all internet-bound traffic to the NAT gateway.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 281,
    "question": "A company is using AWS Key Management Service (AWS KMS) keys to encrypt AWS Lambda environment variables. A solutions architect needs to ensure that the required permissions are in place to decrypt and use the environment variables.\n\nWhich steps must the solutions architect take to implement the correct permissions? (Choose two.) 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Add AWS KMS permissions in the Lambda resource policy.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Add AWS KMS permissions in the Lambda execution role.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Add AWS KMS permissions in the Lambda function policy.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Allow the Lambda execution role in the AWS KMS key policy.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 5,
        "answer": "Allow the Lambda resource policy in the AWS KMS key policy.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2,
      4
    ],
    "multiple": true,
    "correctAnswerExplanations": [
      {
        "answer": "Add AWS KMS permissions in the Lambda execution role.",
        "explanation": ""
      },
      {
        "answer": "Allow the Lambda execution role in the AWS KMS key policy.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 282,
    "question": "A company has a financial application that produces reports. The reports average 50 KB in size and are stored in Amazon S3. The reports are frequently accessed during the first week after production and must be stored for several years. The reports must be retrievable within 6 hours.\n\nWhich solution meets these requirements MOST cost-effectively? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier after 7 days.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use S3 Intelligent-Tiering. Configure S3 Intelligent-Tiering to transition the reports to S3 Standard-Infrequent Access (S3 Standard-IA) and S3 Glacier.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier Deep Archive after 7 days.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier after 7 days.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 283,
    "question": "A company needs to optimize the cost of its Amazon EC2 instances. The company also needs to change the type and family of its EC2 instances every 2-3 months.\n\nWhat should the company do to meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Purchase Partial Upfront Reserved Instances for a 3-year term.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Purchase a No Upfront Compute Savings Plan for a 1-year term.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Purchase All Upfront Reserved Instances for a 1-year term.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Purchase an All Upfront EC2 Instance Savings Plan for a 1-year term.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Purchase a No Upfront Compute Savings Plan for a 1-year term.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 284,
    "question": "A solutions architect needs to review a company's Amazon S3 buckets to discover personally identifiable information (PII). The company stores the PII data in the us-east-1 Region and us-west-2 Region.\n\nWhich solution will meet these requirements with the LEAST operational overhead? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Configure Amazon Macie in each Region. Create a job to analyze the data that is in Amazon S3.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Configure AWS Security Hub for all Regions. Create an AWS Config rule to analyze the data that is in Amazon S3.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Configure Amazon Inspector to analyze the data that is in Amazon S3.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Configure Amazon GuardDuty to analyze the data that is in Amazon S3.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Configure Amazon Macie in each Region. Create a job to analyze the data that is in Amazon S3.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 285,
    "question": "A company's SAP application has a backend SQL Server database in an on-premises environment. The company wants to migrate its on-premises application and database server to AWS. The company needs an instance type that meets the high demands of its SAP database. On-premises performance data shows that both the SAP application and the database have high memory utilization.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use the compute optimized instance family for the application. Use the memory optimized instance family for the database.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use the storage optimized instance family for both the application and the database.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use the memory optimized instance family for both the application and the database.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use the high performance computing (HPC) optimized instance family for the application. Use the memory optimized instance family for the database.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use the memory optimized instance family for both the application and the database.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 286,
    "question": "A company runs an application in a VPC with public and private subnets. The VPC extends across multiple Availability Zones. The application runs on Amazon EC2 instances in private subnets. The application uses an Amazon Simple Queue Service (Amazon SQS) queue.\n\nA solutions architect needs to design a secure solution to establish a connection between the EC2 instances and the SQS queue.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the private subnets. Add to the endpoint a security group that has an inbound access rule that allows traffic from the EC2 instances that are in the private subnets.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public subnets. Attach to the interface endpoint a VPC endpoint policy that allows access from the EC2 instances that are in the private subnets.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public subnets. Attach an Amazon SQS access policy to the interface VPC endpoint that allows requests from only a specified VPC endpoint.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Implement a gateway endpoint for Amazon SQS. Add a NAT gateway to the private subnets. Attach an IAM role to the EC2 instances that allows access to the SQS queue.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the private subnets. Add to the endpoint a security group that has an inbound access rule that allows traffic from the EC2 instances that are in the private subnets.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 287,
    "question": "A solutions architect is using an AWS CloudFormation template to deploy a three-tier web application. The web application consists of a web tier and an application tier that stores and retrieves user data in Amazon DynamoDB tables. The web and application tiers are hosted on Amazon EC2 instances, and the database tier is not publicly accessible. The application EC2 instances need to access the DynamoDB tables without exposing API credentials in the template.\n\nWhat should the solutions architect do to meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Create an IAM role to read the DynamoDB tables. Associate the role with the application instances by referencing an instance profile.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Create an IAM role that has the required permissions to read and write from the DynamoDB tables. Add the role to the EC2 instance profile, and associate the instance profile with the application instances.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use the parameter section in the AWS CloudFormation template to have the user input access and secret keys from an already-created IAM user that has the required permissions to read and write from the DynamoDB tables.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Create an IAM user in the AWS CloudFormation template that has the required permissions to read and write from the DynamoDB tables. Use the GetAtt function to retrieve the access and secret keys, and pass them to the application instances through the user data.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Create an IAM role that has the required permissions to read and write from the DynamoDB tables. Add the role to the EC2 instance profile, and associate the instance profile with the application instances.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 288,
    "question": "A solutions architect manages an analytics application. The application stores large amounts of semistructured data in an Amazon S3 bucket. The solutions architect wants to use parallel data processing to process the data more quickly. The solutions architect also wants to use information that is stored in an Amazon Redshift database to enrich the data.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Athena to process the S3 data. Use AWS Glue with the Amazon Redshift data to enrich the S3 data.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use Amazon EMR to process the S3 data. Use Amazon EMR with the Amazon Redshift data to enrich the S3 data.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use Amazon EMR to process the S3 data. Use Amazon Kinesis Data Streams to move the S3 data into Amazon Redshift so that the data can be enriched.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use AWS Glue to process the S3 data. Use AWS Lake Formation with the Amazon Redshift data to enrich the S3 data.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon EMR to process the S3 data. Use Amazon EMR with the Amazon Redshift data to enrich the S3 data.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 289,
    "question": "A company has two VPCs that are located in the us-west-2 Region within the same AWS account. The company needs to allow network traffic between these VPCs. Approximately 500 GB of data transfer will occur between the VPCs each month.\n\nWhat is the MOST cost-effective solution to connect these VPCs? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to use the transit gateway for inter-VPC communication.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each VPC to use the VPN tunnel for inter-VPC communication.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter-VPC communication.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of each VPC to use the Direct Connect connection for inter-VPC communication.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter-VPC communication.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 290,
    "question": "A company hosts multiple applications on AWS for different product lines. The applications use different compute resources, including Amazon EC2 instances and Application Load Balancers. The applications run in different AWS accounts under the same organization in AWS Organizations across multiple AWS Regions. Teams for each product line have tagged each compute resource in the individual accounts.\n\nThe company wants more details about the cost for each product line from the consolidated billing feature in Organizations.\n\nWhich combination of steps will meet these requirements? (Choose two.) 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Select a specific AWS generated tag in the AWS Billing console.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Select a specific user-defined tag in the AWS Billing console.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Select a specific user-defined tag in the AWS Resource Groups console.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Activate the selected tag from each AWS account.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 5,
        "answer": "Activate the selected tag from the Organizations management account.",
        "correct": true,
        "explanation": ""
      }
    ],
    "corrects": [
      2,
      5
    ],
    "multiple": true,
    "correctAnswerExplanations": [
      {
        "answer": "Select a specific user-defined tag in the AWS Billing console.",
        "explanation": ""
      },
      {
        "answer": "Activate the selected tag from the Organizations management account.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 291,
    "question": "A company's solutions architect is designing an AWS multi-account solution that uses AWS Organizations. The solutions architect has organized the company's accounts into organizational units (OUs).\n\nThe solutions architect needs a solution that will identify any changes to the OU hierarchy. The solution also needs to notify the company's operations team of any changes.\n\nWhich solution will meet these requirements with the LEAST operational overhead? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Provision the AWS accounts by using AWS Control Tower. Use account drift notifications to identify the changes to the OU hierarchy.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Provision the AWS accounts by using AWS Control Tower. Use AWS Config aggregated rules to identify the changes to the OU hierarchy.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use AWS Service Catalog to create accounts in Organizations. Use an AWS CloudTrail organization trail to identify the changes to the OU hierarchy.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use AWS CloudFormation templates to create accounts in Organizations. Use the drift detection operation on a stack to identify the changes to the OU hierarchy.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Provision the AWS accounts by using AWS Control Tower. Use account drift notifications to identify the changes to the OU hierarchy.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 292,
    "question": "A company's website handles millions of requests each day, and the number of requests continues to increase. A solutions architect needs to improve the response time of the web application. The solutions architect determines that the application needs to decrease latency when retrieving product details from the Amazon DynamoDB table.\n\nWhich solution will meet these requirements with the LEAST amount of operational overhead? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Set up a DynamoDB Accelerator (DAX) cluster. Route all read requests through DAX.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Set up Amazon ElastiCache for Redis between the DynamoDB table and the web application. Route all read requests through Redis.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Set up Amazon ElastiCache for Memcached between the DynamoDB table and the web application. Route all read requests through Memcached.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Set up Amazon DynamoDB Streams on the table, and have AWS Lambda read from the table and populate Amazon ElastiCache. Route all read requests through ElastiCache.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Set up a DynamoDB Accelerator (DAX) cluster. Route all read requests through DAX.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 293,
    "question": "A solutions architect needs to ensure that API calls to Amazon DynamoDB from Amazon EC2 instances in a VPC do not travel across the internet.\n\nWhich combination of steps should the solutions architect take to meet this requirement? (Choose two.) 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Create a route table entry for the endpoint.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Create a gateway endpoint for DynamoDB.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Create an interface endpoint for Amazon EC2.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Create an elastic network interface for the endpoint in each of the subnets of the VPC.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 5,
        "answer": "Create a security group entry in the endpoint's security group to provide access.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1,
      2
    ],
    "multiple": true,
    "correctAnswerExplanations": [
      {
        "answer": "Create a route table entry for the endpoint.",
        "explanation": ""
      },
      {
        "answer": "Create a gateway endpoint for DynamoDB.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 294,
    "question": "A company runs its applications on both Amazon Elastic Kubernetes Service (Amazon EKS) clusters and on-premises Kubernetes clusters. The company wants to view all clusters and workloads from a central location.\n\nWhich solution will meet these requirements with the LEAST operational overhead? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon CloudWatch Container Insights to collect and group the cluster information.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use Amazon EKS Connector to register and connect all Kubernetes clusters.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use AWS Systems Manager to collect and view the cluster information.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use Amazon EKS Anywhere as the primary cluster to view the other clusters with native Kubernetes commands.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon EKS Connector to register and connect all Kubernetes clusters.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 295,
    "question": "A company is building an ecommerce application and needs to store sensitive customer information. The company needs to give customers the ability to complete purchase transactions on the website. The company also needs to ensure that sensitive customer data is protected, even from database administrators.\n\nWhich solution meets these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Store sensitive data in an Amazon Elastic Block Store (Amazon EBS) volume. Use EBS encryption to encrypt the data. Use an IAM instance role to restrict access.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Store sensitive data in Amazon RDS for MySQL. Use AWS Key Management Service (AWS KMS) client-side encryption to encrypt the data.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Store sensitive data in Amazon S3. Use AWS Key Management Service (AWS KMS) server-side encryption to encrypt the data. Use S3 bucket policies to restrict access.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Store sensitive data in Amazon FSx for Windows Server. Mount the file share on application servers. Use Windows file permissions to restrict access.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Store sensitive data in Amazon RDS for MySQL. Use AWS Key Management Service (AWS KMS) client-side encryption to encrypt the data.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 296,
    "question": "A company has an on-premises MySQL database that handles transactional data. The company is migrating the database to the AWS Cloud. The migrated database must maintain compatibility with the company's applications that use the database. The migrated database also must scale automatically during periods of increased demand.\n\nWhich migration solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use native MySQL tools to migrate the database to Amazon RDS for MySQL. Configure elastic storage scaling.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Migrate the database to Amazon Redshift by using the mysqldump utility. Turn on Auto Scaling for the Amazon Redshift cluster.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon Aurora. Turn on Aurora Auto Scaling.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon DynamoDB. Configure an Auto Scaling policy.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon Aurora. Turn on Aurora Auto Scaling.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 297,
    "question": "A company runs multiple Amazon EC2 Linux instances in a VPC across two Availability Zones. The instances host applications that use a hierarchical directory structure. The applications need to read and write rapidly and concurrently to shared storage.\n\nWhat should a solutions architect do to meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon S3 bucket. Allow access from all the EC2 instances in the VPC.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system from each EC2 instance.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Create a file system on a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume. Attach the EBS volume to all the EC2 instances.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Create file systems on Amazon Elastic Block Store (Amazon EBS) volumes that are attached to each EC2 instance. Synchronize the EBS volumes across the different EC2 instances.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system from each EC2 instance.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 298,
    "question": "A solutions architect is designing a workload that will store hourly energy consumption by business tenants in a building. The sensors will feed a database through HTTP requests that will add up usage for each tenant. The solutions architect must use managed services when possible. The workload will receive more features in the future as the solutions architect adds independent components.\n\nWhich solution will meet these requirements with the LEAST operational overhead? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon API Gateway with AWS Lambda functions to receive the data from the sensors, process the data, and store the data in an Amazon DynamoDB table.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use an Elastic Load Balancer that is supported by an Auto Scaling group of Amazon EC2 instances to receive and process the data from the sensors. Use an Amazon S3 bucket to store the processed data.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use Amazon API Gateway with AWS Lambda functions to receive the data from the sensors, process the data, and store the data in a Microsoft SQL Server Express database on an Amazon EC2 instance.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use an Elastic Load Balancer that is supported by an Auto Scaling group of Amazon EC2 instances to receive and process the data from the sensors. Use an Amazon Elastic File System (Amazon EFS) shared file system to store the processed data.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon API Gateway with AWS Lambda functions to receive the data from the sensors, process the data, and store the data in an Amazon DynamoDB table.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 299,
    "question": "A solutions architect is designing the storage architecture for a new web application used for storing and viewing engineering drawings. All application components will be deployed on the AWS infrastructure.\n\nThe application design must support caching to minimize the amount of time that users wait for the engineering drawings to load. The application must be able to store petabytes of data.\n\nWhich combination of storage and caching should the solutions architect use? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Amazon S3 with Amazon CloudFront",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Amazon S3 Glacier with Amazon ElastiCache",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Amazon Elastic Block Store (Amazon EBS) volumes with Amazon CloudFront",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "AWS Storage Gateway with Amazon ElastiCache",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Amazon S3 with Amazon CloudFront",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 300,
    "question": "A solutions architect is designing a multi-tier application for a company. The application's users upload images from a mobile device. The application generates a thumbnail of each image and returns a message to the user to confirm that the image was uploaded successfully.\n\nThe thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to its users to notify them that the original image was received. The solutions architect must design the application to asynchronously dispatch requests to the different application tiers.\n\nWhat should the solutions architect do to meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": " Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image upload process as an event source to invoke the Lambda function.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": " Create an AWS Step Functions workflow. Configure Step Functions to handle the orchestration between the application tiers and alert the user when thumbnail generation is complete.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": " Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": " Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions. Use one subscription with the application to generate the thumbnail after the image upload is complete. Use a second subscription to message the user's mobile app by way of a push notification after thumbnail generation is complete.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": " Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 301,
    "question": "A company has a large workload that runs every Friday evening. The workload runs on Amazon EC2 instances that are in two Availability Zones in the us-east-1 Region. Normally, the company must run no more than two instances at all times. However, the company wants to scale up to six instances each Friday to handle a regularly repeating increased workload.\n\nWhich solution will meet these requirements with the LEAST operational overhead? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Create a reminder in Amazon EventBridge to scale the instances.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Create an Auto Scaling group that has a scheduled action.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Create an Auto Scaling group that uses manual scaling.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Create an Auto Scaling group that uses automatic scaling.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Create an Auto Scaling group that has a scheduled action.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 302,
    "question": "A company is creating a REST API. The company has strict requirements for the use of TLS. The company requires TLSv1.3 on the API endpoints. The company also requires a specific public third-party certificate authority (CA) to sign the TLS certificate.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use a local machine to create a certificate that is signed by the third-party Import the certificate into AWS Certificate Manager (ACM). Create an HTTP API in Amazon API Gateway with a custom domain. Configure the custom domain to use the certificate.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Create a certificate in AWS Certificate Manager (ACM) that is signed by the third-party CA. Create an HTTP API in Amazon API Gateway with a custom domain. Configure the custom domain to use the certificate.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use AWS Certificate Manager (ACM) to create a certificate that is signed by the third-party CA. Import the certificate into AWS Certificate Manager (ACM). Create an AWS Lambda function with a Lambda function URL. Configure the Lambda function URL to use the certificate.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Create a certificate in AWS Certificate Manager (ACM) that is signed by the third-party CA. Create an AWS Lambda function with a Lambda function URL. Configure the Lambda function URL to use the certificate.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use a local machine to create a certificate that is signed by the third-party Import the certificate into AWS Certificate Manager (ACM). Create an HTTP API in Amazon API Gateway with a custom domain. Configure the custom domain to use the certificate.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 303,
    "question": "A company runs an application on AWS. The application receives inconsistent amounts of usage. The application uses AWS Direct Connect to connect to an on-premises MySQL-compatible database. The on-premises database consistently uses a minimum of 2 GiB of memory.\n\nThe company wants to migrate the on-premises database to a managed AWS service. The company wants to use auto scaling capabilities to manage unexpected workload increases.\n\nWhich solution will meet these requirements with the LEAST administrative overhead? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Provision an Amazon DynamoDB database with default read and write capacity settings.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Provision an Amazon Aurora database with a minimum capacity of 1 Aurora capacity unit (ACU).",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Provision an Amazon Aurora Serverless v2 database with a minimum capacity of 1 Aurora capacity unit (ACU).",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Provision an Amazon RDS for MySQL database with 2 GiB of memory.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Provision an Amazon Aurora Serverless v2 database with a minimum capacity of 1 Aurora capacity unit (ACU).",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 304,
    "question": "A company wants to use an event-driven programming model with AWS Lambda. The company wants to reduce startup latency for Lambda functions that run on Java 11. The company does not have strict latency requirements for the applications. The company wants to reduce cold starts and outlier latencies when a function scales up.\n\nWhich solution will meet these requirements MOST cost-effectively? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Configure Lambda provisioned concurrency.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Increase the timeout of the Lambda functions.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Increase the memory of the Lambda functions.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Configure Lambda SnapStart.",
        "correct": true,
        "explanation": ""
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Configure Lambda SnapStart.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 305,
    "question": "A financial services company launched a new application that uses an Amazon RDS for MySQL database. The company uses the application to track stock market trends. The company needs to operate the application for only 2 hours at the end of each week. The company needs to optimize the cost of running the database.\n\nWhich solution will meet these requirements MOST cost-effectively? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the existing RDS for MySQL database to an Aurora Serverless v2 MySQL database cluster.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Migrate the existing RDS for MySQL database to an Aurora MySQL database cluster.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Migrate the existing RDS for MySQL database to an Amazon EC2 instance that runs MySQL. Purchase an instance reservation for the EC2 instance.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Migrate the existing RDS for MySQL database to an Amazon Elastic Container Service (Amazon ECS) cluster that uses MySQL container images to run tasks.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Migrate the existing RDS for MySQL database to an Aurora Serverless v2 MySQL database cluster.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 306,
    "question": "A company deploys its applications on Amazon Elastic Kubernetes Service (Amazon EKS) behind an Application Load Balancer in an AWS Region. The application needs to store data in a PostgreSQL database engine. The company wants the data in the database to be highly available. The company also needs increased capacity for read workloads.\n\nWhich solution will meet these requirements with the MOST operational efficiency? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon DynamoDB database table configured with global tables.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Create an Amazon RDS database with Multi-AZ deployments.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Create an Amazon RDS database with Multi-AZ DB cluster deployment.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Create an Amazon RDS database configured with cross-Region read replicas.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Create an Amazon RDS database with Multi-AZ DB cluster deployment.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 307,
    "question": "A company is building a RESTful serverless web application on AWS by using Amazon API Gateway and AWS Lambda. The users of this web application will be geographically distributed, and the company wants to reduce the latency of API requests to these users.\n\nWhich type of endpoint should a solutions architect use to meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Private endpoint",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Regional endpoint",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Interface VPC endpoint",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Edge-optimized endpoint",
        "correct": true,
        "explanation": ""
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Edge-optimized endpoint",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 308,
    "question": "A company uses an Amazon CloudFront distribution to serve content pages for its website. The company needs to ensure that clients use a TLS certificate when accessing the company's website. The company wants to automate the creation and renewal of the TLS certificates.\n\nWhich solution will meet these requirements with the MOST operational efficiency? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use a CloudFront security policy to create a certificate.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use a CloudFront origin access control (OAC) to create a certificate.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use AWS Certificate Manager (ACM) to create a certificate. Use DNS validation for the domain.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use AWS Certificate Manager (ACM) to create a certificate. Use email validation for the domain.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Certificate Manager (ACM) to create a certificate. Use DNS validation for the domain.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 309,
    "question": "A company deployed a serverless application that uses Amazon DynamoDB as a database layer. The application has experienced a large increase in users. The company wants to improve database response time from milliseconds to microseconds and to cache requests to the database.\n\nWhich solution will meet these requirements with the LEAST operational overhead? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use DynamoDB Accelerator (DAX).",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Migrate the database to Amazon Redshift.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Migrate the database to Amazon RDS.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use Amazon ElastiCache for Redis.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use DynamoDB Accelerator (DAX).",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 310,
    "question": "A company runs an application that uses Amazon RDS for PostgreSQL. The application receives traffic only on weekdays during business hours. The company wants to optimize costs and reduce operational overhead based on this usage.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use the Instance Scheduler on AWS to configure start and stop schedules.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Turn off automatic backups. Create weekly manual snapshots of the database.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Create a custom AWS Lambda function to start and stop the database based on minimum CPU utilization.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Purchase All Upfront reserved DB instances.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use the Instance Scheduler on AWS to configure start and stop schedules.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 311,
    "question": "A company uses locally attached storage to run a latency-sensitive application on premises. The company is using a lift and shift method to move the application to the AWS Cloud. The company does not want to change the application architecture.\n\nWhich solution will meet these requirements MOST cost-effectively? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Configure an Auto Scaling group with an Amazon EC2 instance. Use an Amazon FSx for Lustre file system to run the application.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Host the application on an Amazon EC2 instance. Use an Amazon Elastic Block Store (Amazon EBS) GP2 volume to run the application.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Configure an Auto Scaling group with an Amazon EC2 instance. Use an Amazon FSx for OpenZFS file system to run the application.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Host the application on an Amazon EC2 instance. Use an Amazon Elastic Block Store (Amazon EBS) GP3 volume to run the application.",
        "correct": true,
        "explanation": ""
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Host the application on an Amazon EC2 instance. Use an Amazon Elastic Block Store (Amazon EBS) GP3 volume to run the application.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 312,
    "question": "A company runs a stateful production application on Amazon EC2 instances. The application requires at least two EC2 instances to always be running.\n\nA solutions architect needs to design a highly available and fault-tolerant architecture for the application. The solutions architect creates an Auto Scaling group of EC2 instances.\n\nWhich set of additional steps should the solutions architect take to meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Set the Auto Scaling group's minimum capacity to two. Deploy one On-Demand Instance in one Availability Zone and one On-Demand Instance in a second Availability Zone.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Set the Auto Scaling group's minimum capacity to four. Deploy two On-Demand Instances in one Availability Zone and two On-Demand Instances in a second Availability Zone.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Set the Auto Scaling group's minimum capacity to two. Deploy four Spot Instances in one Availability Zone.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Set the Auto Scaling group's minimum capacity to four. Deploy two On-Demand Instances in one Availability Zone and two Spot Instances in a second Availability Zone.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Set the Auto Scaling group's minimum capacity to four. Deploy two On-Demand Instances in one Availability Zone and two On-Demand Instances in a second Availability Zone.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 313,
    "question": "An ecommerce company uses Amazon Route 53 as its DNS provider. The company hosts its website on premises and in the AWS Cloud. The company's on-premises data center is near the us-west-1 Region. The company uses the eu-central-1 Region to host the website. The company wants to minimize load time for the website as much as possible.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Set up a geolocation routing policy. Send the traffic that is near us-west-1 to the on-premises data center. Send the traffic that is near eu-central-1 to eu-central-1.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Set up a simple routing policy that routes all traffic that is near eu-central-1 to eu-central-1 and routes all traffic that is near the on-premises datacenter to the on-premises data center.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Set up a latency routing policy. Associate the policy with us-west-1.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Set up a weighted routing policy. Split the traffic evenly between eu-central-1 and the on-premises data center.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Set up a geolocation routing policy. Send the traffic that is near us-west-1 to the on-premises data center. Send the traffic that is near eu-central-1 to eu-central-1.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 314,
    "question": "A company has 5 PB of archived data on physical tapes. The company needs to preserve the data on the tapes for another 10 years for compliance purposes. The company wants to migrate to AWS in the next 6 months. The data center that stores the tapes has a 1 Gbps uplink internet connectivity.\n\nWhich solution will meet these requirements MOST cost-effectively? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Read the data from the tapes on premises. Stage the data in a local NFS storage. Use AWS DataSync to migrate the data to Amazon S3 Glacier Flexible Retrieval.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use an on-premises backup application to read the data from the tapes and to write directly to Amazon S3 Glacier Deep Archive.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Order multiple AWS Snowball devices that have Tape Gateway. Copy the physical tapes to virtual tapes in Snowball. Ship the Snowball devices to AWS. Create a lifecycle policy to move the tapes to Amazon S3 Glacier Deep Archive.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Configure an on-premises Tape Gateway. Create virtual tapes in the AWS Cloud. Use backup software to copy the physical tape to the virtual tape.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Order multiple AWS Snowball devices that have Tape Gateway. Copy the physical tapes to virtual tapes in Snowball. Ship the Snowball devices to AWS. Create a lifecycle policy to move the tapes to Amazon S3 Glacier Deep Archive.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 315,
    "question": "A company is deploying an application that processes large quantities of data in parallel. The company plans to use Amazon EC2 instances for the workload. The network architecture must be configurable to prevent groups of nodes from sharing the same underlying hardware.\n\nWhich networking solution meets these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Run the EC2 instances in a spread placement group.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Group the EC2 instances in separate accounts.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Configure the EC2 instances with dedicated tenancy.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Configure the EC2 instances with shared tenancy.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Run the EC2 instances in a spread placement group.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 316,
    "question": "A solutions architect is designing a disaster recovery (DR) strategy to provide Amazon EC2 capacity in a failover AWS Region. Business requirements state that the DR strategy must meet capacity in the failover Region.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Purchase On-Demand Instances in the failover Region.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Purchase an EC2 Savings Plan in the failover Region.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Purchase regional Reserved Instances in the failover Region.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Purchase a Capacity Reservation in the failover Region.",
        "correct": true,
        "explanation": ""
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Purchase a Capacity Reservation in the failover Region.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 317,
    "question": "A company has five organizational units (OUs) as part of its organization in AWS Organizations. Each OU correlates to the five businesses that the company owns. The company's research and development (R&D) business is separating from the company and will need its own organization. A solutions architect creates a separate new management account for this purpose.\n\nWhat should the solutions architect do next in the new management account? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Have the R&D AWS account be part of both organizations during the transition.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Invite the R&D AWS account to be part of the new organization after the R&D AWS account has left the prior organization.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Create a new R&D AWS account in the new organization. Migrate resources from the prior R&D AWS account to the new R&D AWS account.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Have the R&D AWS account join the new organization. Make the new management account a member of the prior organization.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Invite the R&D AWS account to be part of the new organization after the R&D AWS account has left the prior organization.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 318,
    "question": "A company is designing a solution to capture customer activity in different web applications to process analytics and make predictions. Customer activity in the web applications is unpredictable and can increase suddenly. The company requires a solution that integrates with other web applications. The solution must include an authorization step for security purposes.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives in an Amazon Elastic File System (Amazon EFS) file system. Authorization is resolved at the GWLB.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis data stream that stores the information that the company receives in an Amazon S3 bucket. Use an AWS Lambda function to resolve authorization.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis Data Firehose that stores the information that the company receives in an Amazon S3 bucket. Use an API Gateway Lambda authorizer to resolve authorization.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives on an Amazon Elastic File System (Amazon EFS) file system. Use an AWS Lambda function to resolve authorization.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis Data Firehose that stores the information that the company receives in an Amazon S3 bucket. Use an API Gateway Lambda authorizer to resolve authorization.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 319,
    "question": "An ecommerce company wants a disaster recovery solution for its Amazon RDS DB instances that run Microsoft SQL Server Enterprise Edition. The company's current recovery point objective (RPO) and recovery time objective (RTO) are 24 hours.\n\nWhich solution will meet these requirements MOST cost-effectively? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Create a cross-Region read replica and promote the read replica to the primary instance.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use AWS Database Migration Service (AWS DMS) to create RDS cross-Region replication.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use cross-Region replication every 24 hours to copy native backups to an Amazon S3 bucket.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Copy automatic snapshots to another Region every 24 hours.",
        "correct": true,
        "explanation": ""
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Copy automatic snapshots to another Region every 24 hours.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 320,
    "question": "A company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer that has sticky sessions enabled. The web server currently hosts the user session state. The company wants to ensure high availability and avoid user session state loss in the event of a web server outage.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use an Amazon ElastiCache for Memcached instance to store the session data. Update the application to use ElastiCache for Memcached to store the session state.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use Amazon ElastiCache for Redis to store the session state. Update the application to use ElastiCache for Redis to store the session state.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use an AWS Storage Gateway cached volume to store session data. Update the application to use AWS Storage Gateway cached volume to store the session state.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use Amazon RDS to store the session state. Update the application to use Amazon RDS to store the session state.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon ElastiCache for Redis to store the session state. Update the application to use ElastiCache for Redis to store the session state.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 321,
    "question": "A company migrated a MySQL database from the company's on-premises data center to an Amazon RDS for MySQL DB instance. The company sized the RDS DB instance to meet the company's average daily workload. Once a month, the database performs slowly when the company runs queries for a report. The company wants to have the ability to run reports and maintain the performance of the daily workloads.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Create a read replica of the database. Direct the queries to the read replica.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Create a backup of the database. Restore the backup to another DB instance. Direct the queries to the new database.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Export the data to Amazon S3. Use Amazon Athena to query the S3 bucket.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Resize the DB instance to accommodate the additional workload.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Create a read replica of the database. Direct the queries to the read replica.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 322,
    "question": "A company runs a container application by using Amazon Elastic Kubernetes Service (Amazon EKS). The application includes microservices that manage customers and place orders. The company needs to route incoming requests to the appropriate microservices.\n\nWhich solution will meet this requirement MOST cost-effectively? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use the AWS Load Balancer Controller to provision a Network Load Balancer.(đắt hơn)",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use the AWS Load Balancer Controller to provision an Application Load Balancer.(rẻ hơn)",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use an AWS Lambda function to connect the requests to Amazon EKS.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use Amazon API Gateway to connect the requests to Amazon EKS.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use the AWS Load Balancer Controller to provision an Application Load Balancer.(rẻ hơn)",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 323,
    "question": "A company uses AWS and sells access to copyrighted images. The company’s global customer base needs to be able to access these images quickly. The company must deny access to users from specific countries. The company wants to minimize costs as much as possible.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon S3 to store the images. Turn on multi-factor authentication (MFA) and public bucket access. Provide customers with a link to the S3 bucket.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use Amazon S3 to store the images. Create an IAM user for each customer. Add the users to a group that has permission to access the S3 bucket.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use Amazon EC2 instances that are behind Application Load Balancers (ALBs) to store the images. Deploy the instances only in the countries the company services. Provide customers with links to the ALBs for their specific country's instances.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use Amazon S3 to store the images. Use Amazon CloudFront to distribute the images with geographic restrictions.",
        "correct": true,
        "explanation": ""
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon S3 to store the images. Use Amazon CloudFront to distribute the images with geographic restrictions.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 324,
    "question": "A solutions architect is designing a highly available Amazon ElastiCache for Redis based solution. The solutions architect needs to ensure that failures do not result in performance degradation or loss of data locally and within an AWS Region. The solution needs to provide high availability at the node level and at the Region level.\n\nWhich solution will meet these requirements? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Use Multi-AZ Redis replication groups with shards that contain multiple nodes.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Use Redis shards that contain multiple nodes with Redis append only files (AOF) turned on.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Use a Multi-AZ Redis cluster with more than one read replica in the replication group.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Use Redis shards that contain multiple nodes with Auto Scaling turned on.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Use a Multi-AZ Redis cluster with more than one read replica in the replication group.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  },
  {
    "id": 325,
    "question": "A company plans to migrate to AWS and use Amazon EC2 On-Demand Instances for its application. During the migration testing phase, a technical team observes that the application takes a long time to launch and load memory to become fully productive.\n\nWhich solution will reduce the launch time of the application during the next testing phase? 1 point",
    "answers": [
      {
        "id": 1,
        "answer": "Launch two or more EC2 On-Demand Instances. Turn on auto scaling features and make the EC2 On-Demand Instances available during the next testing phase.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 2,
        "answer": "Launch EC2 Spot Instances to support the application and to scale the application so it is available during the next testing phase.",
        "correct": false,
        "explanation": ""
      },
      {
        "id": 3,
        "answer": "Launch the EC2 On-Demand Instances with hibernation turned on. Configure EC2 Auto Scaling warm pools during the next testing phase.",
        "correct": true,
        "explanation": ""
      },
      {
        "id": 4,
        "answer": "Launch EC2 On-Demand Instances with Capacity Reservations. Start additional EC2 instances during the next testing phase.",
        "correct": false,
        "explanation": ""
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "correctAnswerExplanations": [
      {
        "answer": "Launch the EC2 On-Demand Instances with hibernation turned on. Configure EC2 Auto Scaling warm pools during the next testing phase.",
        "explanation": ""
      }
    ],
    "domain": "",
    "incorrectAnswerExplanations": [],
    "reference": ""
  }
]