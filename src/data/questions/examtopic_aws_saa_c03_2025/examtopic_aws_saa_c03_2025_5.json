[
  {
    "id": 261,
    "question": "A company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones.<br><br>The company wants to provide its customers with different versions of content based on the devices that the customers use to access the website.<br><br>Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Configure Amazon CloudFront to cache multiple versions of the content.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Configure a host header in a Network Load Balancer to forward traffic to different instances.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up host-based routing to different EC2 instances.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up path-based routing to different EC2 instances.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 262,
    "question": "A company plans to use Amazon ElastiCache for its multi-tier web application. A solutions architect creates a Cache VPC for the ElastiCache cluster and an App VPC for the application’s Amazon EC2 instances. Both VPCs are in the us-east-1 Region.<br><br>The solutions architect must implement a solution to provide the application’s EC2 instances with access to the ElastiCache cluster.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCache cluster’s security group to allow inbound connection from the application’s security group.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application’s security group.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the peering connection’s security group to allow inbound connection from the application’s security group.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an inbound rule for the Transit VPC’s security group to allow inbound connection from the application’s security group.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 263,
    "question": "A company is building an application that consists of several microservices. The company has decided to use container technologies to deploy its software on AWS. The company needs a solution that minimizes the amount of ongoing effort for maintenance and scaling. The company cannot manage additional infrastructure.<br><br>Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability Zones.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch type. Specify a desired task number level of greater than or equal to 2.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a desired task number level of greater than or equal to 2.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability Zones. Create a deployment that specifies two or more replicas for each microservice.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      4
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 264,
    "question": "A company has a web application hosted over 10 Amazon EC2 instances with traffic directed by Amazon Route 53. The company occasionally experiences a timeout error when attempting to browse the application. The networking team finds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error.<br><br>What should a solutions architect implement to overcome these timeout errors?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 265,
    "question": "A solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time.<br><br>Which solution meets these requirements and is MOST secure?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 266,
    "question": "A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy endpoints.<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 267,
    "question": "A company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-real time and must store the data in a centralized location in Apache Parquet format for further processing.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWS Lambda function to send the data to the Kinesis Data Analytics application.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 268,
    "question": "A gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The company wants to improve the user experience while minimizing changes to the application’s architecture.<br><br>What should a solutions architect do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon ElastiCache in front of the database.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use RDS Proxy between the application and the database.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Migrate the application from EC2 instances to AWS Lambda.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 269,
    "question": "An ecommerce company has noticed performance degradation of its Amazon RDS based web application. The performance degradation is attributed to an increase in the number of read-only SQL queries triggered by business analysts. A solutions architect needs to solve the problem with minimal changes to the existing web application.<br><br>What should the solutions architect recommend?",
    "answers": [
      {
        "id": 1,
        "answer": "Export the data to Amazon DynamoDB and have the business analysts run their queries.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Load the data into Amazon ElastiCache and have the business analysts run their queries.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a read replica of the primary database and have the business analysts run their queries.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Copy the data into an Amazon Redshift cluster and have the business analysts run their queries.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 270,
    "question": "A company is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions architect needs to ensure that the data is encrypted at rest before the data is uploaded to the S3 buckets. The data also must be encrypted in transit.<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create bucket policies that require the use of server-side encryption with S3 managed encryption keys (SSE-S3) for S3 uploads.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Enable the security option to encrypt the S3 buckets through the use of a default AWS Key Management Service (AWS KMS) key.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 271,
    "question": "A solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before the desired Amazon EC2 capacity is reached. The peak capacity is the ‘same every night and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-effective solution that will allow for the desired EC2 capacity to be reached quickly and allow the Auto Scaling group to scale down after the batch jobs are complete.<br><br>What should the solutions architect do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Increase the minimum capacity for the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Increase the maximum capacity for the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure scheduled scaling to scale up to the desired compute level.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Change the scaling policy to add more EC2 instances during each scaling operation.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 272,
    "question": "A company serves a dynamic website from a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages to serve customers around the world. The website’s architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in other parts of the world.<br><br>The website needs to serve requests quickly and efficiently regardless of a user’s location. However, the company does not want to recreate the existing architecture across multiple Regions.<br><br>What should a solutions architect do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable the API cache based on the Accept-Language request header.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a geolocation routing policy.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 273,
    "question": "A rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions architect must create a disaster recovery (DR) strategy that includes a different AWS Region. The company wants its database to be up to date in the DR Region with the least possible latency. The remaining infrastructure in the DR Region needs to run at reduced capacity and must be able to scale up if necessary.<br><br>Which solution will meet these requirements with the LOWEST recovery time objective (RTO)?",
    "answers": [
      {
        "id": 1,
        "answer": "Use an Amazon Aurora global database with a pilot light deployment.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an Amazon Aurora global database with a warm standby deployment.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 274,
    "question": "A company runs an application on Amazon EC2 instances. The company needs to implement a disaster recovery (DR) solution for the application. The DR solution needs to have a recovery time objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible AWS resources during normal operations.<br><br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "answers": [
      {
        "id": 1,
        "answer": "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability Zone active at all times.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 275,
    "question": "A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning.<br><br>How should the scaling be changed to address the staff complaints and keep costs to a minimum?",
    "answers": [
      {
        "id": 1,
        "answer": "Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 276,
    "question": "A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application’ s data layer that uses Oracle-specific PL/SQL functions. Traffic to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the minimum healthy instance count only. The company predicts that traffic will continue to increase at a steady but unpredictable rate before leveling off.<br><br>What should a solutions architect do to ensure the system can automatically scale for the increased traffic? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Configure storage Auto Scaling on the RDS for Oracle instance.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Migrate the database to Amazon Aurora to use Auto Scaling storage.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure an alarm on the RDS for Oracle instance for low free storage space.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure the Auto Scaling group to use the average CPU as the scaling metric.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Configure the Auto Scaling group to use the average free memory as the scaling metric.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      4
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 277,
    "question": "A company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive.<br><br>Which storage solution is MOST cost-effective?",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Storage Gateway for files to store and process the video content.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS Storage Gateway for volumes to store and process the video content.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 278,
    "question": "A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-traffic queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any financial information is present in the employee data.<br><br>Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription.",
        "correct": true
      }
    ],
    "corrects": [
      2,
      5
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 279,
    "question": "A company has an application that is backed by an Amazon DynamoDB table. The company’s compliance requirements specify that database backups must be taken every month, must be available for 6 months, and must be retained for 7 years.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after 6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the script on the first day of each month. Create a second script that will run on the second day of each month to transition DynamoDB backups that are older than 6 months to cold storage and to delete backups that are older than 7 years.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the command on the first day of each month with a cron expression. Specify in the command to transition the backups to cold storage after 6 months and to delete the backups after 7 years.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 280,
    "question": "A company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company’s Amazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations.<br><br>What should a solutions architect do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 281,
    "question": "A company runs a fleet of web servers using an Amazon RDS for PostgreSQL DB instance. After a routine compliance check, the company sets a standard that requires a recovery point objective (RPO) of less than 1 second for all its production databases.<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Enable a Multi-AZ deployment for the DB instance.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Enable auto scaling for the DB instance in one Availability Zone.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate Availability Zone.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS DMS) change data capture (CDC) tasks.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 282,
    "question": "A company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets directs web traffic to the EC2 instances. The company wants to implement new security measures to restrict inbound traffic from the ALB to the EC2 instances while preventing access from any other source inside or outside the private subnet of the EC2 instances.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure a route in a route table to direct traffic from the internet to the private IP addresses of the EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure the security group for the EC2 instances to only allow traffic that comes from the security group for the ALB.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP addresses.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure the security group for the ALB to allow any TCP traffic on any port.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 283,
    "question": "A research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs intermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an SMB file system.<br><br>The company maintains two synchronized file systems. This strategy is causing data duplication and inefficient resource usage. The company needs to migrate the applications to AWS without making code changes to either application.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon Simple Queue Service (Amazon SQS) to exchange data between the applications.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 284,
    "question": "As part of budget planning, management wants a report of AWS billed items listed by user. The data will be used to create department budgets. A solutions architect needs to determine the most efficient way to obtain this report information.<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Run a query with Amazon Athena to generate the report.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a report in Cost Explorer and download the report.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Access the bill details from the billing dashboard and download the bill.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES).",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 285,
    "question": "A company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each month.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 286,
    "question": "A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website uses a database backend. The company notices that the website does not reflect updates that have been made in the website’s Git repository. The company checks the continuous integration and continuous delivery (CI/CD) pipeline between the Git repository and Amazon S3. The company verifies that the webhooks are configured properly and that the CI/CD pipeline is sending messages that indicate successful deployments.<br><br>A solutions architect needs to implement a solution that displays the updates on the website.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Add an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Invalidate the CloudFront cache.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use AWS Certificate Manager (ACM) to validate the website’s SSL certificate.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 287,
    "question": "A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specific features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers.<br><br>How should a solutions architect design the architecture to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 288,
    "question": "A company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make any changes to the application.<br><br>What should a solutions architect do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon S3 Standard bucket with access to the web servers.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume to all web servers.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 289,
    "question": "A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the same AWS account.<br><br>Which solution will meet these requirements in the MOST secure manner?",
    "answers": [
      {
        "id": 1,
        "answer": "Apply an S3 bucket policy that grants read access to the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Embed an access key and a secret key in the Lambda function’s code to grant the required IAM permissions for read access to the S3 bucket.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to all S3 buckets in the account.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 290,
    "question": "A company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are in an Auto Scaling group that scales in response to user demand. The company wants to optimize cost savings without making a long-term commitment.<br><br>Which EC2 instance purchasing option should a solutions architect recommend to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Dedicated Instances only",
        "correct": false
      },
      {
        "id": 2,
        "answer": "On-Demand Instances only",
        "correct": false
      },
      {
        "id": 3,
        "answer": "A mix of On-Demand Instances and Spot Instances",
        "correct": true
      },
      {
        "id": 4,
        "answer": "A mix of On-Demand Instances and Reserved Instances",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 291,
    "question": "A media company uses Amazon CloudFront for its publicly available streaming video content. The company wants to secure the video content that is hosted in Amazon S3 by controlling who has access. Some of the company’s users are using a custom HTTP client that does not support cookies. Some of the company’s users are unable to change the hardcoded URLs that they are using for access.<br><br>Which services or methods will meet these requirements with the LEAST impact to the users? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Signed cookies",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Signed URLs",
        "correct": true
      },
      {
        "id": 3,
        "answer": "AWS AppSync",
        "correct": false
      },
      {
        "id": 4,
        "answer": "JSON Web Token (JWT)",
        "correct": false
      },
      {
        "id": 5,
        "answer": "AWS Secrets Manager",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 292,
    "question": "A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data.<br><br>Which solutions will meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 293,
    "question": "A company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely transferred.<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-premises systems to mount the Snowball S3 endpoint to provide local access to the data.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball Edge file interface to provide on-premises systems with local access to the data.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software appliance on premises and configure a percentage of data to cache locally. Mount the gateway storage volumes to provide local access to the data.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 294,
    "question": "An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Traffic must not traverse the internet.<br><br>How should a solutions architect configure access to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a private hosted zone by using Amazon Route 53.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up a gateway VPC endpoint for Amazon S3 in the VPC.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Configure the EC2 instances to use a NAT gateway to access the S3 bucket.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 295,
    "question": "An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identifiable information (PII). The company wants to use the data in three applications. Only one of the applications needs to process the PII. The PII must be removed before the other two applications process the data.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point each application to its respective DynamoDB table.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 296,
    "question": "A development team has launched a new application that is hosted on Amazon EC2 instances inside a development VPC. A solutions architect needs to create a new VPC in the same account. The new VPC will be peered with the development VPC. The VPC CIDR block for the development VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC peering connection to the development VPC.<br><br>What is the SMALLEST CIDR block that meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "10.0.1.0/32",
        "correct": false
      },
      {
        "id": 2,
        "answer": "192.168.0.0/24",
        "correct": false
      },
      {
        "id": 3,
        "answer": "192.168.1.0/32",
        "correct": false
      },
      {
        "id": 4,
        "answer": "10.0.1.0/24",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 297,
    "question": "A company deploys an application on five Amazon EC2 instances. An Application Load Balancer (ALB) distributes traffic to the instances by using a target group. The average CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%.<br><br>A solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPUUtilization metric is below 20%. Configure the second CloudWatch alarm to enter the ALARM state when the average CPUUtilization matric is above 50%. Configure the alarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2 instances that are running.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 298,
    "question": "A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance.<br><br>The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone.<br><br>Which solution will make the application highly available?",
    "answers": [
      {
        "id": 1,
        "answer": "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 299,
    "question": "A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-millisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds of Amazon EC2 instances that run Amazon Linux will distribute and process the data.<br><br>Which solution will meet the performance requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon FSx for NetApp ONTAP file system. Sat each volume’ tiering policy to ALL. Import the raw data into the file system. Mount the fila system on the EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent HDD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon FSx for NetApp ONTAP file system. Set each volume’s tiering policy to NONE. Import the raw data into the file system. Mount the file system on the EC2 instances.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 300,
    "question": "A company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a week. The application’s database storage continues to grow over time.<br><br>What should a solutions architect do to meet these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 301,
    "question": "A university research laboratory needs to migrate 30 TB of data from an on-premises Windows file server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that many other departments in the university share.<br><br>The laboratory wants to implement a data migration service that will maximize the performance of the data transfer. However, the laboratory needs to be able to control the amount of bandwidth that the service uses to minimize the impact on other departments. The data migration must take place within the next 5 days.<br><br>Which AWS solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "AWS Snowcone",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon FSx File Gateway",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS DataSync",
        "correct": true
      },
      {
        "id": 4,
        "answer": "AWS Transfer Family",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 302,
    "question": "A company wants to create a mobile app that allows users to stream slow-motion video clips on their mobile devices. Currently, the app captures video clips and uploads the video clips in raw format into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. However, the videos are large in their raw format.<br><br>Users are experiencing issues with buffering and playback on mobile devices. The company wants to implement solutions to maximize the performance and scalability of the app while minimizing operational overhead.<br><br>Which combination of solutions will meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy Amazon CloudFront for content delivery and caching.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use AWS DataSync to replicate the video files across AW'S Regions in other S3 buckets.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy an Auto Sealing group of Amazon EC2 instances in Local Zones for content delivery and caching.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Deploy an Auto Scaling group of Amazon EC2 instances to convert the video files to more appropriate formats.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 303,
    "question": "A company is launching a new application deployed on an Amazon Elastic Container Service (Amazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is monitoring CPU and memory usage because it is expecting high traffic to the application upon its launch. However, the company wants to reduce costs when utilization decreases.<br><br>What should a solutions architect recommend?",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon EC2 Auto Scaling to scale at certain periods based on previous traffic patterns.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon CloudWatch alarm.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 304,
    "question": "A company recently created a disaster recovery site in a different AWS Region. The company needs to transfer large amounts of data back and forth between NFS file systems in the two Regions on a periodic basis.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS DataSync.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use AWS Snowball devices.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up an SFTP server on Amazon EC2.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Database Migration Service (AWS DMS).",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 305,
    "question": "A company is designing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed.<br><br>Which AWS solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an Amazon S3 bucket. Assign an IAM role to the application to grant access to the S3 bucket. Mount the S3 bucket to the application server.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 306,
    "question": "A company wants to run an in-memory database for a latency-sensitive application that runs on Amazon EC2 instances. The application processes more than 100,000 transactions each minute and requires high network throughput. A solutions architect needs to provide a cost-effective network design that minimizes data transfer charges.<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement group with cluster strategy when launching EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Launch all EC2 instances in different Availability Zones within the same AWS Region. Specify a placement group with partition strategy when launching EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy an Auto Scaling group to launch EC2 instances in different Availability Zones based on a network utilization target.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy an Auto Scaling group with a step scaling policy to launch EC2 instances in different Availability Zones.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 307,
    "question": "A company that primarily runs its application servers on premises has decided to migrate to AWS. The company wants to minimize its need to scale its Internet Small Computer Systems Interface (iSCSI) storage on premises. The company wants only its recently accessed data to remain stored locally.<br><br>Which AWS solution should the company use to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Amazon S3 File Gateway",
        "correct": false
      },
      {
        "id": 2,
        "answer": "AWS Storage Gateway Tape Gateway",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Storage Gateway Volume Gateway stored volumes",
        "correct": false
      },
      {
        "id": 4,
        "answer": "AWS Storage Gateway Volume Gateway cached volumes",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 308,
    "question": "A company has multiple AWS accounts that use consolidated billing. The company runs several active high performance Amazon RDS for Oracle On-Demand DB instances for 90 days. The company’s finance team has access to AWS Trusted Advisor in the consolidated billing account and all other AWS accounts.<br><br>The finance team needs to use the appropriate AWS account to access the Trusted Advisor check recommendations for RDS. The finance team must review the appropriate Trusted Advisor check to reduce RDS costs.<br><br>Which combination of steps should the finance team take to meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Use the Trusted Advisor recommendations from the account where the RDS instances are running.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance checks at the same time.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Review the Trusted Advisor check for Amazon RDS Idle DB Instances.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Review the Trusted Advisor check for Amazon Redshift Reserved Node Optimization.",
        "correct": false
      }
    ],
    "corrects": [
      2,
      4
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 309,
    "question": "A solutions architect needs to optimize storage costs. The solutions architect must identify any Amazon S3 buckets that are no longer being accessed or are rarely accessed.<br><br>Which solution will accomplish this goal with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access patterns by using the metrics data with Amazon Athena.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using CloudTrail logs that are integrated with Amazon CloudWatch Logs.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 310,
    "question": "A company sells datasets to customers who do research in artificial intelligence and machine learning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket in the us-east-1 Region. The company hosts a web application that the customers use to purchase access to a given dataset. The web application is deployed on multiple Amazon EC2 instances behind an Application Load Balancer. After a purchase is made, customers receive an S3 signed URL that allows access to the files.<br><br>The customers are distributed across North America and Europe. The company wants to reduce the cost that is associated with data transfers and wants to maintain or improve performance.<br><br>What should a solutions architect do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure S3 Transfer Acceleration on the existing S3 bucket. Direct customer requests to the S3 Transfer Acceleration endpoint. Continue to use S3 signed URLs for access control.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct customer requests to the CloudFront URL. Switch to CloudFront signed URLs for access control.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between the buckets. Direct customer requests to the closest Region. Continue to use S3 signed URLs for access control.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Modify the web application to enable streaming of the datasets to end users. Configure the web application to read the data from the existing S3 bucket. Implement access control directly in the application.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 311,
    "question": "A company is using AWS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational efficiency and must minimize maintenance.<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create multiple Amazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to use the Kinesis Client Library (KCL) to pool messages from its own data stream.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic for each quote type. Subscribe the Lambda function to its associated SNS topic. Configure the application to publish requests for quotes to the appropriate SNS topic.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to deliver data streams to an Amazon OpenSearch Service cluster. Configure the application to send messages to the proper delivery stream. Configure each backend group of application servers to search for the messages from OpenSearch Service and process them accordingly.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 312,
    "question": "A company has an application that runs on several Amazon EC2 instances. Each EC2 instance has multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The application’s EC2 instance configuration and data need to be backed up nightly. The application also needs to be recoverable in a different AWS Region.<br><br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "answers": [
      {
        "id": 1,
        "answer": "Write an AWS Lambda function that schedules nightly snapshots of the application’s EBS volumes and copies the snapshots to a different Region.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EC2 instances as resources.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EBS volumes as resources.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Write an AWS Lambda function that schedules nightly snapshots of the application's EBS volumes and copies the snapshots to a different Availability Zone.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 313,
    "question": "A company is building a mobile app on AWS. The company wants to expand its reach to millions of users. The company needs to build a platform so that authorized users can watch the company’s content on their mobile devices.<br><br>What should a solutions architect recommend to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS) keys to stream content.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up IPsec VPN between the mobile app and the AWS environment to stream content.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon CloudFront. Provide signed URLs to stream content.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Set up AWS Client VPN between the mobile app and the AWS environment to stream content.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 314,
    "question": "A company has an on-premises MySQL database used by the global sales team with infrequent access patterns. The sales team requires the database to have minimal downtime. A database administrator wants to migrate this database to AWS without selecting a particular instance type in anticipation of more users in the future.<br><br>Which service should a solutions architect recommend?",
    "answers": [
      {
        "id": 1,
        "answer": "Amazon Aurora MySQL",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon Aurora Serverless for MySQL",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Amazon Redshift Spectrum",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon RDS for MySQL",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 315,
    "question": "A company experienced a breach that affected several applications in its on-premises data center. The attacker took advantage of vulnerabilities in the custom applications that were running on the servers. The company is now migrating its applications to run on Amazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends a report that details the findings.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda function to log any findings to AWS CloudTrail.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities. Log any findings to AWS CloudTrail.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the findings.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the findings.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 316,
    "question": "A company uses an Amazon EC2 instance to run a script to poll for and process messages in an Amazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process a growing number of messages that are added to the queue.<br><br>What should a solutions architect recommend to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Increase the size of the EC2 instance to process messages faster.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use AWS Systems Manager Run Command to run the script on demand.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 317,
    "question": "A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces.<br><br>The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the DynamoDB table.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an Amazon Redshift table.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 318,
    "question": "A company recently migrated its entire IT environment to the AWS Cloud. The company discovers that users are provisioning oversized Amazon EC2 instances and modifying security group rules without using the appropriate change control process. A solutions architect must devise a strategy to track and audit these inventory and configuration changes.<br><br>Which actions should the solutions architect take to meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Enable AWS CloudTrail and use it for auditing.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use data lifecycle policies for the Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Enable AWS Trusted Advisor and reference the security dashboard.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Enable AWS Config and create rules for auditing and compliance purposes.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Restore previous resource configurations with an AWS CloudFormation template.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      4
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 319,
    "question": "A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems administrators have used shared SSH keys to manage the instances. After a recent audit, the company’s security team is mandating the removal of all shared keys. A solutions architect must design a solution that provides secure access to the EC2 instances.<br><br>Which solution will meet this requirement with the LEAST amount of administrative overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Systems Manager Session Manager to connect to the EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only SSH access from the bastion instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda function to generate a temporary SSH key.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 320,
    "question": "A company is using a fleet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-flight is lost. The company’s data science team wants to query ingested data in near-real time.<br><br>Which solution provides near-real-time data querying that is scalable with minimal data loss?",
    "answers": [
      {
        "id": 1,
        "answer": "Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 321,
    "question": "What should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?",
    "answers": [
      {
        "id": 1,
        "answer": "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 322,
    "question": "A solutions architect is designing a multi-tier application for a company. The application's users upload images from a mobile device. The application generates a thumbnail of each image and returns a message to the user to confirm that the image was uploaded successfully.<br><br>The thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to its users to notify them that the original image was received. The solutions architect must design the application to asynchronously dispatch requests to the different application tiers.<br><br>What should the solutions architect do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image upload process as an event source to invoke the Lambda function.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an AWS Step Functions workflow. Configure Step Functions to handle the orchestration between the application tiers and alert the user when thumbnail generation is complete.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions. Use one subscription with the application to generate the thumbnail after the image upload is complete. Use a second subscription to message the user's mobile app by way of a push notification after thumbnail generation is complete.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 323,
    "question": "A company’s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance.<br><br>A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company’s security team to analyze.<br><br>Which system architecture should the solutions architect recommend?",
    "answers": [
      {
        "id": 1,
        "answer": "Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 324,
    "question": "A company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The file storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes (TB) of data.<br><br>The company wants to ensure that end users retain immediate access to all file types from the on-premises systems without experiencing latency.<br><br>Which solution will meet these requirements with the LEAST amount of change to the company's existing infrastructure?",
    "answers": [
      {
        "id": 1,
        "answer": "Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the local cache to 10 TB. Modify existing applications to access the files through the NFS protocol. To recover from a disaster, provision an Amazon EC2 instance and mount the S3 bucket that contains the files.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all existing data to a virtual tape library. Configure the data backup solution to run nightly after the initial backup is complete. To recover from a disaster, provision an Amazon EC2 instance and restore the data to an Amazon Elastic Block Store (Amazon EBS) volume from the volumes in the virtual tape library.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the Volume Gateway cached volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 325,
    "question": "A company is hosting a web application from an Amazon S3 bucket. The application uses Amazon Cognito as an identity provider to authenticate users and return a JSON Web Token (JWT) that provides access to protected resources that are stored in another S3 bucket.<br><br>Upon deployment of the application, users report errors and are unable to access the protected content. A solutions architect must resolve this issue by providing proper permissions so that users can access the protected content.<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Update the S3 ACL to allow the application to access the protected content.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket from affecting the ability of users to access the protected content.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Update the Amazon Cognito pool to use custom attribute mappings within the identity pool and grant users the proper permissions to access the protected content.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  }
]