[
  {
    "id": 717,
    "question": "A company wants to migrate an on-premises legacy application to AWS. The application ingests customer order files from an on-premises enterprise resource planning (ERP) system. The application then uploads the files to an SFTP server. The application uses a scheduled job that checks for order files every hour.<br><br>The company already has an AWS account that has connectivity to the on-premises network. The new application on AWS must support integration with the existing ERP system. The new application must be secure and resilient and must use the SFTP protocol to process orders from the ERP system immediately.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS Transfer Family SFTP internet-facing server in two Availability Zones. Use Amazon S3 storage. Create an AWS Lambda function to process order files. Use S3 Event Notifications to send s3:ObjectCreated:* events to the Lambda function.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an AWS Transfer Family SFTP internet-facing server in one Availability Zone. Use Amazon Elastic File System (Amazon EFS) storage. Create an AWS Lambda function to process order files. Use a Transfer Family managed workflow to invoke the Lambda function.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an AWS Transfer Family SFTP internal server in two Availability Zones. Use Amazon Elastic File System (Amazon EFS) storage. Create an AWS Step Functions state machine to process order files. Use Amazon EventBridge Scheduler to invoke the state machine to periodically check Amazon EFS for order files.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an AWS Transfer Family SFTP internal server in two Availability Zones. Use Amazon S3 storage. Create an AWS Lambda function to process order files. Use a Transfer Family managed workflow to invoke the Lambda function.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 718,
    "question": "A company’s applications use Apache Hadoop and Apache Spark to process data on premises. The existing infrastructure is not scalable and is complex to manage.<br><br>A solutions architect must design a scalable solution that reduces operational complexity. The solution must keep the data processing on premises.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Site-to-Site VPN to access the on-premises Hadoop Distributed File System (HDFS) data and application. Use an Amazon EMR cluster to process the data.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS DataSync to connect to the on-premises Hadoop Distributed File System (HDFS) cluster. Create an Amazon EMR cluster to process the data.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Migrate the Apache Hadoop application and the Apache Spark application to Amazon EMR clusters on AWS Outposts. Use the EMR clusters to process the data.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use an AWS Snowball device to migrate the data to an Amazon S3 bucket. Create an Amazon EMR cluster to process the data.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 719,
    "question": "A company is migrating a large amount of data from on-premises storage to AWS. Windows, Mac, and Linux based Amazon EC2 instances in the same AWS Region will access the data by using SMB and NFS storage protocols. The company will access a portion of the data routinely. The company will access the remaining data infrequently.<br><br>The company needs to design a solution to host the data.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon FSx for ONTAP instance. Create an FSx for ONTAP file system with a root volume that uses the auto tiering policy. Migrate the data to the FSx for ONTAP volume.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an Amazon S3 bucket that uses S3 Intelligent-Tiering. Migrate the data to the S3 bucket by using an AWS Storage Gateway Amazon S3 File Gateway.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon FSx for OpenZFS file system. Migrate the data to the new volume.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 720,
    "question": "A manufacturing company runs its report generation application on AWS. The application generates each report in about 20 minutes. The application is built as a monolith that runs on a single Amazon EC2 instance. The application requires frequent updates to its tightly coupled modules. The application becomes complex to maintain as the company adds new features.<br><br>Each time the company patches a software module, the application experiences downtime. Report generation must restart from the beginning after any interruptions. The company wants to redesign the application so that the application can be flexible, scalable, and gradually improved. The company wants to minimize application downtime.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Run the application on AWS Lambda as a single function with maximum provisioned concurrency.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Run the application on Amazon EC2 Spot Instances as microservices with a Spot Fleet default allocation strategy.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Run the application on Amazon Elastic Container Service (Amazon ECS) as microservices with service auto scaling.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Run the application on AWS Elastic Beanstalk as a single application environment with an all-at-once deployment strategy.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 721,
    "question": "A company wants to rearchitect a large-scale web application to a serverless microservices architecture. The application uses Amazon EC2 instances and is written in Python.<br><br>The company selected one component of the web application to test as a microservice. The component supports hundreds of requests each second. The company wants to create and test the microservice on an AWS solution that supports Python. The solution must also scale automatically and require minimal infrastructure and minimal operational support.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Use a Spot Fleet with auto scaling of EC2 instances that run the most recent Amazon Linux operating system.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an AWS Elastic Beanstalk web server environment that has high availability configured.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon Elastic Kubernetes Service (Amazon EKS). Launch Auto Scaling groups of self-managed EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use an AWS Lambda function that runs custom developed code.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 722,
    "question": "A company has an AWS Direct Connect connection from its on-premises location to an AWS account. The AWS account has 30 different VPCs in the same AWS Region. The VPCs use private virtual interfaces (VIFs). Each VPC has a CIDR block that does not overlap with other networks under the company's control.<br><br>The company wants to centrally manage the networking architecture while still allowing each VPC to communicate with all other VPCs and on-premises networks.<br><br>Which solution will meet these requirements with the LEAST amount of operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a transit gateway, and associate the Direct Connect connection with a new transit VIF. Turn on the transit gateway's route propagation feature.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a Direct Connect gateway. Recreate the private VIFs to use the new gateway. Associate each VPC by creating new virtual private gateways.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a transit VPConnect the Direct Connect connection to the transit VPCreate a peering connection between all other VPCs in the Region. Update the route tables.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create AWS Site-to-Site VPN connections from on premises to each VPC. Ensure that both VPN tunnels are UP for each connection. Turn on the route propagation feature.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 723,
    "question": "A company has applications that run on Amazon EC2 instances. The EC2 instances connect to Amazon RDS databases by using an IAM role that has associated policies. The company wants to use AWS Systems Manager to patch the EC2 instances without disrupting the running applications.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a new IAM role. Attach the AmazonSSMManagedInstanceCore policy to the new IAM role. Attach the new IAM role to the EC2 instances and the existing IAM role.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an IAM user. Attach the AmazonSSMManagedInstanceCore policy to the IAM user. Configure Systems Manager to use the IAM user to manage the EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Enable Default Host Configuration Management in Systems Manager to manage the EC2 instances.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Remove the existing policies from the existing IAM role. Add the AmazonSSMManagedInstanceCore policy to the existing IAM role.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 724,
    "question": "A company runs container applications by using Amazon Elastic Kubernetes Service (Amazon EKS) and the Kubernetes Horizontal Pod Autoscaler. The workload is not consistent throughout the day. A solutions architect notices that the number of nodes does not automatically scale out when the existing nodes have reached maximum capacity in the cluster, which causes performance issues.<br><br>Which solution will resolve this issue with the LEAST administrative overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Scale out the nodes by tracking the memory usage.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use an AWS Lambda function to resize the EKS cluster automatically.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use an Amazon EC2 Auto Scaling group to distribute the workload.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 725,
    "question": "A company maintains about 300 TB in Amazon S3 Standard storage month after month. The S3 objects are each typically around 50 GB in size and are frequently replaced with multipart uploads by their global application. The number and size of S3 objects remain constant, but the company's S3 storage costs are increasing each month.<br><br>How should a solutions architect reduce costs in this situation?",
    "answers": [
      {
        "id": 1,
        "answer": "Switch from multipart uploads to Amazon S3 Transfer Acceleration.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Enable an S3 Lifecycle policy that deletes incomplete multipart uploads.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Configure S3 inventory to prevent objects from being archived too quickly.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure Amazon CloudFront to reduce the number of objects stored in Amazon S3.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 726,
    "question": "A company has deployed a multiplayer game for mobile devices. The game requires live location tracking of players based on latitude and longitude. The data store for the game must support rapid updates and retrieval of locations.<br><br>The game uses an Amazon RDS for PostgreSQL DB instance with read replicas to store the location data. During peak usage periods, the database is unable to maintain the performance that is needed for reading and writing updates. The game's user base is increasing rapidly.<br><br>What should a solutions architect do to improve the performance of the data tier?",
    "answers": [
      {
        "id": 1,
        "answer": "Take a snapshot of the existing DB instance. Restore the snapshot with Multi-AZ enabled.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Migrate from Amazon RDS to Amazon OpenSearch Service with OpenSearch Dashboards.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy Amazon DynamoDB Accelerator (DAX) in front of the existing DB instance. Modify the game to use DAX.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy an Amazon ElastiCache for Redis cluster in front of the existing DB instance. Modify the game to use Redis.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 727,
    "question": "A company stores critical data in Amazon DynamoDB tables in the company's AWS account. An IT administrator accidentally deleted a DynamoDB table. The deletion caused a significant loss of data and disrupted the company's operations. The company wants to prevent this type of disruption in the future.<br><br>Which solution will meet this requirement with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure a trail in AWS CloudTrail. Create an Amazon EventBridge rule for delete actions. Create an AWS Lambda function to automatically restore deleted DynamoDB tables.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a backup and restore plan for the DynamoDB tables. Recover the DynamoDB tables manually.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure deletion protection on the DynamoDB tables.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Enable point-in-time recovery on the DynamoDB tables.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 728,
    "question": "A company has an on-premises data center that is running out of storage capacity. The company wants to migrate its storage infrastructure to AWS while minimizing bandwidth costs. The solution must allow for immediate retrieval of data at no additional cost.<br><br>How can these requirements be met?",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy Amazon S3 Glacier Vault and enable expedited retrieval. Enable provisioned retrieval capacity for the workload.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy AWS Storage Gateway using cached volumes. Use Storage Gateway to store data in Amazon S3 while retaining copies of frequently accessed data subsets locally.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Deploy AWS Storage Gateway using stored volumes to store data locally. Use Storage Gateway to asynchronously back up point-in-time snapshots of the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy AWS Direct Connect to connect with the on-premises data center. Configure AWS Storage Gateway to store data locally. Use Storage Gateway to asynchronously back up point-in-time snapshots of the data to Amazon S3.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 729,
    "question": "A company runs a three-tier web application in a VPC across multiple Availability Zones. Amazon EC2 instances run in an Auto Scaling group for the application tier.<br><br>The company needs to make an automated scaling plan that will analyze each resource's daily and weekly historical workload trends. The configuration must scale resources appropriately according to both the forecast and live changes in utilization.<br><br>Which scaling strategy should a solutions architect recommend to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Implement dynamic scaling with step scaling based on average CPU utilization from the EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Enable predictive scaling to forecast and scale. Configure dynamic scaling with target tracking",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an automated scheduled scaling action based on the traffic patterns of the web application.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up a simple scaling policy. Increase the cooldown period based on the EC2 instance startup time.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 730,
    "question": "A package delivery company has an application that uses Amazon EC2 instances and an Amazon Aurora MySQL DB cluster. As the application becomes more popular, EC2 instance usage increases only slightly. DB cluster usage increases at a much faster rate.<br><br>The company adds a read replica, which reduces the DB cluster usage for a short period of time. However, the load continues to increase. The operations that cause the increase in DB cluster usage are all repeated read statements that are related to delivery details. The company needs to alleviate the effect of repeated reads on the DB cluster.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Implement an Amazon ElastiCache for Redis cluster between the application and the DB cluster.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Add an additional read replica to the DB cluster.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure Aurora Auto Scaling for the Aurora read replicas.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Modify the DB cluster to have multiple writer instances.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 731,
    "question": "A company has an application that uses an Amazon DynamoDB table for storage. A solutions architect discovers that many requests to the table are not returning the latest data. The company's users have not reported any other issues with database performance. Latency is in an acceptable range.<br><br>Which design change should the solutions architect recommend?",
    "answers": [
      {
        "id": 1,
        "answer": "Add read replicas to the table.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use a global secondary index (GSI).",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Request strongly consistent reads for the table.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Request eventually consistent reads for the table.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 732,
    "question": "A company has deployed its application on Amazon EC2 instances with an Amazon RDS database. The company used the principle of least privilege to configure the database access credentials. The company's security team wants to protect the application and the database from SQL injection and other web-based attacks.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Use security groups and network ACLs to secure the database and application servers.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS WAF to protect the application. Use RDS parameter groups to configure the security settings.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use AWS Network Firewall to protect the application and the database.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use different database accounts in the application code for different functions. Avoid granting excessive privileges to the database users.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 733,
    "question": "An ecommerce company runs applications in AWS accounts that are part of an organization in AWS Organizations. The applications run on Amazon Aurora PostgreSQL databases across all the accounts. The company needs to prevent malicious activity and must identify abnormal failed and incomplete login attempts to the databases.<br><br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "answers": [
      {
        "id": 1,
        "answer": "Attach service control policies (SCPs) to the root of the organization to identity the failed login attempts.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Enable the Amazon RDS Protection feature in Amazon GuardDuty for the member accounts of the organization.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Publish the Aurora general logs to a log group in Amazon CloudWatch Logs. Export the log data to a central Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Publish all the Aurora PostgreSQL database events in AWS CloudTrail to a central Amazon S3 bucket.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 734,
    "question": "A company has an AWS Direct Connect connection from its corporate data center to its VPC in the us-east-1 Region. The company recently acquired a corporation that has several VPCs and a Direct Connect connection between its on-premises data center and the eu-west-2 Region. The CIDR blocks for the VPCs of the company and the corporation do not overlap. The company requires connectivity between two Regions and the data centers. The company needs a solution that is scalable while reducing operational overhead.<br><br>What should a solutions architect do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Set up inter-Region VPC peering between the VPC in us-east-1 and the VPCs in eu-west-2.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create private virtual interfaces from the Direct Connect connection in us-east-1 to the VPCs in eu-west-2.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Establish VPN appliances in a fully meshed VPN network hosted by Amazon EC2. Use AWS VPN CloudHub to send and receive data between the data centers and each VPC.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Connect the existing Direct Connect connection to a Direct Connect gateway. Route traffic from the virtual private gateways of the VPCs in each Region to the Direct Connect gateway.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 735,
    "question": "A company is developing a mobile game that streams score updates to a backend processor and then posts results on a leaderboard. A solutions architect needs to design a solution that can handle large traffic spikes, process the mobile game updates in order of receipt, and store the processed updates in a highly available database. The company also wants to minimize the management overhead required to maintain the solution.<br><br>What should the solutions architect do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Push score updates to Amazon Kinesis Data Streams. Process the updates in Kinesis Data Streams with AWS Lambda. Store the processed updates in Amazon DynamoDB.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Push score updates to Amazon Kinesis Data Streams. Process the updates with a fleet of Amazon EC2 instances set up for Auto Scaling. Store the processed updates in Amazon Redshift.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to process the updates. Store the processed updates in a SQL database running on Amazon EC2.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue. Use a fleet of Amazon EC2 instances with Auto Scaling to process the updates in the SQS queue. Store the processed updates in an Amazon RDS Multi-AZ DB instance.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 736,
    "question": "A company has multiple AWS accounts with applications deployed in the us-west-2 Region. Application logs are stored within Amazon S3 buckets in each account. The company wants to build a centralized log analysis solution that uses a single S3 bucket. Logs must not leave us-west-2, and the company wants to incur minimal operational overhead.<br><br>Which solution meets these requirements and is MOST cost-effective?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an S3 Lifecycle policy that copies the objects from one of the application S3 buckets to the centralized S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use S3 Same-Region Replication to replicate logs from the S3 buckets to another S3 bucket in us-west-2. Use this S3 bucket for log analysis.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Write a script that uses the PutObject API operation every day to copy the entire contents of the buckets to another S3 bucket in us-west-2. Use this S3 bucket for log analysis.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Write AWS Lambda functions in these accounts that are triggered every time logs are delivered to the S3 buckets (s3:ObjectCreated:* event). Copy the logs to another S3 bucket in us-west-2. Use this S3 bucket for log analysis.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 737,
    "question": "A company has an application that delivers on-demand training videos to students around the world. The application also allows authorized content developers to upload videos. The data is stored in an Amazon S3 bucket in the us-east-2 Region.<br><br>The company has created an S3 bucket in the eu-west-2 Region and an S3 bucket in the ap-southeast-1 Region. The company wants to replicate the data to the new S3 buckets. The company needs to minimize latency for developers who upload videos and students who stream videos near eu-west-2 and ap-southeast-1.<br><br>Which combination of steps will meet these requirements with the FEWEST changes to the application? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket. Configure one-way replication from the us-east-2 S3 bucket to the ap-southeast-1 S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket. Configure one-way replication from the eu-west-2 S3 bucket to the ap-southeast-1 S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure two-way (bidirectional) replication among the S3 buckets that are in all three Regions.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name (ARN) of the Multi-Region Access Point for video streaming. Do not modify the application for video uploads.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name (ARN) of the Multi-Region Access Point for video streaming and uploads.",
        "correct": true
      }
    ],
    "corrects": [
      3,
      5
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 738,
    "question": "A company has a new mobile app. Anywhere in the world, users can see local news on topics they choose. Users also can post photos and videos from inside the app.<br><br>Users access content often in the first minutes after the content is posted. New content quickly replaces older content, and then the older content disappears. The local nature of the news means that users consume 90% of the content within the AWS Region where it is uploaded.<br><br>Which solution will optimize the user experience by providing the LOWEST latency for content uploads?",
    "answers": [
      {
        "id": 1,
        "answer": "Upload and store content in Amazon S3. Use Amazon CloudFront for the uploads.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Upload and store content in Amazon S3. Use S3 Transfer Acceleration for the uploads.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Upload content to Amazon EC2 instances in the Region that is closest to the user. Copy the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Upload and store content in Amazon S3 in the Region that is closest to the user. Use multiple distributions of Amazon CloudFront.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 739,
    "question": "A company is building a new application that uses serverless architecture. The architecture will consist of an Amazon API Gateway REST API and AWS Lambda functions to manage incoming requests.<br><br>The company wants to add a service that can send messages received from the API Gateway REST API to multiple target Lambda functions for processing. The service must offer message filtering that gives the target Lambda functions the ability to receive only the messages the functions need.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Send the requests from the API Gateway REST API to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure the target Lambda functions to poll the different SQS queues.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Send the requests from the API Gateway REST API to Amazon EventBridge. Configure EventBridge to invoke the target Lambda functions.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Send the requests from the API Gateway REST API to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Configure Amazon MSK to publish the messages to the target Lambda functions.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Send the requests from the API Gateway REST API to multiple Amazon Simple Queue Service (Amazon SQS) queues. Configure the target Lambda functions to poll the different SQS queues.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 740,
    "question": "A company migrated millions of archival files to Amazon S3. A solutions architect needs to implement a solution that will encrypt all the archival data by using a customer-provided key. The solution must encrypt existing unencrypted objects and future objects.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a list of unencrypted objects by filtering an Amazon S3 Inventory report. Configure an S3 Batch Operations job to encrypt the objects from the list with a server-side encryption with a customer-provided key (SSE-C). Configure the S3 default encryption feature to use a server-side encryption with a customer-provided key (SSE-C).",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use S3 Storage Lens metrics to identify unencrypted S3 buckets. Configure the S3 default encryption feature to use a server-side encryption with AWS KMS keys (SSE-KMS).",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a list of unencrypted objects by filtering the AWS usage report for Amazon S3. Configure an AWS Batch job to encrypt the objects from the list with a server-side encryption with AWS KMS keys (SSE-KMS). Configure the S3 default encryption feature to use a server-side encryption with AWS KMS keys (SSE-KMS).",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a list of unencrypted objects by filtering the AWS usage report for Amazon S3. Configure the S3 default encryption feature to use a server-side encryption with a customer-provided key (SSE-C).",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 741,
    "question": "The DNS provider that hosts a company's domain name records is experiencing outages that cause service disruption for a website running on AWS. The company needs to migrate to a more resilient managed DNS service and wants the service to run on AWS.<br><br>What should a solutions architect do to rapidly migrate the DNS hosting service?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon Route 53 public hosted zone for the domain name. Import the zone file containing the domain records hosted by the previous provider.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an Amazon Route 53 private hosted zone for the domain name. Import the zone file containing the domain records hosted by the previous provider.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a Simple AD directory in AWS. Enable zone transfer between the DNS provider and AWS Directory Service for Microsoft Active Directory for the domain records.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon Route 53 Resolver inbound endpoint in the VPC. Specify the IP addresses that the provider's DNS will forward DNS queries to. Configure the provider's DNS to forward DNS queries for the domain to the IP addresses that are specified in the inbound endpoint.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 742,
    "question": "A company is building an application on AWS that connects to an Amazon RDS database. The company wants to manage the application configuration and to securely store and retrieve credentials for the database and other services.<br><br>Which solution will meet these requirements with the LEAST administrative overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS AppConfig to store and manage the application configuration. Use AWS Secrets Manager to store and retrieve the credentials.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use AWS Lambda to store and manage the application configuration. Use AWS Systems Manager Parameter Store to store and retrieve the credentials.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use an encrypted application configuration file. Store the file in Amazon S3 for the application configuration. Create another S3 file to store and retrieve the credentials.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS AppConfig to store and manage the application configuration. Use Amazon RDS to store and retrieve the credentials.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 743,
    "question": "To meet security requirements, a company needs to encrypt all of its application data in transit while communicating with an Amazon RDS MySQL DB instance. A recent security audit revealed that encryption at rest is enabled using AWS Key Management Service (AWS KMS), but data in transit is not enabled.<br><br>What should a solutions architect do to satisfy the security requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Enable IAM database authentication on the database.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Provide self-signed certificates. Use the certificates in all connections to the RDS instance.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Take a snapshot of the RDS instance. Restore the snapshot to a new instance with encryption enabled.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Download AWS-provided root certificates. Provide the certificates in all connections to the RDS instance.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 744,
    "question": "A company is designing a new web service that will run on Amazon EC2 instances behind an Elastic Load Balancing (ELB) load balancer. However, many of the web service clients can only reach IP addresses authorized on their firewalls.<br><br>What should a solutions architect recommend to meet the clients’ needs?",
    "answers": [
      {
        "id": 1,
        "answer": "A Network Load Balancer with an associated Elastic IP address.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "An Application Load Balancer with an associated Elastic IP address.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "An EC2 instance with a public IP address running as a proxy in front of the load balancer.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 745,
    "question": "A company has established a new AWS account. The account is newly provisioned and no changes have been made to the default settings. The company is concerned about the security of the AWS account root user.<br><br>What should be done to secure the root user?",
    "answers": [
      {
        "id": 1,
        "answer": "Create IAM users for daily administrative tasks. Disable the root user.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create IAM users for daily administrative tasks. Enable multi-factor authentication on the root user.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Generate an access key for the root user. Use the access key for daily administration tasks instead of the AWS Management Console.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Provide the root user credentials to the most senior solutions architect. Have the solutions architect use the root user for daily administration tasks.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 746,
    "question": "A company is deploying an application that processes streaming data in near-real time. The company plans to use Amazon EC2 instances for the workload. The network architecture must be configurable to provide the lowest possible latency between nodes.<br><br>Which combination of network solutions will meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Enable and configure enhanced networking on each EC2 instance.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Group the EC2 instances in separate accounts.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Run the EC2 instances in a cluster placement group.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Attach multiple elastic network interfaces to each EC2 instance.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Use Amazon Elastic Block Store (Amazon EBS) optimized instance types.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 747,
    "question": "A financial services company wants to shut down two data centers and migrate more than 100 TB of data to AWS. The data has an intricate directory structure with millions of small files stored in deep hierarchies of subfolders. Most of the data is unstructured, and the company’s file storage consists of SMB-based storage types from multiple vendors. The company does not want to change its applications to access the data after migration.<br><br>What should a solutions architect do to meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Direct Connect to migrate the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS DataSync to migrate the data to Amazon FSx for Lustre.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS DataSync to migrate the data to Amazon FSx for Windows File Server.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use AWS Direct Connect to migrate the data on-premises file storage to an AWS Storage Gateway volume gateway.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 748,
    "question": "A company uses an organization in AWS Organizations to manage AWS accounts that contain applications. The company sets up a dedicated monitoring member account in the organization. The company wants to query and visualize observability data across the accounts by using Amazon CloudWatch.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Enable CloudWatch cross-account observability for the monitoring account. Deploy an AWS CloudFormation template provided by the monitoring account in each AWS account to share the data with the monitoring account.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Set up service control policies (SCPs) to provide access to CloudWatch in the monitoring account under the Organizations root organizational unit (OU).",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure a new IAM user in the monitoring account. In each AWS account, configure an IAM policy to have access to query and visualize the CloudWatch data in the account. Attach the new IAM policy to the new IAM user.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a new IAM user in the monitoring account. Create cross-account IAM policies in each AWS account. Attach the IAM policies to the new IAM user.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 749,
    "question": "A company’s website is used to sell products to the public. The site runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). There is also an Amazon CloudFront distribution, and AWS WAF is being used to protect against SQL injection attacks. The ALB is the origin for the CloudFront distribution. A recent review of security logs revealed an external malicious IP that needs to be blocked from accessing the website.<br><br>What should a solutions architect do to protect the application?",
    "answers": [
      {
        "id": 1,
        "answer": "Modify the network ACL on the CloudFront distribution to add a deny rule for the malicious IP address.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 750,
    "question": "A company sets up an organization in AWS Organizations that contains 10 AWS accounts. A solutions architect must design a solution to provide access to the accounts for several thousand employees. The company has an existing identity provider (IdP). The company wants to use the existing IdP for authentication to AWS.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create IAM users for the employees in the required AWS accounts. Connect IAM users to the existing IdP. Configure federated authentication for the IAM users.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up AWS account root users with user email addresses and passwords that are synchronized from the existing IdP.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure AWS IAM Identity Center (AWS Single Sign-On). Connect IAM Identity Center to the existing IdP. Provision users and groups from the existing IdP.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use AWS Resource Access Manager (AWS RAM) to share access to the AWS accounts with the users in the existing IdP.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 751,
    "question": "A solutions architect is designing an AWS Identity and Access Management (IAM) authorization model for a company's AWS account. The company has designated five specific employees to have full access to AWS services and resources in the AWS account.<br><br>The solutions architect has created an IAM user for each of the five designated employees and has created an IAM user group.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Attach the AdministratorAccess resource-based policy to the IAM user group. Place each of the five designated employee IAM users in the IAM user group.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Attach the SystemAdministrator identity-based policy to the IAM user group. Place each of the five designated employee IAM users in the IAM user group.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Attach the AdministratorAccess identity-based policy to the IAM user group. Place each of the five designated employee IAM users in the IAM user group.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Attach the SystemAdministrator resource-based policy to the IAM user group. Place each of the five designated employee IAM users in the IAM user group.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 752,
    "question": "A company has a multi-tier payment processing application that is based on virtual machines (VMs). The communication between the tiers occurs asynchronously through a third-party middleware solution that guarantees exactly-once delivery.<br><br>The company needs a solution that requires the least amount of infrastructure management. The solution must guarantee exactly-once delivery for application messaging.<br><br>Which combination of actions will meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Lambda for the compute layers in the architecture.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon EC2 instances for the compute layers in the architecture.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon Simple Notification Service (Amazon SNS) as the messaging component between the compute layers.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the messaging component between the compute layers.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Use containers that are based on Amazon Elastic Kubernetes Service (Amazon EKS) for the compute layers in the architecture.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      4
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 753,
    "question": "A company has a nightly batch processing routine that analyzes report files that an on-premises file system receives daily through SFTP. The company wants to move the solution to the AWS Cloud. The solution must be highly available and resilient. The solution also must minimize operational effort.<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy AWS Transfer for SFTP and an Amazon Elastic File System (Amazon EFS) file system for storage. Use an Amazon EC2 instance in an Auto Scaling group with a scheduled scaling policy to run the batch operation.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Deploy an Amazon EC2 instance that runs Linux and an SFTP service. Use an Amazon Elastic Block Store (Amazon EBS) volume for storage. Use an Auto Scaling group with the minimum number of instances and desired number of instances set to 1.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy an Amazon EC2 instance that runs Linux and an SFTP service. Use an Amazon Elastic File System (Amazon EFS) file system for storage. Use an Auto Scaling group with the minimum number of instances and desired number of instances set to 1.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy AWS Transfer for SFTP and an Amazon S3 bucket for storage. Modify the application to pull the batch files from Amazon S3 to an Amazon EC2 instance for processing. Use an EC2 instance in an Auto Scaling group with a scheduled scaling policy to run the batch operation.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 754,
    "question": "A company has users all around the world accessing its HTTP-based application deployed on Amazon EC2 instances in multiple AWS Regions. The company wants to improve the availability and performance of the application. The company also wants to protect the application against common web exploits that may affect availability, compromise security, or consume excessive resources. Static IP addresses are required.<br><br>What should a solutions architect recommend to accomplish this?",
    "answers": [
      {
        "id": 1,
        "answer": "Put the EC2 instances behind Network Load Balancers (NLBs) in each Region. Deploy AWS WAF on the NLBs. Create an accelerator using AWS Global Accelerator and register the NLBs as endpoints.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Put the EC2 instances behind Application Load Balancers (ALBs) in each Region. Deploy AWS WAF on the ALBs. Create an accelerator using AWS Global Accelerator and register the ALBs as endpoints.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Put the EC2 instances behind Network Load Balancers (NLBs) in each Region. Deploy AWS WAF on the NLBs. Create an Amazon CloudFront distribution with an origin that uses Amazon Route 53 latency-based routing to route requests to the NLBs.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Put the EC2 instances behind Application Load Balancers (ALBs) in each Region. Create an Amazon CloudFront distribution with an origin that uses Amazon Route 53 latency-based routing to route requests to the ALBs. Deploy AWS WAF on the CloudFront distribution.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 755,
    "question": "A company’s data platform uses an Amazon Aurora MySQL database. The database has multiple read replicas and multiple DB instances across different Availability Zones. Users have recently reported errors from the database that indicate that there are too many connections. The company wants to reduce the failover time by 20% when a read replica is promoted to primary writer.<br><br>Which solution will meet this requirement?",
    "answers": [
      {
        "id": 1,
        "answer": "Switch from Aurora to Amazon RDS with Multi-AZ cluster deployment.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon RDS Proxy in front of the Aurora database.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Switch to Amazon DynamoDB with DynamoDB Accelerator (DAX) for read connections.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Switch to Amazon Redshift with relocation capability.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 756,
    "question": "A company stores text files in Amazon S3. The text files include customer chat messages, date and time information, and customer personally identifiable information (PII).<br><br>The company needs a solution to provide samples of the conversations to an external service provider for quality control. The external service provider needs to randomly pick sample conversations up to the most recent conversation. The company must not share the customer PII with the external service provider. The solution must scale when the number of customer conversations increases.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Object Lambda Access Point. Create an AWS Lambda function that redacts the PII when the function reads the file. Instruct the external service provider to access the Object Lambda Access Point.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a batch process on an Amazon EC2 instance that regularly reads all new files, redacts the PII from the files, and writes the redacted files to a different S3 bucket. Instruct the external service provider to access the bucket that does not contain the PII.<br>B. Create a web application on an Amazon EC2 instance that presents a list of the files, redacts the PII from the files, and allows the external service provider to download new versions of the files that have the PII redacted.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon DynamoDB table. Create an AWS Lambda function that reads only the data in the files that does not contain PII. Configure the Lambda function to store the non-PII data in the DynamoDB table when a new file is written to Amazon S3. Grant the external service provider access to the DynamoDB table.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 757,
    "question": "A company is running a legacy system on an Amazon EC2 instance. The application code cannot be modified, and the system cannot run on more than one instance. A solutions architect must design a resilient solution that can improve the recovery time for the system.<br><br>What should the solutions architect recommend to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Enable termination protection for the EC2 instance.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure the EC2 instance for Multi-AZ deployment.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon CloudWatch alarm to recover the EC2 instance in case of failure.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Launch the EC2 instance with two Amazon Elastic Block Store (Amazon EBS) volumes that use RAID configurations for storage redundancy.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 758,
    "question": "A company wants to deploy its containerized application workloads to a VPC across three Availability Zones. The company needs a solution that is highly available across Availability Zones. The solution must require minimal changes to the application.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Elastic Container Service (Amazon ECS). Configure Amazon ECS Service Auto Scaling to use target tracking scaling. Set the minimum capacity to 3. Set the task placement strategy type to spread with an Availability Zone attribute.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon Elastic Kubernetes Service (Amazon EKS) self-managed nodes. Configure Application Auto Scaling to use target tracking scaling. Set the minimum capacity to 3.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon EC2 Reserved Instances. Launch three EC2 instances in a spread placement group. Configure an Auto Scaling group to use target tracking scaling. Set the minimum capacity to 3.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use an AWS Lambda function. Configure the Lambda function to connect to a VPC. Configure Application Auto Scaling to use Lambda as a scalable target. Set the minimum capacity to 3.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 759,
    "question": "A media company stores movies in Amazon S3. Each movie is stored in a single video file that ranges from 1 GB to 10 GB in size.<br><br>The company must be able to provide the streaming content of a movie within 5 minutes of a user purchase. There is higher demand for movies that are less than 20 years old than for movies that are more than 20 years old. The company wants to minimize hosting service costs based on demand.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Store all media content in Amazon S3. Use S3 Lifecycle policies to move media data into the Infrequent Access tier when the demand for a movie decreases.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Store newer movie video files in S3 Standard. Store older movie video files in S3 Standard-infrequent Access (S3 Standard-IA). When a user orders an older movie, retrieve the video file by using standard retrieval.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Store newer movie video files in S3 Intelligent-Tiering. Store older movie video files in S3 Glacier Flexible Retrieval. When a user orders an older movie, retrieve the video file by using expedited retrieval.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Store newer movie video files in S3 Standard. Store older movie video files in S3 Glacier Flexible Retrieval. When a user orders an older movie, retrieve the video file by using bulk retrieval.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 760,
    "question": "A solutions architect needs to design the architecture for an application that a vendor provides as a Docker container image. The container needs 50 GB of storage available for temporary files. The infrastructure must be serverless.<br><br>Which solution meets these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS Lambda function that uses the Docker container image with an Amazon S3 mounted volume that has more than 50 GB of space.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an AWS Lambda function that uses the Docker container image with an Amazon Elastic Block Store (Amazon EBS) volume that has more than 50 GB of space.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon Elastic Container Service (Amazon ECS) cluster that uses the AWS Fargate launch type. Create a task definition for the container image with an Amazon Elastic File System (Amazon EFS) volume. Create a service with that task definition.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an Amazon Elastic Container Service (Amazon ECS) cluster that uses the Amazon EC2 launch type with an Amazon Elastic Block Store (Amazon EBS) volume that has more than 50 GB of space. Create a task definition for the container image. Create a service with that task definition.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 761,
    "question": "A company needs to use its on-premises LDAP directory service to authenticate its users to the AWS Management Console. The directory service is not compatible with Security Assertion Markup Language (SAML).<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Enable AWS IAM Identity Center (AWS Single Sign-On) between AWS and the on-premises LDAP.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an IAM policy that uses AWS credentials, and integrate the policy into LDAP.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up a process that rotates the IAM credentials whenever LDAP credentials are updated.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Develop an on-premises custom identity broker application or process that uses AWS Security Token Service (AWS STS) to get short-lived credentials.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 762,
    "question": "A company stores multiple Amazon Machine Images (AMIs) in an AWS account to launch its Amazon EC2 instances. The AMIs contain critical data and configurations that are necessary for the company’s operations. The company wants to implement a solution that will recover accidentally deleted AMIs quickly and efficiently.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create Amazon Elastic Block Store (Amazon EBS) snapshots of the AMIs. Store the snapshots in a separate AWS account.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Copy all AMIs to another AWS account periodically.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a retention rule in Recycle Bin.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Upload the AMIs to an Amazon S3 bucket that has Cross-Region Replication.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 763,
    "question": "A company has 150 TB of archived image data stored on-premises that needs to be moved to the AWS Cloud within the next month. The company’s current network connection allows up to 100 Mbps uploads for this purpose during the night only.<br><br>What is the MOST cost-effective mechanism to move this data and meet the migration deadline?",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Snowmobile to ship the data to AWS.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Order multiple AWS Snowball devices to ship the data to AWS.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Enable Amazon S3 Transfer Acceleration and securely upload the data.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon S3 VPC endpoint and establish a VPN to upload the data.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 764,
    "question": "A company wants to migrate its three-tier application from on premises to AWS. The web tier and the application tier are running on third-party virtual machines (VMs). The database tier is running on MySQL.<br><br>The company needs to migrate the application by making the fewest possible changes to the architecture. The company also needs a database solution that can restore data to a specific point in time.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the web tier and the application tier to Amazon EC2 instances in private subnets. Migrate the database tier to Amazon RDS for MySQL in private subnets.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Migrate the web tier to Amazon EC2 instances in public subnets. Migrate the application tier to EC2 instances in private subnets. Migrate the database tier to Amazon Aurora MySQL in private subnets.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Migrate the web tier to Amazon EC2 instances in public subnets. Migrate the application tier to EC2 instances in private subnets. Migrate the database tier to Amazon RDS for MySQL in private subnets.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Migrate the web tier and the application tier to Amazon EC2 instances in public subnets. Migrate the database tier to Amazon Aurora MySQL in public subnets.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 765,
    "question": "A development team is collaborating with another company to create an integrated product. The other company needs to access an Amazon Simple Queue Service (Amazon SQS) queue that is contained in the development team's account. The other company wants to poll the queue without giving up its own account permissions to do so.<br><br>How should a solutions architect provide access to the SQS queue?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an instance profile that provides the other company access to the SQS queue.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an IAM policy that provides the other company access to the SQS queue.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an SQS access policy that provides the other company access to the SQS queue.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an Amazon Simple Notification Service (Amazon SNS) access policy that provides the other company access to the SQS queue.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 766,
    "question": "A company’s developers want a secure way to gain SSH access on the company's Amazon EC2 instances that run the latest version of Amazon Linux. The developers work remotely and in the corporate office.<br><br>The company wants to use AWS services as a part of the solution. The EC2 instances are hosted in a VPC private subnet and access the internet through a NAT gateway that is deployed in a public subnet.<br><br>What should a solutions architect do to meet these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a bastion host in the same subnet as the EC2 instances. Grant the ec2:CreateVpnConnection IAM permission to the developers. Install EC2 Instance Connect so that the developers can connect to the EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an AWS Site-to-Site VPN connection between the corporate network and the VPC. Instruct the developers to use the Site-to-Site VPN connection to access the EC2 instances when the developers are on the corporate network. Instruct the developers to set up another VPN connection for access when they work remotely.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a bastion host in the public subnet of the VPConfigure the security groups and SSH keys of the bastion host to only allow connections and SSH authentication from the developers’ corporate and remote networks. Instruct the developers to connect through the bastion host by using SSH to reach the EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Attach the AmazonSSMManagedInstanceCore IAM policy to an IAM role that is associated with the EC2 instances. Instruct the developers to use AWS Systems Manager Session Manager to access the EC2 instances.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 767,
    "question": "A pharmaceutical company is developing a new drug. The volume of data that the company generates has grown exponentially over the past few months. The company's researchers regularly require a subset of the entire dataset to be immediately available with minimal lag. However, the entire dataset does not need to be accessed on a daily basis. All the data currently resides in on-premises storage arrays, and the company wants to reduce ongoing capital expenses.<br><br>Which storage solution should a solutions architect recommend to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Run AWS DataSync as a scheduled cron job to migrate the data to an Amazon S3 bucket on an ongoing basis.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy an AWS Storage Gateway file gateway with an Amazon S3 bucket as the target storage. Migrate the data to the Storage Gateway appliance.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy an AWS Storage Gateway volume gateway with cached volumes with an Amazon S3 bucket as the target storage. Migrate the data to the Storage Gateway appliance.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure an AWS Site-to-Site VPN connection from the on-premises environment to AWS. Migrate data to an Amazon Elastic File System (Amazon EFS) file system.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 768,
    "question": "A company has a business-critical application that runs on Amazon EC2 instances. The application stores data in an Amazon DynamoDB table. The company must be able to revert the table to any point within the last 24 hours.<br><br>Which solution meets these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure point-in-time recovery for the table.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use AWS Backup for the table.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use an AWS Lambda function to make an on-demand backup of the table every hour.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Turn on streams on the table to capture a log of all changes to the table in the last 24 hours. Store a copy of the stream in an Amazon S3 bucket.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 769,
    "question": "A company hosts an application used to upload files to an Amazon S3 bucket. Once uploaded, the files are processed to extract metadata, which takes less than 5 seconds. The volume and frequency of the uploads varies from a few files each hour to hundreds of concurrent uploads. The company has asked a solutions architect to design a cost-effective architecture that will meet these requirements.<br><br>What should the solutions architect recommend?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure AWS CloudTrail trails to log S3 API calls. Use AWS AppSync to process the files.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure an object-created event notification within the S3 bucket to invoke an AWS Lambda function to process the files.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Configure Amazon Kinesis Data Streams to process and send data to Amazon S3. Invoke an AWS Lambda function to process the files.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure an Amazon Simple Notification Service (Amazon SNS) topic to process the files uploaded to Amazon S3. Invoke an AWS Lambda function to process the files.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 770,
    "question": "A company’s application is deployed on Amazon EC2 instances and uses AWS Lambda functions for an event-driven architecture. The company uses nonproduction development environments in a different AWS account to test new features before the company deploys the features to production.<br><br>The production instances show constant usage because of customers in different time zones. The company uses nonproduction instances only during business hours on weekdays. The company does not use the nonproduction instances on the weekends. The company wants to optimize the costs to run its application on AWS.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Use On-Demand Instances for the production instances. Use Dedicated Hosts for the nonproduction instances on weekends only.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Reserved Instances for the production instances and the nonproduction instances. Shut down the nonproduction instances when not in use.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Compute Savings Plans for the production instances. Use On-Demand Instances for the nonproduction instances. Shut down the nonproduction instances when not in use.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use Dedicated Hosts for the production instances. Use EC2 Instance Savings Plans for the nonproduction instances.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 771,
    "question": "A company stores data in an on-premises Oracle relational database. The company needs to make the data available in Amazon Aurora PostgreSQL for analysis. The company uses an AWS Site-to-Site VPN connection to connect its on-premises network to AWS.<br><br>The company must capture the changes that occur to the source database during the migration to Aurora PostgreSQL.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Use the AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora PostgreSQL schema. Use the AWS Database Migration Service (AWS DMS) full-load migration task to migrate the data.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS DataSync to migrate the data to an Amazon S3 bucket. Import the S3 data to Aurora PostgreSQL by using the Aurora PostgreSQL aws_s3 extension.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use the AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora PostgreSQL schema. Use AWS Database Migration Service (AWS DMS) to migrate the existing data and replicate the ongoing changes.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use an AWS Snowball device to migrate the data to an Amazon S3 bucket. Import the S3 data to Aurora PostgreSQL by using the Aurora PostgreSQL aws_s3 extension.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 772,
    "question": "A company built an application with Docker containers and needs to run the application in the AWS Cloud. The company wants to use a managed service to host the application.<br><br>The solution must scale in and out appropriately according to demand on the individual container services. The solution also must not result in additional operational overhead or infrastructure to manage.<br><br>Which solutions will meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Provision an Amazon API Gateway API. Connect the API to AWS Lambda to run the containers.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 773,
    "question": "An ecommerce company is running a seasonal online sale. The company hosts its website on Amazon EC2 instances spanning multiple Availability Zones. The company wants its website to manage sudden traffic increases during the sale.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Auto Scaling group that is large enough to handle peak traffic load. Stop half of the Amazon EC2 instances. Configure the Auto Scaling group to use the stopped instances to scale out when traffic increases.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Auto Scaling group for the website. Set the minimum size of the Auto Scaling group so that it can handle high traffic volumes without the need to scale out.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon CloudFront and Amazon ElastiCache to cache dynamic content with an Auto Scaling group set as the origin. Configure the Auto Scaling group with the instances necessary to populate CloudFront and ElastiCache. Scale in after the cache is fully populated.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure an Auto Scaling group to scale out as traffic increases. Create a launch template to start new instances from a preconfigured Amazon Machine Image (AMI).",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 774,
    "question": "A solutions architect must provide an automated solution for a company's compliance policy that states security groups cannot include a rule that allows SSH from 0.0.0.0/0. The company needs to be notified if there is any breach in the policy. A solution is needed as soon as possible.<br><br>What should the solutions architect do to meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Write an AWS Lambda script that monitors security groups for SSH being open to 0.0.0.0/0 addresses and creates a notification every time it finds one.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Enable the restricted-ssh AWS Config managed rule and generate an Amazon Simple Notification Service (Amazon SNS) notification when a noncompliant rule is created.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an IAM role with permissions to globally open security groups and network ACLs. Create an Amazon Simple Notification Service (Amazon SNS) topic to generate a notification every time the role is assumed by a user.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure a service control policy (SCP) that prevents non-administrative users from creating or editing security groups. Create a notification in the ticketing system when a user requests a rule that needs administrator permissions.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 775,
    "question": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes.<br><br>A company has deployed an application in an AWS account. The application consists of microservices that run on AWS Lambda and Amazon Elastic Kubernetes Service (Amazon EKS). A separate team supports each microservice. The company has multiple AWS accounts and wants to give each team its own account for its microservices.<br><br>A solutions architect needs to design a solution that will provide service-to-service communication over HTTPS (port 443). The solution also must provide a service registry for service discovery.<br><br>Which solution will meet these requirements with the LEAST administrative overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an inspection VPC. Deploy an AWS Network Firewall firewall to the inspection VPC. Attach the inspection VPC to a new transit gateway. Route VPC-to-VPC traffic to the inspection VPC. Apply firewall rules to allow only HTTPS communication.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a VPC Lattice service network. Associate the microservices with the service network. Define HTTPS listeners for each service. Register microservice compute resources as targets. Identify VPCs that need to communicate with the services. Associate those VPCs with the service network.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a Network Load Balancer (NLB) with an HTTPS listener and target groups for each microservice. Create an AWS PrivateLink endpoint service for each microservice. Create an interface VPC endpoint in each VPC that needs to consume that microservice.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create peering connections between VPCs that contain microservices. Create a prefix list for each service that requires a connection to a client. Create route tables to route traffic to the appropriate VPC. Create security groups to allow only HTTPS communication.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 776,
    "question": "A company has a mobile game that reads most of its metadata from an Amazon RDS DB instance. As the game increased in popularity, developers noticed slowdowns related to the game's metadata load times. Performance metrics indicate that simply scaling the database will not help. A solutions architect must explore all options that include capabilities for snapshots, replication, and sub-millisecond response times.<br><br>What should the solutions architect recommend to solve these issues?",
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the database to Amazon Aurora with Aurora Replicas.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Migrate the database to Amazon DynamoDB with global tables.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Add an Amazon ElastiCache for Redis layer in front of the database.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Add an Amazon ElastiCache for Memcached layer in front of the database.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 777,
    "question": "A company uses AWS Organizations for its multi-account AWS setup. The security organizational unit (OU) of the company needs to share approved Amazon Machine Images (AMIs) with the development OU. The AMIs are created by using AWS Key Management Service (AWS KMS) encrypted snapshots.<br><br>Which solution will meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Add the development team's OU Amazon Resource Name (ARN) to the launch permission list for the AMIs.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Add the Organizations root Amazon Resource Name (ARN) to the launch permission list for the AMIs.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Update the key policy to allow the development team's OU to use the AWS KMS keys that are used to decrypt the snapshots.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Add the development team’s account Amazon Resource Name (ARN) to the launch permission list for the AMIs.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Recreate the AWS KMS key. Add a key policy to allow the Organizations root Amazon Resource Name (ARN) to use the AWS KMS key.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 778,
    "question": "A data analytics company has 80 offices that are distributed globally. Each office hosts 1 PB of data and has between 1 and 2 Gbps of internet bandwidth.<br><br>The company needs to perform a one-time migration of a large amount of data from its offices to Amazon S3. The company must complete the migration within 4 weeks.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Establish a new 10 Gbps AWS Direct Connect connection to each office. Transfer the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use multiple AWS Snowball Edge storage-optimized devices to store and transfer the data to Amazon S3.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use an AWS Snowmobile to store and transfer the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up an AWS Storage Gateway Volume Gateway to transfer the data to Amazon S3.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 779,
    "question": "A company has an Amazon Elastic File System (Amazon EFS) file system that contains a reference dataset. The company has applications on Amazon EC2 instances that need to read the dataset. However, the applications must not be able to change the dataset. The company wants to use IAM access control to prevent the applications from being able to modify or delete the dataset.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Mount the EFS file system in read-only mode from within the EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a resource policy for the EFS file system that denies the elasticfilesystem:ClientWrite action to the IAM roles that are attached to the EC2 instances.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an identity policy for the EFS file system that denies the elasticfilesystem:ClientWrite action on the EFS file system.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an EFS access point for each application. Use Portable Operating System Interface (POSIX) file permissions to allow read-only access to files in the root directory.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 780,
    "question": "A company has hired an external vendor to perform work in the company’s AWS account. The vendor uses an automated tool that is hosted in an AWS account that the vendor owns. The vendor does not have IAM access to the company’s AWS account. The company needs to grant the vendor access to the company’s AWS account.<br><br>Which solution will meet these requirements MOST securely?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an IAM role in the company’s account to delegate access to the vendor’s IAM role. Attach the appropriate IAM policies to the role for the permissions that the vendor requires.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an IAM user in the company’s account with a password that meets the password complexity requirements. Attach the appropriate IAM policies to the user for the permissions that the vendor requires.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an IAM group in the company’s account. Add the automated tool’s IAM user from the vendor account to the group. Attach the appropriate IAM policies to the group for the permissions that the vendor requires.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an IAM user in the company’s account that has a permission boundary that allows the vendor’s account. Attach the appropriate IAM policies to the user for the permissions that the vendor requires.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 781,
    "question": "A company wants to run its experimental workloads in the AWS Cloud. The company has a budget for cloud spending. The company's CFO is concerned about cloud spending accountability for each department. The CFO wants to receive notification when the spending threshold reaches 60% of the budget.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Use cost allocation tags on AWS resources to label owners. Create usage budgets in AWS Budgets. Add an alert threshold to receive notification when spending exceeds 60% of the budget.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use AWS Cost Explorer forecasts to determine resource owners. Use AWS Cost Anomaly Detection to create alert threshold notifications when spending exceeds 60% of the budget.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use cost allocation tags on AWS resources to label owners. Use AWS Support API on AWS Trusted Advisor to create alert threshold notifications when spending exceeds 60% of the budget.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Cost Explorer forecasts to determine resource owners. Create usage budgets in AWS Budgets. Add an alert threshold to receive notification when spending exceeds 60% of the budget.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  }
]