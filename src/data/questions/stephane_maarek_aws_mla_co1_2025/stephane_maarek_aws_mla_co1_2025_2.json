[
  {
    "id": 1,
    "question": "<p>Your company is starting a new machine learning project, and the data preparation tasks are being handled by a team of business analysts. These analysts are more comfortable working with visual tools rather than writing code, and they need to combine, transform, and clean large datasets efficiently. The goal is to use a SageMaker tool that allows them to perform these tasks using a visual, point-and-click interface.</p>\n\n<p>Which SageMaker tool(s) would best suit the team's requirements for preparing and analyzing data without writing code?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon SageMaker Data Wrangler within Amazon SageMaker JumpStart</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Prepare data with SQL in Amazon SageMaker Studio. The SQL extension connections to Amazon Athena for working with datasets</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon SageMaker Data Wrangler within Amazon SageMaker Canvas</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Prepare data using Amazon EMR in Studio</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning (ML)",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon SageMaker Data Wrangler within Amazon SageMaker Canvas</strong></p>\n\n<p>Use Amazon SageMaker Data Wrangler in Amazon SageMaker Canvas to prepare, featurize and analyze your data. You can integrate a Data Wrangler data preparation flow into your machine learning (ML) workflows to simplify and streamline data pre-processing and feature engineering using little to no coding. You can also add your own Python scripts and transformations to customize workflows.</p>\n\n<ol>\n<li><p>Data Flow – Create a data flow to define a series of ML data prep steps. You can use a flow to combine datasets from different data sources, identify the number and types of transformations you want to apply to datasets, and define a data prep workflow that can be integrated into an ML pipeline.</p></li>\n<li><p>Transform – Clean and transform your dataset using standard transforms like string, vector, and numeric data formatting tools. Featurize your data using transforms like text and date/time embedding and categorical encoding.</p></li>\n<li><p>Generate Data Insights – Automatically verify data quality and detect abnormalities in your data with Data Wrangler Data Quality and Insights Report.</p></li>\n<li><p>Analyze – Analyze features in your dataset at any point in your flow. Data Wrangler includes built-in data visualization tools like scatter plots and histograms, as well as data analysis tools like target leakage analysis and quick modeling to understand feature correlation.</p></li>\n<li><p>Export – Export your data preparation workflow to a different location.</p></li>\n</ol>\n\n<p>SageMaker Canvas features for ML lifecycle:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q57-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q57-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/sagemaker/canvas/\">https://aws.amazon.com/sagemaker/canvas/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Prepare data using Amazon EMR in Studio</strong> - The integration between Amazon EMR and Amazon SageMaker Studio provides a scalable environment for large-scale data preparation for machine learning. This option is viable only if users are comfortable writing code.</p>\n\n<p><strong>Use Amazon SageMaker Data Wrangler within Amazon SageMaker JumpStart</strong> - Amazon SageMaker JumpStart is a machine learning (ML) hub that can help you accelerate your ML journey. With SageMaker JumpStart, you can evaluate, compare, and select FMs quickly based on pre-defined quality and responsibility metrics to perform tasks like article summarization and image generation. SageMaker JumpStart is not the right tool for the given use case.</p>\n\n<p><strong>Prepare data with SQL in Amazon SageMaker Studio. The SQL extension connections to Amazon Athena for working with datasets</strong> - Amazon SageMaker Studio provides a built-in SQL extension. This extension allows data scientists to perform tasks such as sampling, exploratory analysis, and feature engineering directly within the JupyterLab notebooks. However, this option does not offer any no-code features.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-data-prep.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-data-prep.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SageMaker Data Wrangler within Amazon SageMaker Canvas</strong>"
      },
      {
        "answer": "",
        "explanation": "Use Amazon SageMaker Data Wrangler in Amazon SageMaker Canvas to prepare, featurize and analyze your data. You can integrate a Data Wrangler data preparation flow into your machine learning (ML) workflows to simplify and streamline data pre-processing and feature engineering using little to no coding. You can also add your own Python scripts and transformations to customize workflows."
      },
      {},
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q57-i1.jpg",
        "answer": "",
        "explanation": "SageMaker Canvas features for ML lifecycle:"
      },
      {
        "link": "https://aws.amazon.com/sagemaker/canvas/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Prepare data using Amazon EMR in Studio</strong> - The integration between Amazon EMR and Amazon SageMaker Studio provides a scalable environment for large-scale data preparation for machine learning. This option is viable only if users are comfortable writing code."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SageMaker Data Wrangler within Amazon SageMaker JumpStart</strong> - Amazon SageMaker JumpStart is a machine learning (ML) hub that can help you accelerate your ML journey. With SageMaker JumpStart, you can evaluate, compare, and select FMs quickly based on pre-defined quality and responsibility metrics to perform tasks like article summarization and image generation. SageMaker JumpStart is not the right tool for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Prepare data with SQL in Amazon SageMaker Studio. The SQL extension connections to Amazon Athena for working with datasets</strong> - Amazon SageMaker Studio provides a built-in SQL extension. This extension allows data scientists to perform tasks such as sampling, exploratory analysis, and feature engineering directly within the JupyterLab notebooks. However, this option does not offer any no-code features."
      }
    ],
    "references": [
      "https://aws.amazon.com/sagemaker/canvas/",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-data-prep.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html"
    ]
  },
  {
    "id": 2,
    "question": "<p>You are a machine learning engineer tasked with deploying a machine learning model to a mobile device for real-time object detection. The model currently performs well but is too large to fit within the device's memory and computational constraints. To ensure the model can run efficiently on the device without sacrificing too much accuracy, you need to reduce its size.</p>\n\n<p>Given these constraints, which combination of techniques is the MOST LIKELY to effectively reduce the model’s size while maintaining acceptable performance?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Prune the model by removing less important neurons and layers, and apply quantization to reduce the precision of the model’s weights from 32-bit to 8-bit</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Apply dimensionality reduction using PCA on the input data, and use model ensembling to combine smaller models, which collectively match the performance of the original model</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use feature selection to reduce the number of input features, compress the model by converting the model weights from floating-point to integer format</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Reduce the size of the training dataset, use dropout to decrease model complexity, thereby shrinking the deployed model size</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Prune the model by removing less important neurons and layers, and apply quantization to reduce the precision of the model’s weights from 32-bit to 8-bit</strong></p>\n\n<p>Model pruning can significantly reduce model size without sacrificing accuracy. The idea is simple: identify the redundant parameters in the model that contribute little to the training process.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q11-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q11-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/machine-learning/pruning-machine-learning-models-with-amazon-sagemaker-debugger-and-amazon-sagemaker-experiments/\">https://aws.amazon.com/blogs/machine-learning/pruning-machine-learning-models-with-amazon-sagemaker-debugger-and-amazon-sagemaker-experiments/</a><p></p>\n\n<p>Quantization is a general term that refers to using technologies that store numbers and perform calculations on them in more compact and lower precision form than their original format (e.g., 32-bit floating point)</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q11-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q11-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q11-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q11-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p>via - <a href=\"https://aws.amazon.com/blogs/machine-learning/leveraging-low-precision-and-quantization-for-deep-learning-using-the-amazon-ec2-c5-instance-and-bigdl/\">https://aws.amazon.com/blogs/machine-learning/leveraging-low-precision-and-quantization-for-deep-learning-using-the-amazon-ec2-c5-instance-and-bigdl/</a></p>\n\n<p>Pruning and quantization are both effective methods for reducing model size. Pruning removes parts of the model that contribute little to overall performance, such as unnecessary neurons or layers, which reduces the model’s complexity. Quantization further reduces the model size by decreasing the precision of weights (e.g., from 32-bit floating-point to 8-bit integers), significantly lowering memory requirements without drastically impacting accuracy.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Apply dimensionality reduction using PCA on the input data, and use model ensembling to combine smaller models, which collectively match the performance of the original model</strong> - Dimensionality reduction using PCA can reduce the input data’s complexity but does not directly shrink the model itself. Model ensembling generally increases model size because it involves combining multiple models, making this approach unsuitable for reducing size.</p>\n\n<p><strong>Use feature selection to reduce the number of input features, compress the model by converting the model weights from floating-point to integer format</strong> - Feature selection can reduce input data size. However, converting weights from floating-point to integer format might be less precise than quantization methods like 8-bit quantization, which are specifically optimized for model compression.</p>\n\n<p><strong>Reduce the size of the training dataset, use dropout to decrease model complexity, thereby shrinking the deployed model size</strong> - Reducing the training dataset size and using dropout are strategies to prevent overfitting, not to reduce the deployed model size.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/leveraging-low-precision-and-quantization-for-deep-learning-using-the-amazon-ec2-c5-instance-and-bigdl/\">https://aws.amazon.com/blogs/machine-learning/leveraging-low-precision-and-quantization-for-deep-learning-using-the-amazon-ec2-c5-instance-and-bigdl/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/pruning-machine-learning-models-with-amazon-sagemaker-debugger-and-amazon-sagemaker-experiments/\">https://aws.amazon.com/blogs/machine-learning/pruning-machine-learning-models-with-amazon-sagemaker-debugger-and-amazon-sagemaker-experiments/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Prune the model by removing less important neurons and layers, and apply quantization to reduce the precision of the model’s weights from 32-bit to 8-bit</strong>"
      },
      {
        "answer": "",
        "explanation": "Model pruning can significantly reduce model size without sacrificing accuracy. The idea is simple: identify the redundant parameters in the model that contribute little to the training process."
      },
      {
        "link": "https://aws.amazon.com/blogs/machine-learning/pruning-machine-learning-models-with-amazon-sagemaker-debugger-and-amazon-sagemaker-experiments/"
      },
      {
        "answer": "",
        "explanation": "Quantization is a general term that refers to using technologies that store numbers and perform calculations on them in more compact and lower precision form than their original format (e.g., 32-bit floating point)"
      },
      {
        "link": "https://aws.amazon.com/blogs/machine-learning/leveraging-low-precision-and-quantization-for-deep-learning-using-the-amazon-ec2-c5-instance-and-bigdl/",
        "answer": "",
        "explanation": "via - <a href=\"https://aws.amazon.com/blogs/machine-learning/leveraging-low-precision-and-quantization-for-deep-learning-using-the-amazon-ec2-c5-instance-and-bigdl/\">https://aws.amazon.com/blogs/machine-learning/leveraging-low-precision-and-quantization-for-deep-learning-using-the-amazon-ec2-c5-instance-and-bigdl/</a>"
      },
      {
        "answer": "",
        "explanation": "Pruning and quantization are both effective methods for reducing model size. Pruning removes parts of the model that contribute little to overall performance, such as unnecessary neurons or layers, which reduces the model’s complexity. Quantization further reduces the model size by decreasing the precision of weights (e.g., from 32-bit floating-point to 8-bit integers), significantly lowering memory requirements without drastically impacting accuracy."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Apply dimensionality reduction using PCA on the input data, and use model ensembling to combine smaller models, which collectively match the performance of the original model</strong> - Dimensionality reduction using PCA can reduce the input data’s complexity but does not directly shrink the model itself. Model ensembling generally increases model size because it involves combining multiple models, making this approach unsuitable for reducing size."
      },
      {
        "answer": "",
        "explanation": "<strong>Use feature selection to reduce the number of input features, compress the model by converting the model weights from floating-point to integer format</strong> - Feature selection can reduce input data size. However, converting weights from floating-point to integer format might be less precise than quantization methods like 8-bit quantization, which are specifically optimized for model compression."
      },
      {
        "answer": "",
        "explanation": "<strong>Reduce the size of the training dataset, use dropout to decrease model complexity, thereby shrinking the deployed model size</strong> - Reducing the training dataset size and using dropout are strategies to prevent overfitting, not to reduce the deployed model size."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/machine-learning/pruning-machine-learning-models-with-amazon-sagemaker-debugger-and-amazon-sagemaker-experiments/",
      "https://aws.amazon.com/blogs/machine-learning/leveraging-low-precision-and-quantization-for-deep-learning-using-the-amazon-ec2-c5-instance-and-bigdl/"
    ]
  },
  {
    "id": 3,
    "question": "<p>You are a data scientist working for a healthcare company that develops predictive models for diagnosing diseases based on patient data. Due to regulatory requirements and the critical nature of healthcare decisions, model interpretability is a top priority. The company needs to ensure that the predictions made by the model can be explained to both medical professionals and regulatory bodies.</p>\n\n<p>You are evaluating different algorithms in Amazon SageMaker for your model, balancing the trade-off between accuracy and interpretability. The initial trials show that more complex models like deep neural networks (DNNs) yield higher accuracy but are less interpretable, whereas simpler models like logistic regression provide clearer insights but may not perform as well on the dataset.</p>\n\n<p>Given these considerations, which of the following approaches is MOST APPROPRIATE for achieving both interpretability and acceptable performance?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Choose a logistic regression model due to its high interpretability and supplement it with additional data preprocessing to improve accuracy</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy an ensemble of models including a complex model for accuracy and a simpler model for interpretability, using model stacking in SageMaker</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Select a deep neural network (DNN) model and use SHAP (SHapley Additive exPlanations) to provide interpretability</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a tree-based algorithm like XGBoost, which offers a balance between accuracy and interpretability with feature importance</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a tree-based algorithm like XGBoost, which offers a balance between accuracy and interpretability with feature importance</strong></p>\n\n<p>The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that tries to accurately predict a target variable by combining multiple estimates from a set of simpler models. The XGBoost algorithm performs well in machine learning competitions for the following reasons:</p>\n\n<p>Its robust handling of a variety of data types, relationships, distributions.</p>\n\n<p>The variety of hyperparameters that you can fine-tune.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q3-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q3-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/model-explainability-aws-ai-ml/interpretability-versus-explainability.html\">https://docs.aws.amazon.com/whitepapers/latest/model-explainability-aws-ai-ml/interpretability-versus-explainability.html</a><p></p>\n\n<p>XGBoost is a tree-based algorithm that naturally provides feature importance, making it easier to interpret which features are influencing the model's predictions. This strikes a balance between achieving high accuracy and maintaining interpretability, which is crucial in healthcare applications. Therefore, this represents the best option for the given use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Select a deep neural network (DNN) model and use SHAP (SHapley Additive exPlanations) to provide interpretability</strong> - You can use Shapley values to determine the contribution that each feature made to model predictions. These attributions can be provided for specific predictions and at a global level for the model as a whole. For example, if you used an ML model for college admissions, the explanations could help determine whether the GPA or the SAT score was the feature most responsible for the model’s predictions, and then you can determine how responsible each feature was for determining an admission decision about a particular student.</p>\n\n<p>While SHAP (Shapley values) can provide interpretability for complex models like DNNs, the explanations can be more challenging to understand for non-technical stakeholders, especially in a high-stakes environment like healthcare. This approach may not fully address the interpretability requirement.</p>\n\n<p><strong>Choose a logistic regression model due to its high interpretability and supplement it with additional data preprocessing to improve accuracy</strong> - Logistic regression is highly interpretable, but it may not perform well on complex datasets compared to more sophisticated algorithms. While data preprocessing can improve its performance, there is often a significant trade-off in accuracy.</p>\n\n<p><strong>Deploy an ensemble of models including a complex model for accuracy and a simpler model for interpretability, using model stacking in SageMaker</strong> - Using an ensemble model may complicate interpretability, especially when combining different types of models. Although this approach might improve accuracy, the resulting complexity could hinder the ability to explain predictions clearly.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/ml-model-interpretability/overview.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/ml-model-interpretability/overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/model-explainability-aws-ai-ml/interpretability-versus-explainability.html\">https://docs.aws.amazon.com/whitepapers/latest/model-explainability-aws-ai-ml/interpretability-versus-explainability.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-shapley-values.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-shapley-values.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use a tree-based algorithm like XGBoost, which offers a balance between accuracy and interpretability with feature importance</strong>"
      },
      {
        "answer": "",
        "explanation": "The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that tries to accurately predict a target variable by combining multiple estimates from a set of simpler models. The XGBoost algorithm performs well in machine learning competitions for the following reasons:"
      },
      {
        "answer": "",
        "explanation": "Its robust handling of a variety of data types, relationships, distributions."
      },
      {
        "answer": "",
        "explanation": "The variety of hyperparameters that you can fine-tune."
      },
      {
        "link": "https://docs.aws.amazon.com/whitepapers/latest/model-explainability-aws-ai-ml/interpretability-versus-explainability.html"
      },
      {
        "answer": "",
        "explanation": "XGBoost is a tree-based algorithm that naturally provides feature importance, making it easier to interpret which features are influencing the model's predictions. This strikes a balance between achieving high accuracy and maintaining interpretability, which is crucial in healthcare applications. Therefore, this represents the best option for the given use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Select a deep neural network (DNN) model and use SHAP (SHapley Additive exPlanations) to provide interpretability</strong> - You can use Shapley values to determine the contribution that each feature made to model predictions. These attributions can be provided for specific predictions and at a global level for the model as a whole. For example, if you used an ML model for college admissions, the explanations could help determine whether the GPA or the SAT score was the feature most responsible for the model’s predictions, and then you can determine how responsible each feature was for determining an admission decision about a particular student."
      },
      {
        "answer": "",
        "explanation": "While SHAP (Shapley values) can provide interpretability for complex models like DNNs, the explanations can be more challenging to understand for non-technical stakeholders, especially in a high-stakes environment like healthcare. This approach may not fully address the interpretability requirement."
      },
      {
        "answer": "",
        "explanation": "<strong>Choose a logistic regression model due to its high interpretability and supplement it with additional data preprocessing to improve accuracy</strong> - Logistic regression is highly interpretable, but it may not perform well on complex datasets compared to more sophisticated algorithms. While data preprocessing can improve its performance, there is often a significant trade-off in accuracy."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy an ensemble of models including a complex model for accuracy and a simpler model for interpretability, using model stacking in SageMaker</strong> - Using an ensemble model may complicate interpretability, especially when combining different types of models. Although this approach might improve accuracy, the resulting complexity could hinder the ability to explain predictions clearly."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/model-explainability-aws-ai-ml/interpretability-versus-explainability.html",
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/ml-model-interpretability/overview.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-shapley-values.html"
    ]
  },
  {
    "id": 4,
    "question": "<p>A healthcare organization is developing a deep learning model on Amazon SageMaker to predict patient outcomes. The training dataset is substantial, and the organization aims to optimize the model's hyperparameters to minimize the validation dataset's loss function.</p>\n\n<p>Which hyperparameter tuning strategy should the organization use to achieve this goal with the least computational effort?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Random Forest</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Hyperband</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Gradient descent</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Grid search</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Hyperband</strong></p>\n\n<p>Hyperband is a multi-fidelity based tuning strategy that dynamically reallocates resources. Hyperband uses both intermediate and final results of training jobs to re-allocate epochs to well-utilized hyperparameter configurations and automatically stops those that underperform. It also seamlessly scales to using many parallel training jobs. These features can significantly speed up hyperparameter tuning over random search and Bayesian optimization strategies.</p>\n\n<p>By allocating fewer resources to underperforming configurations, Hyperband reduces the total computation time compared to exhaustive methods like grid search.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Grid search</strong> - With grid search, you specify a list of hyperparameters and a performance metric, and the algorithm works through all possible combinations to determine the best fit. Grid search works well, but it’s relatively tedious and computationally intensive, especially with large numbers of hyperparameters.</p>\n\n<p><strong>Gradient descent</strong> - Gradient descent is not a hyperparameter tuning strategy for a deep learning model because it is an optimization algorithm used to minimize the loss function by adjusting model parameters (weights and biases) during training. Hyperparameter tuning, on the other hand, involves selecting optimal values for parameters that are not learned during training, such as learning rate, batch size, number of layers, or activation functions. While gradient descent facilitates the learning process by guiding how the model's weights are updated, hyperparameter tuning determines the settings that define the model's architecture and training process.</p>\n\n<p><strong>Random Forest</strong> - It is a machine learning algorithm used for classification and regression tasks. It is an ensemble learning method that builds multiple decision trees and merges their results (e.g., via averaging or voting) to improve predictive accuracy and control overfitting. Note that 'random search' is a hyperparameter optimization (HPO) technique in Amazon SageMaker. It selects random combinations of hyperparameters within predefined ranges and evaluates the model’s performance for each combination.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/what-is/hyperparameter-tuning/\">https://aws.amazon.com/what-is/hyperparameter-tuning/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Hyperband</strong>"
      },
      {
        "answer": "",
        "explanation": "Hyperband is a multi-fidelity based tuning strategy that dynamically reallocates resources. Hyperband uses both intermediate and final results of training jobs to re-allocate epochs to well-utilized hyperparameter configurations and automatically stops those that underperform. It also seamlessly scales to using many parallel training jobs. These features can significantly speed up hyperparameter tuning over random search and Bayesian optimization strategies."
      },
      {
        "answer": "",
        "explanation": "By allocating fewer resources to underperforming configurations, Hyperband reduces the total computation time compared to exhaustive methods like grid search."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Grid search</strong> - With grid search, you specify a list of hyperparameters and a performance metric, and the algorithm works through all possible combinations to determine the best fit. Grid search works well, but it’s relatively tedious and computationally intensive, especially with large numbers of hyperparameters."
      },
      {
        "answer": "",
        "explanation": "<strong>Gradient descent</strong> - Gradient descent is not a hyperparameter tuning strategy for a deep learning model because it is an optimization algorithm used to minimize the loss function by adjusting model parameters (weights and biases) during training. Hyperparameter tuning, on the other hand, involves selecting optimal values for parameters that are not learned during training, such as learning rate, batch size, number of layers, or activation functions. While gradient descent facilitates the learning process by guiding how the model's weights are updated, hyperparameter tuning determines the settings that define the model's architecture and training process."
      },
      {
        "answer": "",
        "explanation": "<strong>Random Forest</strong> - It is a machine learning algorithm used for classification and regression tasks. It is an ensemble learning method that builds multiple decision trees and merges their results (e.g., via averaging or voting) to improve predictive accuracy and control overfitting. Note that 'random search' is a hyperparameter optimization (HPO) technique in Amazon SageMaker. It selects random combinations of hyperparameters within predefined ranges and evaluates the model’s performance for each combination."
      }
    ],
    "references": [
      "https://aws.amazon.com/what-is/hyperparameter-tuning/",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html"
    ]
  },
  {
    "id": 5,
    "question": "<p>You are a Senior ML Engineer at a global logistics company that heavily relies on machine learning models for optimizing delivery routes, predicting demand, and detecting anomalies in real-time. The company is rapidly expanding, and you are tasked with building a maintainable, scalable, and cost-effective ML infrastructure that can handle increasing data volumes and evolving model requirements. You must implement best practices to ensure that the infrastructure can support ongoing development, deployment, monitoring, and scaling of multiple models across different regions.</p>\n\n<p>Which of the following strategies should you implement to create a maintainable, scalable, and cost-effective ML infrastructure for your company using AWS services? (Select three)</p>",
    "corrects": [
      1,
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store all model artifacts and data in Amazon S3, and use versioning to manage changes over time, ensuring that models can be easily rolled back if needed</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Provision fixed resources for each model to avoid unexpected costs, ensuring that the infrastructure is always available for each model</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize infrastructure as code (IaC) with AWS CloudFormation to automate the deployment and management of ML resources, making it easy to replicate and scale infrastructure across regions</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use a monolithic architecture to manage all machine learning models in a single environment, simplifying management and reducing overhead</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Implement a microservices-based architecture with Amazon SageMaker endpoints, where each model is deployed independently, allowing for isolated scaling and updates</p>",
        "correct": true
      },
      {
        "id": 6,
        "answer": "<p>Store all model artifacts and data in Amazon CodeCommit for version control and managing changes over time</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Implement a microservices-based architecture with Amazon SageMaker endpoints, where each model is deployed independently, allowing for isolated scaling and updates</strong></p>\n\n<p>A microservices-based architecture with Amazon SageMaker endpoints allows each model to be deployed, managed, and scaled independently. This approach enhances maintainability by isolating different components, making it easier to update models or scale specific services without affecting others. It also supports a more scalable and flexible infrastructure.</p>\n\n<p><strong>Utilize infrastructure as code (IaC) with AWS CloudFormation to automate the deployment and management of ML resources, making it easy to replicate and scale infrastructure across regions</strong></p>\n\n<p>Utilizing infrastructure as code (IaC) with AWS CloudFormation enables you to automate the deployment and management of your ML infrastructure. This approach ensures consistency across environments, simplifies scaling, and allows for rapid deployment in multiple regions. IaC also enhances maintainability by providing a version-controlled, repeatable process for managing infrastructure changes.</p>\n\n<p><strong>Store all model artifacts and data in Amazon S3, and use versioning to manage changes over time, ensuring that models can be easily rolled back if needed</strong></p>\n\n<p>Storing model artifacts and data in Amazon S3 with versioning is a good practice for maintaining model history and enabling rollbacks.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a monolithic architecture to manage all machine learning models in a single environment, simplifying management and reducing overhead</strong> - A monolithic architecture can simplify management in the short term but becomes difficult to maintain and scale as the number of models and services grows. It also limits flexibility in updating or scaling individual models, leading to potential bottlenecks and higher costs.</p>\n\n<p><strong>Provision fixed resources for each model to avoid unexpected costs, ensuring that the infrastructure is always available for each model</strong> -  Provisioning fixed resources for each model may lead to underutilization or overprovisioning, resulting in higher costs. Dynamic resource allocation, such as using auto-scaling or spot instances, is generally more cost-effective and scalable.</p>\n\n<p><strong>Store all model artifacts and data in Amazon CodeCommit for version control and managing changes over time</strong> - Amazon CodeCommit is the right fit for code-specific version control. You should not use CodeCommit to store model related data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cloudformation-overview.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cloudformation-overview.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/codecommit/\">https://aws.amazon.com/codecommit/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Implement a microservices-based architecture with Amazon SageMaker endpoints, where each model is deployed independently, allowing for isolated scaling and updates</strong>"
      },
      {
        "answer": "",
        "explanation": "A microservices-based architecture with Amazon SageMaker endpoints allows each model to be deployed, managed, and scaled independently. This approach enhances maintainability by isolating different components, making it easier to update models or scale specific services without affecting others. It also supports a more scalable and flexible infrastructure."
      },
      {
        "answer": "",
        "explanation": "<strong>Utilize infrastructure as code (IaC) with AWS CloudFormation to automate the deployment and management of ML resources, making it easy to replicate and scale infrastructure across regions</strong>"
      },
      {
        "answer": "",
        "explanation": "Utilizing infrastructure as code (IaC) with AWS CloudFormation enables you to automate the deployment and management of your ML infrastructure. This approach ensures consistency across environments, simplifies scaling, and allows for rapid deployment in multiple regions. IaC also enhances maintainability by providing a version-controlled, repeatable process for managing infrastructure changes."
      },
      {
        "answer": "",
        "explanation": "<strong>Store all model artifacts and data in Amazon S3, and use versioning to manage changes over time, ensuring that models can be easily rolled back if needed</strong>"
      },
      {
        "answer": "",
        "explanation": "Storing model artifacts and data in Amazon S3 with versioning is a good practice for maintaining model history and enabling rollbacks."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use a monolithic architecture to manage all machine learning models in a single environment, simplifying management and reducing overhead</strong> - A monolithic architecture can simplify management in the short term but becomes difficult to maintain and scale as the number of models and services grows. It also limits flexibility in updating or scaling individual models, leading to potential bottlenecks and higher costs."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision fixed resources for each model to avoid unexpected costs, ensuring that the infrastructure is always available for each model</strong> -  Provisioning fixed resources for each model may lead to underutilization or overprovisioning, resulting in higher costs. Dynamic resource allocation, such as using auto-scaling or spot instances, is generally more cost-effective and scalable."
      },
      {
        "answer": "",
        "explanation": "<strong>Store all model artifacts and data in Amazon CodeCommit for version control and managing changes over time</strong> - Amazon CodeCommit is the right fit for code-specific version control. You should not use CodeCommit to store model related data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cloudformation-overview.html",
      "https://aws.amazon.com/codecommit/"
    ]
  },
  {
    "id": 6,
    "question": "<p>A cloud architect is tasked with deploying a pre-trained AI model for a customer-facing application. The demand for the model's services fluctuates throughout the day, with occasional peaks in traffic and long periods of low activity. The architect needs to find a solution that scales automatically to handle high traffic while minimizing costs during idle periods.</p>\n\n<p>Which solution will fulfill these requirements with minimal management overhead?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the model to an Amazon SageMaker endpoint using Provisioned Concurrency with Serverless Inference to cater to dynamically changing traffic patterns</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy the model by using Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon EventBridge to trigger auto scaling</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy the model to an Amazon SageMaker endpoint. Create SageMaker endpoint auto scaling policies based on Amazon CloudWatch metrics to adjust the number of instances dynamically</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Deploy the model to a group of Amazon Elastic Container Service (Amazon ECS) managed EC2 instances and set up an Application Load Balancer (ALB) in front of the ECS fleet to distribute incoming traffic</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the model to an Amazon SageMaker endpoint. Create SageMaker endpoint auto scaling policies based on Amazon CloudWatch metrics to adjust the number of instances dynamically</strong></p>\n\n<p>Amazon SageMaker AI supports automatic scaling (auto scaling) for your hosted models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, auto scaling brings more instances online. When the workload decreases, auto scaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.</p>\n\n<p>To automatically scale as workload changes occur, you have two options: target tracking and step scaling policies.</p>\n\n<p>In most cases, AWS recommends using target tracking scaling policies. With target tracking, you choose an Amazon CloudWatch metric and target value. Auto scaling creates and manages the CloudWatch alarms for the scaling policy and calculates the scaling adjustment based on the metric and the target value. The policy adds and removes the number of instances as required to keep the metric at, or close to, the specified target value. For example, a scaling policy that uses the predefined InvocationsPerInstance metric with a target value of 70 can keep InvocationsPerInstance at, or close to 70.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the model to a group of Amazon Elastic Container Service (Amazon ECS) managed EC2 instances and set up an Application Load Balancer (ALB) in front of the ECS fleet to distribute incoming traffic</strong></p>\n\n<p><strong>Deploy the model by using Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon EventBridge to trigger auto scaling</strong></p>\n\n<p>Leveraging Amazon Elastic Container Service (Amazon ECS) to deploy the inference endpoint entails significant management overhead, so both these options are ruled out.</p>\n\n<p><strong>Deploy the model to an Amazon SageMaker endpoint using Provisioned Concurrency with Serverless Inference to cater to dynamically changing traffic patterns</strong> - Serverless Inference with provisioned concurrency is a cost-effective option when you have predictable bursts in your traffic. Provisioned concurrency allows you to deploy models on serverless endpoints with predictable performance, and high scalability by keeping your endpoints warm. So, this option is not cost-optimal for the given use case, as you have occasional peaks in traffic that are unpredictable.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-policy.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-policy.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/application/userguide/target-tracking-scaling-policy-overview.html\">https://docs.aws.amazon.com/autoscaling/application/userguide/target-tracking-scaling-policy-overview.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the model to an Amazon SageMaker endpoint. Create SageMaker endpoint auto scaling policies based on Amazon CloudWatch metrics to adjust the number of instances dynamically</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon SageMaker AI supports automatic scaling (auto scaling) for your hosted models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, auto scaling brings more instances online. When the workload decreases, auto scaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using."
      },
      {
        "answer": "",
        "explanation": "To automatically scale as workload changes occur, you have two options: target tracking and step scaling policies."
      },
      {
        "answer": "",
        "explanation": "In most cases, AWS recommends using target tracking scaling policies. With target tracking, you choose an Amazon CloudWatch metric and target value. Auto scaling creates and manages the CloudWatch alarms for the scaling policy and calculates the scaling adjustment based on the metric and the target value. The policy adds and removes the number of instances as required to keep the metric at, or close to, the specified target value. For example, a scaling policy that uses the predefined InvocationsPerInstance metric with a target value of 70 can keep InvocationsPerInstance at, or close to 70."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the model to a group of Amazon Elastic Container Service (Amazon ECS) managed EC2 instances and set up an Application Load Balancer (ALB) in front of the ECS fleet to distribute incoming traffic</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy the model by using Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon EventBridge to trigger auto scaling</strong>"
      },
      {
        "answer": "",
        "explanation": "Leveraging Amazon Elastic Container Service (Amazon ECS) to deploy the inference endpoint entails significant management overhead, so both these options are ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy the model to an Amazon SageMaker endpoint using Provisioned Concurrency with Serverless Inference to cater to dynamically changing traffic patterns</strong> - Serverless Inference with provisioned concurrency is a cost-effective option when you have predictable bursts in your traffic. Provisioned concurrency allows you to deploy models on serverless endpoints with predictable performance, and high scalability by keeping your endpoints warm. So, this option is not cost-optimal for the given use case, as you have occasional peaks in traffic that are unpredictable."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-policy.html",
      "https://docs.aws.amazon.com/autoscaling/application/userguide/target-tracking-scaling-policy-overview.html"
    ]
  },
  {
    "id": 7,
    "question": "<p>You are a data scientist at an insurance company that uses a machine learning model to assess the risk of potential clients and set insurance premiums accordingly. The model was trained on data from the past few years, but recently, the company has expanded its services to new regions with different demographic characteristics. You are concerned that these changes in the data distribution might affect the model's performance and lead to biased or inaccurate predictions. To address this, you decide to use Amazon SageMaker Clarify to monitor and detect any significant shifts in data distribution that could impact the model.</p>\n\n<p>Which of the following actions is the MOST EFFECTIVE for detecting changes in data distribution using SageMaker Clarify and mitigating their impact on model performance?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement a random sampling process to manually review a subset of incoming data each month, comparing it with the original training data to check for distribution changes</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up a continuous monitoring job with SageMaker Clarify to track changes in feature distribution over time and alert you when a significant feature attribution drift is detected, allowing you to investigate and potentially retrain the model</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Clarify to perform a one-time bias analysis during model training, ensuring that the model is initially fair and accurate, and manually monitor future data distribution changes</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Clarify’s bias detection capabilities to analyze the model’s output and identify any disparities between different demographic groups, retraining the model only if significant bias is detected</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up a continuous monitoring job with SageMaker Clarify to track changes in feature distribution over time and alert you when a significant feature attribution drift is detected, allowing you to investigate and potentially retrain the model</strong></p>\n\n<p>A drift in the distribution of live data for models in production can result in a corresponding drift in the feature attribution values, just as it could cause a drift in bias when monitoring bias metrics. Amazon SageMaker Clarify feature attribution monitoring helps data scientists and ML engineers monitor predictions for feature attribution drift on a regular basis.</p>\n\n<p>Continuous monitoring with SageMaker Clarify is the most effective approach for detecting changes in data distribution. By tracking feature distributions over time, you can identify when a significant shift occurs, investigate its impact on model performance, and decide if retraining is necessary. This proactive approach helps ensure that your model remains accurate and fair as the underlying data evolves.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q32-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q32-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-feature-attribution-drift.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-feature-attribution-drift.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use SageMaker Clarify’s bias detection capabilities to analyze the model’s output and identify any disparities between different demographic groups, retraining the model only if significant bias is detected</strong> - While SageMaker Clarify’s bias detection is useful, focusing solely on bias in the model’s output doesn’t address the broader issue of shifts in feature distribution that can impact overall model performance. Continuous monitoring is needed to detect such changes proactively.</p>\n\n<p><strong>Implement a random sampling process to manually review a subset of incoming data each month, comparing it with the original training data to check for distribution changes</strong> - Manual reviews of data can be labor-intensive, error-prone, and may not catch distribution changes in a timely manner. Automated monitoring with SageMaker Clarify is more efficient and reliable.</p>\n\n<p><strong>Use SageMaker Clarify to perform a one-time bias analysis during model training, ensuring that the model is initially fair and accurate, and manually monitor future data distribution changes</strong> - A one-time bias analysis during training helps ensure initial fairness, but it doesn’t address ongoing changes in data distribution after the model is deployed. Continuous monitoring is necessary to maintain model performance over time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-feature-attribution-drift.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-feature-attribution-drift.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up a continuous monitoring job with SageMaker Clarify to track changes in feature distribution over time and alert you when a significant feature attribution drift is detected, allowing you to investigate and potentially retrain the model</strong>"
      },
      {
        "answer": "",
        "explanation": "A drift in the distribution of live data for models in production can result in a corresponding drift in the feature attribution values, just as it could cause a drift in bias when monitoring bias metrics. Amazon SageMaker Clarify feature attribution monitoring helps data scientists and ML engineers monitor predictions for feature attribution drift on a regular basis."
      },
      {
        "answer": "",
        "explanation": "Continuous monitoring with SageMaker Clarify is the most effective approach for detecting changes in data distribution. By tracking feature distributions over time, you can identify when a significant shift occurs, investigate its impact on model performance, and decide if retraining is necessary. This proactive approach helps ensure that your model remains accurate and fair as the underlying data evolves."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-feature-attribution-drift.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use SageMaker Clarify’s bias detection capabilities to analyze the model’s output and identify any disparities between different demographic groups, retraining the model only if significant bias is detected</strong> - While SageMaker Clarify’s bias detection is useful, focusing solely on bias in the model’s output doesn’t address the broader issue of shifts in feature distribution that can impact overall model performance. Continuous monitoring is needed to detect such changes proactively."
      },
      {
        "answer": "",
        "explanation": "<strong>Implement a random sampling process to manually review a subset of incoming data each month, comparing it with the original training data to check for distribution changes</strong> - Manual reviews of data can be labor-intensive, error-prone, and may not catch distribution changes in a timely manner. Automated monitoring with SageMaker Clarify is more efficient and reliable."
      },
      {
        "answer": "",
        "explanation": "<strong>Use SageMaker Clarify to perform a one-time bias analysis during model training, ensuring that the model is initially fair and accurate, and manually monitor future data distribution changes</strong> - A one-time bias analysis during training helps ensure initial fairness, but it doesn’t address ongoing changes in data distribution after the model is deployed. Continuous monitoring is necessary to maintain model performance over time."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-feature-attribution-drift.html"
    ]
  },
  {
    "id": 8,
    "question": "<p>What is Feature Engineering in the context of machine learning?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Feature Engineering involves selecting, modifying, or creating features from raw data to improve the performance of machine learning models, and it is important because it can significantly enhance model accuracy and efficiency</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Feature Engineering is the process of tuning hyperparameters in a machine learning model, and it is important because it optimizes the model’s performance</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Feature Engineering refers to the visualization of data to understand patterns, and it is important because it helps in identifying trends in the dataset</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Feature Engineering is the process of collecting raw data, and it is important because it ensures the availability of data for model training</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning (ML)",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Feature Engineering involves selecting, modifying, or creating features from raw data to improve the performance of machine learning models, and it is important because it can significantly enhance model accuracy and efficiency</strong></p>\n\n<p>Feature Engineering is the process of selecting, modifying, or creating new features from raw data to enhance the performance of machine learning models. It is crucial because it can lead to significant improvements in model accuracy and efficiency by providing the model with better representations of the data.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q60-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q60-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/feature-engineering.html\">https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/feature-engineering.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Feature Engineering is the process of collecting raw data, and it is important because it ensures the availability of data for model training</strong> - Feature Engineering is not about collecting raw data; it focuses on transforming raw data into meaningful features for model training.</p>\n\n<p><strong>Feature Engineering is the process of tuning hyperparameters in a machine learning model, and it is important because it optimizes the model’s performance</strong> - Hyperparameter tuning is the process of selecting the best set of hyperparameters for a machine learning model. Hyperparameters are the external configurations of the model that are set before training and cannot be learned from the data. Feature Engineering is not related to Hyperparameter tuning.</p>\n\n<p><strong>Feature Engineering refers to the visualization of data to understand patterns, and it is important because it helps in identifying trends in the dataset</strong> - While data visualization is important, Feature Engineering specifically refers to transforming raw data into useful features, not just visualizing data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/feature-engineering.html\">https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/feature-engineering.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/what-is/feature-engineering/\">https://aws.amazon.com/what-is/feature-engineering/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Feature Engineering involves selecting, modifying, or creating features from raw data to improve the performance of machine learning models, and it is important because it can significantly enhance model accuracy and efficiency</strong>"
      },
      {
        "answer": "",
        "explanation": "Feature Engineering is the process of selecting, modifying, or creating new features from raw data to enhance the performance of machine learning models. It is crucial because it can lead to significant improvements in model accuracy and efficiency by providing the model with better representations of the data."
      },
      {
        "link": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/feature-engineering.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Feature Engineering is the process of collecting raw data, and it is important because it ensures the availability of data for model training</strong> - Feature Engineering is not about collecting raw data; it focuses on transforming raw data into meaningful features for model training."
      },
      {
        "answer": "",
        "explanation": "<strong>Feature Engineering is the process of tuning hyperparameters in a machine learning model, and it is important because it optimizes the model’s performance</strong> - Hyperparameter tuning is the process of selecting the best set of hyperparameters for a machine learning model. Hyperparameters are the external configurations of the model that are set before training and cannot be learned from the data. Feature Engineering is not related to Hyperparameter tuning."
      },
      {
        "answer": "",
        "explanation": "<strong>Feature Engineering refers to the visualization of data to understand patterns, and it is important because it helps in identifying trends in the dataset</strong> - While data visualization is important, Feature Engineering specifically refers to transforming raw data into useful features, not just visualizing data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/feature-engineering.html",
      "https://aws.amazon.com/what-is/feature-engineering/"
    ]
  },
  {
    "id": 9,
    "question": "<p>You are a Machine Learning Operations (MLOps) Engineer at a large technology company that runs multiple machine learning workloads across different environments. Your company has a variety of ML use cases, including continuous real-time predictions, scheduled batch processing for weekly model retraining, and small-scale experimentation with multiple hyperparameter tuning jobs that can tolerate failure.</p>\n\n<p>Which of the following strategies represents the best use of spot instances, on-demand instances, and reserved instances for different machine learning workloads, considering the requirements for cost optimization, reliability, and performance? (Select two)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use reserved instances for real-time predictions that require high availability</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use reserved instances for scheduled batch processing for model retraining</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use on-demand instances for real-time predictions that require high availability</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use spot instances for hyperparameter tuning jobs where interruptions can be tolerated</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Use on-demand instances for hyperparameter tuning jobs where interruptions can be tolerated</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use spot instances for hyperparameter tuning jobs where interruptions can be tolerated</strong></p>\n\n<p>Spot instances are well-suited for hyperparameter tuning jobs where interruptions are acceptable, as they offer significant cost savings but may be terminated by AWS if the capacity is needed elsewhere.</p>\n\n<p><strong>Use reserved instances for real-time predictions that require high availability</strong></p>\n\n<p>Reserved instances are the best choice for real-time predictions that require high availability, as they ensure that the necessary resources are always available while optimizing costs over time.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q24-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q24-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use on-demand instances for real-time predictions that require high availability</strong> - Although on-demand instances can certainly be used for real-time predictions that require high availability, however, reserved instances are a better fit, as they are more cost-effective.</p>\n\n<p><strong>Use on-demand instances for hyperparameter tuning jobs where interruptions can be tolerated</strong> - Although on-demand instances can certainly be used for hyperparameter tuning jobs where interruptions can be tolerated, however, spot instances are a better fit, as they are more cost-effective.</p>\n\n<p><strong>Use reserved instances for scheduled batch processing for model retraining</strong> - Since the model re-training is done on a weekly schedule, so on-demand instances are a better fit compared to the reserved instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p><a href=\"https://aws.amazon.com/ec2/pricing/reserved-instances/\">https://aws.amazon.com/ec2/pricing/reserved-instances/</a></p>\n\n<p><a href=\"https://aws.amazon.com/ec2/spot/\">https://aws.amazon.com/ec2/spot/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use spot instances for hyperparameter tuning jobs where interruptions can be tolerated</strong>"
      },
      {
        "answer": "",
        "explanation": "Spot instances are well-suited for hyperparameter tuning jobs where interruptions are acceptable, as they offer significant cost savings but may be terminated by AWS if the capacity is needed elsewhere."
      },
      {
        "answer": "",
        "explanation": "<strong>Use reserved instances for real-time predictions that require high availability</strong>"
      },
      {
        "answer": "",
        "explanation": "Reserved instances are the best choice for real-time predictions that require high availability, as they ensure that the necessary resources are always available while optimizing costs over time."
      },
      {
        "link": "https://aws.amazon.com/ec2/pricing/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use on-demand instances for real-time predictions that require high availability</strong> - Although on-demand instances can certainly be used for real-time predictions that require high availability, however, reserved instances are a better fit, as they are more cost-effective."
      },
      {
        "answer": "",
        "explanation": "<strong>Use on-demand instances for hyperparameter tuning jobs where interruptions can be tolerated</strong> - Although on-demand instances can certainly be used for hyperparameter tuning jobs where interruptions can be tolerated, however, spot instances are a better fit, as they are more cost-effective."
      },
      {
        "answer": "",
        "explanation": "<strong>Use reserved instances for scheduled batch processing for model retraining</strong> - Since the model re-training is done on a weekly schedule, so on-demand instances are a better fit compared to the reserved instances."
      }
    ],
    "references": [
      "https://aws.amazon.com/ec2/pricing/",
      "https://aws.amazon.com/ec2/pricing/reserved-instances/",
      "https://aws.amazon.com/ec2/spot/"
    ]
  },
  {
    "id": 10,
    "question": "<p>You are a machine learning engineer at a tech company responsible for maintaining a recommendation engine that drives personalized content for users. The current model has been performing well, but you’ve recently developed a new model that incorporates additional features and advanced techniques. Before fully replacing the existing model, you want to evaluate the new model's performance in a real-world environment. To do this, you decide to use A/B testing to compare the performance of the new model against the current model in terms of business outcomes.</p>\n\n<p>Which of the following approaches is the MOST EFFECTIVE for conducting A/B testing for this use case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the new model as the primary endpoint and keep the current model as a fallback. Use AWS Lambda to route a percentage of traffic to the new model and compare the predictions manually to decide which model to keep</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Randomly assign a percentage of users to each model and measure engagement metrics, such as click-through rates and session duration, using Amazon CloudWatch to aggregate the results and determine the winning model</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Fully deploy the new model and observe its performance over time. If it performs worse than the old model, revert to the previous model and analyze the differences</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy both models behind separate endpoints and use Amazon SageMaker Model Monitor to track key performance metrics like accuracy and latency, choosing the better model based on these metrics</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Randomly assign a percentage of users to each model and measure engagement metrics, such as click-through rates and session duration, using Amazon CloudWatch to aggregate the results and determine the winning model</strong></p>\n\n<p>This option correctly implements A/B testing by randomly assigning users to each model and measuring real-world engagement metrics like click-through rates, conversion rates and session duration. Using Amazon CloudWatch to aggregate and analyze these results allows for a data-driven decision on which model performs better in practice, based on actual user behavior.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q33-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q33-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/\">https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy both models behind separate endpoints and use Amazon SageMaker Model Monitor to track key performance metrics like accuracy and latency, choosing the better model based on these metrics</strong> - Deploying both models behind separate endpoints and tracking metrics with SageMaker Model Monitor can provide valuable insights into technical performance, but it does not measure real-world user engagement. A/B testing is most effective when comparing user-driven metrics, which directly reflect business outcomes.</p>\n\n<p><strong>Deploy the new model as the primary endpoint and keep the current model as a fallback. Use AWS Lambda to route a percentage of traffic to the new model and compare the predictions manually to decide which model to keep</strong> - Routing a percentage of traffic to the new model using AWS Lambda introduces some comparison, but manual evaluation is prone to bias and error. A/B testing should be automated and driven by metrics to ensure an objective comparison.</p>\n\n<p><strong>Fully deploy the new model and observe its performance over time. If it performs worse than the old model, revert to the previous model and analyze the differences</strong> - Fully deploying the new model without a direct comparison to the old model risks disrupting the user experience. A/B testing provides a safer and more controlled environment to assess the new model's performance before making a full switch.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/\">https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Randomly assign a percentage of users to each model and measure engagement metrics, such as click-through rates and session duration, using Amazon CloudWatch to aggregate the results and determine the winning model</strong>"
      },
      {
        "answer": "",
        "explanation": "This option correctly implements A/B testing by randomly assigning users to each model and measuring real-world engagement metrics like click-through rates, conversion rates and session duration. Using Amazon CloudWatch to aggregate and analyze these results allows for a data-driven decision on which model performs better in practice, based on actual user behavior."
      },
      {
        "link": "https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy both models behind separate endpoints and use Amazon SageMaker Model Monitor to track key performance metrics like accuracy and latency, choosing the better model based on these metrics</strong> - Deploying both models behind separate endpoints and tracking metrics with SageMaker Model Monitor can provide valuable insights into technical performance, but it does not measure real-world user engagement. A/B testing is most effective when comparing user-driven metrics, which directly reflect business outcomes."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy the new model as the primary endpoint and keep the current model as a fallback. Use AWS Lambda to route a percentage of traffic to the new model and compare the predictions manually to decide which model to keep</strong> - Routing a percentage of traffic to the new model using AWS Lambda introduces some comparison, but manual evaluation is prone to bias and error. A/B testing should be automated and driven by metrics to ensure an objective comparison."
      },
      {
        "answer": "",
        "explanation": "<strong>Fully deploy the new model and observe its performance over time. If it performs worse than the old model, revert to the previous model and analyze the differences</strong> - Fully deploying the new model without a direct comparison to the old model risks disrupting the user experience. A/B testing provides a safer and more controlled environment to assess the new model's performance before making a full switch."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/"
    ]
  },
  {
    "id": 11,
    "question": "<p>A company is using a fleet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in flight is lost. The company’s data science team wants to query ingested data in near-real time.</p>\n\n<p>Which solution provides near-real-time data querying that is scalable with minimal data loss?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Write to Amazon S3 bucket and use Amazon Redshift Spectrum to query the data</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Publish data to Amazon Kinesis Data Streams, use Kinesis Data Analytics to query the data</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning (ML)",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Publish data to Amazon Kinesis Data Streams, use Kinesis Data Analytics to query the data</strong></p>\n\n<p>With Amazon Kinesis Data Analytics for SQL Applications, you can process and analyze streaming data using standard SQL. The service enables you to quickly author and run powerful SQL code against streaming sources to perform time series analytics, feed real-time dashboards, and create real-time metrics.</p>\n\n<p>To get started with Kinesis Data Analytics, you create a Kinesis Data Analytics application that continuously reads and processes streaming data. The service supports ingesting data from Amazon Kinesis Data Streams and Amazon Data Firehose streaming sources. Kinesis Data Analytics supports Amazon Data Firehose (Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk), AWS Lambda, and Amazon Kinesis Data Streams as destinations.</p>\n\n<p>Kinesis Data Stream and Kinesis Data Analytics:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q50-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q50-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/\">https://aws.amazon.com/kinesis/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data</strong></p>\n\n<p><strong>Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Write to Amazon S3 bucket and use Amazon Redshift Spectrum to query the data</strong></p>\n\n<p>Amazon Elastic Block Store (EBS) is not typically used for streaming data. EBS is designed as block-level storage for use with Amazon EC2 instances, providing persistent storage that remains available independent of the lifecycle of the EC2 instance. It is ideal for workloads requiring consistent and low-latency storage performance, such as databases, file systems, and applications that need block storage. Therefore, both these options are incorrect.</p>\n\n<p><strong>Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data</strong> - The instance store is temporary storage and the data will be lost when the EC2 instance reboots.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html\">https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html</a></p>\n\n<p><a href=\"https://repost.aws/questions/QUpj1ciWvKTfCN8UGJ-UdqnQ/kinesis-data-ingestion\">https://repost.aws/questions/QUpj1ciWvKTfCN8UGJ-UdqnQ/kinesis-data-ingestion</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Publish data to Amazon Kinesis Data Streams, use Kinesis Data Analytics to query the data</strong>"
      },
      {
        "answer": "",
        "explanation": "With Amazon Kinesis Data Analytics for SQL Applications, you can process and analyze streaming data using standard SQL. The service enables you to quickly author and run powerful SQL code against streaming sources to perform time series analytics, feed real-time dashboards, and create real-time metrics."
      },
      {
        "answer": "",
        "explanation": "To get started with Kinesis Data Analytics, you create a Kinesis Data Analytics application that continuously reads and processes streaming data. The service supports ingesting data from Amazon Kinesis Data Streams and Amazon Data Firehose streaming sources. Kinesis Data Analytics supports Amazon Data Firehose (Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk), AWS Lambda, and Amazon Kinesis Data Streams as destinations."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q50-i1.jpg",
        "answer": "",
        "explanation": "Kinesis Data Stream and Kinesis Data Analytics:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Write to Amazon S3 bucket and use Amazon Redshift Spectrum to query the data</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic Block Store (EBS) is not typically used for streaming data. EBS is designed as block-level storage for use with Amazon EC2 instances, providing persistent storage that remains available independent of the lifecycle of the EC2 instance. It is ideal for workloads requiring consistent and low-latency storage performance, such as databases, file systems, and applications that need block storage. Therefore, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data</strong> - The instance store is temporary storage and the data will be lost when the EC2 instance reboots."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/",
      "https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html",
      "https://repost.aws/questions/QUpj1ciWvKTfCN8UGJ-UdqnQ/kinesis-data-ingestion"
    ]
  },
  {
    "id": 12,
    "question": "<p>You are a machine learning engineer tasked with building a deep learning model to classify images for an autonomous vehicle project. The dataset is massive, consisting of millions of labeled images. Initial training runs on a single GPU instance in Amazon SageMaker are taking too long, and the training costs are rising. You need to reduce the model training time without compromising performance significantly.</p>\n\n<p>Which of the following approaches is the MOST LIKELY to effectively reduce the training time while maintaining model performance?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement distributed training using multiple GPU instances to parallelize the training process, reducing the overall time</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Reduce the size of the training dataset to speed up training, even if it means using fewer examples per class</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable early stopping to halt training when the model’s performance on the validation set stops improving, thereby avoiding overfitting</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Switch to a smaller instance type to reduce computational costs, accepting a longer training time as a trade-off</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement distributed training using multiple GPU instances to parallelize the training process, reducing the overall time</strong></p>\n\n<p>Distributed training allows you to split the workload across multiple GPU instances, significantly reducing training time by processing more data in parallel. Amazon SageMaker supports distributed training, making this an effective approach for large datasets and complex models.</p>\n\n<p>SageMaker provides distributed training libraries and supports various distributed training options for deep learning tasks such as computer vision (CV) and natural language processing (NLP). With SageMaker’s distributed training libraries, you can run highly scalable and cost-effective custom data parallel and model parallel deep learning training jobs.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q6-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q6-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable early stopping to halt training when the model’s performance on the validation set stops improving, thereby avoiding overfitting</strong> - Early stopping is a useful technique to prevent overfitting by stopping training once the validation performance plateaus. While this can reduce training time, its effectiveness depends on the model's behavior and may not significantly shorten training time if the model converges slowly, as exemplified by the long training runs on a single GPU instance for the given use case.</p>\n\n<p><strong>Switch to a smaller instance type to reduce computational costs, accepting a longer training time as a trade-off</strong> - Switching to a smaller instance type might reduce costs, but it will likely increase training time, which is counterproductive to the goal of reducing overall training time.</p>\n\n<p><strong>Reduce the size of the training dataset to speed up training, even if it means using fewer examples per class</strong> - Reducing the dataset size could speed up training, but it would likely compromise model performance by reducing the amount of data the model can learn from, especially in a scenario where data diversity is critical, such as image classification for autonomous vehicles.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Implement distributed training using multiple GPU instances to parallelize the training process, reducing the overall time</strong>"
      },
      {
        "answer": "",
        "explanation": "Distributed training allows you to split the workload across multiple GPU instances, significantly reducing training time by processing more data in parallel. Amazon SageMaker supports distributed training, making this an effective approach for large datasets and complex models."
      },
      {
        "answer": "",
        "explanation": "SageMaker provides distributed training libraries and supports various distributed training options for deep learning tasks such as computer vision (CV) and natural language processing (NLP). With SageMaker’s distributed training libraries, you can run highly scalable and cost-effective custom data parallel and model parallel deep learning training jobs."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable early stopping to halt training when the model’s performance on the validation set stops improving, thereby avoiding overfitting</strong> - Early stopping is a useful technique to prevent overfitting by stopping training once the validation performance plateaus. While this can reduce training time, its effectiveness depends on the model's behavior and may not significantly shorten training time if the model converges slowly, as exemplified by the long training runs on a single GPU instance for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Switch to a smaller instance type to reduce computational costs, accepting a longer training time as a trade-off</strong> - Switching to a smaller instance type might reduce costs, but it will likely increase training time, which is counterproductive to the goal of reducing overall training time."
      },
      {
        "answer": "",
        "explanation": "<strong>Reduce the size of the training dataset to speed up training, even if it means using fewer examples per class</strong> - Reducing the dataset size could speed up training, but it would likely compromise model performance by reducing the amount of data the model can learn from, especially in a scenario where data diversity is critical, such as image classification for autonomous vehicles."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html"
    ]
  },
  {
    "id": 13,
    "question": "<p>A company uses a generative model to analyze animal images in the training dataset to record variables like different ear shapes, eye shapes, tail features, and skin patterns.</p>\n\n<p>Which of the following tasks can the generative model perform?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The model can recreate new animal images that were not in the training dataset</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>The model can classify a single species of animals such as cats</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The model can identify any image from the training dataset</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The model can classify multiple species of animals such as cats, dogs, etc</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The model can recreate new animal images that were not in the training dataset</strong></p>\n\n<p>Generative artificial intelligence (generative AI) is a type of AI that can create new content and ideas, including conversations, stories, images, videos, and music. AI technologies attempt to mimic human intelligence in nontraditional computing tasks like image recognition, natural language processing (NLP), and translation.</p>\n\n<p>Generative models can analyze animal images to record variables like different ear shapes, eye shapes, tail features, and skin patterns. They learn features and their relations to understand what different animals look like in general. They can then recreate new animal images that were not in the training set.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q56-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q56-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/what-is/generative-ai/\">https://aws.amazon.com/what-is/generative-ai/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The model can classify a single species of animals such as cats</strong></p>\n\n<p><strong>The model can classify multiple species of animals such as cats, dogs, etc</strong></p>\n\n<p>Traditional machine learning models were discriminative or focused on classifying data points. They attempted to determine the relationship between known and unknown factors. For example, they look at images—known data like pixel arrangement, line, color, and shape—and map them to words—the unknown factor. Only discriminative models can act as single-class classifiers or multi-class classifiers. Therefore, both these options are incorrect.</p>\n\n<p><strong>The model can identify any image from the training dataset</strong> - This option has been added as a distractor. A generative model is not an image-matching algorithm. It cannot identify an image from the training dataset.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/what-is/generative-ai/\">https://aws.amazon.com/what-is/generative-ai/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The model can recreate new animal images that were not in the training dataset</strong>"
      },
      {
        "answer": "",
        "explanation": "Generative artificial intelligence (generative AI) is a type of AI that can create new content and ideas, including conversations, stories, images, videos, and music. AI technologies attempt to mimic human intelligence in nontraditional computing tasks like image recognition, natural language processing (NLP), and translation."
      },
      {
        "answer": "",
        "explanation": "Generative models can analyze animal images to record variables like different ear shapes, eye shapes, tail features, and skin patterns. They learn features and their relations to understand what different animals look like in general. They can then recreate new animal images that were not in the training set."
      },
      {
        "link": "https://aws.amazon.com/what-is/generative-ai/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The model can classify a single species of animals such as cats</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>The model can classify multiple species of animals such as cats, dogs, etc</strong>"
      },
      {
        "answer": "",
        "explanation": "Traditional machine learning models were discriminative or focused on classifying data points. They attempted to determine the relationship between known and unknown factors. For example, they look at images—known data like pixel arrangement, line, color, and shape—and map them to words—the unknown factor. Only discriminative models can act as single-class classifiers or multi-class classifiers. Therefore, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>The model can identify any image from the training dataset</strong> - This option has been added as a distractor. A generative model is not an image-matching algorithm. It cannot identify an image from the training dataset."
      }
    ],
    "references": [
      "https://aws.amazon.com/what-is/generative-ai/"
    ]
  },
  {
    "id": 14,
    "question": "<p>You are a machine learning engineer working for an e-commerce company. You have developed a recommendation model that predicts products customers are likely to buy based on their browsing history and past purchases. The model initially performs well, but after deploying it in production, you notice two issues: the model's performance degrades over time as new data is added (catastrophic forgetting) and the model shows signs of overfitting during retraining on updated datasets.</p>\n\n<p>Given these challenges, which of the following strategies is the MOST LIKELY to help prevent overfitting and catastrophic forgetting while maintaining model accuracy?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Regularly update the training dataset with new data, apply L2 regularization to manage overfitting, and use an ensemble of models to prevent catastrophic forgetting</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Apply L2 regularization to reduce overfitting, use dropout to prevent underfitting, and retrain the model on the entire dataset periodically to avoid catastrophic forgetting</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use early stopping during training to prevent overfitting, incorporate new data incrementally through transfer learning to mitigate catastrophic forgetting, and apply L1 regularization to ensure feature selection</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Reduce the model complexity by decreasing the number of features, apply data augmentation to handle underfitting, and leverage L1 regularization to address catastrophic forgetting</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use early stopping during training to prevent overfitting, incorporate new data incrementally through transfer learning to mitigate catastrophic forgetting, and apply L1 regularization to ensure feature selection</strong></p>\n\n<p>Early stopping is a proven method to prevent overfitting by halting training when the model’s performance on the validation set stops improving. Incorporating new data incrementally through transfer learning helps to mitigate catastrophic forgetting by allowing the model to learn new information while retaining its prior knowledge.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q9-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q9-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html</a><p></p>\n\n<p>Regularization helps prevent linear models from overfitting training data examples by penalizing extreme weight values. L1 regularization reduces the number of features used in the model by pushing the weight of features that would otherwise have very small weights to zero. L1 regularization produces sparse models and reduces the amount of noise in the model. L2 regularization results in smaller overall weight values, which stabilizes the weights when there is high correlation between the features.</p>\n\n<p>L1 regularization is beneficial for feature selection, which can improve model generalization and prevent both overfitting and underfitting.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q9-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q9-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/machine-learning/automatically-retrain-neural-networks-with-renate/\">https://aws.amazon.com/blogs/machine-learning/automatically-retrain-neural-networks-with-renate/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Apply L2 regularization to reduce overfitting, use dropout to prevent underfitting, and retrain the model on the entire dataset periodically to avoid catastrophic forgetting</strong> - While L2 regularization is effective for reducing overfitting, using dropout typically addresses overfitting, not underfitting. Retraining the model on the entire dataset periodically may help, but it could still lead to catastrophic forgetting as the model might \"forget\" previous knowledge when learning from new data.</p>\n\n<p><strong>Reduce the model complexity by decreasing the number of features, apply data augmentation to handle underfitting, and leverage L1 regularization to address catastrophic forgetting</strong> - Reducing model complexity can help with overfitting, but it might lead to underfitting if too many features are removed. Data augmentation is more relevant for addressing overfitting, particularly in image or text data, rather than underfitting. L1 regularization is effective for reducing overfitting, and not for addressing catastrophic forgetting.</p>\n\n<p><strong>Regularly update the training dataset with new data, apply L2 regularization to manage overfitting, and use an ensemble of models to prevent catastrophic forgetting</strong> - Regularly updating the dataset can help with model performance over time, but it might not address catastrophic forgetting unless combined with techniques like transfer learning. L2 regularization and ensembling are useful strategies, but ensembling does not specifically prevent catastrophic forgetting.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/automatically-retrain-neural-networks-with-renate/\">https://aws.amazon.com/blogs/machine-learning/automatically-retrain-neural-networks-with-renate/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use early stopping during training to prevent overfitting, incorporate new data incrementally through transfer learning to mitigate catastrophic forgetting, and apply L1 regularization to ensure feature selection</strong>"
      },
      {
        "answer": "",
        "explanation": "Early stopping is a proven method to prevent overfitting by halting training when the model’s performance on the validation set stops improving. Incorporating new data incrementally through transfer learning helps to mitigate catastrophic forgetting by allowing the model to learn new information while retaining its prior knowledge."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html"
      },
      {
        "answer": "",
        "explanation": "Regularization helps prevent linear models from overfitting training data examples by penalizing extreme weight values. L1 regularization reduces the number of features used in the model by pushing the weight of features that would otherwise have very small weights to zero. L1 regularization produces sparse models and reduces the amount of noise in the model. L2 regularization results in smaller overall weight values, which stabilizes the weights when there is high correlation between the features."
      },
      {
        "answer": "",
        "explanation": "L1 regularization is beneficial for feature selection, which can improve model generalization and prevent both overfitting and underfitting."
      },
      {
        "link": "https://aws.amazon.com/blogs/machine-learning/automatically-retrain-neural-networks-with-renate/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Apply L2 regularization to reduce overfitting, use dropout to prevent underfitting, and retrain the model on the entire dataset periodically to avoid catastrophic forgetting</strong> - While L2 regularization is effective for reducing overfitting, using dropout typically addresses overfitting, not underfitting. Retraining the model on the entire dataset periodically may help, but it could still lead to catastrophic forgetting as the model might \"forget\" previous knowledge when learning from new data."
      },
      {
        "answer": "",
        "explanation": "<strong>Reduce the model complexity by decreasing the number of features, apply data augmentation to handle underfitting, and leverage L1 regularization to address catastrophic forgetting</strong> - Reducing model complexity can help with overfitting, but it might lead to underfitting if too many features are removed. Data augmentation is more relevant for addressing overfitting, particularly in image or text data, rather than underfitting. L1 regularization is effective for reducing overfitting, and not for addressing catastrophic forgetting."
      },
      {
        "answer": "",
        "explanation": "<strong>Regularly update the training dataset with new data, apply L2 regularization to manage overfitting, and use an ensemble of models to prevent catastrophic forgetting</strong> - Regularly updating the dataset can help with model performance over time, but it might not address catastrophic forgetting unless combined with techniques like transfer learning. L2 regularization and ensembling are useful strategies, but ensembling does not specifically prevent catastrophic forgetting."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html",
      "https://aws.amazon.com/blogs/machine-learning/automatically-retrain-neural-networks-with-renate/",
      "https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters.html"
    ]
  },
  {
    "id": 15,
    "question": "<p>A data analytics company is using an Amazon EMR cluster to process large volumes of log data collected from various sources. The logs must be processed in batches, and the results are stored in Amazon S3 for downstream analytics. The company requires a cluster configuration that ensures no data loss during processing while being as cost-effective as possible.</p>\n\n<p>Which instance purchasing option will meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an On-Demand instance for the primary node and Spot instances for the core nodes as well as the task nodes to reduce costs while ensuring fault tolerance</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Spot instances for primary and core nodes, and On-Demand instances for task nodes to balance cost and availability</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an On-Demand instance for the primary node along with the core nodes, and Spot instances for task nodes to reduce costs while ensuring fault tolerance</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Spot instances for all primary, core, and task nodes to minimize costs and automatically handle failures by replacing interrupted nodes</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use an On-Demand instance for the primary node along with the core nodes, and Spot instances for task nodes to reduce costs while ensuring fault tolerance</strong></p>\n\n<p>Amazon EMR cluster architecture consists of the following nodes:</p>\n\n<p>Primary Node - Manages the cluster and must always be reliable. An On-Demand instance ensures continuous operation.</p>\n\n<p>Core Nodes - Perform data processing and store intermediate results in HDFS. These nodes must also be fault-tolerant, requiring On-Demand instances.</p>\n\n<p>Task Nodes - Execute additional processing without storing data, making Spot instances a cost-effective choice since their loss does not affect data durability.</p>\n\n<p>This configuration combines fault tolerance for critical components (primary and core nodes) with cost savings for processing (task nodes).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Spot instances for all primary, core, and task nodes to minimize costs and automatically handle failures by replacing interrupted nodes</strong> - Spot interruptions can cause instability for primary and core nodes, risking data loss and cluster failures.</p>\n\n<p><strong>Use Spot instances for primary and core nodes, and On-Demand instances for task nodes to balance cost and availability</strong> - Spot instances for primary and core nodes risk data loss due to interruptions, violating the fault-tolerance requirement.</p>\n\n<p><strong>Use an On-Demand instance for the primary node and Spot instances for the core nodes as well as the task nodes to reduce costs while ensuring fault tolerance</strong> - Spot instances for core nodes risk data loss due to interruptions, violating the fault-tolerance requirement.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/ec2/spot/use-case/emr/\">https://aws.amazon.com/ec2/spot/use-case/emr/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use an On-Demand instance for the primary node along with the core nodes, and Spot instances for task nodes to reduce costs while ensuring fault tolerance</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon EMR cluster architecture consists of the following nodes:"
      },
      {
        "answer": "",
        "explanation": "Primary Node - Manages the cluster and must always be reliable. An On-Demand instance ensures continuous operation."
      },
      {
        "answer": "",
        "explanation": "Core Nodes - Perform data processing and store intermediate results in HDFS. These nodes must also be fault-tolerant, requiring On-Demand instances."
      },
      {
        "answer": "",
        "explanation": "Task Nodes - Execute additional processing without storing data, making Spot instances a cost-effective choice since their loss does not affect data durability."
      },
      {
        "answer": "",
        "explanation": "This configuration combines fault tolerance for critical components (primary and core nodes) with cost savings for processing (task nodes)."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Spot instances for all primary, core, and task nodes to minimize costs and automatically handle failures by replacing interrupted nodes</strong> - Spot interruptions can cause instability for primary and core nodes, risking data loss and cluster failures."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Spot instances for primary and core nodes, and On-Demand instances for task nodes to balance cost and availability</strong> - Spot instances for primary and core nodes risk data loss due to interruptions, violating the fault-tolerance requirement."
      },
      {
        "answer": "",
        "explanation": "<strong>Use an On-Demand instance for the primary node and Spot instances for the core nodes as well as the task nodes to reduce costs while ensuring fault tolerance</strong> - Spot instances for core nodes risk data loss due to interruptions, violating the fault-tolerance requirement."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html",
      "https://aws.amazon.com/ec2/spot/use-case/emr/"
    ]
  },
  {
    "id": 16,
    "question": "<p>You are a data scientist at an e-commerce company working to develop a recommendation system for customers. After building several models, including collaborative filtering, content-based filtering, and a deep learning model, you find that each model excels in different scenarios. For example, the collaborative filtering model works well for returning customers with rich interaction data, while the content-based filtering model performs better for new customers with little interaction history. Your goal is to combine these models to create a recommendation system that provides more accurate and personalized recommendations across all customer segments.</p>\n\n<p>Which of the following strategies is the MOST LIKELY to achieve this goal?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use stacking, where the predictions from the collaborative filtering and content-based filtering models are fed into a deep learning model as inputs, allowing the deep learning model to make the final recommendation</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement a hybrid model that combines the predictions of collaborative filtering, content-based filtering, and deep learning using a weighted average, where weights are based on model performance for different customer segments</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Apply boosting by sequentially training the collaborative filtering, content-based filtering, and deep learning models, where each model corrects the errors of the previous one</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a bagging approach to train multiple instances of the deep learning model on different subsets of the data and average their predictions to improve overall performance</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement a hybrid model that combines the predictions of collaborative filtering, content-based filtering, and deep learning using a weighted average, where weights are based on model performance for different customer segments</strong></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q10-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q10-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/machine-learning/efficiently-train-tune-and-deploy-custom-ensembles-using-amazon-sagemaker/\">https://aws.amazon.com/blogs/machine-learning/efficiently-train-tune-and-deploy-custom-ensembles-using-amazon-sagemaker/</a><p></p>\n\n<p>In bagging, data scientists improve the accuracy of weak learners by training several of them at once on multiple datasets. In contrast, boosting trains weak learners one after another. Stacking involves training a meta-model on the predictions of several base models. This approach can significantly improve performance because the meta-model learns to leverage the strengths of each base model while compensating for their weaknesses.</p>\n\n<p>For the given use case, a hybrid model that combines the predictions of different models using a weighted average is the most appropriate approach. By assigning weights based on each model’s performance for specific customer segments, you can ensure that the recommendation system leverages the strengths of each model, providing more accurate and personalized recommendations across all customer segments.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a bagging approach to train multiple instances of the deep learning model on different subsets of the data and average their predictions to improve overall performance</strong> - Bagging is typically used to reduce variance and improve stability for a single type of model, like decision trees. However, it does not directly address the need to combine different models that perform well in different scenarios, which is key for your recommendation system.</p>\n\n<p><strong>Apply boosting by sequentially training the collaborative filtering, content-based filtering, and deep learning models, where each model corrects the errors of the previous one</strong> - Boosting is useful for improving the performance of weak learners by training them sequentially, but it is not designed to combine different types of models like collaborative filtering, content-based filtering, and deep learning, each of which has strengths in different areas.</p>\n\n<p><strong>Use stacking, where the predictions from the collaborative filtering and content-based filtering models are fed into a deep learning model as inputs, allowing the deep learning model to make the final recommendation</strong> - Stacking is a powerful technique for combining models, but in this case, the deep learning model is not necessarily better suited as a meta-model for making the final recommendation. A weighted hybrid model is more effective when different models excel in different contexts, as it allows you to balance their contributions based on performance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/efficiently-train-tune-and-deploy-custom-ensembles-using-amazon-sagemaker/\">https://aws.amazon.com/blogs/machine-learning/efficiently-train-tune-and-deploy-custom-ensembles-using-amazon-sagemaker/</a></p>\n\n<p><a href=\"https://aws.amazon.com/what-is/boosting/\">https://aws.amazon.com/what-is/boosting/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Implement a hybrid model that combines the predictions of collaborative filtering, content-based filtering, and deep learning using a weighted average, where weights are based on model performance for different customer segments</strong>"
      },
      {
        "link": "https://aws.amazon.com/blogs/machine-learning/efficiently-train-tune-and-deploy-custom-ensembles-using-amazon-sagemaker/"
      },
      {
        "answer": "",
        "explanation": "In bagging, data scientists improve the accuracy of weak learners by training several of them at once on multiple datasets. In contrast, boosting trains weak learners one after another. Stacking involves training a meta-model on the predictions of several base models. This approach can significantly improve performance because the meta-model learns to leverage the strengths of each base model while compensating for their weaknesses."
      },
      {
        "answer": "",
        "explanation": "For the given use case, a hybrid model that combines the predictions of different models using a weighted average is the most appropriate approach. By assigning weights based on each model’s performance for specific customer segments, you can ensure that the recommendation system leverages the strengths of each model, providing more accurate and personalized recommendations across all customer segments."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use a bagging approach to train multiple instances of the deep learning model on different subsets of the data and average their predictions to improve overall performance</strong> - Bagging is typically used to reduce variance and improve stability for a single type of model, like decision trees. However, it does not directly address the need to combine different models that perform well in different scenarios, which is key for your recommendation system."
      },
      {
        "answer": "",
        "explanation": "<strong>Apply boosting by sequentially training the collaborative filtering, content-based filtering, and deep learning models, where each model corrects the errors of the previous one</strong> - Boosting is useful for improving the performance of weak learners by training them sequentially, but it is not designed to combine different types of models like collaborative filtering, content-based filtering, and deep learning, each of which has strengths in different areas."
      },
      {
        "answer": "",
        "explanation": "<strong>Use stacking, where the predictions from the collaborative filtering and content-based filtering models are fed into a deep learning model as inputs, allowing the deep learning model to make the final recommendation</strong> - Stacking is a powerful technique for combining models, but in this case, the deep learning model is not necessarily better suited as a meta-model for making the final recommendation. A weighted hybrid model is more effective when different models excel in different contexts, as it allows you to balance their contributions based on performance."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/machine-learning/efficiently-train-tune-and-deploy-custom-ensembles-using-amazon-sagemaker/",
      "https://aws.amazon.com/what-is/boosting/"
    ]
  },
  {
    "id": 17,
    "question": "<p>A telecommunications company collects data about customer interactions and stores it in an Amazon S3 bucket. The company uses Amazon Athena to query the data and identify patterns. The dataset includes a churn indicator as the target variable, and the company wants to assess whether an ML model can accurately predict customer churn.</p>\n\n<p>Which solution will provide this information with the LEAST development effort?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Download the dataset locally, preprocess the data manually, and use a third-party AutoML library to train and evaluate models</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon SageMaker Autopilot to automatically analyze the dataset, generate candidate models, and evaluate their ability to predict customer churn</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Bedrock with a pre-trained foundation model to directly analyze the dataset and predict churn without training a custom model</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage Amazon SageMaker Studio to build a custom workflow for preprocessing, feature engineering, training, and evaluation of predictive models</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon SageMaker Autopilot to automatically analyze the dataset, generate candidate models, and evaluate their ability to predict customer churn</strong></p>\n\n<p>Amazon SageMaker Autopilot is the most efficient solution because:</p>\n\n<p>It provides end-to-end automation for machine learning workflows, including data preprocessing, feature engineering, model training, and evaluation.</p>\n\n<p>It evaluates multiple candidate models to determine their predictive power with minimal manual effort.</p>\n\n<p>It is fully managed, making it ideal for teams looking to quickly assess a dataset's suitability for predictive modeling.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Bedrock with a pre-trained foundation model to directly analyze the dataset and predict churn without training a custom model</strong> - Amazon Bedrock provides pre-trained foundation models for NLP and generative tasks. It is not designed for custom predictive modeling on tabular datasets like customer churn data.</p>\n\n<p><strong>Download the dataset locally, preprocess the data manually, and use a third-party AutoML library to train and evaluate models</strong> - This approach involves significant manual effort, including data preprocessing and infrastructure setup, making it less efficient than using SageMaker Autopilot.</p>\n\n<p><strong>Leverage Amazon SageMaker Studio to build a custom workflow for preprocessing, feature engineering, training, and evaluation of predictive models</strong> - While SageMaker Studio provides flexibility, it requires substantial development effort compared to SageMaker Autopilot, which automates the entire process.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sagemaker-ai/autopilot/\">https://aws.amazon.com/sagemaker-ai/autopilot/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/use-auto-ml.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/use-auto-ml.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SageMaker Autopilot to automatically analyze the dataset, generate candidate models, and evaluate their ability to predict customer churn</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon SageMaker Autopilot is the most efficient solution because:"
      },
      {
        "answer": "",
        "explanation": "It provides end-to-end automation for machine learning workflows, including data preprocessing, feature engineering, model training, and evaluation."
      },
      {
        "answer": "",
        "explanation": "It evaluates multiple candidate models to determine their predictive power with minimal manual effort."
      },
      {
        "answer": "",
        "explanation": "It is fully managed, making it ideal for teams looking to quickly assess a dataset's suitability for predictive modeling."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Bedrock with a pre-trained foundation model to directly analyze the dataset and predict churn without training a custom model</strong> - Amazon Bedrock provides pre-trained foundation models for NLP and generative tasks. It is not designed for custom predictive modeling on tabular datasets like customer churn data."
      },
      {
        "answer": "",
        "explanation": "<strong>Download the dataset locally, preprocess the data manually, and use a third-party AutoML library to train and evaluate models</strong> - This approach involves significant manual effort, including data preprocessing and infrastructure setup, making it less efficient than using SageMaker Autopilot."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon SageMaker Studio to build a custom workflow for preprocessing, feature engineering, training, and evaluation of predictive models</strong> - While SageMaker Studio provides flexibility, it requires substantial development effort compared to SageMaker Autopilot, which automates the entire process."
      }
    ],
    "references": [
      "https://aws.amazon.com/sagemaker-ai/autopilot/",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/use-auto-ml.html"
    ]
  },
  {
    "id": 18,
    "question": "<p>You are a machine learning engineer responsible for optimizing the cost and performance of an ML model deployed on Amazon SageMaker. The model serves real-time predictions for an e-commerce platform, and the current instance type is providing reliable performance but at a higher cost than anticipated. Your goal is to determine the most cost-effective instance type that still meets the performance requirements for low-latency predictions.</p>\n\n<p>Which approach is the MOST EFFECTIVE for rightsizing the instance family and size for your SageMaker endpoint?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Inference Recommender to select the lowest-cost instance type, regardless of performance, and configure autoscaling to handle any additional load during peak times</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Inference Recommender to run load tests across various instance types and configurations, compare the performance and cost of each, and select the instance type that offers the best balance between cost and performance</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Manually review the performance metrics from Amazon CloudWatch for the current instance and experiment with different instance types by redeploying the model on each until you find the optimal one</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Compute Optimizer to analyze the current instance’s CPU and memory usage, and automatically switch to the smallest recommended instance type that matches the utilization metrics</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use SageMaker Inference Recommender to run load tests across various instance types and configurations, compare the performance and cost of each, and select the instance type that offers the best balance between cost and performance</strong></p>\n\n<p>SageMaker Inference Recommender is specifically designed to help you select the best instance type for your model by running load tests across various configurations. It provides detailed insights into how different instance types perform under real-world conditions, allowing you to make an informed decision that balances cost and performance. This approach ensures that you rightsize your instance while meeting both cost and performance requirements.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q41-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q41-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-prospective.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-prospective.html</a><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q41-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q41-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-load-test.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-load-test.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Compute Optimizer to analyze the current instance’s CPU and memory usage, and automatically switch to the smallest recommended instance type that matches the utilization metrics</strong> - AWS Compute Optimizer is effective for analyzing general-purpose EC2 instances, but it doesn’t provide the ML-specific insights needed for optimizing SageMaker endpoints. Simply switching to the smallest recommended instance could lead to insufficient performance for real-time predictions.</p>\n\n<p><strong>Manually review the performance metrics from Amazon CloudWatch for the current instance and experiment with different instance types by redeploying the model on each until you find the optimal one</strong> - Manually experimenting with instance types is time-consuming and may lead to suboptimal choices. SageMaker Inference Recommender automates this process and provides more precise recommendations based on your model’s actual performance.</p>\n\n<p><strong>Use SageMaker Inference Recommender to select the lowest-cost instance type, regardless of performance, and configure autoscaling to handle any additional load during peak times</strong> - While cost is important, selecting the lowest-cost instance without considering performance issues could degrade the user experience. SageMaker Inference Recommender provides a more balanced approach by considering both cost and performance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-prospective.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-prospective.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-load-test.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-load-test.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/compute-optimizer/\">https://aws.amazon.com/compute-optimizer/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use SageMaker Inference Recommender to run load tests across various instance types and configurations, compare the performance and cost of each, and select the instance type that offers the best balance between cost and performance</strong>"
      },
      {
        "answer": "",
        "explanation": "SageMaker Inference Recommender is specifically designed to help you select the best instance type for your model by running load tests across various configurations. It provides detailed insights into how different instance types perform under real-world conditions, allowing you to make an informed decision that balances cost and performance. This approach ensures that you rightsize your instance while meeting both cost and performance requirements."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-prospective.html"
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-load-test.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Compute Optimizer to analyze the current instance’s CPU and memory usage, and automatically switch to the smallest recommended instance type that matches the utilization metrics</strong> - AWS Compute Optimizer is effective for analyzing general-purpose EC2 instances, but it doesn’t provide the ML-specific insights needed for optimizing SageMaker endpoints. Simply switching to the smallest recommended instance could lead to insufficient performance for real-time predictions."
      },
      {
        "answer": "",
        "explanation": "<strong>Manually review the performance metrics from Amazon CloudWatch for the current instance and experiment with different instance types by redeploying the model on each until you find the optimal one</strong> - Manually experimenting with instance types is time-consuming and may lead to suboptimal choices. SageMaker Inference Recommender automates this process and provides more precise recommendations based on your model’s actual performance."
      },
      {
        "answer": "",
        "explanation": "<strong>Use SageMaker Inference Recommender to select the lowest-cost instance type, regardless of performance, and configure autoscaling to handle any additional load during peak times</strong> - While cost is important, selecting the lowest-cost instance without considering performance issues could degrade the user experience. SageMaker Inference Recommender provides a more balanced approach by considering both cost and performance."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-prospective.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-load-test.html",
      "https://aws.amazon.com/compute-optimizer/"
    ]
  },
  {
    "id": 19,
    "question": "<p>You are a data scientist at a financial services company tasked with deploying a lightweight machine learning model that predicts creditworthiness based on a customer’s transaction history. The model needs to provide real-time predictions with minimal latency, and the traffic pattern is unpredictable, with occasional spikes during business hours. The company is cost-conscious and prefers a serverless architecture to minimize infrastructure management overhead.</p>\n\n<p>Which approach is the MOST SUITABLE for deploying this solution, and why?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an Amazon EC2 instance to host the model, with AWS Lambda functions handling the communication between the API Gateway and the EC2 instance for prediction requests</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy the model using Amazon ECS (Elastic Container Service) and configure an AWS Lambda to trigger the ECS service on-demand, ensuring that the model is only running during peak traffic periods</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy the model directly within AWS Lambda as a function, and expose it through an API Gateway endpoint, allowing the function to scale automatically with traffic and provide real-time predictions</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Deploy the model as a SageMaker endpoint for real-time inference, and configure AWS Lambda to preprocess incoming requests before sending them to the SageMaker endpoint for prediction</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the model directly within AWS Lambda as a function, and expose it through an API Gateway endpoint, allowing the function to scale automatically with traffic and provide real-time predictions</strong></p>\n\n<p>Deploying the model within AWS Lambda as a function and exposing it through an API Gateway endpoint is ideal for lightweight, serverless, real-time inference. Lambda’s automatic scaling and pay-per-use model align well with unpredictable traffic patterns and the need for cost efficiency.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q27-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q27-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/\">https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the model as a SageMaker endpoint for real-time inference, and configure AWS Lambda to preprocess incoming requests before sending them to the SageMaker endpoint for prediction</strong> - While deploying the model as a SageMaker endpoint is suitable for more complex models requiring managed infrastructure, this approach might be overkill for a lightweight model, especially if you want to minimize costs and management overhead. Lambda functions can serve the model directly in a more cost-effective manner.</p>\n\n<p><strong>Use an Amazon EC2 instance to host the model, with AWS Lambda functions handling the communication between the API Gateway and the EC2 instance for prediction requests</strong> - Hosting the model on an Amazon EC2 instance with Lambda managing communication adds unnecessary complexity and overhead. EC2-based deployments require more management and may not be as cost-effective for serverless and real-time use cases.</p>\n\n<p><strong>Deploy the model using Amazon ECS (Elastic Container Service) and configure an AWS Lambda to trigger the ECS service on-demand, ensuring that the model is only running during peak traffic periods</strong> - Using Amazon ECS triggered by AWS Lambda adds complexity and may not provide the same level of real-time responsiveness as directly deploying the model in Lambda.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/\">https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the model directly within AWS Lambda as a function, and expose it through an API Gateway endpoint, allowing the function to scale automatically with traffic and provide real-time predictions</strong>"
      },
      {
        "answer": "",
        "explanation": "Deploying the model within AWS Lambda as a function and exposing it through an API Gateway endpoint is ideal for lightweight, serverless, real-time inference. Lambda’s automatic scaling and pay-per-use model align well with unpredictable traffic patterns and the need for cost efficiency."
      },
      {
        "link": "https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the model as a SageMaker endpoint for real-time inference, and configure AWS Lambda to preprocess incoming requests before sending them to the SageMaker endpoint for prediction</strong> - While deploying the model as a SageMaker endpoint is suitable for more complex models requiring managed infrastructure, this approach might be overkill for a lightweight model, especially if you want to minimize costs and management overhead. Lambda functions can serve the model directly in a more cost-effective manner."
      },
      {
        "answer": "",
        "explanation": "<strong>Use an Amazon EC2 instance to host the model, with AWS Lambda functions handling the communication between the API Gateway and the EC2 instance for prediction requests</strong> - Hosting the model on an Amazon EC2 instance with Lambda managing communication adds unnecessary complexity and overhead. EC2-based deployments require more management and may not be as cost-effective for serverless and real-time use cases."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy the model using Amazon ECS (Elastic Container Service) and configure an AWS Lambda to trigger the ECS service on-demand, ensuring that the model is only running during peak traffic periods</strong> - Using Amazon ECS triggered by AWS Lambda adds complexity and may not provide the same level of real-time responsiveness as directly deploying the model in Lambda."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/"
    ]
  },
  {
    "id": 20,
    "question": "<p>You are a data scientist at a healthcare company working on deploying a machine learning model that predicts patient outcomes based on real-time data from wearable devices. The model needs to be containerized for easy deployment and scaling across different environments, including development, testing, and production. The company wants to ensure that container images are managed efficiently, securely, and consistently across all environments.</p>\n\n<p>Given these requirements, which combination of AWS services is the MOST SUITABLE for building, storing, deploying, and maintaining the containerized ML solution?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon ECS to manage and deploy the containerized model, Amazon S3 to store container images, and manually push updates to the containers using the AWS CLI</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon ECR to store the container images, Amazon EKS for orchestrating the containers, and AWS CodePipeline for automating the CI/CD pipeline, ensuring that updates to the model are seamlessly deployed</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon ECR to store container images, manually deploy containers on Amazon EC2 instances, and use AWS CloudFormation to manage the infrastructure configuration</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Docker Hub to store the container images, Amazon EKS for orchestrating the containers, and AWS Lambda to trigger updates to the containers when new images are pushed</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon ECR to store the container images, Amazon EKS for orchestrating the containers, and AWS CodePipeline for automating the CI/CD pipeline, ensuring that updates to the model are seamlessly deployed</strong></p>\n\n<p>Amazon ECR is a fully managed container registry that integrates seamlessly with Amazon EKS, allowing you to securely store and manage container images. Amazon EKS provides a scalable and managed Kubernetes environment for orchestrating containers. AWS CodePipeline can be used to automate the continuous integration and delivery (CI/CD) process, ensuring that new versions of the model are automatically built, tested, and deployed without manual intervention. This combination ensures consistency, security, and scalability across the ML solution.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q28-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q28-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/build-secure-enterprise-ml-platform/automation-pipelines.html\">https://docs.aws.amazon.com/whitepapers/latest/build-secure-enterprise-ml-platform/automation-pipelines.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon ECS to manage and deploy the containerized model, Amazon S3 to store container images, and manually push updates to the containers using the AWS CLI</strong> - While Amazon ECS is a powerful container management service, storing container images in Amazon S3 is not recommended since S3 is not optimized for container image storage. Additionally, manually pushing updates via the AWS CLI lacks automation and can introduce errors, making it less suitable for a production environment.</p>\n\n<p><strong>Use Amazon ECR to store container images, manually deploy containers on Amazon EC2 instances, and use AWS CloudFormation to manage the infrastructure configuration</strong> - Manually deploying containers on Amazon EC2 instances and managing infrastructure with AWS CloudFormation adds unnecessary complexity and management overhead. Using Amazon EKS for orchestration is more efficient and scalable for containerized workloads.</p>\n\n<p><strong>Use Docker Hub to store the container images, Amazon EKS for orchestrating the containers, and AWS Lambda to trigger updates to the containers when new images are pushed</strong> - Docker Hub is a widely used container registry, but for enterprise solutions on AWS, Amazon ECR is more secure and integrates better with other AWS services. Using AWS Lambda to trigger updates is unconventional and less efficient compared to using a CI/CD pipeline with AWS CodePipeline.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/build-secure-enterprise-ml-platform/automation-pipelines.html\">https://docs.aws.amazon.com/whitepapers/latest/build-secure-enterprise-ml-platform/automation-pipelines.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/build-a-ci-cd-pipeline-for-deploying-custom-machine-learning-models-using-aws-services/\">https://aws.amazon.com/blogs/machine-learning/build-a-ci-cd-pipeline-for-deploying-custom-machine-learning-models-using-aws-services/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon ECR to store the container images, Amazon EKS for orchestrating the containers, and AWS CodePipeline for automating the CI/CD pipeline, ensuring that updates to the model are seamlessly deployed</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon ECR is a fully managed container registry that integrates seamlessly with Amazon EKS, allowing you to securely store and manage container images. Amazon EKS provides a scalable and managed Kubernetes environment for orchestrating containers. AWS CodePipeline can be used to automate the continuous integration and delivery (CI/CD) process, ensuring that new versions of the model are automatically built, tested, and deployed without manual intervention. This combination ensures consistency, security, and scalability across the ML solution."
      },
      {
        "link": "https://docs.aws.amazon.com/whitepapers/latest/build-secure-enterprise-ml-platform/automation-pipelines.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon ECS to manage and deploy the containerized model, Amazon S3 to store container images, and manually push updates to the containers using the AWS CLI</strong> - While Amazon ECS is a powerful container management service, storing container images in Amazon S3 is not recommended since S3 is not optimized for container image storage. Additionally, manually pushing updates via the AWS CLI lacks automation and can introduce errors, making it less suitable for a production environment."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon ECR to store container images, manually deploy containers on Amazon EC2 instances, and use AWS CloudFormation to manage the infrastructure configuration</strong> - Manually deploying containers on Amazon EC2 instances and managing infrastructure with AWS CloudFormation adds unnecessary complexity and management overhead. Using Amazon EKS for orchestration is more efficient and scalable for containerized workloads."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Docker Hub to store the container images, Amazon EKS for orchestrating the containers, and AWS Lambda to trigger updates to the containers when new images are pushed</strong> - Docker Hub is a widely used container registry, but for enterprise solutions on AWS, Amazon ECR is more secure and integrates better with other AWS services. Using AWS Lambda to trigger updates is unconventional and less efficient compared to using a CI/CD pipeline with AWS CodePipeline."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/build-secure-enterprise-ml-platform/automation-pipelines.html",
      "https://aws.amazon.com/blogs/machine-learning/build-a-ci-cd-pipeline-for-deploying-custom-machine-learning-models-using-aws-services/"
    ]
  },
  {
    "id": 21,
    "question": "<p>A financial services company deployed a machine learning model using Amazon SageMaker Asynchronous Inference in the past with successful performance. Now, the company needs to deploy a new ML model that detects fraudulent credit card transactions in real-time within their banking application. However, when using SageMaker Asynchronous Inference for this new model, the performance is poor and does not meet the real-time requirements. Additionally, the company wants to receive notifications whenever there is a deviation in the model's quality.</p>\n\n<p>As an AWS Certified Machine Learning Engineer Associate, what do you recommend? </p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Retain SageMaker Asynchronous Inference and enable autoscaling for the endpoint to improve performance, while using Amazon SQS for notifications</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Batch Transform for inference and set up a monitoring system with SageMaker Model Monitor to send notifications for quality deviations</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Increase the instance size and response timeout settings for SageMaker Asynchronous Inference and use Amazon SNS for quality deviation notifications</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Switch to SageMaker Real-Time Inference for the deployment and use SageMaker Model Monitor for model quality deviations</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Switch to SageMaker Real-Time Inference for the deployment and use SageMaker Model Monitor for model quality deviations</strong></p>\n\n<p>SageMaker Real-Time Inference is designed for low-latency applications, making it ideal for real-time fraud detection. Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker AI hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling.</p>\n\n<p>Model quality monitoring jobs monitor the performance of a model by comparing the predictions that the model makes with the actual Ground Truth labels that the model attempts to predict. To do this, model quality monitoring merges data that is captured from real-time or batch inference with actual labels that you store in an Amazon S3 bucket, and then compares the predictions with the actual labels. If you set the value of the enable_cloudwatch_metrics to True when you create the monitoring schedule, model quality monitoring jobs can send all metrics to CloudWatch, which can further be used to send notifications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the instance size and response timeout settings for SageMaker Asynchronous Inference and use Amazon SNS for quality deviation notifications</strong> - While increasing the instance size and timeout may improve the performance, Asynchronous Inference is not suitable for real-time inference use cases.</p>\n\n<p><strong>Use SageMaker Batch Transform for inference and set up a monitoring system with SageMaker Model Monitor to send notifications for quality deviations</strong> - Batch Transform is intended for offline, large-scale batch processing and is not suitable for real-time fraud detection applications.</p>\n\n<p><strong>Retain SageMaker Asynchronous Inference and enable autoscaling for the endpoint to improve performance, while using Amazon SQS for notifications</strong> - While autoscaling can help handle higher loads, Asynchronous Inference is inherently not designed for real-time inference applications, making it a poor fit for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Switch to SageMaker Real-Time Inference for the deployment and use SageMaker Model Monitor for model quality deviations</strong>"
      },
      {
        "answer": "",
        "explanation": "SageMaker Real-Time Inference is designed for low-latency applications, making it ideal for real-time fraud detection. Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker AI hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling."
      },
      {
        "answer": "",
        "explanation": "Model quality monitoring jobs monitor the performance of a model by comparing the predictions that the model makes with the actual Ground Truth labels that the model attempts to predict. To do this, model quality monitoring merges data that is captured from real-time or batch inference with actual labels that you store in an Amazon S3 bucket, and then compares the predictions with the actual labels. If you set the value of the enable_cloudwatch_metrics to True when you create the monitoring schedule, model quality monitoring jobs can send all metrics to CloudWatch, which can further be used to send notifications."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Increase the instance size and response timeout settings for SageMaker Asynchronous Inference and use Amazon SNS for quality deviation notifications</strong> - While increasing the instance size and timeout may improve the performance, Asynchronous Inference is not suitable for real-time inference use cases."
      },
      {
        "answer": "",
        "explanation": "<strong>Use SageMaker Batch Transform for inference and set up a monitoring system with SageMaker Model Monitor to send notifications for quality deviations</strong> - Batch Transform is intended for offline, large-scale batch processing and is not suitable for real-time fraud detection applications."
      },
      {
        "answer": "",
        "explanation": "<strong>Retain SageMaker Asynchronous Inference and enable autoscaling for the endpoint to improve performance, while using Amazon SQS for notifications</strong> - While autoscaling can help handle higher loads, Asynchronous Inference is inherently not designed for real-time inference applications, making it a poor fit for this use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html"
    ]
  },
  {
    "id": 22,
    "question": "<p>You are a DevOps engineer responsible for maintaining a serverless machine learning application that provides real-time predictions using AWS Lambda. Recently, users have reported increased latency when interacting with the application, especially during peak usage hours. You need to quickly identify the root cause of the latency and resolve the performance issues to ensure the application remains responsive.</p>\n\n<p>Which combination of monitoring and observability tools is the MOST EFFECTIVE for troubleshooting the latency and performance issues in this serverless application?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon CloudWatch Alarms to set thresholds for Lambda duration and error rates, and configure AWS X-Ray to periodically sample traces from the application for analysis</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable detailed monitoring in Amazon CloudWatch to track Lambda invocations, errors, and throttles, and manually inspect the Lambda code to identify performance bottlenecks</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy Amazon CloudWatch Logs Insights to query and analyze the application logs for errors, and use AWS Config to review recent changes to the infrastructure that might have introduced latency</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS X-Ray to trace requests across the entire application, identify bottlenecks, and visualize the end-to-end latency for each request. Combine this with Amazon CloudWatch Lambda Insights to monitor the Lambda function’s memory usage, CPU usage, and invocation times</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS X-Ray to trace requests across the entire application, identify bottlenecks, and visualize the end-to-end latency for each request. Combine this with Amazon CloudWatch Lambda Insights to monitor the Lambda function’s memory usage, CPU usage, and invocation times</strong></p>\n\n<p>This approach leverages AWS X-Ray to trace the entire request path, providing detailed insights into where latency occurs, whether in the Lambda function, external APIs, or other integrated services. AWS X-Ray’s visualization helps identify bottlenecks and latency sources.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/images/xray/latest/devguide/images/xray-get-started.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/images/xray/latest/devguide/images/xray-get-started.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-gettingstarted.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-gettingstarted.html</a><p></p>\n\n<p>Amazon CloudWatch Lambda Insights provides detailed metrics on Lambda function performance, including memory usage, CPU usage, and invocation times, allowing you to pinpoint performance issues specific to the Lambda environment. This combination is powerful for diagnosing and resolving latency issues in a serverless architecture.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q35-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q35-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable detailed monitoring in Amazon CloudWatch to track Lambda invocations, errors, and throttles, and manually inspect the Lambda code to identify performance bottlenecks</strong> - Detailed monitoring in Amazon CloudWatch provides useful metrics, but it lacks the deep trace analysis that AWS X-Ray offers. Manually inspecting code is time-consuming and may not accurately identify the performance bottlenecks.</p>\n\n<p><strong>Deploy Amazon CloudWatch Logs Insights to query and analyze the application logs for errors, and use AWS Config to review recent changes to the infrastructure that might have introduced latency</strong> - Amazon CloudWatch Logs Insights is effective for analyzing logs, but it doesn’t provide the same end-to-end tracing capabilities as AWS X-Ray. AWS Config is useful for tracking infrastructure changes, but it may not directly help identify latency issues within the application.</p>\n\n<p><strong>Use Amazon CloudWatch Alarms to set thresholds for Lambda duration and error rates, and configure AWS X-Ray to periodically sample traces from the application for analysis</strong> - While setting alarms for Lambda duration and error rates is important, relying on periodically sampled traces with AWS X-Ray may miss intermittent issues. Continuous tracing and monitoring with AWS X-Ray and Lambda Insights offer a more comprehensive solution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-gettingstarted.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-gettingstarted.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/aws-lambda-support-for-aws-x-ray/\">https://aws.amazon.com/blogs/aws/aws-lambda-support-for-aws-x-ray/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-metrics-basic-detailed.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-metrics-basic-detailed.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS X-Ray to trace requests across the entire application, identify bottlenecks, and visualize the end-to-end latency for each request. Combine this with Amazon CloudWatch Lambda Insights to monitor the Lambda function’s memory usage, CPU usage, and invocation times</strong>"
      },
      {
        "answer": "",
        "explanation": "This approach leverages AWS X-Ray to trace the entire request path, providing detailed insights into where latency occurs, whether in the Lambda function, external APIs, or other integrated services. AWS X-Ray’s visualization helps identify bottlenecks and latency sources."
      },
      {
        "link": "https://docs.aws.amazon.com/xray/latest/devguide/xray-gettingstarted.html"
      },
      {
        "answer": "",
        "explanation": "Amazon CloudWatch Lambda Insights provides detailed metrics on Lambda function performance, including memory usage, CPU usage, and invocation times, allowing you to pinpoint performance issues specific to the Lambda environment. This combination is powerful for diagnosing and resolving latency issues in a serverless architecture."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable detailed monitoring in Amazon CloudWatch to track Lambda invocations, errors, and throttles, and manually inspect the Lambda code to identify performance bottlenecks</strong> - Detailed monitoring in Amazon CloudWatch provides useful metrics, but it lacks the deep trace analysis that AWS X-Ray offers. Manually inspecting code is time-consuming and may not accurately identify the performance bottlenecks."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy Amazon CloudWatch Logs Insights to query and analyze the application logs for errors, and use AWS Config to review recent changes to the infrastructure that might have introduced latency</strong> - Amazon CloudWatch Logs Insights is effective for analyzing logs, but it doesn’t provide the same end-to-end tracing capabilities as AWS X-Ray. AWS Config is useful for tracking infrastructure changes, but it may not directly help identify latency issues within the application."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon CloudWatch Alarms to set thresholds for Lambda duration and error rates, and configure AWS X-Ray to periodically sample traces from the application for analysis</strong> - While setting alarms for Lambda duration and error rates is important, relying on periodically sampled traces with AWS X-Ray may miss intermittent issues. Continuous tracing and monitoring with AWS X-Ray and Lambda Insights offer a more comprehensive solution."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/xray/latest/devguide/xray-gettingstarted.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html",
      "https://aws.amazon.com/blogs/aws/aws-lambda-support-for-aws-x-ray/",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-metrics-basic-detailed.html"
    ]
  },
  {
    "id": 23,
    "question": "<p>A travel agency has collected a large volume of customer feedback recordings from calls made to its customer support team after launching a new vacation package. The agency wants to evaluate the success of the vacation package by analyzing the sentiment of customer feedback. The ML engineer needs to process and analyze the recordings to identify positive and negative sentiments in the least amount of time.</p>\n\n<p>Which action should the ML engineer take?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Build a custom sentiment analysis model using Amazon SageMaker and train it with labeled customer feedback data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Glue to preprocess the feedback recordings and implement a sentiment analysis solution using open-source libraries</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Transcribe to convert the customer feedback recordings into text, and then use Amazon Comprehend to analyze the text for sentiment</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Manually transcribe the recordings, convert them to English using Amazon Translate, and run a custom script to analyze sentiment</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Transcribe to convert the customer feedback recordings into text, and then use Amazon Comprehend to analyze the text for sentiment</strong></p>\n\n<p>This solution uses AWS services optimized for the workflow:</p>\n\n<p>Amazon Transcribe - Converts audio feedback recordings into text, enabling text-based analysis.</p>\n\n<p>Amazon Comprehend - Analyzes text for sentiment (positive, negative, neutral, mixed), providing actionable insights.</p>\n\n<p>This solution leverages fully managed AWS services that minimize the need for custom coding and manual preprocessing. Also, the integration of Transcribe and Comprehend enables efficient and scalable sentiment analysis with minimal operational overhead.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Build a custom sentiment analysis model using Amazon SageMaker and train it with labeled customer feedback data</strong> - Training a custom model requires significant time and effort to prepare the data, train the model, and validate its accuracy. Managed services like Comprehend eliminate this overhead for sentiment analysis.</p>\n\n<p><strong>Manually transcribe the recordings, convert them to English using Amazon Translate, and run a custom script to analyze sentiment</strong> - Manually transcribing the recordings introduces unnecessary delays. Additionally, Amazon Comprehend supports multiple languages natively, making translation unnecessary.</p>\n\n<p><strong>Use AWS Glue to preprocess the feedback recordings and implement a sentiment analysis solution using open-source libraries</strong> - Amazon Polly converts text to speech, not speech to text. Writing a custom script for sentiment analysis adds complexity compared to using Amazon Comprehend.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/transcribe/\">https://aws.amazon.com/transcribe/</a></p>\n\n<p><a href=\"https://aws.amazon.com/comprehend/\">https://aws.amazon.com/comprehend/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/generate-high-quality-meeting-notes-using-amazon-transcribe-and-amazon-comprehend/\">https://aws.amazon.com/blogs/machine-learning/generate-high-quality-meeting-notes-using-amazon-transcribe-and-amazon-comprehend/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Transcribe to convert the customer feedback recordings into text, and then use Amazon Comprehend to analyze the text for sentiment</strong>"
      },
      {
        "answer": "",
        "explanation": "This solution uses AWS services optimized for the workflow:"
      },
      {
        "answer": "",
        "explanation": "Amazon Transcribe - Converts audio feedback recordings into text, enabling text-based analysis."
      },
      {
        "answer": "",
        "explanation": "Amazon Comprehend - Analyzes text for sentiment (positive, negative, neutral, mixed), providing actionable insights."
      },
      {
        "answer": "",
        "explanation": "This solution leverages fully managed AWS services that minimize the need for custom coding and manual preprocessing. Also, the integration of Transcribe and Comprehend enables efficient and scalable sentiment analysis with minimal operational overhead."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Build a custom sentiment analysis model using Amazon SageMaker and train it with labeled customer feedback data</strong> - Training a custom model requires significant time and effort to prepare the data, train the model, and validate its accuracy. Managed services like Comprehend eliminate this overhead for sentiment analysis."
      },
      {
        "answer": "",
        "explanation": "<strong>Manually transcribe the recordings, convert them to English using Amazon Translate, and run a custom script to analyze sentiment</strong> - Manually transcribing the recordings introduces unnecessary delays. Additionally, Amazon Comprehend supports multiple languages natively, making translation unnecessary."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue to preprocess the feedback recordings and implement a sentiment analysis solution using open-source libraries</strong> - Amazon Polly converts text to speech, not speech to text. Writing a custom script for sentiment analysis adds complexity compared to using Amazon Comprehend."
      }
    ],
    "references": [
      "https://aws.amazon.com/transcribe/",
      "https://aws.amazon.com/comprehend/",
      "https://aws.amazon.com/blogs/machine-learning/generate-high-quality-meeting-notes-using-amazon-transcribe-and-amazon-comprehend/"
    ]
  },
  {
    "id": 24,
    "question": "<p>You are an ML engineer working for a logistics company that uses machine learning models to optimize delivery routes, predict maintenance needs, and forecast demand. The company wants to deploy several models into production, each serving different business functions but running on the same infrastructure to minimize costs. These models differ in the frequency of updates. The company is considering whether to use a multi-model deployment approach or a multi-container deployment approach on Amazon SageMaker to manage these models efficiently.</p>\n\n<p>Given these requirements, which deployment strategy is MOST SUITABLE for managing these diverse models?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a multi-model deployment on a single SageMaker endpoint to host all models together, allowing you to dynamically load and serve models as needed without needing separate endpoints</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Deploy each model individually using separate SageMaker endpoints, ensuring each model has dedicated resources and can be scaled independently</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Implement a multi-container deployment strategy on a single SageMaker endpoint, where each model runs in its own container, allowing you to manage resource allocation more precisely across models</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a hybrid approach where frequently updated models are deployed using multi-model endpoints and more complex models are deployed using multi-container endpoints, balancing flexibility and resource management</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a multi-model deployment on a single SageMaker endpoint to host all models together, allowing you to dynamically load and serve models as needed without needing separate endpoints</strong></p>\n\n<p>Multi-model endpoints are ideal for hosting a large number of models that use the same ML framework on a shared serving container. If you have a mix of frequently and infrequently accessed models, a multi-model endpoint can efficiently serve this traffic with fewer resources and higher cost savings. Your application should be tolerant of occasional cold start-related latency penalties that occur when invoking infrequently used models.</p>\n\n<p>Multi-model endpoints support hosting both CPU and GPU backed models. By using GPU backed models, you can lower your model deployment costs through increased usage of the endpoint and its underlying accelerated compute instances.</p>\n\n<p>Multi-model endpoints also enable time-sharing of memory resources across your models. This works best when the models are fairly similar in size and invocation latency. When this is the case, multi-model endpoints can effectively use instances across all models. If you have models that have significantly higher transactions per second (TPS) or latency requirements, we recommend hosting them on dedicated endpoints.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q22-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q22-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy each model individually using separate SageMaker endpoints, ensuring each model has dedicated resources and can be scaled independently</strong> - Deploying each model on a separate endpoint provides dedicated resources and independent scaling but can lead to higher costs and complexity, especially when managing many models.</p>\n\n<p><strong>Implement a multi-container deployment strategy on a single SageMaker endpoint, where each model runs in its own container, allowing you to manage resource allocation more precisely across models</strong> - A multi-container deployment is useful when models have different dependencies or require isolation for security reasons. However, it is more complex to manage than multi-model deployment and may be overkill if your models do not require strict separation or have different runtime environments.</p>\n\n<p><strong>Use a hybrid approach where frequently updated models are deployed using multi-model endpoints and more complex models are deployed using multi-container endpoints, balancing flexibility and resource management</strong> - While a hybrid approach might seem to provide flexibility, it adds complexity in terms of management and deployment. It’s better to choose a single approach that meets most of your needs unless there are specific reasons to segregate models by deployment type.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-direct.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-direct.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use a multi-model deployment on a single SageMaker endpoint to host all models together, allowing you to dynamically load and serve models as needed without needing separate endpoints</strong>"
      },
      {
        "answer": "",
        "explanation": "Multi-model endpoints are ideal for hosting a large number of models that use the same ML framework on a shared serving container. If you have a mix of frequently and infrequently accessed models, a multi-model endpoint can efficiently serve this traffic with fewer resources and higher cost savings. Your application should be tolerant of occasional cold start-related latency penalties that occur when invoking infrequently used models."
      },
      {
        "answer": "",
        "explanation": "Multi-model endpoints support hosting both CPU and GPU backed models. By using GPU backed models, you can lower your model deployment costs through increased usage of the endpoint and its underlying accelerated compute instances."
      },
      {
        "answer": "",
        "explanation": "Multi-model endpoints also enable time-sharing of memory resources across your models. This works best when the models are fairly similar in size and invocation latency. When this is the case, multi-model endpoints can effectively use instances across all models. If you have models that have significantly higher transactions per second (TPS) or latency requirements, we recommend hosting them on dedicated endpoints."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy each model individually using separate SageMaker endpoints, ensuring each model has dedicated resources and can be scaled independently</strong> - Deploying each model on a separate endpoint provides dedicated resources and independent scaling but can lead to higher costs and complexity, especially when managing many models."
      },
      {
        "answer": "",
        "explanation": "<strong>Implement a multi-container deployment strategy on a single SageMaker endpoint, where each model runs in its own container, allowing you to manage resource allocation more precisely across models</strong> - A multi-container deployment is useful when models have different dependencies or require isolation for security reasons. However, it is more complex to manage than multi-model deployment and may be overkill if your models do not require strict separation or have different runtime environments."
      },
      {
        "answer": "",
        "explanation": "<strong>Use a hybrid approach where frequently updated models are deployed using multi-model endpoints and more complex models are deployed using multi-container endpoints, balancing flexibility and resource management</strong> - While a hybrid approach might seem to provide flexibility, it adds complexity in terms of management and deployment. It’s better to choose a single approach that meets most of your needs unless there are specific reasons to segregate models by deployment type."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-direct.html"
    ]
  },
  {
    "id": 25,
    "question": "<p>Which of the following are examples of supervised learning? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Clustering</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Linear regression</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Association rule learning</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Document classification</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Neural network</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct options:</p>\n\n<p>Supervised learning algorithms train on sample data that specifies both the algorithm's input and output. For example, the data could be images of handwritten numbers that are annotated to indicate which numbers they represent. Given sufficient labeled data, the supervised learning system would eventually recognize the clusters of pixels and shapes associated with each handwritten number.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q47-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q47-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/compare/the-difference-between-machine-learning-supervised-and-unsupervised/\">https://aws.amazon.com/compare/the-difference-between-machine-learning-supervised-and-unsupervised/</a><p></p>\n\n<p><strong>Linear regression</strong></p>\n\n<p>Linear regression refers to supervised learning models that, based on one or more inputs, predict a value from a continuous scale. An example of linear regression is predicting a house price. You could predict a house’s price based on its location, age, and number of rooms after you train a model on a set of historical sales training data with those variables.</p>\n\n<p><strong>Neural network</strong></p>\n\n<p>A neural network solution is a more complex supervised learning technique. To produce a given outcome, it takes some given inputs and performs one or more layers of mathematical transformation based on adjusting data weightings. An example of a neural network technique is predicting a digit from a handwritten image.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Document classification</strong> - This is an example of semi-supervised learning. Semi-supervised learning is when you apply both supervised and unsupervised learning techniques to a common problem. This technique relies on using a small amount of labeled data and a large amount of unlabeled data to train systems. When applying categories to a large document base, there may be too many documents to physically label. For example, these could be countless reports, transcripts, or specifications. Training on the unlabeled data helps identify similar documents for labeling.</p>\n\n<p><strong>Association rule learning</strong> - This is an example of unsupervised learning. Association rule learning techniques uncover rule-based relationships between inputs in a dataset. For example, the Apriori algorithm conducts market basket analysis to identify rules like coffee and milk often being purchased together.</p>\n\n<p><strong>Clustering</strong> - Clustering is an unsupervised learning technique that groups certain data inputs, so they may be categorized as a whole. There are various types of clustering algorithms depending on the input data. An example of clustering is identifying different types of network traffic to predict potential security incidents.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/what-is/machine-learning/\">https://aws.amazon.com/what-is/machine-learning/</a></p>\n\n<p><a href=\"https://aws.amazon.com/compare/the-difference-between-machine-learning-supervised-and-unsupervised/\">https://aws.amazon.com/compare/the-difference-between-machine-learning-supervised-and-unsupervised/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Supervised learning algorithms train on sample data that specifies both the algorithm's input and output. For example, the data could be images of handwritten numbers that are annotated to indicate which numbers they represent. Given sufficient labeled data, the supervised learning system would eventually recognize the clusters of pixels and shapes associated with each handwritten number."
      },
      {
        "link": "https://aws.amazon.com/compare/the-difference-between-machine-learning-supervised-and-unsupervised/"
      },
      {
        "answer": "",
        "explanation": "<strong>Linear regression</strong>"
      },
      {
        "answer": "",
        "explanation": "Linear regression refers to supervised learning models that, based on one or more inputs, predict a value from a continuous scale. An example of linear regression is predicting a house price. You could predict a house’s price based on its location, age, and number of rooms after you train a model on a set of historical sales training data with those variables."
      },
      {
        "answer": "",
        "explanation": "<strong>Neural network</strong>"
      },
      {
        "answer": "",
        "explanation": "A neural network solution is a more complex supervised learning technique. To produce a given outcome, it takes some given inputs and performs one or more layers of mathematical transformation based on adjusting data weightings. An example of a neural network technique is predicting a digit from a handwritten image."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Document classification</strong> - This is an example of semi-supervised learning. Semi-supervised learning is when you apply both supervised and unsupervised learning techniques to a common problem. This technique relies on using a small amount of labeled data and a large amount of unlabeled data to train systems. When applying categories to a large document base, there may be too many documents to physically label. For example, these could be countless reports, transcripts, or specifications. Training on the unlabeled data helps identify similar documents for labeling."
      },
      {
        "answer": "",
        "explanation": "<strong>Association rule learning</strong> - This is an example of unsupervised learning. Association rule learning techniques uncover rule-based relationships between inputs in a dataset. For example, the Apriori algorithm conducts market basket analysis to identify rules like coffee and milk often being purchased together."
      },
      {
        "answer": "",
        "explanation": "<strong>Clustering</strong> - Clustering is an unsupervised learning technique that groups certain data inputs, so they may be categorized as a whole. There are various types of clustering algorithms depending on the input data. An example of clustering is identifying different types of network traffic to predict potential security incidents."
      }
    ],
    "references": [
      "https://aws.amazon.com/compare/the-difference-between-machine-learning-supervised-and-unsupervised/",
      "https://aws.amazon.com/what-is/machine-learning/"
    ]
  },
  {
    "id": 26,
    "question": "<p>Which of the following highlights the differences between model parameters and hyperparameters in the context of generative AI?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Hyperparameters are values that define a model and its behavior in interpreting input and generating responses. Model parameters are values that can be adjusted for model customization to control the training process</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Model parameters are values that define a model and its behavior in interpreting input and generating responses. Hyperparameters are values that can be adjusted for model customization to control the training process</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Both Hyperparameters and model parameters are values that can be adjusted for model customization to control the training process</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Both Hyperparameters and model parameters are values that define a model and its behavior in interpreting input and generating responses</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning (ML)",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Model parameters are values that define a model and its behavior in interpreting input and generating responses. Hyperparameters are values that can be adjusted for model customization to control the training process</strong></p>\n\n<p>Hyperparameters are values that can be adjusted for model customization to control the training process and, consequently, the output custom model. In other words, hyperparameters are external configurations set before the training process begins. They control the training process and the structure of the model but are not adjusted by the training algorithm itself. Examples include the learning rate, the number of layers in a neural network, etc.</p>\n\n<p>Model parameters are values that define a model and its behavior in interpreting input and generating responses. Model parameters are controlled and updated by providers. You can also update model parameters to create a new model through the process of model customization. In other words, Model parameters are the internal variables of the model that are learned and adjusted during the training process. These parameters directly influence the output of the model for a given input. Examples include the weights and biases in a neural network.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q53-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q53-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html\">https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Both Hyperparameters and model parameters are values that can be adjusted for model customization to control the training process</strong></p>\n\n<p><strong>Both Hyperparameters and model parameters are values that define a model and its behavior in interpreting input and generating responses</strong></p>\n\n<p><strong>Hyperparameters are values that define a model and its behavior in interpreting input and generating responses. Model parameters are values that can be adjusted for model customization to control the training process</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html\">https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Model parameters are values that define a model and its behavior in interpreting input and generating responses. Hyperparameters are values that can be adjusted for model customization to control the training process</strong>"
      },
      {
        "answer": "",
        "explanation": "Hyperparameters are values that can be adjusted for model customization to control the training process and, consequently, the output custom model. In other words, hyperparameters are external configurations set before the training process begins. They control the training process and the structure of the model but are not adjusted by the training algorithm itself. Examples include the learning rate, the number of layers in a neural network, etc."
      },
      {
        "answer": "",
        "explanation": "Model parameters are values that define a model and its behavior in interpreting input and generating responses. Model parameters are controlled and updated by providers. You can also update model parameters to create a new model through the process of model customization. In other words, Model parameters are the internal variables of the model that are learned and adjusted during the training process. These parameters directly influence the output of the model for a given input. Examples include the weights and biases in a neural network."
      },
      {
        "link": "https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Both Hyperparameters and model parameters are values that can be adjusted for model customization to control the training process</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Both Hyperparameters and model parameters are values that define a model and its behavior in interpreting input and generating responses</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Hyperparameters are values that define a model and its behavior in interpreting input and generating responses. Model parameters are values that can be adjusted for model customization to control the training process</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html"
    ]
  },
  {
    "id": 27,
    "question": "<p>A healthcare company uses a binary classification model to predict whether patients are at risk of developing a particular condition. The model is currently in production, but the company plans to develop a new version of the model to improve its accuracy. The ML engineer is tasked with recalibrating the model to maximize correct predictions for both patients at risk (positive labels) and patients not at risk (negative labels). The engineer must choose an appropriate metric to evaluate and adjust the model for these requirements.</p>\n\n<p>Which metric should the ML engineer use for recalibrating the model?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Accuracy</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Root mean squared error (RMSE)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Precision</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Recall</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Accuracy</strong></p>\n\n<p>Amazon SageMaker Autopilot produces metrics that measure the predictive quality of machine learning model candidates. The metrics calculated for candidates are specified using an array of <code>MetricDatum</code> types.</p>\n\n<p>The ratio of the number of correctly classified items to the total number of (correctly and incorrectly) classified items. It is used for both binary and multiclass classification. Accuracy measures how close the predicted class values are to the actual values. Values for accuracy metrics vary between zero (0) and one (1). A value of 1 indicates perfect accuracy, and 0 indicates perfect inaccuracy.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Precision</strong> - Precision measures how well an algorithm predicts the true positives (TP) out of all of the positives that it identifies. It is defined as follows: Precision = TP/(TP+FP), with values ranging from zero (0) to one (1), and is used in binary classification. Precision is an important metric when the cost of a false positive is high.</p>\n\n<p><strong>Root mean squared error (RMSE)</strong> - Root mean squared error (RMSE) measures the square root of the squared difference between predicted and actual values, and is averaged over all values. It is used in regression analysis to understand model prediction error. It's an important metric to indicate the presence of large model errors and outliers.</p>\n\n<p><strong>Recall</strong> - Recall measures how well an algorithm correctly predicts all of the true positives (TP) in a dataset. A true positive is a positive prediction that is also an actual positive value in the data. Recall is defined as follows: Recall = TP/(TP+FN), with values ranging from 0 to 1. Higher scores reflect a better ability of the model to predict true positives (TP) in the data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Accuracy</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon SageMaker Autopilot produces metrics that measure the predictive quality of machine learning model candidates. The metrics calculated for candidates are specified using an array of <code>MetricDatum</code> types."
      },
      {
        "answer": "",
        "explanation": "The ratio of the number of correctly classified items to the total number of (correctly and incorrectly) classified items. It is used for both binary and multiclass classification. Accuracy measures how close the predicted class values are to the actual values. Values for accuracy metrics vary between zero (0) and one (1). A value of 1 indicates perfect accuracy, and 0 indicates perfect inaccuracy."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Precision</strong> - Precision measures how well an algorithm predicts the true positives (TP) out of all of the positives that it identifies. It is defined as follows: Precision = TP/(TP+FP), with values ranging from zero (0) to one (1), and is used in binary classification. Precision is an important metric when the cost of a false positive is high."
      },
      {
        "answer": "",
        "explanation": "<strong>Root mean squared error (RMSE)</strong> - Root mean squared error (RMSE) measures the square root of the squared difference between predicted and actual values, and is averaged over all values. It is used in regression analysis to understand model prediction error. It's an important metric to indicate the presence of large model errors and outliers."
      },
      {
        "answer": "",
        "explanation": "<strong>Recall</strong> - Recall measures how well an algorithm correctly predicts all of the true positives (TP) in a dataset. A true positive is a positive prediction that is also an actual positive value in the data. Recall is defined as follows: Recall = TP/(TP+FN), with values ranging from 0 to 1. Higher scores reflect a better ability of the model to predict true positives (TP) in the data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html"
    ]
  },
  {
    "id": 28,
    "question": "<p>A healthcare analytics company has implemented a data ingestion pipeline to process patient monitoring data from wearable devices. The pipeline uses Amazon Kinesis Data Firehose to ingest data into Amazon OpenSearch Service. Currently, the Firehose stream has a buffer interval of 60 seconds, and an OpenSearch dashboard displays real-time alerts about patients' health metrics based on the ingested data. The company needs to optimize the pipeline to achieve sub-second latency for the alerts on the dashboard to respond quickly to critical patient health events.</p>\n\n<p>What do you recommend as the most optimal solution?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Switch from Kinesis Data Firehose to Apache Spark Streaming to preprocess patient monitoring data and load it into OpenSearch Service in real time</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable caching on the OpenSearch dashboard to reduce latency by serving previously ingested data for alert generation</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Reduce the Kinesis Data Firehose buffer interval to zero seconds by leveraging 'buffering hints', thereby enabling immediate data delivery to OpenSearch Service</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Replace the Firehose stream with Amazon Kinesis Data Streams and use AWS Lambda to write the data to OpenSearch Service with lower latency</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Reduce the Kinesis Data Firehose buffer interval to zero seconds by leveraging 'buffering hints', thereby enabling immediate data delivery to OpenSearch Service</strong></p>\n\n<p>Kinesis Data Firehose provides configurable buffer intervals for data delivery to destinations. Setting the buffer interval to zero seconds ensures data is delivered to OpenSearch Service as soon as it is ingested, minimizing latency. You can create a buffering hint by setting a value of zero for <code>IntervalInSeconds</code> parameter. Firehose will buffer incoming data for the specified period of time, in seconds, before delivering it to the destination. This adjustment eliminates batching delays and allows the OpenSearch dashboard to display near real-time health alerts, meeting the sub-second latency requirement.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable caching on the OpenSearch dashboard to reduce latency by serving previously ingested data for alert generation</strong> - Caching can reduce query latency but does not address the data ingestion delay caused by the 60-second Firehose buffer interval.</p>\n\n<p><strong>Replace the Firehose stream with Amazon Kinesis Data Streams and use AWS Lambda to write the data to OpenSearch Service with lower latency</strong> - While Kinesis Data Streams and Lambda could reduce latency, this solution increases operational complexity compared to simply adjusting the Firehose buffer interval.</p>\n\n<p><strong>Switch from Kinesis Data Firehose to Apache Spark Streaming to preprocess patient monitoring data and load it into OpenSearch Service in real time</strong> - While Apache Spark Streaming could reduce latency, this solution increases operational complexity as you need to write custom code to implement the solution. So, this option is not the best fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-kinesis-data-firehose-zero-buffering/\">https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-kinesis-data-firehose-zero-buffering/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/APIReference/API_BufferingHints.html\">https://docs.aws.amazon.com/firehose/latest/APIReference/API_BufferingHints.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Reduce the Kinesis Data Firehose buffer interval to zero seconds by leveraging 'buffering hints', thereby enabling immediate data delivery to OpenSearch Service</strong>"
      },
      {
        "answer": "",
        "explanation": "Kinesis Data Firehose provides configurable buffer intervals for data delivery to destinations. Setting the buffer interval to zero seconds ensures data is delivered to OpenSearch Service as soon as it is ingested, minimizing latency. You can create a buffering hint by setting a value of zero for <code>IntervalInSeconds</code> parameter. Firehose will buffer incoming data for the specified period of time, in seconds, before delivering it to the destination. This adjustment eliminates batching delays and allows the OpenSearch dashboard to display near real-time health alerts, meeting the sub-second latency requirement."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable caching on the OpenSearch dashboard to reduce latency by serving previously ingested data for alert generation</strong> - Caching can reduce query latency but does not address the data ingestion delay caused by the 60-second Firehose buffer interval."
      },
      {
        "answer": "",
        "explanation": "<strong>Replace the Firehose stream with Amazon Kinesis Data Streams and use AWS Lambda to write the data to OpenSearch Service with lower latency</strong> - While Kinesis Data Streams and Lambda could reduce latency, this solution increases operational complexity compared to simply adjusting the Firehose buffer interval."
      },
      {
        "answer": "",
        "explanation": "<strong>Switch from Kinesis Data Firehose to Apache Spark Streaming to preprocess patient monitoring data and load it into OpenSearch Service in real time</strong> - While Apache Spark Streaming could reduce latency, this solution increases operational complexity as you need to write custom code to implement the solution. So, this option is not the best fit."
      }
    ],
    "references": [
      "https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-kinesis-data-firehose-zero-buffering/",
      "https://docs.aws.amazon.com/firehose/latest/APIReference/API_BufferingHints.html",
      "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
    ]
  },
  {
    "id": 29,
    "question": "<p>You are a data scientist working on a predictive maintenance model for an industrial manufacturing company. The model is designed to predict equipment failures based on sensor data collected over time. During the development process, you notice that the model performs exceptionally well on the training data but struggles to generalize to new, unseen data. Additionally, there are some indications that the model might not be fully capturing the complexity of the problem. To ensure the model performs well in production, you need to identify whether it is overfitting, underfitting, or both.</p>\n\n<p>Which of the following strategies is the MOST EFFECTIVE for identifying overfitting and underfitting in your model?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Perform cross-validation with different subsets of the data; if the model’s performance varies significantly across folds, the model is underfitting</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Compare the training and validation loss curves over time; if the validation loss is much higher than the training loss, the model is likely overfitting</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Reduce the number of features in the model; if performance improves, the model was previously overfitting</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Analyze the model’s performance on a separate test set; if the model performs well on both the training and test sets, it is neither overfitting nor underfitting</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Compare the training and validation loss curves over time; if the validation loss is much higher than the training loss, the model is likely overfitting</strong></p>\n\n<p>Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt1-q12-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt1-q12-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html</a><p></p>\n\n<p>Comparing the training and validation loss curves is an effective way to identify overfitting. If the validation loss is significantly higher than the training loss, it indicates that the model is overfitting the training data and failing to generalize to unseen data. This is a clear sign that the model is too complex or trained for too many epochs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Reduce the number of features in the model; if performance improves, the model was previously overfitting</strong> - Reducing the number of features might reduce overfitting, but it’s not a diagnostic method. It’s a corrective measure rather than a strategy to identify overfitting or underfitting. The model’s behavior on training versus validation data is a better indicator of overfitting or underfitting.</p>\n\n<p><strong>Perform cross-validation with different subsets of the data; if the model’s performance varies significantly across folds, the model is underfitting</strong> - Cross-validation is a useful technique for assessing model performance, but significant variation in performance across folds is more indicative of data variance rather than underfitting. Underfitting is generally identified when the model performs poorly on both the training and validation sets.</p>\n\n<p><strong>Analyze the model’s performance on a separate test set; if the model performs well on both the training and test sets, it is neither overfitting nor underfitting</strong> - Analyzing performance on a test set is important, but it only confirms the final model’s ability to generalize. If the model performs well on both training and test data, it likely indicates a good fit, but this does not help identify overfitting or underfitting during the training process itself.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/what-is/overfitting/\">https://aws.amazon.com/what-is/overfitting/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Compare the training and validation loss curves over time; if the validation loss is much higher than the training loss, the model is likely overfitting</strong>"
      },
      {
        "answer": "",
        "explanation": "Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples."
      },
      {
        "link": "https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html"
      },
      {
        "answer": "",
        "explanation": "Comparing the training and validation loss curves is an effective way to identify overfitting. If the validation loss is significantly higher than the training loss, it indicates that the model is overfitting the training data and failing to generalize to unseen data. This is a clear sign that the model is too complex or trained for too many epochs."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Reduce the number of features in the model; if performance improves, the model was previously overfitting</strong> - Reducing the number of features might reduce overfitting, but it’s not a diagnostic method. It’s a corrective measure rather than a strategy to identify overfitting or underfitting. The model’s behavior on training versus validation data is a better indicator of overfitting or underfitting."
      },
      {
        "answer": "",
        "explanation": "<strong>Perform cross-validation with different subsets of the data; if the model’s performance varies significantly across folds, the model is underfitting</strong> - Cross-validation is a useful technique for assessing model performance, but significant variation in performance across folds is more indicative of data variance rather than underfitting. Underfitting is generally identified when the model performs poorly on both the training and validation sets."
      },
      {
        "answer": "",
        "explanation": "<strong>Analyze the model’s performance on a separate test set; if the model performs well on both the training and test sets, it is neither overfitting nor underfitting</strong> - Analyzing performance on a test set is important, but it only confirms the final model’s ability to generalize. If the model performs well on both training and test data, it likely indicates a good fit, but this does not help identify overfitting or underfitting during the training process itself."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html",
      "https://aws.amazon.com/what-is/overfitting/"
    ]
  },
  {
    "id": 30,
    "question": "<p>You are a machine learning engineer at a healthcare startup that uses an Amazon SageMaker endpoint to deliver real-time diagnostics based on patient data. The model needs to handle a high volume of requests with low latency to ensure timely results. Recently, the startup has experienced rapid growth, leading to occasional periods of high traffic where users experience increased latency and, in some cases, request timeouts. You also need to be mindful of cost, as the startup operates on a tight budget.</p>\n\n<p>Which approach is the MOST EFFECTIVE for troubleshooting and resolving the capacity concerns while balancing cost and performance?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>nable auto-scaling for the SageMaker endpoint to automatically adjust the number of instances based on request load, and set a budget alert in AWS Budgets to monitor cost increases as traffic scales</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Lambda with provisioned concurrency to handle the requests, ensuring that the function is always ready to serve traffic. Configure Lambda to auto-scale based on traffic but limit the maximum concurrency to control costs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Request a service quota increase for the SageMaker endpoint to allow for more instances during peak traffic, and set up a CloudWatch Alarm to notify you when utilization exceeds 80% of the current quota</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Increase the instance size for the SageMaker endpoint to handle more requests per instance, and manually monitor performance and costs using Amazon CloudWatch metrics</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>nable auto-scaling for the SageMaker endpoint to automatically adjust the number of instances based on request load, and set a budget alert in AWS Budgets to monitor cost increases as traffic scales</strong></p>\n\n<p>Enabling auto-scaling on the SageMaker endpoint allows the system to automatically adjust the number of instances based on incoming traffic, ensuring that it can handle spikes without degrading performance. With a target tracking scaling policy, you choose an Amazon CloudWatch metric and target value. Auto scaling creates and manages the CloudWatch alarms for the scaling policy and calculates the scaling adjustment based on the metric and the target value. The policy adds and removes the number of instances as required to keep the metric at, or close to, the specified target value. For example, a scaling policy that uses the predefined InvocationsPerInstance metric with a target value of 70 can keep InvocationsPerInstance at, or close to 70.</p>\n\n<p>By setting up a budget alert in AWS Budgets, you can monitor cost increases and ensure that scaling does not exceed your budget. This approach provides a balance between performance and cost, allowing the system to scale dynamically while keeping expenses in check.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q43-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q43-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html\">https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the instance size for the SageMaker endpoint to handle more requests per instance, and manually monitor performance and costs using Amazon CloudWatch metrics</strong> - Increasing the instance size might temporarily address performance issues, but it is less flexible and could lead to over-provisioning, resulting in unnecessary costs during periods of low traffic. Manual monitoring is also less efficient and can lead to delays in addressing issues.</p>\n\n<p><strong>Use AWS Lambda with provisioned concurrency to handle the requests, ensuring that the function is always ready to serve traffic. Configure Lambda to auto-scale based on traffic but limit the maximum concurrency to control costs</strong> - Provisioned concurrency is a feature that keeps Lambda functions initialized and hyper-ready to respond in double-digit milliseconds. This is ideal for implementing interactive services, such as web and mobile backends, latency-sensitive microservices, or synchronous APIs. Using AWS Lambda with provisioned concurrency is suitable for serverless workloads, but it may not be the best fit for a SageMaker model that requires consistent performance at scale. Additionally, limiting concurrency to control costs could lead to performance bottlenecks during high-traffic periods.</p>\n\n<p><strong>Request a service quota increase for the SageMaker endpoint to allow for more instances during peak traffic, and set up a CloudWatch Alarm to notify you when utilization exceeds 80% of the current quota</strong> - Your AWS account has default quotas, formerly referred to as limits, for each AWS service. Requesting a service quota increase can help if the current quota is a limiting factor, but it doesn’t directly address cost control or the need for dynamic scaling. It’s a reactive approach that doesn’t provide the flexibility of auto-scaling combined with budget monitoring.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html\">https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/\">https://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html\">https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>nable auto-scaling for the SageMaker endpoint to automatically adjust the number of instances based on request load, and set a budget alert in AWS Budgets to monitor cost increases as traffic scales</strong>"
      },
      {
        "answer": "",
        "explanation": "Enabling auto-scaling on the SageMaker endpoint allows the system to automatically adjust the number of instances based on incoming traffic, ensuring that it can handle spikes without degrading performance. With a target tracking scaling policy, you choose an Amazon CloudWatch metric and target value. Auto scaling creates and manages the CloudWatch alarms for the scaling policy and calculates the scaling adjustment based on the metric and the target value. The policy adds and removes the number of instances as required to keep the metric at, or close to, the specified target value. For example, a scaling policy that uses the predefined InvocationsPerInstance metric with a target value of 70 can keep InvocationsPerInstance at, or close to 70."
      },
      {
        "answer": "",
        "explanation": "By setting up a budget alert in AWS Budgets, you can monitor cost increases and ensure that scaling does not exceed your budget. This approach provides a balance between performance and cost, allowing the system to scale dynamically while keeping expenses in check."
      },
      {
        "link": "https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Increase the instance size for the SageMaker endpoint to handle more requests per instance, and manually monitor performance and costs using Amazon CloudWatch metrics</strong> - Increasing the instance size might temporarily address performance issues, but it is less flexible and could lead to over-provisioning, resulting in unnecessary costs during periods of low traffic. Manual monitoring is also less efficient and can lead to delays in addressing issues."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Lambda with provisioned concurrency to handle the requests, ensuring that the function is always ready to serve traffic. Configure Lambda to auto-scale based on traffic but limit the maximum concurrency to control costs</strong> - Provisioned concurrency is a feature that keeps Lambda functions initialized and hyper-ready to respond in double-digit milliseconds. This is ideal for implementing interactive services, such as web and mobile backends, latency-sensitive microservices, or synchronous APIs. Using AWS Lambda with provisioned concurrency is suitable for serverless workloads, but it may not be the best fit for a SageMaker model that requires consistent performance at scale. Additionally, limiting concurrency to control costs could lead to performance bottlenecks during high-traffic periods."
      },
      {
        "answer": "",
        "explanation": "<strong>Request a service quota increase for the SageMaker endpoint to allow for more instances during peak traffic, and set up a CloudWatch Alarm to notify you when utilization exceeds 80% of the current quota</strong> - Your AWS account has default quotas, formerly referred to as limits, for each AWS service. Requesting a service quota increase can help if the current quota is a limiting factor, but it doesn’t directly address cost control or the need for dynamic scaling. It’s a reactive approach that doesn’t provide the flexibility of auto-scaling combined with budget monitoring."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html",
      "https://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/",
      "https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html"
    ]
  },
  {
    "id": 31,
    "question": "<p>You are a data scientist working on a machine learning project to predict customer lifetime value (CLV) for an e-commerce company. Before deploying a complex model like a deep neural network, you need to establish a performance baseline to measure the effectiveness of your advanced models. You decide to use Amazon SageMaker to create this baseline efficiently. The goal is to build a simple model that can be easily implemented and provide a reference point for evaluating the performance of more sophisticated models later.</p>\n\n<p>Which of the following approaches is the MOST EFFECTIVE for creating a performance baseline using Amazon SageMaker?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy a SageMaker BlazingText model to create word embeddings from customer reviews, which can be used as a baseline for evaluating CLV predictions</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker JumpStart to deploy a pre-trained model for customer segmentation, which can serve as a baseline for your CLV prediction model</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Implement SageMaker Autopilot to automatically explore various models and select the best one as the baseline, allowing you to skip manual model selection</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Train a basic linear learner model using Amazon SageMaker, focusing on key features like customer age, purchase frequency, and average order value, to establish a baseline for CLV prediction</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Train a basic linear learner model using Amazon SageMaker, focusing on key features like customer age, purchase frequency, and average order value, to establish a baseline for CLV prediction</strong></p>\n\n<p>Using SageMaker’s linear learner algorithm is an effective approach for creating a simple and interpretable baseline. This method allows you to establish a performance benchmark using key features that are directly related to predicting CLV. The linear learner is quick to train and provides a clear point of comparison for more complex models.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use SageMaker JumpStart to deploy a pre-trained model for customer segmentation, which can serve as a baseline for your CLV prediction model</strong> - SageMaker JumpStart is great for quickly deploying pre-built models, but a pre-trained customer segmentation model is not directly aligned with predicting CLV. It would not serve as an appropriate baseline for this specific task.</p>\n\n<p><strong>Implement SageMaker Autopilot to automatically explore various models and select the best one as the baseline, allowing you to skip manual model selection</strong> - SageMaker Autopilot can automatically explore and select the best models, but it typically aims for optimal performance rather than establishing a simple baseline. While Autopilot is useful, starting with a basic model like linear learner helps you understand the incremental value of more complex models.</p>\n\n<p><strong>Deploy a SageMaker BlazingText model to create word embeddings from customer reviews, which can be used as a baseline for evaluating CLV predictions</strong> - BlazingText is designed for natural language processing tasks like text classification and word embedding, which are not directly applicable to creating a CLV prediction baseline. It’s not the right tool for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/sagemaker/jumpstart/\">https://aws.amazon.com/sagemaker/jumpstart/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Train a basic linear learner model using Amazon SageMaker, focusing on key features like customer age, purchase frequency, and average order value, to establish a baseline for CLV prediction</strong>"
      },
      {
        "answer": "",
        "explanation": "Using SageMaker’s linear learner algorithm is an effective approach for creating a simple and interpretable baseline. This method allows you to establish a performance benchmark using key features that are directly related to predicting CLV. The linear learner is quick to train and provides a clear point of comparison for more complex models."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use SageMaker JumpStart to deploy a pre-trained model for customer segmentation, which can serve as a baseline for your CLV prediction model</strong> - SageMaker JumpStart is great for quickly deploying pre-built models, but a pre-trained customer segmentation model is not directly aligned with predicting CLV. It would not serve as an appropriate baseline for this specific task."
      },
      {
        "answer": "",
        "explanation": "<strong>Implement SageMaker Autopilot to automatically explore various models and select the best one as the baseline, allowing you to skip manual model selection</strong> - SageMaker Autopilot can automatically explore and select the best models, but it typically aims for optimal performance rather than establishing a simple baseline. While Autopilot is useful, starting with a basic model like linear learner helps you understand the incremental value of more complex models."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy a SageMaker BlazingText model to create word embeddings from customer reviews, which can be used as a baseline for evaluating CLV predictions</strong> - BlazingText is designed for natural language processing tasks like text classification and word embedding, which are not directly applicable to creating a CLV prediction baseline. It’s not the right tool for this use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html",
      "https://aws.amazon.com/sagemaker/jumpstart/",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html"
    ]
  },
  {
    "id": 32,
    "question": "<p>Which of the following summarizes the differences between a token and an embedding in the context of generative AI?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A token is a sequence of characters that a model can interpret or predict as a single unit of meaning, whereas, an embedding is a vector of numerical values that represents condensed information obtained by transforming input into that vector</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Both token and embedding refer to a vector of numerical values that represents condensed information obtained by transforming input into that vector</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Both token and embedding refer to a sequence of characters that a model can interpret or predict as a single unit of meaning</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>An embedding is a sequence of characters that a model can interpret or predict as a single unit of meaning, whereas, a token is a vector of numerical values that represents condensed information obtained by transforming input into that vector</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning (ML)",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>A token is a sequence of characters that a model can interpret or predict as a single unit of meaning, whereas, an embedding is a vector of numerical values that represents condensed information obtained by transforming input into that vector</strong></p>\n\n<p>Embedding – The process of condensing information by transforming input into a vector of numerical values, known as the embeddings, in order to compare the similarity between different objects by using a shared numerical representation. For example, sentences can be compared to determine the similarity in meaning, images can be compared to determine visual similarity, or text and image can be compared to see if they're relevant to each other.</p>\n\n<p>Token – A sequence of characters that a model can interpret or predict as a single unit of meaning. For example, with text models, a token could correspond not just to a word, but also to a part of a word with grammatical meaning (such as \"-ed\"), a punctuation mark (such as \"?\"), or a common phrase (such as \"a lot\").</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q54-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q54-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html\">https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Both token and embedding refer to a sequence of characters that a model can interpret or predict as a single unit of meaning</strong></p>\n\n<p><strong>An embedding is a sequence of characters that a model can interpret or predict as a single unit of meaning, whereas, a token is a vector of numerical values that represents condensed information obtained by transforming input into that vector</strong></p>\n\n<p><strong>Both token and embedding refer to a vector of numerical values that represents condensed information obtained by transforming input into that vector</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>A token is a sequence of characters that a model can interpret or predict as a single unit of meaning, whereas, an embedding is a vector of numerical values that represents condensed information obtained by transforming input into that vector</strong>"
      },
      {
        "answer": "",
        "explanation": "Embedding – The process of condensing information by transforming input into a vector of numerical values, known as the embeddings, in order to compare the similarity between different objects by using a shared numerical representation. For example, sentences can be compared to determine the similarity in meaning, images can be compared to determine visual similarity, or text and image can be compared to see if they're relevant to each other."
      },
      {
        "answer": "",
        "explanation": "Token – A sequence of characters that a model can interpret or predict as a single unit of meaning. For example, with text models, a token could correspond not just to a word, but also to a part of a word with grammatical meaning (such as \"-ed\"), a punctuation mark (such as \"?\"), or a common phrase (such as \"a lot\")."
      },
      {
        "link": "https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Both token and embedding refer to a sequence of characters that a model can interpret or predict as a single unit of meaning</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>An embedding is a sequence of characters that a model can interpret or predict as a single unit of meaning, whereas, a token is a vector of numerical values that represents condensed information obtained by transforming input into that vector</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Both token and embedding refer to a vector of numerical values that represents condensed information obtained by transforming input into that vector</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html"
    ]
  },
  {
    "id": 33,
    "question": "<p>You are a machine learning engineer responsible for maintaining an ML pipeline that processes customer transaction data to detect fraudulent activity. The pipeline includes several stages: data ingestion, data preprocessing, model inference, and result storage. The pipeline runs continuously, processing large volumes of data in near real-time. Recently, you’ve noticed that some transactions are being incorrectly classified as non-fraudulent, raising concerns about potential anomalies or errors in the data processing or model inference stages.</p>\n\n<p>Given the critical nature of the pipeline, which approach is the MOST EFFECTIVE for addressing these issues?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Lambda functions to log and audit every transaction processed by the pipeline, storing detailed logs in Amazon S3 for manual analysis when anomalies are suspected</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement Amazon SageMaker Model Monitor to track the distribution of input data features and model predictions over time, alerting you when deviations from expected patterns occur</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Schedule periodic manual reviews of a random sample of processed transactions to check for anomalies or errors, documenting any issues for further investigation</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up Amazon CloudWatch alarms to monitor CPU and memory usage across the pipeline’s compute resources, and trigger notifications if these metrics exceed predefined thresholds</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement Amazon SageMaker Model Monitor to track the distribution of input data features and model predictions over time, alerting you when deviations from expected patterns occur</strong></p>\n\n<p>Amazon SageMaker Model Monitor is designed to track the distribution of input data features and monitor model predictions for anomalies. It provides alerts when deviations from expected patterns are detected, making it an effective tool for detecting data drift, concept drift, and other issues that could lead to incorrect classifications.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q31-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q31-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up Amazon CloudWatch alarms to monitor CPU and memory usage across the pipeline’s compute resources, and trigger notifications if these metrics exceed predefined thresholds</strong> - While monitoring CPU and memory usage can help identify performance issues, it does not directly address anomalies or errors in data processing or model inference. These metrics are not sufficient to detect issues like data drift or changes in model behavior.</p>\n\n<p><strong>Schedule periodic manual reviews of a random sample of processed transactions to check for anomalies or errors, documenting any issues for further investigation</strong> - Manual reviews can be helpful for spot-checking, but they are not scalable or reliable for continuous monitoring, especially in a high-volume, real-time pipeline. Automated monitoring is more effective for catching issues as they arise.</p>\n\n<p><strong>Use AWS Lambda functions to log and audit every transaction processed by the pipeline, storing detailed logs in Amazon S3 for manual analysis when anomalies are suspected</strong> - Logging and auditing every transaction with AWS Lambda functions can provide detailed insights, but it is resource-intensive and does not offer real-time anomaly detection. This approach is better suited for post-mortem analysis rather than proactive monitoring.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Implement Amazon SageMaker Model Monitor to track the distribution of input data features and model predictions over time, alerting you when deviations from expected patterns occur</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon SageMaker Model Monitor is designed to track the distribution of input data features and monitor model predictions for anomalies. It provides alerts when deviations from expected patterns are detected, making it an effective tool for detecting data drift, concept drift, and other issues that could lead to incorrect classifications."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon CloudWatch alarms to monitor CPU and memory usage across the pipeline’s compute resources, and trigger notifications if these metrics exceed predefined thresholds</strong> - While monitoring CPU and memory usage can help identify performance issues, it does not directly address anomalies or errors in data processing or model inference. These metrics are not sufficient to detect issues like data drift or changes in model behavior."
      },
      {
        "answer": "",
        "explanation": "<strong>Schedule periodic manual reviews of a random sample of processed transactions to check for anomalies or errors, documenting any issues for further investigation</strong> - Manual reviews can be helpful for spot-checking, but they are not scalable or reliable for continuous monitoring, especially in a high-volume, real-time pipeline. Automated monitoring is more effective for catching issues as they arise."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Lambda functions to log and audit every transaction processed by the pipeline, storing detailed logs in Amazon S3 for manual analysis when anomalies are suspected</strong> - Logging and auditing every transaction with AWS Lambda functions can provide detailed insights, but it is resource-intensive and does not offer real-time anomaly detection. This approach is better suited for post-mortem analysis rather than proactive monitoring."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html"
    ]
  },
  {
    "id": 34,
    "question": "<p>A global e-commerce company collects video/audio/text based customer feedback in various languages, including German, and stores the data for analysis. The company wants to use a large language model (LLM) to analyze and generate summaries of customer feedback in English. The solution must handle multilingual audio data efficiently and complete the task with the least operational effort.</p>\n\n<p>Which solution will meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Manually preprocess all data formats into plain text and use a custom-trained SageMaker model for summarization in English</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Rekognition to extract metadata from video and audio feedback and combine it with audio data transcription to summarize using Amazon Comprehend</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Transcribe to convert video/audio feedback into German text, Amazon Translate to translate the German text into English, and Amazon Comprehend to analyze sentiment and summarize the feedback in English</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Transcribe to directly summarize the audio and video feedback from German to English text and use Amazon Comprehend to analyze customer sentiment</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Transcribe to convert video/audio feedback into German text, Amazon Translate to translate the German text into English, and Amazon Comprehend to analyze sentiment and summarize the feedback in English</strong></p>\n\n<p>This solution efficiently leverages AWS services designed for specific tasks:</p>\n\n<p>Amazon Transcribe: Converts video/audio feedback into German text, making the content accessible for further processing.</p>\n\n<p>Amazon Translate: Translates the German text into English, ensuring compatibility with downstream analysis tools.</p>\n\n<p>Amazon Comprehend: Analyzes the English text for customer sentiment and generates summaries.</p>\n\n<p>This approach minimizes operational effort by using fully managed services that seamlessly integrate to handle the multilingual data pipeline.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Rekognition to extract metadata from video and audio feedback and combine it with audio data transcription to summarize using Amazon Comprehend</strong> - Rekognition is designed for image/video analysis and not audio analysis. So, this option is incorrect.</p>\n\n<p><strong>Manually preprocess all data formats into plain text and use a custom-trained SageMaker model for summarization in English</strong> - This approach introduces significant manual effort and operational complexity compared to the fully managed services provided by AWS.</p>\n\n<p><strong>Use Amazon Transcribe to directly summarize the audio and video feedback from German to English text and use Amazon Comprehend to analyze customer sentiment</strong> - Amazon Transcribe only converts video/audio to text and does not support direct summarization. Additional tools are required for the summarization task.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/pm/transcribe/\">https://aws.amazon.com/pm/transcribe/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rekognition/\">https://aws.amazon.com/rekognition/</a></p>\n\n<p><a href=\"https://aws.amazon.com/translate/\">https://aws.amazon.com/translate/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/break-through-language-barriers-with-amazon-transcribe-amazon-translate-and-amazon-polly/\">https://aws.amazon.com/blogs/machine-learning/break-through-language-barriers-with-amazon-transcribe-amazon-translate-and-amazon-polly/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Transcribe to convert video/audio feedback into German text, Amazon Translate to translate the German text into English, and Amazon Comprehend to analyze sentiment and summarize the feedback in English</strong>"
      },
      {
        "answer": "",
        "explanation": "This solution efficiently leverages AWS services designed for specific tasks:"
      },
      {
        "answer": "",
        "explanation": "Amazon Transcribe: Converts video/audio feedback into German text, making the content accessible for further processing."
      },
      {
        "answer": "",
        "explanation": "Amazon Translate: Translates the German text into English, ensuring compatibility with downstream analysis tools."
      },
      {
        "answer": "",
        "explanation": "Amazon Comprehend: Analyzes the English text for customer sentiment and generates summaries."
      },
      {
        "answer": "",
        "explanation": "This approach minimizes operational effort by using fully managed services that seamlessly integrate to handle the multilingual data pipeline."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Rekognition to extract metadata from video and audio feedback and combine it with audio data transcription to summarize using Amazon Comprehend</strong> - Rekognition is designed for image/video analysis and not audio analysis. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Manually preprocess all data formats into plain text and use a custom-trained SageMaker model for summarization in English</strong> - This approach introduces significant manual effort and operational complexity compared to the fully managed services provided by AWS."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Transcribe to directly summarize the audio and video feedback from German to English text and use Amazon Comprehend to analyze customer sentiment</strong> - Amazon Transcribe only converts video/audio to text and does not support direct summarization. Additional tools are required for the summarization task."
      }
    ],
    "references": [
      "https://aws.amazon.com/pm/transcribe/",
      "https://aws.amazon.com/rekognition/",
      "https://aws.amazon.com/translate/",
      "https://aws.amazon.com/blogs/machine-learning/break-through-language-barriers-with-amazon-transcribe-amazon-translate-and-amazon-polly/"
    ]
  },
  {
    "id": 35,
    "question": "<p>You are a Machine Learning Engineer at a healthcare company working on a binary classification model to predict whether a patient has a particular disease based on several medical features. The consequences of misclassifications are severe: false positives lead to unnecessary and expensive follow-up tests, while false negatives could result in a failure to provide critical treatment. You need to evaluate the model using appropriate metrics to balance the risks associated with these types of errors.</p>\n\n<p>Given the critical nature of the application, which combination of evaluation metrics should you prioritize to minimize both false positives and false negatives while ensuring that the model is reliable for deployment? (Select two)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Prioritize accuracy, as it provides a general sense of how well the model is performing across all predictions</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Evaluate the model using the Area Under the ROC Curve (AUC) to understand its performance across different classification thresholds</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Prioritize recall to reduce the number of false negatives, ensuring that as many patients with the disease as possible are correctly identified</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Focus on precision to reduce the number of false positives, thus avoiding unnecessary follow-up tests for patients who do not have the disease</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use the F1 score to balance the trade-off between precision and recall, ensuring that both false positives and false negatives are considered</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use the F1 score to balance the trade-off between precision and recall, ensuring that both false positives and false negatives are considered</strong></p>\n\n<p>The F1 score is the harmonic mean of precision and recall, and it provides a single metric that balances both false positives and false negatives. It is particularly useful in scenarios where both types of errors have significant consequences, as in this healthcare scenario.</p>\n\n<p><strong>Prioritize recall to reduce the number of false negatives, ensuring that as many patients with the disease as possible are correctly identified</strong></p>\n\n<p>Given the severe risk associated with false negatives (failing to identify a patient with the disease), recall is a crucial metric. By prioritizing recall, you ensure that the model captures as many actual positive cases as possible, reducing the chance of missing a patient who needs treatment.</p>\n\n<p>Key metrics to measure machine learning model performance:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q13-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q13-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q13-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q13-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Prioritize accuracy, as it provides a general sense of how well the model is performing across all predictions</strong> - Accuracy alone is not a sufficient metric in this scenario because it does not differentiate between the types of errors (false positives and false negatives). High accuracy might still result in unacceptable levels of false negatives or false positives.</p>\n\n<p><strong>Focus on precision to reduce the number of false positives, thus avoiding unnecessary follow-up tests for patients who do not have the disease</strong> - Focusing solely on precision would reduce false positives but might increase false negatives, which could be catastrophic in this healthcare scenario where missing a disease diagnosis could lead to severe consequences.</p>\n\n<p><strong>Evaluate the model using the Area Under the ROC Curve (AUC) to understand its performance across different classification thresholds</strong> - AUC-ROC is valuable for understanding the model's performance across thresholds, but it doesn't provide the same direct focus on balancing precision and recall as the F1 score does. In a healthcare scenario with high stakes, the direct balance offered by F1 score and a focus on recall are more critical.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the F1 score to balance the trade-off between precision and recall, ensuring that both false positives and false negatives are considered</strong>"
      },
      {
        "answer": "",
        "explanation": "The F1 score is the harmonic mean of precision and recall, and it provides a single metric that balances both false positives and false negatives. It is particularly useful in scenarios where both types of errors have significant consequences, as in this healthcare scenario."
      },
      {
        "answer": "",
        "explanation": "<strong>Prioritize recall to reduce the number of false negatives, ensuring that as many patients with the disease as possible are correctly identified</strong>"
      },
      {
        "answer": "",
        "explanation": "Given the severe risk associated with false negatives (failing to identify a patient with the disease), recall is a crucial metric. By prioritizing recall, you ensure that the model captures as many actual positive cases as possible, reducing the chance of missing a patient who needs treatment."
      },
      {
        "answer": "",
        "explanation": "Key metrics to measure machine learning model performance:"
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html",
        "answer": "",
        "explanation": "via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Prioritize accuracy, as it provides a general sense of how well the model is performing across all predictions</strong> - Accuracy alone is not a sufficient metric in this scenario because it does not differentiate between the types of errors (false positives and false negatives). High accuracy might still result in unacceptable levels of false negatives or false positives."
      },
      {
        "answer": "",
        "explanation": "<strong>Focus on precision to reduce the number of false positives, thus avoiding unnecessary follow-up tests for patients who do not have the disease</strong> - Focusing solely on precision would reduce false positives but might increase false negatives, which could be catastrophic in this healthcare scenario where missing a disease diagnosis could lead to severe consequences."
      },
      {
        "answer": "",
        "explanation": "<strong>Evaluate the model using the Area Under the ROC Curve (AUC) to understand its performance across different classification thresholds</strong> - AUC-ROC is valuable for understanding the model's performance across thresholds, but it doesn't provide the same direct focus on balancing precision and recall as the F1 score does. In a healthcare scenario with high stakes, the direct balance offered by F1 score and a focus on recall are more critical."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html",
      "https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html"
    ]
  },
  {
    "id": 36,
    "question": "<p>A telecommunications company receives real-time network performance data streams from thousands of devices across its infrastructure. The data streams consist of thousands of JSON records per second, detailing metrics like latency, packet loss, and jitter. The company needs to implement a scalable solution on AWS to monitor these metrics and detect anomalies in real time with the least operational overhead.</p>\n\n<p>Which solution will meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Kinesis Data Streams to ingest the data, process it with Apache Flink to analyze the streams in real time, and apply the RANDOM_CUT_FOREST algorithm to detect anomalies</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Kinesis Firehose for real-time streaming of the network metrics data into an Amazon S3 bucket, then periodically run an AWS Glue ETL job to detect anomalies using a custom Python script</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Ingest the data using Amazon Kinesis Data Streams, analyze the network metrics using Amazon Comprehend, and identify anomalies with a SageMaker model</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Lambda to process the network metrics streams in real time and write a custom function to detect anomalies directly from the data</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to ingest the data, process it with Apache Flink to analyze the streams in real time, and apply the RANDOM_CUT_FOREST algorithm to detect anomalies</strong></p>\n\n<p>This solution uses AWS services designed for real-time processing and anomaly detection:</p>\n\n<p>Amazon Kinesis Data Streams - Provides a scalable, low-latency service for ingesting high-volume, real-time network metrics.</p>\n\n<p>Apache Flink on Kinesis Data Analytics - Processes the data streams in real time and offers powerful frameworks for data analysis.</p>\n\n<p>RANDOM_CUT_FOREST algorithm - A built-in anomaly detection algorithm within Apache Flink that minimizes the need for custom code or external models.</p>\n\n<p>This combination handles high-throughput data streams and detects anomalies efficiently, with minimal operational overhead.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Kinesis Firehose for real-time streaming of the network metrics data into an Amazon S3 bucket, then periodically run an AWS Glue ETL job to detect anomalies using a custom Python script</strong> - Kinesis Firehose delivers data in real-time to destinations like Amazon S3 but using AWS Glue for periodic jobs introduces latency and is unsuitable for real-time anomaly detection.</p>\n\n<p><strong>Ingest the data using Amazon Kinesis Data Streams, analyze the network metrics using Amazon Comprehend, and identify anomalies with a SageMaker model</strong> - Amazon Comprehend is a natural language processing tool and cannot analyze numerical or structured data like network metrics. Additionally, SageMaker for real-time analysis adds unnecessary complexity compared to Apache Flink’s built-in capabilities.</p>\n\n<p><strong>Use AWS Lambda to process the network metrics streams in real time and write a custom function to detect anomalies directly from the data</strong> - While Lambda can handle real-time data, it is not optimized for high-throughput or resource-intensive computations like anomaly detection. Scaling Lambda for such tasks adds operational complexity.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/real-time-anomaly-detection-via-random-cut-forest-in-amazon-managed-service-for-apache-flink/\">https://aws.amazon.com/blogs/big-data/real-time-anomaly-detection-via-random-cut-forest-in-amazon-managed-service-for-apache-flink/</a></p>\n\n<p><a href=\"https://aws.amazon.com/managed-service-apache-flink/\">https://aws.amazon.com/managed-service-apache-flink/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Streams to ingest the data, process it with Apache Flink to analyze the streams in real time, and apply the RANDOM_CUT_FOREST algorithm to detect anomalies</strong>"
      },
      {
        "answer": "",
        "explanation": "This solution uses AWS services designed for real-time processing and anomaly detection:"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams - Provides a scalable, low-latency service for ingesting high-volume, real-time network metrics."
      },
      {
        "answer": "",
        "explanation": "Apache Flink on Kinesis Data Analytics - Processes the data streams in real time and offers powerful frameworks for data analysis."
      },
      {
        "answer": "",
        "explanation": "RANDOM_CUT_FOREST algorithm - A built-in anomaly detection algorithm within Apache Flink that minimizes the need for custom code or external models."
      },
      {
        "answer": "",
        "explanation": "This combination handles high-throughput data streams and detects anomalies efficiently, with minimal operational overhead."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Firehose for real-time streaming of the network metrics data into an Amazon S3 bucket, then periodically run an AWS Glue ETL job to detect anomalies using a custom Python script</strong> - Kinesis Firehose delivers data in real-time to destinations like Amazon S3 but using AWS Glue for periodic jobs introduces latency and is unsuitable for real-time anomaly detection."
      },
      {
        "answer": "",
        "explanation": "<strong>Ingest the data using Amazon Kinesis Data Streams, analyze the network metrics using Amazon Comprehend, and identify anomalies with a SageMaker model</strong> - Amazon Comprehend is a natural language processing tool and cannot analyze numerical or structured data like network metrics. Additionally, SageMaker for real-time analysis adds unnecessary complexity compared to Apache Flink’s built-in capabilities."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Lambda to process the network metrics streams in real time and write a custom function to detect anomalies directly from the data</strong> - While Lambda can handle real-time data, it is not optimized for high-throughput or resource-intensive computations like anomaly detection. Scaling Lambda for such tasks adds operational complexity."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/real-time-anomaly-detection-via-random-cut-forest-in-amazon-managed-service-for-apache-flink/",
      "https://aws.amazon.com/managed-service-apache-flink/",
      "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
    ]
  },
  {
    "id": 37,
    "question": "<p>You are the senior project manager at a global e-commerce company that runs multiple machine learning projects, including recommendation systems, fraud detection, and demand forecasting. The company has a cloud budget that is tightly monitored, and you are required to provide detailed reports on the costs associated with each ML project. To do this effectively, you need to track and allocate costs across different teams and projects, ensuring that each project stays within its allocated budget.</p>\n\n<p>Which of the following approaches is the MOST EFFECTIVE for tracking and allocating costs across your ML projects using AWS services?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up AWS Budgets for each project and rely on the alerts when the budget threshold is exceeded. Use these alerts to monitor costs and manually adjust resource usage as needed</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create separate AWS accounts for each ML project, allowing costs to be isolated and tracked at the account level. Manually aggregate the costs for reporting purposes using monthly billing statements</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Implement resource tagging across all AWS resources used by your ML projects, including SageMaker instances, S3 buckets, and Lambda functions. Use AWS Cost Explorer to filter costs by tags such as project, team, and environment to generate detailed cost reports</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon CloudWatch to monitor usage metrics for each resource and manually calculate the associated costs based on the metrics. Allocate costs by assigning each resource to a specific project or team</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement resource tagging across all AWS resources used by your ML projects, including SageMaker instances, S3 buckets, and Lambda functions. Use AWS Cost Explorer to filter costs by tags such as project, team, and environment to generate detailed cost reports</strong></p>\n\n<p>Resource tagging is a best practice for cost tracking and allocation in AWS.</p>\n\n<p>AWS Cost Explorer allows you to analyze your past AWS spending, identify cost trends, and forecast future costs based on historical data. This tool is valuable for budgeting and financial planning, helping you make informed decisions about resource allocation and cost management. By tagging resources with metadata such as project name, team, and environment, you can use AWS Cost Explorer to break down costs by these tags, providing detailed insights into where your budget is being spent.</p>\n\n<p>This approach allows for automated, granular tracking and reporting of costs across multiple ML projects, making it easier to stay within budget.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q38-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q38-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create separate AWS accounts for each ML project, allowing costs to be isolated and tracked at the account level. Manually aggregate the costs for reporting purposes using monthly billing statements</strong> - Creating separate AWS accounts for each project provides isolation but can lead to higher management overhead and complexity. Aggregating costs manually across accounts is time-consuming and less efficient than using resource tags within a single account or consolidated billing setup.</p>\n\n<p><strong>Use Amazon CloudWatch to monitor usage metrics for each resource and manually calculate the associated costs based on the metrics. Allocate costs by assigning each resource to a specific project or team</strong> - While CloudWatch provides valuable usage metrics, manually calculating costs from these metrics is cumbersome and error-prone. This approach does not offer the same level of automation or granularity as tagging and using Cost Explorer.</p>\n\n<p><strong>Set up AWS Budgets for each project and rely on the alerts when the budget threshold is exceeded. Use these alerts to monitor costs and manually adjust resource usage as needed</strong> - AWS Budgets is useful for setting and monitoring budget thresholds, but it is reactive rather than proactive. It provides alerts but lacks the detailed cost breakdowns needed for effective tracking and allocation across multiple projects.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/aws-cost-management/aws-cost-explorer/\">https://aws.amazon.com/aws-cost-management/aws-cost-explorer/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Implement resource tagging across all AWS resources used by your ML projects, including SageMaker instances, S3 buckets, and Lambda functions. Use AWS Cost Explorer to filter costs by tags such as project, team, and environment to generate detailed cost reports</strong>"
      },
      {
        "answer": "",
        "explanation": "Resource tagging is a best practice for cost tracking and allocation in AWS."
      },
      {
        "answer": "",
        "explanation": "AWS Cost Explorer allows you to analyze your past AWS spending, identify cost trends, and forecast future costs based on historical data. This tool is valuable for budgeting and financial planning, helping you make informed decisions about resource allocation and cost management. By tagging resources with metadata such as project name, team, and environment, you can use AWS Cost Explorer to break down costs by these tags, providing detailed insights into where your budget is being spent."
      },
      {
        "answer": "",
        "explanation": "This approach allows for automated, granular tracking and reporting of costs across multiple ML projects, making it easier to stay within budget."
      },
      {
        "link": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create separate AWS accounts for each ML project, allowing costs to be isolated and tracked at the account level. Manually aggregate the costs for reporting purposes using monthly billing statements</strong> - Creating separate AWS accounts for each project provides isolation but can lead to higher management overhead and complexity. Aggregating costs manually across accounts is time-consuming and less efficient than using resource tags within a single account or consolidated billing setup."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon CloudWatch to monitor usage metrics for each resource and manually calculate the associated costs based on the metrics. Allocate costs by assigning each resource to a specific project or team</strong> - While CloudWatch provides valuable usage metrics, manually calculating costs from these metrics is cumbersome and error-prone. This approach does not offer the same level of automation or granularity as tagging and using Cost Explorer."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up AWS Budgets for each project and rely on the alerts when the budget threshold is exceeded. Use these alerts to monitor costs and manually adjust resource usage as needed</strong> - AWS Budgets is useful for setting and monitoring budget thresholds, but it is reactive rather than proactive. It provides alerts but lacks the detailed cost breakdowns needed for effective tracking and allocation across multiple projects."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html",
      "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html",
      "https://aws.amazon.com/aws-cost-management/aws-cost-explorer/"
    ]
  },
  {
    "id": 38,
    "question": "<p>A healthcare organization has deployed a predictive ML model in production using Amazon SageMaker. To ensure the model's performance, the organization has enabled SageMaker Model Monitor to track data quality. After recent update to the model, a data scientist observes data quality issues flagged by Model Monitor checks.</p>\n\n<p>What steps should the data scientist take to address the data quality issues identified by Model Monitor?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Modify the model's hyperparameters and redeploy the updated version to production</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Generate a new baseline using the latest dataset and configure Model Monitor to use this updated baseline for future evaluations</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Ingest Ground Truth labels and use these as an updated baseline for future evaluations</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The constraint_violations.json file lists the violations detected in the current dataset. Manually correct the data issues and rerun the model</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Generate a new baseline using the latest dataset and configure Model Monitor to use this updated baseline for future evaluations</strong></p>\n\n<p>A baseline is used as a reference to compare real-time or batch predictions from the model. It computes statistics and metrics along with constraints on them. During monitoring, all of these are used in conjunction to identify violations.</p>\n\n<p>Data quality monitoring automatically monitors machine learning (ML) models in production and notifies you when data quality issues arise. ML models in production have to make predictions on real-life data that is not carefully curated like most training datasets. If the statistical nature of the data that your model receives while in production drifts away from the nature of the baseline data it was trained on, the model begins to lose accuracy in its predictions. Amazon SageMaker Model Monitor uses rules to detect data drift and alerts you when it happens. A new baseline should be created with the latest dataset and configure Model Monitor to use this updated baseline for future evaluations.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Modify the model's hyperparameters and redeploy the updated version to production</strong> - Hyperparameters are used during the training phase of model development. Adjusting hyperparameters does not address data quality issues for the given use case, hence this option is incorrect.</p>\n\n<p><strong>Ingest Ground Truth labels and use these as an updated baseline for future evaluations</strong> - Model quality monitoring jobs monitor the performance of a model by comparing the predictions that the model makes with the actual Ground Truth labels that the model attempts to predict. Ground Truth labels are useful for model quality monitoring and not for data quality monitoring.</p>\n\n<p><strong>The constraint_violations.json file lists the violations detected in the current dataset. Manually correct the data issues and rerun the model</strong> - Manually fixing all the data violations listed in the constraint_violations.json file is impractical, making this option unsuitable.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-faqs.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-faqs.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Generate a new baseline using the latest dataset and configure Model Monitor to use this updated baseline for future evaluations</strong>"
      },
      {
        "answer": "",
        "explanation": "A baseline is used as a reference to compare real-time or batch predictions from the model. It computes statistics and metrics along with constraints on them. During monitoring, all of these are used in conjunction to identify violations."
      },
      {
        "answer": "",
        "explanation": "Data quality monitoring automatically monitors machine learning (ML) models in production and notifies you when data quality issues arise. ML models in production have to make predictions on real-life data that is not carefully curated like most training datasets. If the statistical nature of the data that your model receives while in production drifts away from the nature of the baseline data it was trained on, the model begins to lose accuracy in its predictions. Amazon SageMaker Model Monitor uses rules to detect data drift and alerts you when it happens. A new baseline should be created with the latest dataset and configure Model Monitor to use this updated baseline for future evaluations."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Modify the model's hyperparameters and redeploy the updated version to production</strong> - Hyperparameters are used during the training phase of model development. Adjusting hyperparameters does not address data quality issues for the given use case, hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Ingest Ground Truth labels and use these as an updated baseline for future evaluations</strong> - Model quality monitoring jobs monitor the performance of a model by comparing the predictions that the model makes with the actual Ground Truth labels that the model attempts to predict. Ground Truth labels are useful for model quality monitoring and not for data quality monitoring."
      },
      {
        "answer": "",
        "explanation": "<strong>The constraint_violations.json file lists the violations detected in the current dataset. Manually correct the data issues and rerun the model</strong> - Manually fixing all the data violations listed in the constraint_violations.json file is impractical, making this option unsuitable."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-faqs.html"
    ]
  },
  {
    "id": 39,
    "question": "<p>You are working as a data scientist at a company that specializes in predictive analytics. You are tasked with training a deep learning model using Amazon SageMaker to predict customer churn. The dataset you have is large and contains millions of records. The training process is taking longer than expected, and you suspect that the hyperparameters need fine-tuning. You want to balance the training time while ensuring the model converges effectively. You have set the batch size to 256, epochs to 50, and learning rate to 0.01. However, the training job is still not performing as expected.</p>\n\n<p>Given this scenario, which of the following adjustments is MOST LIKELY to reduce the training time without compromising model performance?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Decrease the learning rate and increase the batch size</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Increase the number of epochs and maintain the batch size</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Increase the batch size and decrease the number of epochs</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Decrease the batch size and increase the number of epochs</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Increase the batch size and decrease the number of epochs</strong></p>\n\n<p>Hyperparameters are external configuration variables that data scientists use to manage machine learning model training. Sometimes called model hyperparameters, the hyperparameters are manually set before training a model. They're different from parameters, which are internal parameters automatically derived during the learning process and not set by data scientists.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q1-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q1-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/what-is/hyperparameter-tuning/\">https://aws.amazon.com/what-is/hyperparameter-tuning/</a><p></p>\n\n<p>Increasing the batch size allows more data to be processed in parallel, which can lead to faster training times because the model updates its weights less frequently. Reducing the number of epochs compensates for the larger batch size by ensuring the model doesn't overfit. This combination can significantly reduce training time while still allowing the model to converge effectively, provided the learning rate remains optimal.</p>\n\n<p>Optimize the learning process of your models with hyperparameters:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q1-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q1-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-llms-finetuning-hyperparameters.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-llms-finetuning-hyperparameters.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Decrease the batch size and increase the number of epochs</strong> - Decreasing the batch size means the model will update weights more frequently, but this will increase the training time due to smaller batches. Increasing the number of epochs will further slow down the training, which is counterproductive given the goal of reducing training time.</p>\n\n<p><strong>Increase the number of epochs and maintain the batch size</strong> - Increasing the number of epochs without adjusting the batch size could lead to longer training times, potentially causing overfitting without necessarily improving performance.</p>\n\n<p><strong>Decrease the learning rate and increase the batch size</strong> - Decreasing the learning rate typically results in a more stable but slower convergence process. Coupling this with an increased batch size may unnecessarily prolong training time without a significant gain in model performance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/what-is/hyperparameter-tuning/\">https://aws.amazon.com/what-is/hyperparameter-tuning/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-llms-finetuning-hyperparameters.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-llms-finetuning-hyperparameters.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Increase the batch size and decrease the number of epochs</strong>"
      },
      {
        "answer": "",
        "explanation": "Hyperparameters are external configuration variables that data scientists use to manage machine learning model training. Sometimes called model hyperparameters, the hyperparameters are manually set before training a model. They're different from parameters, which are internal parameters automatically derived during the learning process and not set by data scientists."
      },
      {
        "link": "https://aws.amazon.com/what-is/hyperparameter-tuning/"
      },
      {
        "answer": "",
        "explanation": "Increasing the batch size allows more data to be processed in parallel, which can lead to faster training times because the model updates its weights less frequently. Reducing the number of epochs compensates for the larger batch size by ensuring the model doesn't overfit. This combination can significantly reduce training time while still allowing the model to converge effectively, provided the learning rate remains optimal."
      },
      {
        "answer": "",
        "explanation": "Optimize the learning process of your models with hyperparameters:"
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-llms-finetuning-hyperparameters.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Decrease the batch size and increase the number of epochs</strong> - Decreasing the batch size means the model will update weights more frequently, but this will increase the training time due to smaller batches. Increasing the number of epochs will further slow down the training, which is counterproductive given the goal of reducing training time."
      },
      {
        "answer": "",
        "explanation": "<strong>Increase the number of epochs and maintain the batch size</strong> - Increasing the number of epochs without adjusting the batch size could lead to longer training times, potentially causing overfitting without necessarily improving performance."
      },
      {
        "answer": "",
        "explanation": "<strong>Decrease the learning rate and increase the batch size</strong> - Decreasing the learning rate typically results in a more stable but slower convergence process. Coupling this with an increased batch size may unnecessarily prolong training time without a significant gain in model performance."
      }
    ],
    "references": [
      "https://aws.amazon.com/what-is/hyperparameter-tuning/",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-llms-finetuning-hyperparameters.html"
    ]
  },
  {
    "id": 40,
    "question": "<p>You are a machine learning engineer responsible for deploying a customer churn prediction model using Amazon SageMaker into production at a telecommunications company. The model is critical for proactive customer retention efforts, so maintaining high availability and reliability is essential. Given the model’s importance, you must implement best practices for deployment, including versioning and rollback strategies, to ensure that any issues with the new model version can be quickly addressed without impacting the business.</p>\n\n<p>Which of the following approaches BEST exemplifies deployment best practices in this scenario?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a blue/green deployment strategy to deploy the new model version in parallel with the existing version, allowing you to switch traffic to the new model gradually and roll back if necessary</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Deploy the new model version directly to production, replacing the existing model, and monitor its performance closely. If issues arise, retrain the model immediately and redeploy</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Version the model by tagging the new version, deploy it to production, and use weighted traffic splitting to send a small percentage of traffic to the new model. If no issues are detected, gradually increase traffic to the new version</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the new model version to a staging environment, test it thoroughly, and then manually replace the existing production model. If any issues occur, update the model in staging and redeploy</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a blue/green deployment strategy to deploy the new model version in parallel with the existing version, allowing you to switch traffic to the new model gradually and roll back if necessary</strong></p>\n\n<p>A blue/green deployment strategy is a best practice in model deployment. It allows you to deploy the new model version in parallel with the existing one, gradually shifting traffic to the new version while monitoring its performance. If issues are detected, you can quickly roll back to the previous version without disrupting service.</p>\n\n<p>In a blue/green deployment, SageMaker provisions a new fleet with the updates (the green fleet). Then, SageMaker shifts traffic from the old fleet (the blue fleet) to the green fleet. Once the green fleet operates smoothly for a set evaluation period (called the baking period), SageMaker terminates the blue fleet. You can specify Amazon CloudWatch alarms that SageMaker uses to monitor the green fleet. If an issue with the updated code trips any of the alarms, SageMaker initiates an auto-rollback to the blue fleet in order to maintain availability thereby minimizing risk.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q18-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q18-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the new model version directly to production, replacing the existing model, and monitor its performance closely. If issues arise, retrain the model immediately and redeploy</strong> - Deploying the new model directly to production without a rollback strategy is risky. If issues arise, the model would need to be retrained and redeployed, which could lead to significant downtime and business impact.</p>\n\n<p><strong>Deploy the new model version to a staging environment, test it thoroughly, and then manually replace the existing production model. If any issues occur, update the model in staging and redeploy</strong> - Deploying to a staging environment is a good practice for testing, but manually replacing the production model increases the risk of downtime and errors. A more automated and controlled strategy like blue/green deployment is preferable.</p>\n\n<p><strong>Version the model by tagging the new version, deploy it to production, and use weighted traffic splitting to send a small percentage of traffic to the new model. If no issues are detected, gradually increase traffic to the new version</strong> - Versioning and using weighted traffic splitting is also a good practice, but it’s more complex and might not provide the same immediate rollback capability as blue/green deployment. However, this approach is viable in certain scenarios where gradual rollout is necessary.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-rolling.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-rolling.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use a blue/green deployment strategy to deploy the new model version in parallel with the existing version, allowing you to switch traffic to the new model gradually and roll back if necessary</strong>"
      },
      {
        "answer": "",
        "explanation": "A blue/green deployment strategy is a best practice in model deployment. It allows you to deploy the new model version in parallel with the existing one, gradually shifting traffic to the new version while monitoring its performance. If issues are detected, you can quickly roll back to the previous version without disrupting service."
      },
      {
        "answer": "",
        "explanation": "In a blue/green deployment, SageMaker provisions a new fleet with the updates (the green fleet). Then, SageMaker shifts traffic from the old fleet (the blue fleet) to the green fleet. Once the green fleet operates smoothly for a set evaluation period (called the baking period), SageMaker terminates the blue fleet. You can specify Amazon CloudWatch alarms that SageMaker uses to monitor the green fleet. If an issue with the updated code trips any of the alarms, SageMaker initiates an auto-rollback to the blue fleet in order to maintain availability thereby minimizing risk."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the new model version directly to production, replacing the existing model, and monitor its performance closely. If issues arise, retrain the model immediately and redeploy</strong> - Deploying the new model directly to production without a rollback strategy is risky. If issues arise, the model would need to be retrained and redeployed, which could lead to significant downtime and business impact."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy the new model version to a staging environment, test it thoroughly, and then manually replace the existing production model. If any issues occur, update the model in staging and redeploy</strong> - Deploying to a staging environment is a good practice for testing, but manually replacing the production model increases the risk of downtime and errors. A more automated and controlled strategy like blue/green deployment is preferable."
      },
      {
        "answer": "",
        "explanation": "<strong>Version the model by tagging the new version, deploy it to production, and use weighted traffic splitting to send a small percentage of traffic to the new model. If no issues are detected, gradually increase traffic to the new version</strong> - Versioning and using weighted traffic splitting is also a good practice, but it’s more complex and might not provide the same immediate rollback capability as blue/green deployment. However, this approach is viable in certain scenarios where gradual rollout is necessary."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-rolling.html"
    ]
  },
  {
    "id": 41,
    "question": "<p>You are responsible for deploying a machine learning model on AWS SageMaker for a real-time prediction application. The application requires low latency and high throughput. During deployment, you notice that the model’s response time is slower than expected, and the throughput is not meeting the required levels. You have already optimized the model itself, so the next step is to optimize the deployment environment. You are currently using a single instance of the <code>ml.m5.large</code> instance type with the default endpoint configuration.</p>\n\n<p>Which of the following changes is MOST LIKELY to improve the model’s response time and throughput?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable Auto Scaling with a target metric for the instance utilization</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Increase the instance count to two and enable asynchronous inference</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Switch to an <code>ml.m5.2xlarge</code> instance type and use multi-AZ deployment</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Change the instance type to <code>ml.p2.xlarge</code> and add multi-model support</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable Auto Scaling with a target metric for the instance utilization</strong></p>\n\n<p>Amazon SageMaker supports automatic scaling (auto scaling) for your hosted models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, auto scaling brings more instances online. When the workload decreases, auto scaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.</p>\n\n<p>Enabling Auto Scaling allows the endpoint to dynamically adjust the number of instances based on actual traffic. By targeting instance utilization, the deployment can automatically scale out during peak times and scale in during low demand, improving both response time and throughput without over-provisioning. With target tracking, you choose an Amazon CloudWatch metric and target value. Auto scaling creates and manages the CloudWatch alarms for the scaling policy and calculates the scaling adjustment based on the metric and the target value. The policy adds and removes the number of instances as required to keep the metric at, or close to, the specified target value. For example, a scaling policy that uses the predefined InvocationsPerInstance metric with a target value of 70 can keep InvocationsPerInstance at, or close to 70.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q2-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q2-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the instance type to <code>ml.p2.xlarge</code> and add multi-model support</strong> - While changing to an <code>ml.p2.xlarge</code> instance type, which is optimized for GPU, could improve performance for compute-intensive models, it may not be necessary for all types of models, especially if the model is CPU-bound. Adding multi-model support may further complicate the deployment without addressing the core issue of latency and throughput. Multi-model endpoints provide a scalable and cost-effective solution to deploying large numbers of models. They use the same fleet of resources and a shared serving container to host all of your models. This reduces hosting costs by improving endpoint utilization compared with using single-model endpoints. It also reduces deployment overhead because Amazon SageMaker manages loading models in memory and scaling them based on the traffic patterns to your endpoint.</p>\n\n<p>The following diagram shows how multi-model endpoints work compared to single-model endpoints.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/multi-model-endpoints-diagram.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/multi-model-endpoints-diagram.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html</a><p></p>\n\n<p><strong>Increase the instance count to two and enable asynchronous inference</strong> - Asynchronous inference is typically used when latency is less of a concern, which contradicts the requirements of real-time prediction. Increasing the instance count without addressing scalability could help throughput but may not effectively reduce latency.</p>\n\n<p><strong>Switch to an <code>ml.m5.2xlarge</code> instance type and use multi-AZ deployment</strong> - Switching to a more powerful <code>ml.m5.2xlarge</code> instance type and using multi-AZ deployment could improve performance, but this option mainly adds redundancy and fault tolerance rather than optimizing response time and throughput directly.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable Auto Scaling with a target metric for the instance utilization</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon SageMaker supports automatic scaling (auto scaling) for your hosted models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, auto scaling brings more instances online. When the workload decreases, auto scaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using."
      },
      {
        "answer": "",
        "explanation": "Enabling Auto Scaling allows the endpoint to dynamically adjust the number of instances based on actual traffic. By targeting instance utilization, the deployment can automatically scale out during peak times and scale in during low demand, improving both response time and throughput without over-provisioning. With target tracking, you choose an Amazon CloudWatch metric and target value. Auto scaling creates and manages the CloudWatch alarms for the scaling policy and calculates the scaling adjustment based on the metric and the target value. The policy adds and removes the number of instances as required to keep the metric at, or close to, the specified target value. For example, a scaling policy that uses the predefined InvocationsPerInstance metric with a target value of 70 can keep InvocationsPerInstance at, or close to 70."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Change the instance type to <code>ml.p2.xlarge</code> and add multi-model support</strong> - While changing to an <code>ml.p2.xlarge</code> instance type, which is optimized for GPU, could improve performance for compute-intensive models, it may not be necessary for all types of models, especially if the model is CPU-bound. Adding multi-model support may further complicate the deployment without addressing the core issue of latency and throughput. Multi-model endpoints provide a scalable and cost-effective solution to deploying large numbers of models. They use the same fleet of resources and a shared serving container to host all of your models. This reduces hosting costs by improving endpoint utilization compared with using single-model endpoints. It also reduces deployment overhead because Amazon SageMaker manages loading models in memory and scaling them based on the traffic patterns to your endpoint."
      },
      {
        "answer": "",
        "explanation": "The following diagram shows how multi-model endpoints work compared to single-model endpoints."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Increase the instance count to two and enable asynchronous inference</strong> - Asynchronous inference is typically used when latency is less of a concern, which contradicts the requirements of real-time prediction. Increasing the instance count without addressing scalability could help throughput but may not effectively reduce latency."
      },
      {
        "answer": "",
        "explanation": "<strong>Switch to an <code>ml.m5.2xlarge</code> instance type and use multi-AZ deployment</strong> - Switching to a more powerful <code>ml.m5.2xlarge</code> instance type and using multi-AZ deployment could improve performance, but this option mainly adds redundancy and fault tolerance rather than optimizing response time and throughput directly."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html"
    ]
  },
  {
    "id": 42,
    "question": "<p>You are a Data Scientist working for an e-commerce platform that uses a machine learning model to recommend products to customers. The model has been in production for over a year and was initially performing well. However, you have recently noticed a decrease in the model's accuracy, particularly when recommending products to new customers. This decline suggests that the model may be experiencing drift due to changing customer preferences and market trends. To address this issue, you need to implement a strategy for detecting and managing model drift using Amazon SageMaker.</p>\n\n<p>Which of the following strategies should you implement to effectively detect and manage model drift in your product recommendation model using Amazon SageMaker? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Retrain the model with the new training data, ensuring that the model remains up to date with new customer preferences</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon SageMaker Clarify to continuously monitor and mitigate bias in the model, and initiate model retraining due to changes in data distribution</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy multiple versions of the model simultaneously using Amazon SageMaker multi-model endpoints, and switch between them based on performance metrics</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Manually review model performance every quarter and initiate retraining only if a significant drop in accuracy is observed, minimizing unnecessary retraining costs</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use Amazon SageMaker Model Monitor to set up monitoring for data quality and data drift, enabling you to receive alerts and initiate model retraining when the distribution of input data changes significantly from the training data</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use Amazon SageMaker Model Monitor to set up monitoring for data quality and data drift, enabling you to receive alerts and initiate model retraining when the distribution of input data changes significantly from the training data</strong></p>\n\n<p><strong>Retrain the model with the new training data, ensuring that the model remains up to date with new customer preferences</strong></p>\n\n<p>Amazon SageMaker Model Monitor allows you to continuously monitor the input data for data quality and data drift. By setting up alerts, you can detect when the input data distribution has changed significantly from the training data, which is a key indicator of model drift. This proactive approach helps in taking timely action, such as retraining the model or adjusting it to account for new patterns in the data.</p>\n\n<p>With SageMaker Model Monitor, you can set alerts that notify you when there are deviations in the model quality. Early and proactive detection of these deviations lets you to take corrective actions. You can take actions like retraining models, auditing upstream systems, or fixing quality issues without having to monitor models manually or build additional tooling.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q29-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q29-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy multiple versions of the model simultaneously using Amazon SageMaker multi-model endpoints, and switch between them based on performance metrics</strong> - Deploying multiple versions of the same model using Amazon SageMaker multi-model endpoints is resource-intensive and cost-inefficient. So, this option is ruled out.</p>\n\n<p><strong>Manually review model performance every quarter and initiate retraining only if a significant drop in accuracy is observed, minimizing unnecessary retraining costs</strong> - Manually reviewing performance every quarter is reactive and might not catch drift early enough, leading to prolonged periods of suboptimal model performance. Automated monitoring with Model Monitor provides a more timely and systematic approach to detecting drift.</p>\n\n<p><strong>Use Amazon SageMaker Clarify to continuously monitor and mitigate bias in the model, and initiate model retraining due to changes in data distribution</strong> - While Amazon SageMaker Clarify is useful for monitoring and mitigating bias, it is not specifically designed for detecting model drift caused by changes in data distribution. Model Monitor is the more appropriate tool for addressing drift in this context.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/retraining-models-on-new-data.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/retraining-models-on-new-data.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/\">https://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/architecture/detecting-data-drift-using-amazon-sagemaker/\">https://aws.amazon.com/blogs/architecture/detecting-data-drift-using-amazon-sagemaker/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SageMaker Model Monitor to set up monitoring for data quality and data drift, enabling you to receive alerts and initiate model retraining when the distribution of input data changes significantly from the training data</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Retrain the model with the new training data, ensuring that the model remains up to date with new customer preferences</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon SageMaker Model Monitor allows you to continuously monitor the input data for data quality and data drift. By setting up alerts, you can detect when the input data distribution has changed significantly from the training data, which is a key indicator of model drift. This proactive approach helps in taking timely action, such as retraining the model or adjusting it to account for new patterns in the data."
      },
      {
        "answer": "",
        "explanation": "With SageMaker Model Monitor, you can set alerts that notify you when there are deviations in the model quality. Early and proactive detection of these deviations lets you to take corrective actions. You can take actions like retraining models, auditing upstream systems, or fixing quality issues without having to monitor models manually or build additional tooling."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy multiple versions of the model simultaneously using Amazon SageMaker multi-model endpoints, and switch between them based on performance metrics</strong> - Deploying multiple versions of the same model using Amazon SageMaker multi-model endpoints is resource-intensive and cost-inefficient. So, this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Manually review model performance every quarter and initiate retraining only if a significant drop in accuracy is observed, minimizing unnecessary retraining costs</strong> - Manually reviewing performance every quarter is reactive and might not catch drift early enough, leading to prolonged periods of suboptimal model performance. Automated monitoring with Model Monitor provides a more timely and systematic approach to detecting drift."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SageMaker Clarify to continuously monitor and mitigate bias in the model, and initiate model retraining due to changes in data distribution</strong> - While Amazon SageMaker Clarify is useful for monitoring and mitigating bias, it is not specifically designed for detecting model drift caused by changes in data distribution. Model Monitor is the more appropriate tool for addressing drift in this context."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html",
      "https://docs.aws.amazon.com/machine-learning/latest/dg/retraining-models-on-new-data.html",
      "https://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/",
      "https://aws.amazon.com/blogs/architecture/detecting-data-drift-using-amazon-sagemaker/"
    ]
  },
  {
    "id": 43,
    "question": "<p>A healthcare company is automating the deployment of a machine learning solution to predict patient health risks. The ML engineer needs to use AWS CloudFormation to define a model that will be hosted on an Amazon SageMaker endpoint and serve real-time inference requests.</p>\n\n<p>Which resource should the ML engineer create in the CloudFormation template to address this requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use <code>AWS::SageMaker::Endpoint</code> to define the SageMaker endpoint that will host the model and serve inference requests</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use <code>AWS::EC2::Instance</code> to host the ML model manually and handle inference requests</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use <code>AWS::SageMaker::Model</code> to specify the ML model, including the model artifacts and the inference container configuration for hosting</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use <code>AWS::SageMaker::EndpointConfig</code> to define the configuration for the SageMaker endpoint without specifying the model</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use <code>AWS::SageMaker::Model</code> to specify the ML model, including the model artifacts and the inference container configuration for hosting</strong></p>\n\n<p>The <code>AWS::SageMaker::Model</code> CloudFormation resource is specifically designed to define an ML model hosted on Amazon SageMaker. It includes configuration details such as:</p>\n\n<p>Model artifacts - Stored in Amazon S3.</p>\n\n<p>Inference container - The container image that contains the inference code.</p>\n\n<p>IAM Role - Permissions to allow SageMaker to access the resources required for hosting.</p>\n\n<p>This resource is the foundation for creating a SageMaker endpoint, as it defines the model that the endpoint will host.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use <code>AWS::SageMaker::EndpointConfig</code> to define the configuration for the SageMaker endpoint without specifying the model</strong> - The <code>AWS::SageMaker::EndpointConfig</code> resource specifies how an endpoint is configured (e.g., instance type and count) but requires a model defined by <code>AWS::SageMaker::Model</code> to function. It cannot directly define the model.</p>\n\n<p><strong>Use <code>AWS::SageMaker::Endpoint</code> to define the SageMaker endpoint that will host the model and serve inference requests</strong> - While <code>AWS::SageMaker::Endpoint</code> is required to host the model, it depends on a model resource (<code>AWS::SageMaker::Model</code>). It cannot directly define the model or its configuration.</p>\n\n<p><strong>Use <code>AWS::EC2::Instance</code> to host the ML model manually and handle inference requests</strong> - Hosting a model manually on an EC2 instance adds operational overhead and does not leverage SageMaker’s managed hosting capabilities, such as auto-scaling and endpoint management.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-model.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-model.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-endpoint.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-endpoint.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-endpointconfig.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-endpointconfig.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use <code>AWS::SageMaker::Model</code> to specify the ML model, including the model artifacts and the inference container configuration for hosting</strong>"
      },
      {
        "answer": "",
        "explanation": "The <code>AWS::SageMaker::Model</code> CloudFormation resource is specifically designed to define an ML model hosted on Amazon SageMaker. It includes configuration details such as:"
      },
      {
        "answer": "",
        "explanation": "Model artifacts - Stored in Amazon S3."
      },
      {
        "answer": "",
        "explanation": "Inference container - The container image that contains the inference code."
      },
      {
        "answer": "",
        "explanation": "IAM Role - Permissions to allow SageMaker to access the resources required for hosting."
      },
      {
        "answer": "",
        "explanation": "This resource is the foundation for creating a SageMaker endpoint, as it defines the model that the endpoint will host."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use <code>AWS::SageMaker::EndpointConfig</code> to define the configuration for the SageMaker endpoint without specifying the model</strong> - The <code>AWS::SageMaker::EndpointConfig</code> resource specifies how an endpoint is configured (e.g., instance type and count) but requires a model defined by <code>AWS::SageMaker::Model</code> to function. It cannot directly define the model."
      },
      {
        "answer": "",
        "explanation": "<strong>Use <code>AWS::SageMaker::Endpoint</code> to define the SageMaker endpoint that will host the model and serve inference requests</strong> - While <code>AWS::SageMaker::Endpoint</code> is required to host the model, it depends on a model resource (<code>AWS::SageMaker::Model</code>). It cannot directly define the model or its configuration."
      },
      {
        "answer": "",
        "explanation": "<strong>Use <code>AWS::EC2::Instance</code> to host the ML model manually and handle inference requests</strong> - Hosting a model manually on an EC2 instance adds operational overhead and does not leverage SageMaker’s managed hosting capabilities, such as auto-scaling and endpoint management."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-model.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-endpoint.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-endpointconfig.html"
    ]
  },
  {
    "id": 44,
    "question": "<p>A media analytics company processes raw video metadata stored in Amazon S3 buckets to generate insights on audience engagement. The company needs to create data ingestion pipelines for processing this metadata and ML model deployment pipelines to analyze and predict audience behavior patterns. The solution must efficiently handle large-scale data processing and integrate seamlessly with ML workflows.</p>\n\n<p>Which solution will meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Studio Classic for both data ingestion pipelines and ML model deployment pipelines to simplify the process</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Glue to preprocess raw video metadata in Amazon S3 and create a custom script to deploy models manually using SageMaker endpoints</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Kinesis Data Firehose to stream video metadata from Amazon S3 into the ingestion pipeline and deploy models using AWS Lambda</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Glue to create data ingestion pipelines for scalable data processing and SageMaker Studio Classic to manage ML model deployment pipelines</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Glue to create data ingestion pipelines for scalable data processing and SageMaker Studio Classic to manage ML model deployment pipelines</strong></p>\n\n<p>AWS Glue provides serverless, scalable ETL capabilities to preprocess and transform large amounts of raw metadata from S3, enabling efficient ingestion pipelines. SageMaker Studio Classic offers an integrated environment for creating, training, deploying, and monitoring ML models. Studio Classic simplifies the process of building and managing model deployment pipelines. Therefore, the combination of AWS Glue for data ingestion and SageMaker Studio Classic for ML pipelines ensures a streamlined, scalable solution optimized for media analytics.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Kinesis Data Firehose to stream video metadata from Amazon S3 into the ingestion pipeline and deploy models using AWS Lambda</strong> - Kinesis Data Firehose is designed for streaming data and is not optimized for large-scale batch ingestion from Amazon S3. AWS Lambda has runtime and resource limitations, making it unsuitable for managing complex ML deployment pipelines.</p>\n\n<p><strong>Use AWS Glue to preprocess raw video metadata in Amazon S3 and create a custom script to deploy models manually using SageMaker endpoints</strong> - While AWS Glue is suitable for data preprocessing, custom scripts for model deployment require significant manual effort and are less efficient than using SageMaker Studio Classic for deployment pipelines.</p>\n\n<p><strong>Use SageMaker Studio Classic for both data ingestion pipelines and ML model deployment pipelines to simplify the process</strong> - While SageMaker Studio Classic is excellent for ML workflows, it is not designed for large-scale ETL tasks, where AWS Glue provides a better solution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue to create data ingestion pipelines for scalable data processing and SageMaker Studio Classic to manage ML model deployment pipelines</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Glue provides serverless, scalable ETL capabilities to preprocess and transform large amounts of raw metadata from S3, enabling efficient ingestion pipelines. SageMaker Studio Classic offers an integrated environment for creating, training, deploying, and monitoring ML models. Studio Classic simplifies the process of building and managing model deployment pipelines. Therefore, the combination of AWS Glue for data ingestion and SageMaker Studio Classic for ML pipelines ensures a streamlined, scalable solution optimized for media analytics."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Kinesis Data Firehose to stream video metadata from Amazon S3 into the ingestion pipeline and deploy models using AWS Lambda</strong> - Kinesis Data Firehose is designed for streaming data and is not optimized for large-scale batch ingestion from Amazon S3. AWS Lambda has runtime and resource limitations, making it unsuitable for managing complex ML deployment pipelines."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue to preprocess raw video metadata in Amazon S3 and create a custom script to deploy models manually using SageMaker endpoints</strong> - While AWS Glue is suitable for data preprocessing, custom scripts for model deployment require significant manual effort and are less efficient than using SageMaker Studio Classic for deployment pipelines."
      },
      {
        "answer": "",
        "explanation": "<strong>Use SageMaker Studio Classic for both data ingestion pipelines and ML model deployment pipelines to simplify the process</strong> - While SageMaker Studio Classic is excellent for ML workflows, it is not designed for large-scale ETL tasks, where AWS Glue provides a better solution."
      }
    ],
    "references": [
      "https://aws.amazon.com/glue/",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
    ]
  },
  {
    "id": 45,
    "question": "<p>A retail company uses a recommendation model deployed on an Amazon SageMaker endpoint to provide product suggestions to customers in real time. The company has developed a new version of the recommendation model and wants to evaluate its performance using live customer data without affecting the current production model. The company needs to determine whether the new model provides better recommendations before replacing the production model.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the new model to a separate Amazon SageMaker endpoint and use a custom application to split live traffic between the endpoints for evaluation purposes</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon SageMaker multi-model endpoints to deploy both the current and new models simultaneously and manually analyze the logs for prediction comparisons</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon SageMaker shadow testing to route a copy of live customer data to the new model for evaluation while maintaining the production model’s operation. Compare predictions from both models to assess performance</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Replace the current production model with the new model on the existing SageMaker endpoint, monitor its performance, and roll back if issues are detected</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon SageMaker shadow testing to route a copy of live customer data to the new model for evaluation while maintaining the production model’s operation. Compare predictions from both models to assess performance</strong></p>\n\n<p>Amazon SageMaker shadow testing is the most efficient and reliable solution for evaluating a new model's performance using live data without impacting production systems. In shadow testing, a copy of the live traffic is routed to the new model for predictions while the production model continues to serve end users. This setup allows the company to compare predictions from both models in real time and assess the new model’s performance under actual usage conditions.</p>\n\n<p>Shadow testing ensures that the end-user experience remains unaffected, as the new model operates in parallel without influencing the production results. Additionally, this approach minimizes risks by enabling side-by-side evaluation before deciding whether to replace the production model. The automated nature of shadow testing also reduces operational overhead compared to custom or manual solutions, making it the ideal choice for this scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the new model to a separate Amazon SageMaker endpoint and use a custom application to split live traffic between the endpoints for evaluation purposes</strong> - This approach requires significant custom development effort to split and manage live traffic. It introduces additional complexity compared to SageMaker’s built-in shadow testing feature.</p>\n\n<p><strong>Replace the current production model with the new model on the existing SageMaker endpoint, monitor its performance, and roll back if issues are detected</strong> - Replacing the production model directly risks exposing users to potential performance issues with the new model. It does not allow for a side-by-side comparison without impacting end users.</p>\n\n<p><strong>Use Amazon SageMaker multi-model endpoints to deploy both the current and new models simultaneously and manually analyze the logs for prediction comparisons</strong> - Multi-model endpoints are designed for hosting multiple models cost-effectively, but they do not provide built-in mechanisms for routing live traffic or performing comparisons automatically.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/shadow-tests.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/shadow-tests.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SageMaker shadow testing to route a copy of live customer data to the new model for evaluation while maintaining the production model’s operation. Compare predictions from both models to assess performance</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon SageMaker shadow testing is the most efficient and reliable solution for evaluating a new model's performance using live data without impacting production systems. In shadow testing, a copy of the live traffic is routed to the new model for predictions while the production model continues to serve end users. This setup allows the company to compare predictions from both models in real time and assess the new model’s performance under actual usage conditions."
      },
      {
        "answer": "",
        "explanation": "Shadow testing ensures that the end-user experience remains unaffected, as the new model operates in parallel without influencing the production results. Additionally, this approach minimizes risks by enabling side-by-side evaluation before deciding whether to replace the production model. The automated nature of shadow testing also reduces operational overhead compared to custom or manual solutions, making it the ideal choice for this scenario."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the new model to a separate Amazon SageMaker endpoint and use a custom application to split live traffic between the endpoints for evaluation purposes</strong> - This approach requires significant custom development effort to split and manage live traffic. It introduces additional complexity compared to SageMaker’s built-in shadow testing feature."
      },
      {
        "answer": "",
        "explanation": "<strong>Replace the current production model with the new model on the existing SageMaker endpoint, monitor its performance, and roll back if issues are detected</strong> - Replacing the production model directly risks exposing users to potential performance issues with the new model. It does not allow for a side-by-side comparison without impacting end users."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SageMaker multi-model endpoints to deploy both the current and new models simultaneously and manually analyze the logs for prediction comparisons</strong> - Multi-model endpoints are designed for hosting multiple models cost-effectively, but they do not provide built-in mechanisms for routing live traffic or performing comparisons automatically."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/shadow-tests.html"
    ]
  },
  {
    "id": 46,
    "question": "<p>You are a machine learning engineer responsible for managing a fraud detection model deployed on Amazon SageMaker. The model needs to be retrained periodically as new transaction data becomes available and as data distribution shifts over time. To ensure compliance and traceability, the company requires that all re-training activities, including data access and model updates, be logged and monitored. Additionally, the security team wants to be notified of any unauthorized attempts to retrain the model.</p>\n\n<p>How can you use AWS CloudTrail to meet these guidelines?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable AWS CloudTrail to log all API calls related to SageMaker, including CreateTrainingJob and UpdateEndpoint, and configure CloudWatch Alarms to notify the security team if unauthorized API calls are detected</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable detailed monitoring in Amazon CloudWatch to track all SageMaker activities, and use CloudWatch Logs to store and analyze the logs for any suspicious activity related to re-training</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable AWS CloudWatch to log all API calls related to SageMaker, including CreateTrainingJob and UpdateEndpoint, and configure CloudWatch Alarms to notify the security team if unauthorized API calls are detected</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure AWS CloudTrail to log only the data access events that trigger re-training, and manually trigger model re-training when changes in the data are detected through these logs</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable AWS CloudTrail to log all API calls related to SageMaker, including CreateTrainingJob and UpdateEndpoint, and configure CloudWatch Alarms to notify the security team if unauthorized API calls are detected</strong></p>\n\n<p>This option leverages AWS CloudTrail to log all relevant API activities related to SageMaker, providing full traceability of re-training events and any updates to the model. By configuring CloudWatch Alarms, you can automatically notify the security team if any suspicious or unauthorized actions are detected. This approach meets both compliance and operational requirements.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q36-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q36-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS CloudTrail to log only the data access events that trigger re-training, and manually trigger model re-training when changes in the data are detected through these logs</strong> - Monitoring only data access events in S3 is insufficient because it misses other critical re-training activities, such as creating training jobs or updating endpoints. Manual log reviews are prone to errors and delay in detecting unauthorized activities.</p>\n\n<p><strong>Enable detailed monitoring in Amazon CloudWatch to track all SageMaker activities, and use CloudWatch Logs to store and analyze the logs for any suspicious activity related to re-training</strong> - While CloudWatch is useful for performance monitoring, CloudTrail is specifically designed to log and monitor API activity across AWS services, making it more appropriate for tracking re-training activities and ensuring compliance.</p>\n\n<p><strong>Enable AWS CloudWatch to log all API calls related to SageMaker, including CreateTrainingJob and UpdateEndpoint, and configure CloudWatch Alarms to notify the security team if unauthorized API calls are detected</strong> - You can only use CloudTrail to log and monitor API activity across AWS services. CloudWatch is useful for performance monitoring. So, this option acts as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable AWS CloudTrail to log all API calls related to SageMaker, including CreateTrainingJob and UpdateEndpoint, and configure CloudWatch Alarms to notify the security team if unauthorized API calls are detected</strong>"
      },
      {
        "answer": "",
        "explanation": "This option leverages AWS CloudTrail to log all relevant API activities related to SageMaker, providing full traceability of re-training events and any updates to the model. By configuring CloudWatch Alarms, you can automatically notify the security team if any suspicious or unauthorized actions are detected. This approach meets both compliance and operational requirements."
      },
      {
        "link": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure AWS CloudTrail to log only the data access events that trigger re-training, and manually trigger model re-training when changes in the data are detected through these logs</strong> - Monitoring only data access events in S3 is insufficient because it misses other critical re-training activities, such as creating training jobs or updating endpoints. Manual log reviews are prone to errors and delay in detecting unauthorized activities."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable detailed monitoring in Amazon CloudWatch to track all SageMaker activities, and use CloudWatch Logs to store and analyze the logs for any suspicious activity related to re-training</strong> - While CloudWatch is useful for performance monitoring, CloudTrail is specifically designed to log and monitor API activity across AWS services, making it more appropriate for tracking re-training activities and ensuring compliance."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable AWS CloudWatch to log all API calls related to SageMaker, including CreateTrainingJob and UpdateEndpoint, and configure CloudWatch Alarms to notify the security team if unauthorized API calls are detected</strong> - You can only use CloudTrail to log and monitor API activity across AWS services. CloudWatch is useful for performance monitoring. So, this option acts as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html"
    ]
  },
  {
    "id": 47,
    "question": "<p>You are an ML Engineer developing a model to predict credit card fraud for a financial institution. The dataset you have is extensive, containing millions of transactions, but it is highly imbalanced, with only a small fraction representing fraudulent transactions. To ensure your model generalizes well to unseen data, you need to split your dataset effectively into training, validation, and test sets. You must also avoid overfitting and ensure that the model's performance is accurately measured during development.</p>\n\n<p>Which of the following strategies should you employ to effectively utilize the training, validation, and test sets to develop a robust and reliable machine learning model, given the imbalanced nature of the dataset? (Select two)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Randomly shuffle the data and split it into training, validation, and test sets without considering the class distribution to avoid any bias</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the entire dataset for training to maximize the amount of data the model learns from, and evaluate the model’s performance using cross-validation</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Split the dataset into three sets: a training set for model learning, a validation set for hyperparameter tuning, and a test set for final performance evaluation on unseen data</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Combine the validation and test sets into a single evaluation set to simplify the process and reduce the number of data splits</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use a stratified split to ensure that the training, validation, and test sets each contain a representative distribution of fraudulent and non-fraudulent transactions</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Split the dataset into three sets: a training set for model learning, a validation set for hyperparameter tuning, and a test set for final performance evaluation on unseen data</strong></p>\n\n<p>Splitting the dataset into training, validation, and test sets is a standard practice in machine learning. The training set is used to train the model, the validation set is used for hyperparameter tuning and model selection, and the test set is used for the final evaluation of the model’s performance on unseen data. This approach helps to prevent overfitting and provides an unbiased evaluation of the model.</p>\n\n<p><strong>Use a stratified split to ensure that the training, validation, and test sets each contain a representative distribution of fraudulent and non-fraudulent transactions</strong></p>\n\n<p>Given the imbalanced nature of the dataset, a stratified split is essential to ensure that each set (training, validation, and test) contains a similar distribution of the classes (fraudulent and non-fraudulent transactions). This prevents the model from learning misleading patterns and ensures that performance metrics are reliable.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q16-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q16-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/machine-learning/create-train-test-and-validation-splits-on-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/\">https://aws.amazon.com/blogs/machine-learning/create-train-test-and-validation-splits-on-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the entire dataset for training to maximize the amount of data the model learns from, and evaluate the model’s performance using cross-validation</strong> - Using the entire dataset for training without a separate test set can lead to overfitting, as the model’s performance would be evaluated on data it has already seen. Cross-validation alone cannot replace the need for a final test set for unbiased evaluation.</p>\n\n<p><strong>Randomly shuffle the data and split it into training, validation, and test sets without considering the class distribution to avoid any bias</strong> - Randomly shuffling the data without considering the class distribution can lead to imbalanced splits, where one or more sets may not have enough examples of the minority class (fraudulent transactions). This would make it difficult to accurately assess the model's performance.</p>\n\n<p><strong>Combine the validation and test sets into a single evaluation set to simplify the process and reduce the number of data splits</strong> - Combining the validation and test sets is not recommended because it blurs the distinction between tuning and evaluation. The test set should remain unseen until the final evaluation to provide an unbiased assessment of the model’s performance.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/create-train-test-and-validation-splits-on-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/\">https://aws.amazon.com/blogs/machine-learning/create-train-test-and-validation-splits-on-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Split the dataset into three sets: a training set for model learning, a validation set for hyperparameter tuning, and a test set for final performance evaluation on unseen data</strong>"
      },
      {
        "answer": "",
        "explanation": "Splitting the dataset into training, validation, and test sets is a standard practice in machine learning. The training set is used to train the model, the validation set is used for hyperparameter tuning and model selection, and the test set is used for the final evaluation of the model’s performance on unseen data. This approach helps to prevent overfitting and provides an unbiased evaluation of the model."
      },
      {
        "answer": "",
        "explanation": "<strong>Use a stratified split to ensure that the training, validation, and test sets each contain a representative distribution of fraudulent and non-fraudulent transactions</strong>"
      },
      {
        "answer": "",
        "explanation": "Given the imbalanced nature of the dataset, a stratified split is essential to ensure that each set (training, validation, and test) contains a similar distribution of the classes (fraudulent and non-fraudulent transactions). This prevents the model from learning misleading patterns and ensures that performance metrics are reliable."
      },
      {
        "link": "https://aws.amazon.com/blogs/machine-learning/create-train-test-and-validation-splits-on-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the entire dataset for training to maximize the amount of data the model learns from, and evaluate the model’s performance using cross-validation</strong> - Using the entire dataset for training without a separate test set can lead to overfitting, as the model’s performance would be evaluated on data it has already seen. Cross-validation alone cannot replace the need for a final test set for unbiased evaluation."
      },
      {
        "answer": "",
        "explanation": "<strong>Randomly shuffle the data and split it into training, validation, and test sets without considering the class distribution to avoid any bias</strong> - Randomly shuffling the data without considering the class distribution can lead to imbalanced splits, where one or more sets may not have enough examples of the minority class (fraudulent transactions). This would make it difficult to accurately assess the model's performance."
      },
      {
        "answer": "",
        "explanation": "<strong>Combine the validation and test sets into a single evaluation set to simplify the process and reduce the number of data splits</strong> - Combining the validation and test sets is not recommended because it blurs the distinction between tuning and evaluation. The test set should remain unseen until the final evaluation to provide an unbiased assessment of the model’s performance."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/machine-learning/create-train-test-and-validation-splits-on-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/"
    ]
  },
  {
    "id": 48,
    "question": "<p>A robotics company is training a deep learning model in Amazon SageMaker Studio to optimize the movement of autonomous robots in a warehouse. The training process involves fine-tuning a pre-trained model on a large dataset of movement trajectories. In previous training runs, the company encountered issues such as vanishing gradients, underutilized GPUs, and overfitting, leading to suboptimal model performance and wasted resources.</p>\n\n<p>The company’s ML engineer needs a solution that meets the following requirements with the LEAST operational overhead:</p>\n\n<p>Detect these training issues in real-time.</p>\n\n<p>Automatically react to these issues by stopping training or triggering notifications.</p>\n\n<p>Provide comprehensive metrics for monitoring without increasing operational complexity.</p>\n\n<p>What do you suggest?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage Amazon CloudWatch Logs to monitor GPU usage and manually analyze logs to identify vanishing gradients and overfitting during training</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon SageMaker Experiments to log training metrics and manually evaluate them for signs of vanishing gradients, GPU inefficiency, and overfitting</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon SageMaker Debugger with built-in rules to detect vanishing gradients, GPU underutilization, and overfitting. Configure Debugger to automatically react based on predefined thresholds</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Develop custom Python scripts to track training metrics and set up logic to monitor for issues like vanishing gradients and overfitting in real time</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon SageMaker Debugger with built-in rules to detect vanishing gradients, GPU underutilization, and overfitting. Configure Debugger to automatically react based on predefined thresholds</strong></p>\n\n<p>Amazon SageMaker Debugger provides a low-effort, integrated solution to address common training issues like vanishing gradients, GPU underutilization, and overfitting. By enabling built-in rules, SageMaker Debugger can monitor training jobs in real time, detect issues, and automatically trigger actions such as stopping training or generating alerts. These rules offer deep insights and actionable metrics while integrating seamlessly with SageMaker Studio for visualization. The comprehensive monitoring and automated reactions reduce manual effort and allow the ML engineer to focus on improving the model rather than troubleshooting issues.</p>\n\n<p>Here are the rules relevant to the given use case:</p>\n\n<p>VanishingGradient - This rule detects if the gradients in a trial become extremely small or drop to a zero magnitude. If the mean of the absolute values of the gradients drops below a specified threshold, the rule returns True.</p>\n\n<p>Overfit - This rule detects if your model is being overfit to the training data by comparing the validation and training losses. This rule can be applied either to one of the supported deep learning frameworks (TensorFlow, MXNet, and PyTorch) or to the XGBoost algorithm.</p>\n\n<p>LowGPUUtilization - The LowGPUUtilization rule helps detect if GPU utilization is low or suffers from fluctuations. This is checked for each GPU on each worker. Rule returns True if 95th quantile is below threshold_p95 which indicates underutilization. Rule returns true if 95th quantile is above threshold_p95 and 5th quantile is below threshold_p5 which indicates fluctuations.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon CloudWatch Logs to monitor GPU usage and manually analyze logs to identify vanishing gradients and overfitting during training</strong> - CloudWatch can track GPU usage but does not have built-in capabilities to detect deep learning-specific issues like vanishing gradients and overfitting. This approach also requires manual log analysis, increasing operational overhead.</p>\n\n<p><strong>Use Amazon SageMaker Experiments to log training metrics and manually evaluate them for signs of vanishing gradients, GPU inefficiency, and overfitting</strong> - SageMaker Experiments is useful for tracking and organizing training runs but lacks automated rules to detect and react to specific training issues in real time.</p>\n\n<p><strong>Develop custom Python scripts to track training metrics and set up logic to monitor for issues like vanishing gradients and overfitting in real time</strong> - Writing custom scripts introduces significant development and maintenance overhead. SageMaker Debugger’s built-in rules provide the same functionality with minimal effort.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-profiler-rules.html#low-gpu-utilization\">https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-profiler-rules.html#low-gpu-utilization</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html#overfit\">https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html#overfit</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html#vanishing-gradient\">https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html#vanishing-gradient</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SageMaker Debugger with built-in rules to detect vanishing gradients, GPU underutilization, and overfitting. Configure Debugger to automatically react based on predefined thresholds</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon SageMaker Debugger provides a low-effort, integrated solution to address common training issues like vanishing gradients, GPU underutilization, and overfitting. By enabling built-in rules, SageMaker Debugger can monitor training jobs in real time, detect issues, and automatically trigger actions such as stopping training or generating alerts. These rules offer deep insights and actionable metrics while integrating seamlessly with SageMaker Studio for visualization. The comprehensive monitoring and automated reactions reduce manual effort and allow the ML engineer to focus on improving the model rather than troubleshooting issues."
      },
      {
        "answer": "",
        "explanation": "Here are the rules relevant to the given use case:"
      },
      {
        "answer": "",
        "explanation": "VanishingGradient - This rule detects if the gradients in a trial become extremely small or drop to a zero magnitude. If the mean of the absolute values of the gradients drops below a specified threshold, the rule returns True."
      },
      {
        "answer": "",
        "explanation": "Overfit - This rule detects if your model is being overfit to the training data by comparing the validation and training losses. This rule can be applied either to one of the supported deep learning frameworks (TensorFlow, MXNet, and PyTorch) or to the XGBoost algorithm."
      },
      {
        "answer": "",
        "explanation": "LowGPUUtilization - The LowGPUUtilization rule helps detect if GPU utilization is low or suffers from fluctuations. This is checked for each GPU on each worker. Rule returns True if 95th quantile is below threshold_p95 which indicates underutilization. Rule returns true if 95th quantile is above threshold_p95 and 5th quantile is below threshold_p5 which indicates fluctuations."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon CloudWatch Logs to monitor GPU usage and manually analyze logs to identify vanishing gradients and overfitting during training</strong> - CloudWatch can track GPU usage but does not have built-in capabilities to detect deep learning-specific issues like vanishing gradients and overfitting. This approach also requires manual log analysis, increasing operational overhead."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SageMaker Experiments to log training metrics and manually evaluate them for signs of vanishing gradients, GPU inefficiency, and overfitting</strong> - SageMaker Experiments is useful for tracking and organizing training runs but lacks automated rules to detect and react to specific training issues in real time."
      },
      {
        "answer": "",
        "explanation": "<strong>Develop custom Python scripts to track training metrics and set up logic to monitor for issues like vanishing gradients and overfitting in real time</strong> - Writing custom scripts introduces significant development and maintenance overhead. SageMaker Debugger’s built-in rules provide the same functionality with minimal effort."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-profiler-rules.html#low-gpu-utilization",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html#overfit",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html#vanishing-gradient"
    ]
  },
  {
    "id": 49,
    "question": "<p>You are a data scientist working for an e-commerce company that wants to implement personalized product recommendations for its users. The company has a large dataset of user interactions, including clicks, purchases, and reviews. The goal is to create a recommendation system that can scale to millions of users while providing real-time recommendations based on user behavior. You need to choose the most appropriate built-in algorithm in Amazon SageMaker to achieve this goal.</p>\n\n<p>Given the requirements, which of the following Amazon SageMaker built-in algorithms is the MOST SUITABLE for this use case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>K-Means Algorithm to cluster users into segments and recommend products based on these segments</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Factorization Machines Algorithm to model user-item interactions for collaborative filtering</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>BlazingText Algorithm to analyze the text in user reviews and identify product similarities</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>XGBoost Algorithm to rank the products based on user behavior and demographic features</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Factorization Machines Algorithm to model user-item interactions for collaborative filtering</strong></p>\n\n<p>The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.</p>\n\n<p>Factorization Machines is well-suited for collaborative filtering. It excels at modeling sparse user-item interactions, making it ideal for large-scale recommendation systems where there are many users and items but relatively few interactions for each user-item pair. This algorithm can effectively capture latent factors to provide personalized recommendations.</p>\n\n<p>Mapping use cases to built-in algorithms:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q4-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q4-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>XGBoost Algorithm to rank the products based on user behavior and demographic features</strong> - XGBoost is a powerful algorithm for ranking and classification tasks, but it’s not optimized for collaborative filtering, which is crucial for personalized recommendations in this context.</p>\n\n<p><strong>BlazingText Algorithm to analyze the text in user reviews and identify product similarities</strong> - BlazingText is effective for text classification and word embedding but is not specifically designed for recommendation systems. While it can be used to analyze user reviews, it does not address the core requirement of user-item interaction modeling.</p>\n\n<p><strong>K-Means Algorithm to cluster users into segments and recommend products based on these segments</strong> - K-Means is useful for clustering users into segments, but this approach is more generalized and does not provide the level of personalization required for individual recommendations based on specific user-item interactions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/accelerate-and-improve-recommender-system-training-and-predictions-using-amazon-sagemaker-feature-store/\">https://aws.amazon.com/blogs/machine-learning/accelerate-and-improve-recommender-system-training-and-predictions-using-amazon-sagemaker-feature-store/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Factorization Machines Algorithm to model user-item interactions for collaborative filtering</strong>"
      },
      {
        "answer": "",
        "explanation": "The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation."
      },
      {
        "answer": "",
        "explanation": "Factorization Machines is well-suited for collaborative filtering. It excels at modeling sparse user-item interactions, making it ideal for large-scale recommendation systems where there are many users and items but relatively few interactions for each user-item pair. This algorithm can effectively capture latent factors to provide personalized recommendations."
      },
      {
        "answer": "",
        "explanation": "Mapping use cases to built-in algorithms:"
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>XGBoost Algorithm to rank the products based on user behavior and demographic features</strong> - XGBoost is a powerful algorithm for ranking and classification tasks, but it’s not optimized for collaborative filtering, which is crucial for personalized recommendations in this context."
      },
      {
        "answer": "",
        "explanation": "<strong>BlazingText Algorithm to analyze the text in user reviews and identify product similarities</strong> - BlazingText is effective for text classification and word embedding but is not specifically designed for recommendation systems. While it can be used to analyze user reviews, it does not address the core requirement of user-item interaction modeling."
      },
      {
        "answer": "",
        "explanation": "<strong>K-Means Algorithm to cluster users into segments and recommend products based on these segments</strong> - K-Means is useful for clustering users into segments, but this approach is more generalized and does not provide the level of personalization required for individual recommendations based on specific user-item interactions."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html",
      "https://aws.amazon.com/blogs/machine-learning/accelerate-and-improve-recommender-system-training-and-predictions-using-amazon-sagemaker-feature-store/"
    ]
  },
  {
    "id": 50,
    "question": "<p>Which AWS services are specifically designed to aid in monitoring machine learning models and incorporating human review processes? (Select two)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Augmented AI (Amazon A2I)</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon SageMaker Model Monitor</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon SageMaker Data Wrangler</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon SageMaker Feature Store</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Amazon SageMaker Ground Truth</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning (ML)",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Amazon SageMaker Model Monitor</strong></p>\n\n<p>Amazon SageMaker Model Monitor is a service that continuously monitors the quality of machine learning models in production and helps detect data drift, model quality issues, and anomalies. It ensures that models perform as expected and alerts users to issues that might require human intervention.</p>\n\n<p><strong>Amazon Augmented AI (Amazon A2I)</strong></p>\n\n<p>Amazon Augmented AI (A2I) is a service that helps implement human review workflows for machine learning predictions. It integrates human judgment into ML workflows, allowing for reviews and corrections of model predictions, which is critical for applications requiring high accuracy and accountability.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q64-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q64-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-use-augmented-ai-a2i-human-review-loops.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-use-augmented-ai-a2i-human-review-loops.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon SageMaker Data Wrangler</strong> - Amazon SageMaker Data Wrangler is designed to simplify and streamline the process of data preparation for machine learning, not specifically for monitoring or human review.</p>\n\n<p><strong>Amazon SageMaker Feature Store</strong> - Amazon SageMaker Feature Store is a fully managed repository for storing, updating, and retrieving machine learning features. While it aids in managing features used by models, it does not directly handle monitoring or human review processes.</p>\n\n<p><strong>Amazon SageMaker Ground Truth</strong> - Amazon SageMaker Ground Truth is used for building highly accurate training datasets for machine learning quickly. It does involve human annotators for labeling data, but it is not specifically designed for monitoring or human review of model predictions in production.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-use-augmented-ai-a2i-human-review-loops.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-use-augmented-ai-a2i-human-review-loops.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon SageMaker Model Monitor</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon SageMaker Model Monitor is a service that continuously monitors the quality of machine learning models in production and helps detect data drift, model quality issues, and anomalies. It ensures that models perform as expected and alerts users to issues that might require human intervention."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Augmented AI (Amazon A2I)</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Augmented AI (A2I) is a service that helps implement human review workflows for machine learning predictions. It integrates human judgment into ML workflows, allowing for reviews and corrections of model predictions, which is critical for applications requiring high accuracy and accountability."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-use-augmented-ai-a2i-human-review-loops.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon SageMaker Data Wrangler</strong> - Amazon SageMaker Data Wrangler is designed to simplify and streamline the process of data preparation for machine learning, not specifically for monitoring or human review."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon SageMaker Feature Store</strong> - Amazon SageMaker Feature Store is a fully managed repository for storing, updating, and retrieving machine learning features. While it aids in managing features used by models, it does not directly handle monitoring or human review processes."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon SageMaker Ground Truth</strong> - Amazon SageMaker Ground Truth is used for building highly accurate training datasets for machine learning quickly. It does involve human annotators for labeling data, but it is not specifically designed for monitoring or human review of model predictions in production."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-use-augmented-ai-a2i-human-review-loops.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html"
    ]
  },
  {
    "id": 51,
    "question": "<p>You are a Data Scientist working for a retail company and you have been tasked with developing a demand forecasting model using Amazon SageMaker. The company requires the model to be highly accurate to optimize inventory levels, but you also need to consider constraints on training time and cost due to budget limitations. You have access to multiple SageMaker instance types and options like spot instances to reduce costs, but you must balance these factors against the need for a performant model. Your goal is to choose a configuration that provides an acceptable tradeoff between model performance, training time, and cost.</p>\n\n<p>Which of the following strategies should you consider when balancing model performance, training time, and cost in this scenario using Amazon SageMaker? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy multiple models with different instance types simultaneously, choosing the one that completes training first, regardless of performance or cost</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement distributed training across multiple smaller instances to balance training time and cost while maintaining model performance</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use the largest available instance type to minimize training time, regardless of cost, ensuring that the model is trained as quickly as possible</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a smaller instance type to save on costs, and accept longer training times, as model accuracy is not the highest priority</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Optimize hyperparameters using Amazon SageMaker’s automatic model tuning (hyperparameter optimization) to improve performance, while using spot instances to reduce cost</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Optimize hyperparameters using Amazon SageMaker’s automatic model tuning (hyperparameter optimization) to improve performance, while using spot instances to reduce cost</strong></p>\n\n<p>Amazon SageMaker’s automatic model tuning allows you to optimize hyperparameters, which can significantly improve model performance without manually tuning each parameter. By using spot instances, you can reduce the cost of training while still focusing on achieving high performance. This approach helps balance the tradeoff between cost and performance effectively.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q17-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q17-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html</a><p></p>\n\n<p><strong>Implement distributed training across multiple smaller instances to balance training time and cost while maintaining model performance</strong></p>\n\n<p>Distributed training across multiple smaller instances can help reduce overall training time while managing costs. This strategy allows you to leverage parallel processing without incurring the high costs associated with using a single, large instance. It also ensures that the model training process is efficient without compromising on performance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q17-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q17-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the largest available instance type to minimize training time, regardless of cost, ensuring that the model is trained as quickly as possible</strong> - Using the largest available instance type might minimize training time but can result in prohibitively high costs, which is not suitable given the budget constraints. This strategy does not balance cost-effectiveness with the need for performance.</p>\n\n<p><strong>Use a smaller instance type to save on costs, and accept longer training times, as model accuracy is not the highest priority</strong> - While using a smaller instance type may save on costs, it can significantly increase training time. Given that model accuracy is important for optimizing inventory levels, this tradeoff might not be acceptable.</p>\n\n<p><strong>Deploy multiple models with different instance types simultaneously, choosing the one that completes training first, regardless of performance or cost</strong> - Deploying multiple models simultaneously on different instance types and selecting the one that finishes first is inefficient and can lead to unnecessary costs. Additionally, this approach does not consider the need for balanced performance, cost, and training time.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Optimize hyperparameters using Amazon SageMaker’s automatic model tuning (hyperparameter optimization) to improve performance, while using spot instances to reduce cost</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon SageMaker’s automatic model tuning allows you to optimize hyperparameters, which can significantly improve model performance without manually tuning each parameter. By using spot instances, you can reduce the cost of training while still focusing on achieving high performance. This approach helps balance the tradeoff between cost and performance effectively."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Implement distributed training across multiple smaller instances to balance training time and cost while maintaining model performance</strong>"
      },
      {
        "answer": "",
        "explanation": "Distributed training across multiple smaller instances can help reduce overall training time while managing costs. This strategy allows you to leverage parallel processing without incurring the high costs associated with using a single, large instance. It also ensures that the model training process is efficient without compromising on performance."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the largest available instance type to minimize training time, regardless of cost, ensuring that the model is trained as quickly as possible</strong> - Using the largest available instance type might minimize training time but can result in prohibitively high costs, which is not suitable given the budget constraints. This strategy does not balance cost-effectiveness with the need for performance."
      },
      {
        "answer": "",
        "explanation": "<strong>Use a smaller instance type to save on costs, and accept longer training times, as model accuracy is not the highest priority</strong> - While using a smaller instance type may save on costs, it can significantly increase training time. Given that model accuracy is important for optimizing inventory levels, this tradeoff might not be acceptable."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy multiple models with different instance types simultaneously, choosing the one that completes training first, regardless of performance or cost</strong> - Deploying multiple models simultaneously on different instance types and selecting the one that finishes first is inefficient and can lead to unnecessary costs. Additionally, this approach does not consider the need for balanced performance, cost, and training time."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html"
    ]
  },
  {
    "id": 52,
    "question": "<p>A retail company uses Amazon SageMaker to train ML models for predicting product demand. The company stores training data for different product categories in Amazon S3 within a single AWS account. Each data scientist is assigned to a specific product category and must have access to only their respective category's training data. Data scientists must not be able to access training data from other product categories. The company needs to ensure secure, fine-grained access control for the training data while maintaining centralized management of resources.</p>\n\n<p>Which solution will meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a preprocessing pipeline using AWS Glue to separate training data for each product category into individual S3 buckets. Use bucket policies to grant access to these buckets</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up separate AWS accounts for each product category under an AWS Organizations structure. Restrict each data scientist's access to their assigned account</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create separate S3 buckets for each product category. Use bucket policies to grant access to individual buckets based on each data scientist's IAM role</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use IAM policies with resource-based conditions to grant each data scientist access to specific S3 bucket prefixes that correspond to their assigned product category. Attach these policies to the IAM roles used by SageMaker notebook instances</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use IAM policies with resource-based conditions to grant each data scientist access to specific S3 bucket prefixes that correspond to their assigned product category. Attach these policies to the IAM roles used by SageMaker notebook instances</strong></p>\n\n<p>IAM policies with resource-based conditions, such as s3:prefix, allow fine-grained access control within a single S3 bucket. This approach avoids the need for multiple buckets and simplifies management. By attaching these policies to SageMaker notebook instance roles, data scientists can securely access only their assigned training data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create separate S3 buckets for each product category. Use bucket policies to grant access to individual buckets based on each data scientist's IAM role</strong> - While creating separate S3 buckets for each product category can achieve the goal, it adds operational overhead. Managing multiple buckets and bucket policies for each category increases complexity, especially as the number of product categories grows.</p>\n\n<p><strong>Create a preprocessing pipeline using AWS Glue to separate training data for each product category into individual S3 buckets. Use bucket policies to grant access to these buckets</strong> - This approach introduces unnecessary cost and complexity by duplicating the training data across multiple S3 buckets. AWS Glue is primarily intended for ETL tasks and is not needed for simple access control requirements.</p>\n\n<p><strong>Set up separate AWS accounts for each product category under an AWS Organizations structure. Restrict each data scientist's access to their assigned account</strong> - Creating separate AWS accounts for each product category is excessive and introduces significant administrative overhead. This approach is not cost-effective for managing access control when the data resides in a single AWS account.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use IAM policies with resource-based conditions to grant each data scientist access to specific S3 bucket prefixes that correspond to their assigned product category. Attach these policies to the IAM roles used by SageMaker notebook instances</strong>"
      },
      {
        "answer": "",
        "explanation": "IAM policies with resource-based conditions, such as s3:prefix, allow fine-grained access control within a single S3 bucket. This approach avoids the need for multiple buckets and simplifies management. By attaching these policies to SageMaker notebook instance roles, data scientists can securely access only their assigned training data."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create separate S3 buckets for each product category. Use bucket policies to grant access to individual buckets based on each data scientist's IAM role</strong> - While creating separate S3 buckets for each product category can achieve the goal, it adds operational overhead. Managing multiple buckets and bucket policies for each category increases complexity, especially as the number of product categories grows."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a preprocessing pipeline using AWS Glue to separate training data for each product category into individual S3 buckets. Use bucket policies to grant access to these buckets</strong> - This approach introduces unnecessary cost and complexity by duplicating the training data across multiple S3 buckets. AWS Glue is primarily intended for ETL tasks and is not needed for simple access control requirements."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up separate AWS accounts for each product category under an AWS Organizations structure. Restrict each data scientist's access to their assigned account</strong> - Creating separate AWS accounts for each product category is excessive and introduces significant administrative overhead. This approach is not cost-effective for managing access control when the data resides in a single AWS account."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html"
    ]
  },
  {
    "id": 53,
    "question": "<p>You are an ML Engineer at a financial services company tasked with deploying a machine learning model for real-time fraud detection in production. The model requires low-latency inference to ensure that fraudulent transactions are flagged immediately. However, you also need to conduct extensive testing and experimentation in a separate environment to fine-tune the model and validate its performance before deploying it. You must provision compute resources that are appropriate for both environments, balancing performance, cost, and the specific needs of testing and production.</p>\n\n<p>Which of the following strategies should you implement to effectively provision compute resources for both the production environment and the test environment using Amazon SageMaker, considering the different requirements for each environment? (Select two)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use CPU-based instances in the test environment to save on costs during experimentation</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Leverage AWS Inferentia accelerators in the production environment to meet high throughput and low latency requirements</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Provision CPU-based instances in both production and test environments to reduce costs, as CPU instances are generally cheaper than GPU instances</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use GPU-based instances in both production and test environments to ensure that the model inference and testing are both performed at maximum speed, regardless of cost</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Provision identical instances in both production and test environments to ensure consistent performance between the two, eliminating the risk of discrepancies during deployment</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use CPU-based instances in the test environment to save on costs during experimentation</strong></p>\n\n<p>For the test environment, CPU-based instances can be used to run experiments and validate the model, which helps reduce costs without compromising the ability to test different configurations and models.</p>\n\n<p><strong>Leverage AWS Inferentia accelerators in the production environment to meet high throughput and low latency requirements</strong></p>\n\n<p>AWS Inferentia accelerators are designed by AWS to deliver high performance at the lowest cost in Amazon EC2 for your deep learning (DL) and generative AI inference applications. The first-generation AWS Inferentia accelerator powers Amazon Elastic Compute Cloud (Amazon EC2) Inf1 instances that deliver higher throughput and lower latency than comparable Amazon EC2 instances. They also offer up to 70% lower cost per inference than comparable Amazon EC2 instances. Therefore, you can meet performance requirements in the production environment while optimizing costs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use GPU-based instances in both production and test environments to ensure that the model inference and testing are both performed at maximum speed, regardless of cost</strong> - While using GPU-based instances in both environments ensures high performance, it is not cost-effective. The test environment does not typically require the same level of performance as production, making GPU instances unnecessary and expensive.</p>\n\n<p><strong>Provision CPU-based instances in both production and test environments to reduce costs, as CPU instances are generally cheaper than GPU instances</strong> - Provisioning only CPU-based instances in both environments might save costs but would likely fail to meet the low-latency requirements in production. Inference times could be unacceptably slow, which is critical for real-time fraud detection.</p>\n\n<p><strong>Provision identical instances in both production and test environments to ensure consistent performance between the two, eliminating the risk of discrepancies during deployment</strong> - Although using identical instances in both environments ensures consistency, it is not cost-efficient. The test environment does not need to replicate the full performance of the production environment, so using less powerful and less expensive instances is more appropriate for testing purposes.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/machine-learning/inferentia/\">https://aws.amazon.com/machine-learning/inferentia/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use CPU-based instances in the test environment to save on costs during experimentation</strong>"
      },
      {
        "answer": "",
        "explanation": "For the test environment, CPU-based instances can be used to run experiments and validate the model, which helps reduce costs without compromising the ability to test different configurations and models."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Inferentia accelerators in the production environment to meet high throughput and low latency requirements</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Inferentia accelerators are designed by AWS to deliver high performance at the lowest cost in Amazon EC2 for your deep learning (DL) and generative AI inference applications. The first-generation AWS Inferentia accelerator powers Amazon Elastic Compute Cloud (Amazon EC2) Inf1 instances that deliver higher throughput and lower latency than comparable Amazon EC2 instances. They also offer up to 70% lower cost per inference than comparable Amazon EC2 instances. Therefore, you can meet performance requirements in the production environment while optimizing costs."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use GPU-based instances in both production and test environments to ensure that the model inference and testing are both performed at maximum speed, regardless of cost</strong> - While using GPU-based instances in both environments ensures high performance, it is not cost-effective. The test environment does not typically require the same level of performance as production, making GPU instances unnecessary and expensive."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision CPU-based instances in both production and test environments to reduce costs, as CPU instances are generally cheaper than GPU instances</strong> - Provisioning only CPU-based instances in both environments might save costs but would likely fail to meet the low-latency requirements in production. Inference times could be unacceptably slow, which is critical for real-time fraud detection."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision identical instances in both production and test environments to ensure consistent performance between the two, eliminating the risk of discrepancies during deployment</strong> - Although using identical instances in both environments ensures consistency, it is not cost-efficient. The test environment does not need to replicate the full performance of the production environment, so using less powerful and less expensive instances is more appropriate for testing purposes."
      }
    ],
    "references": [
      "https://aws.amazon.com/machine-learning/inferentia/"
    ]
  },
  {
    "id": 54,
    "question": "<p>You are a data scientist working for a media company that processes large volumes of video and image data to generate personalized content recommendations. The dataset, which is stored in Amazon S3, contains tens of millions of small image files and several terabytes of high-resolution large video files. The training jobs you run on Amazon SageMaker require low-latency access to this data and need to be completed quickly to keep up with the dynamic content pipeline.</p>\n\n<p>Given the characteristics of your data and the requirements for low-latency, high-throughput access, which approach is the MOST APPROPRIATE for this scenario?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon FSx for Lustre to mount the entire dataset as a high-performance file system, providing consistently low-latency access to both the small image files and the large video files</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Fast File mode with Amazon S3 to stream the small image files directly to the training instances on-demand, minimizing the time required to start training</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an FSx for Lustre file system linked with the relevant Amazon S3 bucket folder having the training data for the small image files and apply Fast File mode to the relevant Amazon S3 bucket folder to access the video files, thereby combining the strengths of both approaches</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Fast File mode with Amazon S3 for the large video files, enabling on-demand streaming of data, and store the small image files locally on the training instances to reduce I/O latency</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning (ML)",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an FSx for Lustre file system linked with the relevant Amazon S3 bucket folder having the training data for the small image files and apply Fast File mode to the relevant Amazon S3 bucket folder to access the video files, thereby combining the strengths of both approaches</strong></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q19-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q19-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q19-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q19-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html</a></p>\n\n<p>FSx for Lustre can scale to hundreds of gigabytes of throughput and millions of IOPS with low-latency file retrieval. When starting a training job, SageMaker mounts the FSx for Lustre file system to the training instance file system, then starts your training script. Mounting itself is a relatively fast operation that doesn't depend on the size of the dataset stored in FSx for Lustre.</p>\n\n<p>If your dataset is too large for file mode, has many small files that you can't serialize easily, or uses a random read access pattern, FSx for Lustre is a good option to consider. Its file system scales to hundreds of gigabytes per second (GB/s) of throughput and millions of IOPS, which is ideal when you have many small files.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q19-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q19-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data-best-practices.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data-best-practices.html</a></p>\n\n<p>For the given use case, you can create an FSx for Lustre file system linked with the Amazon S3 bucket folder having the training data for the small image files, like so:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q19-i4.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q19-i4.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html</a></p>\n\n<p>You can then apply Fast File mode for the video files in the relevant Amazon S3 bucket folder, like so:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q19-i5.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q19-i5.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon FSx for Lustre to mount the entire dataset as a high-performance file system, providing consistently low-latency access to both the small image files and the large video files</strong> - Amazon FSx for Lustre is designed for high-performance workloads with large datasets, especially when you need low-latency access to many small files that you can't serialize easily, or uses a random read access pattern. FSx is not the optimal solution to provide low-latency access to many large video files, you should rather use the Fast File mode for the video files in the relevant Amazon S3 bucket folder. So, this option is incorrect.</p>\n\n<p><strong>Use Fast File mode with Amazon S3 to stream the small image files directly to the training instances on-demand, minimizing the time required to start training</strong> - While Fast File mode is effective for large files, it does not provide the low-latency, high-throughput access needed for a large number of small files. So, this option is incorrect.</p>\n\n<p><strong>Use Fast File mode with Amazon S3 for the large video files, enabling on-demand streaming of data, and store the small image files locally on the training instances to reduce I/O latency</strong> - Splitting data management between Fast File mode for large files and local storage for small files adds unnecessary complexity and additional costs without providing a proportional performance improvement, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-sagemaker-fast-file-mode/\">https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-sagemaker-fast-file-mode/</a></p>\n\n<p><a href=\"https://aws.amazon.com/fsx/lustre/faqs/\">https://aws.amazon.com/fsx/lustre/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an FSx for Lustre file system linked with the relevant Amazon S3 bucket folder having the training data for the small image files and apply Fast File mode to the relevant Amazon S3 bucket folder to access the video files, thereby combining the strengths of both approaches</strong>"
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html",
        "answer": "",
        "explanation": "via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html</a>"
      },
      {
        "answer": "",
        "explanation": "FSx for Lustre can scale to hundreds of gigabytes of throughput and millions of IOPS with low-latency file retrieval. When starting a training job, SageMaker mounts the FSx for Lustre file system to the training instance file system, then starts your training script. Mounting itself is a relatively fast operation that doesn't depend on the size of the dataset stored in FSx for Lustre."
      },
      {
        "answer": "",
        "explanation": "If your dataset is too large for file mode, has many small files that you can't serialize easily, or uses a random read access pattern, FSx for Lustre is a good option to consider. Its file system scales to hundreds of gigabytes per second (GB/s) of throughput and millions of IOPS, which is ideal when you have many small files."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data-best-practices.html",
        "answer": "",
        "explanation": "via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data-best-practices.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data-best-practices.html</a>"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can create an FSx for Lustre file system linked with the Amazon S3 bucket folder having the training data for the small image files, like so:"
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html",
        "answer": "",
        "explanation": "via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html</a>"
      },
      {
        "answer": "",
        "explanation": "You can then apply Fast File mode for the video files in the relevant Amazon S3 bucket folder, like so:"
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html",
        "answer": "",
        "explanation": "via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon FSx for Lustre to mount the entire dataset as a high-performance file system, providing consistently low-latency access to both the small image files and the large video files</strong> - Amazon FSx for Lustre is designed for high-performance workloads with large datasets, especially when you need low-latency access to many small files that you can't serialize easily, or uses a random read access pattern. FSx is not the optimal solution to provide low-latency access to many large video files, you should rather use the Fast File mode for the video files in the relevant Amazon S3 bucket folder. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Fast File mode with Amazon S3 to stream the small image files directly to the training instances on-demand, minimizing the time required to start training</strong> - While Fast File mode is effective for large files, it does not provide the low-latency, high-throughput access needed for a large number of small files. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Fast File mode with Amazon S3 for the large video files, enabling on-demand streaming of data, and store the small image files locally on the training instances to reduce I/O latency</strong> - Splitting data management between Fast File mode for large files and local storage for small files adds unnecessary complexity and additional costs without providing a proportional performance improvement, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data-best-practices.html",
      "https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-sagemaker-fast-file-mode/",
      "https://aws.amazon.com/fsx/lustre/faqs/"
    ]
  },
  {
    "id": 55,
    "question": "<p>You are a machine learning engineer at a fintech company that has developed several models for various use cases, including fraud detection, credit scoring, and personalized marketing. Each model has different performance and deployment requirements. The fraud detection model requires real-time predictions with low latency and needs to scale quickly based on incoming transaction volumes. The credit scoring model is computationally intensive but can tolerate batch processing with slightly higher latency. The personalized marketing model needs to be triggered by events and doesn’t require constant availability.</p>\n\n<p>Given these varying requirements, which deployment target is the MOST SUITABLE for each model?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the fraud detection model using SageMaker endpoints for low-latency, real-time predictions, deploy the credit scoring model on Amazon ECS for batch processing, and deploy the personalized marketing model using AWS Lambda for event-driven execution</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Deploy the fraud detection model using AWS Lambda for serverless, on-demand execution, deploy the credit scoring model on Amazon EKS for scalable batch processing, and deploy the personalized marketing model on SageMaker endpoints to handle event-driven inference</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy the fraud detection model on Amazon ECS for auto-scaling based on demand, deploy the credit scoring model using SageMaker endpoints for real-time scoring, and deploy the personalized marketing model on Amazon EKS for event-driven processing</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy all three models on a single Amazon EKS cluster to take advantage of Kubernetes orchestration, ensuring consistent management and scaling across different use cases</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the fraud detection model using SageMaker endpoints for low-latency, real-time predictions, deploy the credit scoring model on Amazon ECS for batch processing, and deploy the personalized marketing model using AWS Lambda for event-driven execution</strong></p>\n\n<p>Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling. SageMaker endpoints are optimized for low-latency, real-time predictions, making them ideal for the fraud detection model.</p>\n\n<p>Amazon ECS provides a service scheduler for long-running tasks and applications. It also provides the ability to run standalone tasks or scheduled tasks for batch jobs or single run tasks. You can specify the task placement strategies and constraints for running tasks that best meet your needs. Amazon ECS is well-suited for batch processing tasks, making it a good choice for the credit scoring model.</p>\n\n<p>AWS Lambda is ideal for the event-driven nature of the personalized marketing model, allowing it to scale on-demand with minimal cost.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q23-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q23-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduling_tasks.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduling_tasks.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the fraud detection model using AWS Lambda for serverless, on-demand execution, deploy the credit scoring model on Amazon EKS for scalable batch processing, and deploy the personalized marketing model on SageMaker endpoints to handle event-driven inference</strong> - AWS Lambda is serverless and ideal for event-driven tasks, but it may not provide the low-latency, real-time performance required for fraud detection. SageMaker endpoints are better suited for this use case. The credit scoring model is better suited for ECS, where batch processing can be efficiently managed, while personalized marketing is a good fit for AWS Lambda.</p>\n\n<p><strong>Deploy all three models on a single Amazon EKS cluster to take advantage of Kubernetes orchestration, ensuring consistent management and scaling across different use cases</strong> - Deploying all models on a single Amazon EKS cluster could be overkill and lead to unnecessary complexity. While Kubernetes provides powerful orchestration, it might be excessive for simple, event-driven or batch workloads.</p>\n\n<p><strong>Deploy the fraud detection model on Amazon ECS for auto-scaling based on demand, deploy the credit scoring model using SageMaker endpoints for real-time scoring, and deploy the personalized marketing model on Amazon EKS for event-driven processing</strong> - While Amazon ECS can handle auto-scaling, it is not as optimized for real-time, low-latency predictions as SageMaker endpoints. Additionally, using SageMaker endpoints for the credit scoring model does not align well with batch processing needs. The personalized marketing model is better suited to AWS Lambda rather than Amazon EKS, which is more complex and designed for containerized applications with continuous workloads.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduling_tasks.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduling_tasks.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the fraud detection model using SageMaker endpoints for low-latency, real-time predictions, deploy the credit scoring model on Amazon ECS for batch processing, and deploy the personalized marketing model using AWS Lambda for event-driven execution</strong>"
      },
      {
        "answer": "",
        "explanation": "Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling. SageMaker endpoints are optimized for low-latency, real-time predictions, making them ideal for the fraud detection model."
      },
      {
        "answer": "",
        "explanation": "Amazon ECS provides a service scheduler for long-running tasks and applications. It also provides the ability to run standalone tasks or scheduled tasks for batch jobs or single run tasks. You can specify the task placement strategies and constraints for running tasks that best meet your needs. Amazon ECS is well-suited for batch processing tasks, making it a good choice for the credit scoring model."
      },
      {
        "answer": "",
        "explanation": "AWS Lambda is ideal for the event-driven nature of the personalized marketing model, allowing it to scale on-demand with minimal cost."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduling_tasks.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the fraud detection model using AWS Lambda for serverless, on-demand execution, deploy the credit scoring model on Amazon EKS for scalable batch processing, and deploy the personalized marketing model on SageMaker endpoints to handle event-driven inference</strong> - AWS Lambda is serverless and ideal for event-driven tasks, but it may not provide the low-latency, real-time performance required for fraud detection. SageMaker endpoints are better suited for this use case. The credit scoring model is better suited for ECS, where batch processing can be efficiently managed, while personalized marketing is a good fit for AWS Lambda."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy all three models on a single Amazon EKS cluster to take advantage of Kubernetes orchestration, ensuring consistent management and scaling across different use cases</strong> - Deploying all models on a single Amazon EKS cluster could be overkill and lead to unnecessary complexity. While Kubernetes provides powerful orchestration, it might be excessive for simple, event-driven or batch workloads."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy the fraud detection model on Amazon ECS for auto-scaling based on demand, deploy the credit scoring model using SageMaker endpoints for real-time scoring, and deploy the personalized marketing model on Amazon EKS for event-driven processing</strong> - While Amazon ECS can handle auto-scaling, it is not as optimized for real-time, low-latency predictions as SageMaker endpoints. Additionally, using SageMaker endpoints for the credit scoring model does not align well with batch processing needs. The personalized marketing model is better suited to AWS Lambda rather than Amazon EKS, which is more complex and designed for containerized applications with continuous workloads."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduling_tasks.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html"
    ]
  },
  {
    "id": 56,
    "question": "<p>A financial services company is using Amazon SageMaker Canvas to build machine learning models for predicting stock price trends. The company stores its data in Amazon S3, and the dataset includes large volumes of transactional records with a complex structure, including nested fields and multiple data types. The data scientist needs to choose a file format that will optimize data processing performance and minimize loading time into SageMaker Canvas.</p>\n\n<p>Which file format will meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>CSV</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>XML</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>JSON</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Apache Parquet</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning (ML)",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Apache Parquet</strong></p>\n\n<p>Apache Parquet is specifically designed for large-scale data processing, making it ideal for machine learning workflows with complex datasets. Its columnar format reduces the amount of data that needs to be processed by enabling efficient selection of relevant columns, thereby minimizing data processing time. Parquet also supports compression and parallel processing, which further enhances its performance when handling large datasets like transactional records. This makes it the most suitable file format for importing data into SageMaker Canvas efficiently.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>JSON</strong> - JSON can represent complex structures but is verbose and inefficient compared to Parquet. Processing JSON files requires more computational resources, increasing data import times.</p>\n\n<p><strong>CSV</strong> - CSV is ideal for flat, tabular data but lacks support for nested structures. It is not optimized for large-scale, complex datasets and often results in slower processing times.</p>\n\n<p><strong>XML</strong> - XML is verbose and inefficient compared to Parquet. It is not optimized for performance, and SageMaker Canvas does not provide enhanced support for XML processing.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/canvas.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/canvas.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-parquet-home.html\">https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-parquet-home.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-sagemaker-canvas-apache-parquet-file-format/\">https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-sagemaker-canvas-apache-parquet-file-format/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Apache Parquet</strong>"
      },
      {
        "answer": "",
        "explanation": "Apache Parquet is specifically designed for large-scale data processing, making it ideal for machine learning workflows with complex datasets. Its columnar format reduces the amount of data that needs to be processed by enabling efficient selection of relevant columns, thereby minimizing data processing time. Parquet also supports compression and parallel processing, which further enhances its performance when handling large datasets like transactional records. This makes it the most suitable file format for importing data into SageMaker Canvas efficiently."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>JSON</strong> - JSON can represent complex structures but is verbose and inefficient compared to Parquet. Processing JSON files requires more computational resources, increasing data import times."
      },
      {
        "answer": "",
        "explanation": "<strong>CSV</strong> - CSV is ideal for flat, tabular data but lacks support for nested structures. It is not optimized for large-scale, complex datasets and often results in slower processing times."
      },
      {
        "answer": "",
        "explanation": "<strong>XML</strong> - XML is verbose and inefficient compared to Parquet. It is not optimized for performance, and SageMaker Canvas does not provide enhanced support for XML processing."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/canvas.html",
      "https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-parquet-home.html",
      "https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-sagemaker-canvas-apache-parquet-file-format/"
    ]
  },
  {
    "id": 57,
    "question": "<p>You are a Cloud Financial Manager at a SaaS company that uses various AWS services to run its applications and machine learning workloads. Your management team has asked you to reduce overall AWS spending while ensuring that critical applications remain highly available and performant. To achieve this, you need to use AWS cost analysis tools to monitor spending, identify cost-saving opportunities, and optimize resource utilization across the organization.</p>\n\n<p>Which of the following actions can you perform using the appropriate AWS cost analysis tools to achieve your goal of reducing costs and optimizing AWS resource utilization? (Select two)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage AWS Trusted Advisor to directly modify and reconfigure resources based on cost optimization recommendations without manual intervention</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Cost Explorer to set custom budgets for cost and usage to govern costs across your organization and receive alerts when costs exceed your defined thresholds</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Cost Explorer to automatically delete unused resources across your AWS environment, ensuring that no unnecessary costs are incurred</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Cost Explorer to analyze historical spending patterns, identify cost trends, and forecast future costs to help with budgeting and planning</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Leverage AWS Trusted Advisor to receive recommendations for cost optimization, such as identifying underutilized or idle resources, and reserved instance purchasing opportunities</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use AWS Cost Explorer to analyze historical spending patterns, identify cost trends, and forecast future costs to help with budgeting and planning</strong></p>\n\n<p>AWS Cost Explorer allows you to analyze your past AWS spending, identify cost trends, and forecast future costs based on historical data. This tool is valuable for budgeting and financial planning, helping you make informed decisions about resource allocation and cost management.</p>\n\n<p><strong>Leverage AWS Trusted Advisor to receive recommendations for cost optimization, such as identifying underutilized or idle resources, and reserved instance purchasing opportunities</strong></p>\n\n<p>AWS Trusted Advisor provides actionable recommendations to optimize your AWS environment, including cost-saving suggestions. It identifies opportunities such as underutilized resources, idle instances, and reserved instance purchasing options that can significantly reduce your AWS costs.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q37-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q37-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Cost Explorer to set custom budgets for cost and usage to govern costs across your organization and receive alerts when costs exceed your defined thresholds</strong> You can only use AWS Budgets to set custom budgets for cost and usage to govern costs across your organization and receive alerts when costs exceed your defined thresholds.</p>\n\n<p><strong>Use AWS Cost Explorer to automatically delete unused resources across your AWS environment, ensuring that no unnecessary costs are incurred</strong> AWS Cost Explorer does not have the capability to automatically delete unused resources. It is a cost analysis tool that helps you visualize and understand your costs but does not manage or modify your AWS resources directly.</p>\n\n<p><strong>Leverage AWS Trusted Advisor to directly modify and reconfigure resources based on cost optimization recommendations without manual intervention</strong> - AWS Trusted Advisor provides recommendations but does not automatically modify or reconfigure resources. Changes must be made manually based on the insights provided by the tool.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/aws-cost-management/aws-cost-explorer/\">https://aws.amazon.com/aws-cost-management/aws-cost-explorer/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html\">https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Cost Explorer to analyze historical spending patterns, identify cost trends, and forecast future costs to help with budgeting and planning</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Cost Explorer allows you to analyze your past AWS spending, identify cost trends, and forecast future costs based on historical data. This tool is valuable for budgeting and financial planning, helping you make informed decisions about resource allocation and cost management."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Trusted Advisor to receive recommendations for cost optimization, such as identifying underutilized or idle resources, and reserved instance purchasing opportunities</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Trusted Advisor provides actionable recommendations to optimize your AWS environment, including cost-saving suggestions. It identifies opportunities such as underutilized resources, idle instances, and reserved instance purchasing options that can significantly reduce your AWS costs."
      },
      {
        "link": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Cost Explorer to set custom budgets for cost and usage to govern costs across your organization and receive alerts when costs exceed your defined thresholds</strong> You can only use AWS Budgets to set custom budgets for cost and usage to govern costs across your organization and receive alerts when costs exceed your defined thresholds."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Cost Explorer to automatically delete unused resources across your AWS environment, ensuring that no unnecessary costs are incurred</strong> AWS Cost Explorer does not have the capability to automatically delete unused resources. It is a cost analysis tool that helps you visualize and understand your costs but does not manage or modify your AWS resources directly."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Trusted Advisor to directly modify and reconfigure resources based on cost optimization recommendations without manual intervention</strong> - AWS Trusted Advisor provides recommendations but does not automatically modify or reconfigure resources. Changes must be made manually based on the insights provided by the tool."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html",
      "https://aws.amazon.com/aws-cost-management/aws-cost-explorer/",
      "https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html"
    ]
  },
  {
    "id": 58,
    "question": "<p>A financial services company is building a fraud detection model using Amazon SageMaker. The ML engineer receives a 40 MB Apache Parquet file as input data. The file contains several correlated columns that are not needed for the model. The engineer needs to drop these unnecessary columns with the least effort while ensuring the data remains compatible with SageMaker for further preprocessing and model training.</p>\n\n<p>What should the ML engineer do?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Convert the Parquet file to CSV, manually edit the file to remove the columns, and re-upload the file to SageMaker</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Data Wrangler to import the Parquet file, drop the unnecessary columns, and save the transformed data</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Glue to transform the Parquet file and drop the unnecessary columns before importing it into SageMaker</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Write a custom Python script using pandas to load the Parquet file, drop the unnecessary columns, and save it back as a Parquet file</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning (ML)",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use SageMaker Data Wrangler to import the Parquet file, drop the unnecessary columns, and save the transformed data</strong></p>\n\n<p>Amazon SageMaker Data Wrangler (Data Wrangler) is a feature of Amazon SageMaker Studio Classic that provides an end-to-end solution to import, prepare, transform, featurize, and analyze data. You can integrate a Data Wrangler data preparation flow into your machine learning (ML) workflows to simplify and streamline data pre-processing and feature engineering (including dropping unused columns) using little to no coding.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Write a custom Python script using pandas to load the Parquet file, drop the unnecessary columns, and save it back as a Parquet file</strong> - While pandas can handle Parquet files, this approach requires custom scripting, which is more effort than using a built-in tool like SageMaker Data Wrangler designed for such tasks.</p>\n\n<p><strong>Convert the Parquet file to CSV, manually edit the file to remove the columns, and re-upload the file to SageMaker</strong> - Manually editing a file is time-consuming and error-prone. It also adds unnecessary complexity by converting the file format twice (Parquet to CSV and back).</p>\n\n<p><strong>Use AWS Glue to transform the Parquet file and drop the unnecessary columns before importing it into SageMaker</strong> - AWS Glue can perform this transformation, but it is a more complex and time-consuming process compared to SageMaker Data Wrangler, especially for a single file of this size.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-getting-started.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-getting-started.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use SageMaker Data Wrangler to import the Parquet file, drop the unnecessary columns, and save the transformed data</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon SageMaker Data Wrangler (Data Wrangler) is a feature of Amazon SageMaker Studio Classic that provides an end-to-end solution to import, prepare, transform, featurize, and analyze data. You can integrate a Data Wrangler data preparation flow into your machine learning (ML) workflows to simplify and streamline data pre-processing and feature engineering (including dropping unused columns) using little to no coding."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Write a custom Python script using pandas to load the Parquet file, drop the unnecessary columns, and save it back as a Parquet file</strong> - While pandas can handle Parquet files, this approach requires custom scripting, which is more effort than using a built-in tool like SageMaker Data Wrangler designed for such tasks."
      },
      {
        "answer": "",
        "explanation": "<strong>Convert the Parquet file to CSV, manually edit the file to remove the columns, and re-upload the file to SageMaker</strong> - Manually editing a file is time-consuming and error-prone. It also adds unnecessary complexity by converting the file format twice (Parquet to CSV and back)."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue to transform the Parquet file and drop the unnecessary columns before importing it into SageMaker</strong> - AWS Glue can perform this transformation, but it is a more complex and time-consuming process compared to SageMaker Data Wrangler, especially for a single file of this size."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-getting-started.html"
    ]
  },
  {
    "id": 59,
    "question": "<p>You are a machine learning engineer at a company building a recommendation system using deep learning. The system is based on a neural network architecture implemented in TensorFlow. Your team wants to train the model on a large dataset stored in Amazon S3 and leverage the scalability of Amazon SageMaker. You plan to use SageMaker script mode to customize the training process while taking advantage of SageMaker’s managed services.</p>\n\n<p>Which of the following actions is the MOST LIKELY to help you successfully train the model using SageMaker script mode with TensorFlow?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker script mode to automatically convert your Python script into a SageMaker Estimator object without specifying the TensorFlow framework version</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Package your TensorFlow model as a .tar.gz file, upload it to Amazon S3, and use SageMaker script mode to directly deploy the model for training</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a custom Docker container with TensorFlow installed and push it to Amazon ECR, then use SageMaker script mode to launch the training job</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Write a training script in Python that adheres to TensorFlow's Estimator API, upload it to an S3 bucket, and use SageMaker script mode with the built-in TensorFlow container to execute the script</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Write a training script in Python that adheres to TensorFlow's Estimator API, upload it to an S3 bucket, and use SageMaker script mode with the built-in TensorFlow container to execute the script</strong></p>\n\n<p>Script mode enables you to write custom training and inference code while still utilizing common ML framework containers maintained by AWS.</p>\n\n<p>SageMaker supports most of the popular ML frameworks through pre-built containers, and has taken the extra step to optimize them to work especially well on AWS compute and network infrastructure in order to achieve near-linear scaling efficiency. These pre-built containers also provide some additional Python packages, such as Pandas and NumPy, so you can write your own code for training an algorithm. These frameworks also allow you to install any Python package hosted on PyPi by including a requirements.txt file with your training code or to include your own code directories.</p>\n\n<p>For the given use case, you need to utilize the SageMaker script mode and write a Python script that leverages the TensorFlow framework (following the Estimator API or a custom training loop). Upload this script to S3, and specify the TensorFlow framework version in SageMaker’s built-in container. SageMaker will then run your script in the managed environment, handling tasks like data loading and distributed training.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q8-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q8-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/\">https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a custom Docker container with TensorFlow installed and push it to Amazon ECR, then use SageMaker script mode to launch the training job</strong> - While using a custom Docker container is a valid approach, it’s not necessary when using SageMaker script mode with supported frameworks like TensorFlow or PyTorch. SageMaker already provides built-in containers for these frameworks, allowing you to focus on your training script without the overhead of managing custom containers.</p>\n\n<p><strong>Package your TensorFlow model as a .tar.gz file, upload it to Amazon S3, and use SageMaker script mode to directly deploy the model for training</strong> - Packaging the model as a .tar.gz file and uploading it to S3 is relevant for deploying a pre-trained model, not for training a model. SageMaker script mode is intended for running custom training scripts, not directly deploying pre-packaged models for training.</p>\n\n<p><strong>Use SageMaker script mode to automatically convert your Python script into a SageMaker Estimator object without specifying the TensorFlow framework version</strong> - SageMaker script mode does not automatically convert your script into an Estimator object. You must write the script according to the framework’s requirements and specify the desired framework version when launching the training job.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/\">https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html</a></p>\n\n<p><a href=\"https://sagemaker.readthedocs.io/en/v2.27.1/frameworks/tensorflow/deploying_tensorflow_serving.html\">https://sagemaker.readthedocs.io/en/v2.27.1/frameworks/tensorflow/deploying_tensorflow_serving.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Write a training script in Python that adheres to TensorFlow's Estimator API, upload it to an S3 bucket, and use SageMaker script mode with the built-in TensorFlow container to execute the script</strong>"
      },
      {
        "answer": "",
        "explanation": "Script mode enables you to write custom training and inference code while still utilizing common ML framework containers maintained by AWS."
      },
      {
        "answer": "",
        "explanation": "SageMaker supports most of the popular ML frameworks through pre-built containers, and has taken the extra step to optimize them to work especially well on AWS compute and network infrastructure in order to achieve near-linear scaling efficiency. These pre-built containers also provide some additional Python packages, such as Pandas and NumPy, so you can write your own code for training an algorithm. These frameworks also allow you to install any Python package hosted on PyPi by including a requirements.txt file with your training code or to include your own code directories."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you need to utilize the SageMaker script mode and write a Python script that leverages the TensorFlow framework (following the Estimator API or a custom training loop). Upload this script to S3, and specify the TensorFlow framework version in SageMaker’s built-in container. SageMaker will then run your script in the managed environment, handling tasks like data loading and distributed training."
      },
      {
        "link": "https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a custom Docker container with TensorFlow installed and push it to Amazon ECR, then use SageMaker script mode to launch the training job</strong> - While using a custom Docker container is a valid approach, it’s not necessary when using SageMaker script mode with supported frameworks like TensorFlow or PyTorch. SageMaker already provides built-in containers for these frameworks, allowing you to focus on your training script without the overhead of managing custom containers."
      },
      {
        "answer": "",
        "explanation": "<strong>Package your TensorFlow model as a .tar.gz file, upload it to Amazon S3, and use SageMaker script mode to directly deploy the model for training</strong> - Packaging the model as a .tar.gz file and uploading it to S3 is relevant for deploying a pre-trained model, not for training a model. SageMaker script mode is intended for running custom training scripts, not directly deploying pre-packaged models for training."
      },
      {
        "answer": "",
        "explanation": "<strong>Use SageMaker script mode to automatically convert your Python script into a SageMaker Estimator object without specifying the TensorFlow framework version</strong> - SageMaker script mode does not automatically convert your script into an Estimator object. You must write the script according to the framework’s requirements and specify the desired framework version when launching the training job."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html",
      "https://sagemaker.readthedocs.io/en/v2.27.1/frameworks/tensorflow/deploying_tensorflow_serving.html"
    ]
  },
  {
    "id": 60,
    "question": "<p>You are a data scientist at a healthcare company that has deployed a machine learning model to predict patient readmission rates. The model plays a crucial role in optimizing patient care and managing hospital resources. After several months in production, the medical team has noticed that the model’s predictions seem less accurate than before, leading to concerns about data quality and model performance. To ensure that the model continues to deliver reliable predictions, you need to implement techniques to monitor both data quality and model performance continuously.</p>\n\n<p>Which of the following approaches is the MOST EFFECTIVE for monitoring data quality and model performance in this scenario?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Schedule periodic retraining of the model on the latest data to ensure it remains accurate, without additional monitoring of data quality or performance metrics</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up a dashboard that tracks model performance metrics, such as precision, recall, and AUC, and use version control to monitor changes in the model's code and training data over time</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Perform manual checks on a sample of the input data each week to ensure data quality and manually track model performance metrics in a spreadsheet for analysis</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Implement data validation rules to check for missing values, outliers, and distribution changes in the input data before feeding it to the model, and use model performance metrics such as accuracy and F1 score to monitor the model's output</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement data validation rules to check for missing values, outliers, and distribution changes in the input data before feeding it to the model, and use model performance metrics such as accuracy and F1 score to monitor the model's output</strong></p>\n\n<p>You may encounter data quality issues such as missing values or having the wrong data type. These issues may not be discovered until quite late in the process after training a ML model. So, you need to perform data validation to proactively check for issues in your data and provides guidance on resolutions. Implementing data validation rules helps ensure that the input data remains clean and consistent, which is crucial for maintaining model performance. By using better quality data, you will end up with a better performing ML model.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q30-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q30-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/machine-learning/identifying-and-avoiding-common-data-issues-while-building-no-code-ml-models-with-amazon-sagemaker-canvas/\">https://aws.amazon.com/blogs/machine-learning/identifying-and-avoiding-common-data-issues-while-building-no-code-ml-models-with-amazon-sagemaker-canvas/</a><p></p>\n\n<p>Monitoring model performance metrics like accuracy and F1 score provides real-time insights into how well the model is performing, allowing you to detect any degradation early and take corrective actions.</p>\n\n<p>Key metrics to measure machine learning model performance:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q30-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q30-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q30-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q30-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Perform manual checks on a sample of the input data each week to ensure data quality and manually track model performance metrics in a spreadsheet for analysis</strong> - While manual checks can catch some issues, they are not scalable or reliable for continuous monitoring. Manually tracking performance metrics is also prone to human error and can delay the identification of problems.</p>\n\n<p><strong>Set up a dashboard that tracks model performance metrics, such as precision, recall, and AUC, and use version control to monitor changes in the model's code and training data over time</strong> - A dashboard is useful for visualizing performance metrics, but it does not directly address data quality. Additionally, while version control is important for tracking changes, it does not actively monitor data quality or model performance.</p>\n\n<p><strong>Schedule periodic retraining of the model on the latest data to ensure it remains accurate, without additional monitoring of data quality or performance metrics</strong> - Periodic retraining is important but does not replace the need for continuous monitoring of data quality and performance. Without monitoring, you may not know when retraining is needed, potentially leading to periods of poor model performance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/identifying-and-avoiding-common-data-issues-while-building-no-code-ml-models-with-amazon-sagemaker-canvas/\">https://aws.amazon.com/blogs/machine-learning/identifying-and-avoiding-common-data-issues-while-building-no-code-ml-models-with-amazon-sagemaker-canvas/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Implement data validation rules to check for missing values, outliers, and distribution changes in the input data before feeding it to the model, and use model performance metrics such as accuracy and F1 score to monitor the model's output</strong>"
      },
      {
        "answer": "",
        "explanation": "You may encounter data quality issues such as missing values or having the wrong data type. These issues may not be discovered until quite late in the process after training a ML model. So, you need to perform data validation to proactively check for issues in your data and provides guidance on resolutions. Implementing data validation rules helps ensure that the input data remains clean and consistent, which is crucial for maintaining model performance. By using better quality data, you will end up with a better performing ML model."
      },
      {
        "link": "https://aws.amazon.com/blogs/machine-learning/identifying-and-avoiding-common-data-issues-while-building-no-code-ml-models-with-amazon-sagemaker-canvas/"
      },
      {
        "answer": "",
        "explanation": "Monitoring model performance metrics like accuracy and F1 score provides real-time insights into how well the model is performing, allowing you to detect any degradation early and take corrective actions."
      },
      {
        "answer": "",
        "explanation": "Key metrics to measure machine learning model performance:"
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html",
        "answer": "",
        "explanation": "via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Perform manual checks on a sample of the input data each week to ensure data quality and manually track model performance metrics in a spreadsheet for analysis</strong> - While manual checks can catch some issues, they are not scalable or reliable for continuous monitoring. Manually tracking performance metrics is also prone to human error and can delay the identification of problems."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up a dashboard that tracks model performance metrics, such as precision, recall, and AUC, and use version control to monitor changes in the model's code and training data over time</strong> - A dashboard is useful for visualizing performance metrics, but it does not directly address data quality. Additionally, while version control is important for tracking changes, it does not actively monitor data quality or model performance."
      },
      {
        "answer": "",
        "explanation": "<strong>Schedule periodic retraining of the model on the latest data to ensure it remains accurate, without additional monitoring of data quality or performance metrics</strong> - Periodic retraining is important but does not replace the need for continuous monitoring of data quality and performance. Without monitoring, you may not know when retraining is needed, potentially leading to periods of poor model performance."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/machine-learning/identifying-and-avoiding-common-data-issues-while-building-no-code-ml-models-with-amazon-sagemaker-canvas/",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html"
    ]
  },
  {
    "id": 61,
    "question": "<p>You are tasked with preparing a dataset for a machine learning model using SageMaker Data Wrangler. You need to perform a transformation that removes outliers from your dataset.</p>\n\n<p>Which of the following actions in SageMaker Data Wrangler would you use to accomplish this task?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Format string</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Filter</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Impute</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>One-hot Encode</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning (ML)",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Filter</strong></p>\n\n<p>You can filter the data in your columns using Data Wrangler. For a column that has the categories, male and female, you can filter out all the male values or all the female values.</p>\n\n<p>You can also add multiple filters. The filters can be applied across multiple columns or the same column. For example, if you're creating a column that only has values within a certain range, you add two different filters. One filter specifies that the column must have values greater than the value that you provide. The other filter specifies that the column must have values less than the value that you provide.</p>\n\n<p>For the given use case, Filter transform will help remove outliers.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q48-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q48-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-filter-data\">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-filter-data</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Impute</strong> - The <code>Impute missing</code> transform is used to create a new column that contains imputed values when missing values are found in input categorical or numerical data. Impute cannot remove outliers.</p>\n\n<p><strong>One-hot Encode</strong> - Encoding categorical data is the process of creating a numerical representation for categories. One-hot encode transformation helps encode data but cannot remove outliers.</p>\n\n<p><strong>Format string</strong> - The Format string transforms contain standard string formatting operations. For example, you can use these operations to remove special characters, normalize string lengths, and update string casing. The format string cannot remove outliers.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Filter</strong>"
      },
      {
        "answer": "",
        "explanation": "You can filter the data in your columns using Data Wrangler. For a column that has the categories, male and female, you can filter out all the male values or all the female values."
      },
      {
        "answer": "",
        "explanation": "You can also add multiple filters. The filters can be applied across multiple columns or the same column. For example, if you're creating a column that only has values within a certain range, you add two different filters. One filter specifies that the column must have values greater than the value that you provide. The other filter specifies that the column must have values less than the value that you provide."
      },
      {
        "answer": "",
        "explanation": "For the given use case, Filter transform will help remove outliers."
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-filter-data"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Impute</strong> - The <code>Impute missing</code> transform is used to create a new column that contains imputed values when missing values are found in input categorical or numerical data. Impute cannot remove outliers."
      },
      {
        "answer": "",
        "explanation": "<strong>One-hot Encode</strong> - Encoding categorical data is the process of creating a numerical representation for categories. One-hot encode transformation helps encode data but cannot remove outliers."
      },
      {
        "answer": "",
        "explanation": "<strong>Format string</strong> - The Format string transforms contain standard string formatting operations. For example, you can use these operations to remove special characters, normalize string lengths, and update string casing. The format string cannot remove outliers."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-filter-data",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html"
    ]
  },
  {
    "id": 62,
    "question": "<p>A publishing company wants to automatically identify and extract meaningful, unique keywords from a large collection of documents using AWS services.</p>\n\n<p>What solution should the ML engineer implement to achieve this goal with minimal operational overhead? </p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage Amazon Rekognition to search and extract insights about the entities and Key phrases present in the documents</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Leverage Amazon Textract to gather insights about the entities and Key phrases present in the documents</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Leverage Amazon Kendra to search and extract meaningful, unique keywords present in the documents</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage Amazon Comprehend custom entity recognition and key phrase extraction to identify and extract relevant keywords</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage Amazon Comprehend custom entity recognition and key phrase extraction to identify and extract relevant keywords</strong></p>\n\n<p>Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find meaning and insights in text. Amazon Comprehend can analyze a document or set of documents to gather insights about it. Some of the insights that Amazon Comprehend develops about a document include:</p>\n\n<ol>\n<li><p>Entities – Amazon Comprehend returns a list of entities, such as people, places, and locations, identified in a document.</p></li>\n<li><p>Events – Amazon Comprehend detects speciﬁc types of events and related details.</p></li>\n<li><p>Key phrases – Amazon Comprehend extracts key phrases that appear in a document. For example, a document about a basketball game might return the names of the teams, the name of the venue, and the final score.</p></li>\n<li><p>Personally identifiable information (PII) – Amazon Comprehend analyzes documents to detect personal data that identify an individual, such as an address, bank account number, or phone number.</p></li>\n<li><p>Dominant language – Amazon Comprehend identifies the dominant language in a document. Amazon Comprehend can identify 100 languages.</p></li>\n<li><p>Sentiment – Amazon Comprehend determines the dominant sentiment of a document. Sentiment can be positive, neutral, negative, or mixed.</p></li>\n<li><p>Targeted Sentiment – Amazon Comprehend determines the sentiment of specific entities mentioned in a document. The sentiment of each mention can be positive, neutral, negative, or mixed.</p></li>\n<li><p>Syntax analysis – Amazon Comprehend parses each word in your document and determines the part of speech for the word. For example, in the sentence \"It is raining today in Seattle,\" \"it\" is identified as a pronoun, \"raining\" is identified as a verb, and \"Seattle\" is identified as a proper noun.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon Textract to gather insights about the entities and Key phrases present in the documents</strong> - Amazon Textract extracts text, handwriting, and data from scanned documents, invoices, letters, and other documents. It supports tabular data and many languages and dialects. Amazon Textract and Amazon Comprehend are often used together for various applications. However, for the given use case, the input is in the form of documents, so Amazon Comprehend alone is sufficient to identify and extract key phrases. Additionally, Amazon Comprehend provides a custom entity recognition feature for more tailored extractions.</p>\n\n<p><strong>Leverage Amazon Rekognition to search and extract insights about the entities and Key phrases present in the documents</strong> - Amazon Rekognition can detect text in images and videos. It can then convert the detected text into machine-readable text. Uses computer vision to analyze images and videos at scale. It can perform object and scene detection, facial analysis, and text detection. Rekognition can also be used for content moderation.</p>\n\n<p>We use Amazon Comprehend for processing and understanding text data, such as analyzing customer reviews, extracting keywords, or identifying entities from text. On the other hand, Amazon Rekognition is used when your focus is on analyzing visual content like detecting objects in images, identifying faces, or moderating videos for inappropriate content.</p>\n\n<p><strong>Leverage Amazon Kendra to search and extract meaningful, unique keywords present in the documents</strong> - Amazon Kendra provides search and Retrieval Augmented Generation (RAG) functionality for your application. It indexes your documents directly—or from your third-party document repository—and intelligently serves relevant information to your users. You can use Amazon Kendra to create an updatable index of documents of a variety of types.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/comprehend/latest/dg/concepts-insights.html\">https://docs.aws.amazon.com/comprehend/latest/dg/concepts-insights.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Comprehend custom entity recognition and key phrase extraction to identify and extract relevant keywords</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find meaning and insights in text. Amazon Comprehend can analyze a document or set of documents to gather insights about it. Some of the insights that Amazon Comprehend develops about a document include:"
      },
      {}
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Textract to gather insights about the entities and Key phrases present in the documents</strong> - Amazon Textract extracts text, handwriting, and data from scanned documents, invoices, letters, and other documents. It supports tabular data and many languages and dialects. Amazon Textract and Amazon Comprehend are often used together for various applications. However, for the given use case, the input is in the form of documents, so Amazon Comprehend alone is sufficient to identify and extract key phrases. Additionally, Amazon Comprehend provides a custom entity recognition feature for more tailored extractions."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Rekognition to search and extract insights about the entities and Key phrases present in the documents</strong> - Amazon Rekognition can detect text in images and videos. It can then convert the detected text into machine-readable text. Uses computer vision to analyze images and videos at scale. It can perform object and scene detection, facial analysis, and text detection. Rekognition can also be used for content moderation."
      },
      {
        "answer": "",
        "explanation": "We use Amazon Comprehend for processing and understanding text data, such as analyzing customer reviews, extracting keywords, or identifying entities from text. On the other hand, Amazon Rekognition is used when your focus is on analyzing visual content like detecting objects in images, identifying faces, or moderating videos for inappropriate content."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Kendra to search and extract meaningful, unique keywords present in the documents</strong> - Amazon Kendra provides search and Retrieval Augmented Generation (RAG) functionality for your application. It indexes your documents directly—or from your third-party document repository—and intelligently serves relevant information to your users. You can use Amazon Kendra to create an updatable index of documents of a variety of types."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/comprehend/latest/dg/concepts-insights.html"
    ]
  },
  {
    "id": 63,
    "question": "<p>A fashion retailer wants to develop a machine learning model to predict the popularity of clothing designs. The dataset includes categorical data about the primary fabric type of each design, such as \"Cotton,\" \"Polyester,\" and \"Silk.\" An ML engineer is preparing the data for a neural network model and needs to preprocess the fabric type information appropriately.</p>\n\n<p>Which technique should the ML engineer use for feature engineering?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Perform min-max normalization on the categorical fabric type data to scale the values between 0 and 1 for compatibility with the model</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Apply one-hot encoding to convert the categorical fabric type data into binary vectors that the neural network can process</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use PCA (Principal Component Analysis) to reduce the dimensionality of the categorical fabric type data before feeding it into the neural network</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use label encoding to assign unique integer values to each fabric type and input these integers directly into the neural network</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning (ML)",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Apply one-hot encoding to convert the categorical fabric type data into binary vectors that the neural network can process</strong></p>\n\n<p>One-hot encoding is the most appropriate method for handling categorical data in this scenario because:</p>\n\n<p>It converts each category into a binary vector representation, ensuring compatibility with the neural network.</p>\n\n<p>It treats each category as independent, avoiding the introduction of artificial ordinal relationships between the fabric types.</p>\n\n<p>This approach ensures the neural network interprets the fabric types correctly without introducing unintended biases.</p>\n\n<p>For example, if the dataset contains the categories \"Cotton,\" \"Polyester,\" and \"Silk,\" one-hot encoding represents them as:</p>\n\n<p>Cotton: [1, 0, 0]</p>\n\n<p>Polyester: [0, 1, 0]</p>\n\n<p>Silk: [0, 0, 1]</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use label encoding to assign unique integer values to each fabric type and input these integers directly into the neural network</strong> - Label encoding assigns integers to categories (e.g., Cotton = 1, Polyester = 2). This implies an ordinal relationship between fabric types, which does not exist and could mislead the neural network.</p>\n\n<p><strong>Perform min-max normalization on the categorical fabric type data to scale the values between 0 and 1 for compatibility with the model</strong> - Min-max normalization is used for continuous numerical data, not for categorical data like colors. This technique does not apply to this problem.</p>\n\n<p><strong>Use PCA (Principal Component Analysis) to reduce the dimensionality of the categorical fabric type data before feeding it into the neural network</strong> - PCA applies to numerical data and cannot be directly used on categorical data without prior encoding.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.ONE_HOT_ENCODING.html\">https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.ONE_HOT_ENCODING.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Apply one-hot encoding to convert the categorical fabric type data into binary vectors that the neural network can process</strong>"
      },
      {
        "answer": "",
        "explanation": "One-hot encoding is the most appropriate method for handling categorical data in this scenario because:"
      },
      {
        "answer": "",
        "explanation": "It converts each category into a binary vector representation, ensuring compatibility with the neural network."
      },
      {
        "answer": "",
        "explanation": "It treats each category as independent, avoiding the introduction of artificial ordinal relationships between the fabric types."
      },
      {
        "answer": "",
        "explanation": "This approach ensures the neural network interprets the fabric types correctly without introducing unintended biases."
      },
      {
        "answer": "",
        "explanation": "For example, if the dataset contains the categories \"Cotton,\" \"Polyester,\" and \"Silk,\" one-hot encoding represents them as:"
      },
      {
        "answer": "",
        "explanation": "Cotton: [1, 0, 0]"
      },
      {
        "answer": "",
        "explanation": "Polyester: [0, 1, 0]"
      },
      {
        "answer": "",
        "explanation": "Silk: [0, 0, 1]"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use label encoding to assign unique integer values to each fabric type and input these integers directly into the neural network</strong> - Label encoding assigns integers to categories (e.g., Cotton = 1, Polyester = 2). This implies an ordinal relationship between fabric types, which does not exist and could mislead the neural network."
      },
      {
        "answer": "",
        "explanation": "<strong>Perform min-max normalization on the categorical fabric type data to scale the values between 0 and 1 for compatibility with the model</strong> - Min-max normalization is used for continuous numerical data, not for categorical data like colors. This technique does not apply to this problem."
      },
      {
        "answer": "",
        "explanation": "<strong>Use PCA (Principal Component Analysis) to reduce the dimensionality of the categorical fabric type data before feeding it into the neural network</strong> - PCA applies to numerical data and cannot be directly used on categorical data without prior encoding."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.ONE_HOT_ENCODING.html"
    ]
  },
  {
    "id": 64,
    "question": "<p>Which of the following is correct regarding the training set, validation set, and test set used in the context of machine learning? (Select two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Test sets are optional</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Validation set is used to determine how well the model generalizes</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Test set is used to determine how well the model generalizes</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Validation sets are optional</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Test set is used for hyperparameter tuning</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning (ML)",
    "explanation": "<p>Correct options:</p>\n\n<p>Data used for ML is typically split into the following datasets:</p>\n\n<p>The training set is used to train the model, the validation set is used for tuning hyperparameters and selecting the best model during the training process, and the test set is used for evaluating the final performance of the model on unseen data.</p>\n\n<p><strong>Validation sets are optional</strong></p>\n\n<p>The validation set introduces new data to the trained model. You can use a validation set to periodically measure model performance as training is happening and also tune any hyperparameters of the model. However, validation datasets are optional.</p>\n\n<p><strong>Test set is used to determine how well the model generalizes</strong></p>\n\n<p>The test set is used on the final trained model to assess its performance on unseen data. This helps determine how well the model generalizes.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Test set is used for hyperparameter tuning</strong> - The test set is used for evaluating the final performance of the model on unseen data.</p>\n\n<p><strong>Test sets are optional</strong> - Only validation sets are optional.</p>\n\n<p><strong>Validation set is used to determine how well the model generalizes</strong> - Only the test set is used to determine how well the model generalizes.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/create-train-test-and-validation-splits-on-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/\">https://aws.amazon.com/blogs/machine-learning/create-train-test-and-validation-splits-on-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Data used for ML is typically split into the following datasets:"
      },
      {
        "answer": "",
        "explanation": "The training set is used to train the model, the validation set is used for tuning hyperparameters and selecting the best model during the training process, and the test set is used for evaluating the final performance of the model on unseen data."
      },
      {
        "answer": "",
        "explanation": "<strong>Validation sets are optional</strong>"
      },
      {
        "answer": "",
        "explanation": "The validation set introduces new data to the trained model. You can use a validation set to periodically measure model performance as training is happening and also tune any hyperparameters of the model. However, validation datasets are optional."
      },
      {
        "answer": "",
        "explanation": "<strong>Test set is used to determine how well the model generalizes</strong>"
      },
      {
        "answer": "",
        "explanation": "The test set is used on the final trained model to assess its performance on unseen data. This helps determine how well the model generalizes."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Test set is used for hyperparameter tuning</strong> - The test set is used for evaluating the final performance of the model on unseen data."
      },
      {
        "answer": "",
        "explanation": "<strong>Test sets are optional</strong> - Only validation sets are optional."
      },
      {
        "answer": "",
        "explanation": "<strong>Validation set is used to determine how well the model generalizes</strong> - Only the test set is used to determine how well the model generalizes."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/machine-learning/create-train-test-and-validation-splits-on-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/"
    ]
  },
  {
    "id": 65,
    "question": "<p>Which of the following is correct regarding the techniques used to improve the performance of a Foundation Model (FM)?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Both Fine-tuning and Retrieval-augmented generation (RAG) change the weights of the FM</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Neither Fine-tuning nor Retrieval-augmented generation (RAG) changes the weights of the FM</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Fine-tuning does not change the weights of the FM whereas Retrieval-augmented generation (RAG) changes the weights of the FM</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Fine-tuning changes the weights of the FM whereas Retrieval-augmented generation (RAG) does not change the weights of the FM</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Fine-tuning changes the weights of the FM whereas Retrieval-augmented generation (RAG) does not change the weights of the FM</strong></p>\n\n<p>Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model.</p>\n\n<p>Another recommended way to first customize a foundation model to a specific use case is through prompt engineering. Providing your foundation model with well-engineered, context-rich prompts can help achieve desired results without any fine-tuning or changing of model weights.</p>\n\n<p>Lastly, fine-tuning is a customization method for FMs that involves further training and does change the weights of your model.</p>\n\n<p>Retrieval Augmented Generation (RAG) allows you to customize a model’s responses when you want the model to consider new knowledge or up-to-date information. When your data changes frequently, like inventory or pricing, it’s not practical to fine-tune and update the model while it’s serving user queries.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q51-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q51-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/machine-learning/best-practices-to-build-generative-ai-applications-on-aws/\">https://aws.amazon.com/blogs/machine-learning/best-practices-to-build-generative-ai-applications-on-aws/</a><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q51-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-mla-pt/assets/pt2-q51-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning.html</a><p></p>\n\n<p>Exam Alert:</p>\n\n<p>Note the following regarding the techniques to improve the performance of a Foundation Model (FM):</p>\n\n<p>Prompt engineering does NOT change the weights of the FM.\nRetrieval-Augmented Generation (RAG) does NOT change the weights of the FM.\nFine-tuning DOES change the weights of the FM.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Neither Fine-tuning nor Retrieval-augmented generation (RAG) changes the weights of the FM</strong></p>\n\n<p><strong>Both Fine-tuning and Retrieval-augmented generation (RAG) change the weights of the FM</strong></p>\n\n<p><strong>Fine-tuning does not change the weights of the FM whereas Retrieval-augmented generation (RAG) changes the weights of the FM</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/best-practices-to-build-generative-ai-applications-on-aws/\">https://aws.amazon.com/blogs/machine-learning/best-practices-to-build-generative-ai-applications-on-aws/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Fine-tuning changes the weights of the FM whereas Retrieval-augmented generation (RAG) does not change the weights of the FM</strong>"
      },
      {
        "answer": "",
        "explanation": "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model."
      },
      {
        "answer": "",
        "explanation": "Another recommended way to first customize a foundation model to a specific use case is through prompt engineering. Providing your foundation model with well-engineered, context-rich prompts can help achieve desired results without any fine-tuning or changing of model weights."
      },
      {
        "answer": "",
        "explanation": "Lastly, fine-tuning is a customization method for FMs that involves further training and does change the weights of your model."
      },
      {
        "answer": "",
        "explanation": "Retrieval Augmented Generation (RAG) allows you to customize a model’s responses when you want the model to consider new knowledge or up-to-date information. When your data changes frequently, like inventory or pricing, it’s not practical to fine-tune and update the model while it’s serving user queries."
      },
      {
        "link": "https://aws.amazon.com/blogs/machine-learning/best-practices-to-build-generative-ai-applications-on-aws/"
      },
      {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning.html"
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "answer": "",
        "explanation": "Note the following regarding the techniques to improve the performance of a Foundation Model (FM):"
      },
      {
        "answer": "",
        "explanation": "Prompt engineering does NOT change the weights of the FM.\nRetrieval-Augmented Generation (RAG) does NOT change the weights of the FM.\nFine-tuning DOES change the weights of the FM."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Neither Fine-tuning nor Retrieval-augmented generation (RAG) changes the weights of the FM</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Both Fine-tuning and Retrieval-augmented generation (RAG) change the weights of the FM</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Fine-tuning does not change the weights of the FM whereas Retrieval-augmented generation (RAG) changes the weights of the FM</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/machine-learning/best-practices-to-build-generative-ai-applications-on-aws/",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning.html"
    ]
  }
]