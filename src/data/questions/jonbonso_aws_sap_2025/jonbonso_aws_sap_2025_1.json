[
  {
    "id": 1,
    "question": "<p>A company has several AWS accounts that are managed using AWS Organizations. The company created only one organizational unit (OU) so all child accounts are members of the Production OU. The Solutions Architects control access to certain AWS services using SCPs that define the restricted services. The SCPs are attached at the root of the organization so that they will be applied to all AWS accounts under the organization. The company recently acquired a small business firm and its existing AWS account was invited to join the organization. Upon onboarding, the administrators of the small business firm cannot apply the required AWS Config rules to meet the parent company’s security policies.</p><p>Which of the following options will allow the administrators to update the AWS Config rules on their AWS account without introducing long-term management overhead?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Instead of using a “deny list” to AWS services on the organization’s root SCPs, use an “allow list” to allow only the required AWS services. Temporarily add the AWS Config service on the “allow list” for the principals of the new account and make the required changes.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add the new account to a temporary Onboarding organization unit (OU) that has an attached SCP allowing changes to AWS Config. Perform the needed changes while on this temporary OU before moving the new account to Production OU.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Update the SCPs applied in the root of the AWS organization and remove the rule that restricts changes to the AWS Config service. Deploy a new AWS Service Catalog to the whole organization containing the company’s AWS Config policies.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Remove the SCPs on the organization’s root and apply them to the Production OU instead. Create a temporary Onboarding OU that has an attached SCP allowing changes to AWS Config. Add the new account to this temporary OU and make the required changes before moving it to Production OU.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Organizations</strong> helps you centrally manage and govern your environment as you grow and scale your AWS resources. Using AWS Organizations, you can programmatically create new AWS accounts and allocate resources, group accounts to organize your workflows, apply policies to accounts or groups for governance, and simplify billing by using a single payment method for all of your accounts.</p><p>With AWS Organizations, you can consolidate multiple AWS accounts into an organization that you create and centrally manage. You can create member accounts and invite existing accounts to join your organization. You can organize those accounts into groups and attach policy-based controls.</p><p><strong><img src=\"https://media.tutorialsdojo.com/sap_aws_orgganization_nested.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></strong></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><strong><img src=\"https://media.tutorialsdojo.com/sap_aws_orgganization_nested.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></strong></div><p></p><p><strong>Service control policies (SCPs)</strong> are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines. SCPs are available only in an organization that has all features enabled.</p><p>An SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user. Any account has only those permissions allowed by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the <code><strong>AdministratorAccess</strong> </code>IAM policy with */* permissions to the user.</p><p>AWS strongly recommends that you don't attach SCPs to the root of your organization without thoroughly testing the impact that the policy has on accounts. Instead, create an OU that you can move your accounts into one at a time, or at least in small numbers, to ensure that you don't inadvertently lock users out of key services.</p><p>Therefore, the correct answer is: <strong>Remove the SCPs on the organization’s root and apply them to the Production OU instead. Create a temporary Onboarding OU that has an attached SCP allowing changes to AWS Config. Add the new account to this temporary OU and make the required changes before moving it to Production OU.</strong> It is not recommended to attach the SCPs to the root of the organization, so it is better to move all the SCPs to the Production OU. This way, the temporary Onboarding OU can have an independent SCP to allow the required changes on AWS Config. Then, you can move the new AWS account to the Production OU.</p><p>The option that says: <strong>Update the SCPs applied in the root of the AWS organization and remove the rule that restricts changes to the AWS Config service. Deploy a new AWS Service Catalog to the whole organization containing the company’s AWS Config policies</strong> is incorrect. Although AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS, this will cause possible problems in the future for the administrators. Removing the AWS Config restriction on the root of the AWS organization's SCP will allow all Admins on all AWS accounts to manage/change/update their own AWS Config rules.</p><p>The option that says: <strong>Add the new account to a temporary Onboarding organization unit (OU) that has an attached SCP allowing changes to AWS Config. Perform the needed changes while on this temporary OU before moving the new account to Production OU</strong> is incorrect. If the SCP applied on the organization's root has a \"deny\" permission, all OUs under the organization will inherit that rule. You cannot override an explicit \"deny\" permission with an explicit \"allow\" applied to the temporary Onboarding OU.</p><p>The option that says: <strong>Instead of using a “deny list” to AWS services on the organization’s root SCPs, use an “allow list” to allow only the required AWS services. Temporarily add the AWS Config service on the “allow list” for the principals of the new account and make the required changes</strong> is incorrect. This is possible, however, it will cause more management problems in the future as you will have to update the \"allow list\" for any service that users may require in the future.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/controltower/latest/userguide/organizations.html\">https://docs.aws.amazon.com/controltower/latest/userguide/organizations.html</a></p><p><a href=\"https://aws.amazon.com/organizations/\">https://aws.amazon.com/organizations/</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p><p><br></p><p><strong>Check out these AWS Organizations and SCP Comparison Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/controltower/latest/userguide/organizations.html",
      "https://aws.amazon.com/organizations/",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
      "https://tutorialsdojo.com/aws-organizations/?src=udemy",
      "https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy"
    ]
  },
  {
    "id": 2,
    "question": "<p>An IT consulting company has multiple AWS accounts for its teams and departments that have been grouped into several organizational units (OUs) using AWS Organizations. The lead solutions architect received a report from the security team that there was a suspected breach in one of the environments wherein a third-party AWS account was suddenly added to the AWS Organization without any prior approval. The external account has high-level access privileges to the accounts that the company owns. Fortunately, no detrimental action was performed yet.</p><p>Which of the following actions should the solutions architect take to properly set up a monitoring system that notifies for any changes to the company AWS accounts? (Select TWO.)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a CloudWatch Dashboard to monitor any changes to your organizations and create an SNS topic that would send you a notification.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a trail in Amazon CloudTrail to capture all API calls to your AWS Organizations, including calls from the AWS Organizations console and from code calls to the AWS Organizations APIs. Use Amazon EventBridge and SNS to raise events when administrator-specified actions occur in an organization and send a notification to you.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure AWS Control Tower to manage and monitor all child accounts under the organization. Use Amazon Inspector to analyze any possible breach and notify the administrators using AWS SNS.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Config to monitor the compliance of your AWS Organizations. Set up an SNS Topic or Amazon EventBridge that will send alerts to you for any changes.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Monitor all changes to your organization using Systems Manager and use Amazon EventBridge to notify you of any new activity to your account.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p><strong>AWS Organizations</strong> can work with CloudWatch Events to raise events when administrator-specified actions occur in an organization. For example, because of the sensitivity of such actions, most administrators would want to be warned every time someone creates a new account in the organization or when an administrator of a member account attempts to leave the organization. You can configure CloudWatch Events rules that look for these actions and then send the generated events to administrator-defined targets. Targets can be an Amazon SNS topic that emails or text messages its subscribers. Combining this with Amazon CloudTrail, you can set an event to trigger whenever a matching API call is received.</p><p>Multi-account, multi-region data aggregation in <strong>AWS Config</strong> enables you to aggregate AWS Config data from multiple accounts and regions into a single account. Multi-account, multi-region data aggregation is useful for central IT administrators to monitor compliance for multiple AWS accounts in the enterprise. An aggregator is a new resource type in AWS Config that collects AWS Config data from multiple source accounts and regions. Create an aggregator in the Region where you want to see the aggregated AWS Config data. While creating an aggregator, you can choose to add either individual account IDs or your organization.</p><p><img src=\"https://media.tutorialsdojo.com/sap_organization_cw_events.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_organization_cw_events.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the following options are the correct answers:</p><p><strong>- Create a trail in Amazon CloudTrail to capture all API calls to your AWS Organizations, including calls from the AWS Organizations console and from code calls to the AWS Organizations APIs. Use Amazon EventBridge and SNS to raise events when administrator-specified actions occur in an organization and send a notification to you.<br></strong></p><p><strong>- Use AWS Config to monitor the compliance of your AWS Organizations. Set up an SNS Topic or Amazon EventBridge that will send alerts to you for any changes.<br></strong></p><p>The option that says: <strong>Monitor all changes to your organization using Systems Manager and use Amazon EventBridge to notify you of any new activity to your account</strong> is incorrect. AWS Systems Manager is a collection of capabilities for configuring and managing your Amazon EC2 instances, on-premises servers and virtual machines, and other AWS resources at scale. This can't be used to monitor the changes to the setup of AWS Organizations.</p><p>The option that says: <strong>Setting up a CloudWatch Dashboard to monitor any changes to your organizations and creating an SNS topic that would send you a notification</strong> is incorrect because a CloudWatch Dashboard is primarily used to monitor your AWS resources and not the configuration of your AWS Organizations. Although you can enable sharing of all CloudWatch Events across all accounts in your organization, this can't be used to monitor if there is a new AWS account added to your AWS Organizations. Most of the time, the Amazon EventBridge service is primarily used to monitor your AWS resources and the applications you run on AWS in real-time.</p><p>The option that says:<strong> Configure AWS Control Tower to manage and monitor all child accounts under the organization. Use Amazon Inspector to analyze any possible breach and notify the administrators using AWS SNS</strong> is incorrect. AWS Control Tower can send logs to CloudWatch and CloudTrail for monitoring AWS accounts in the organization. However, Amazon Inspector is used as an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities, not for monitoring events on individual AWS accounts.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_monitoring.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_monitoring.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_tutorials_cwe.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_tutorials_cwe.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html\">https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html</a></p><p><br></p><p><strong>Check out these AWS Organizations and AWS Config Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p><p><a href=\"https://tutorialsdojo.com/multi-account-multi-region-data-aggregation-on-aws-config/?src=udemy\">https://tutorialsdojo.com/multi-account-multi-region-data-aggregation-on-aws-config/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_monitoring.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_tutorials_cwe.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html",
      "https://tutorialsdojo.com/aws-organizations/?src=udemy",
      "https://tutorialsdojo.com/multi-account-multi-region-data-aggregation-on-aws-config/?src=udemy"
    ]
  },
  {
    "id": 3,
    "question": "<p>A company wants to host its internal web application in AWS. The front-end uses Docker containers and it connects to a MySQL instance as the backend database. The company plans to use AWS-managed container services to reduce the overhead in managing the servers. The application should allow employees to access company documents, which are accessed frequently for the first 3 months and then rarely after that. As part of the company policy, these documents must be retained for at least five years. Because this is an internal web application, the company wants to have the lowest possible cost.</p><p>Which of the following implementations is the most cost-effective solution?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the Docker containers using Amazon Elastic Container Service (ECS) with Amazon EC2 On-Demand instances. Use On-Demand instances as well for the Amazon RDS database and its read replicas. Create an Amazon EFS volume that is mounted on the EC2 instances to store the company documents. Create a cron job that will copy the documents to Amazon S3 Glacier after three months and then create a bucket lifecycle policy that will delete objects older than five years.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy the Docker containers using Amazon Elastic Container Service (ECS) with Amazon EC2 Spot Instances. Ensure that Spot Instance draining is enabled on the ECS agent config. Use Reserved instance for the Amazon RDS database and its read replicas. Create an encrypted Amazon S3 bucket to store the company documents. Create a bucket lifecycle policy that will move the documents to Amazon S3 Glacier after three months and will delete objects older than five years.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Deploy the Docker containers using Amazon Elastic Container Service (ECS) with Amazon EC2 Spot Instances. Use Spot instances for the Amazon RDS database and its read replicas. Create an encrypted ECS volume on the EC2 hosts that is shared with the containers to store the company documents. Set up a cron job that will delete the files after five years.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the Docker containers using Amazon Elastic Kubernetes Service (EKS)with auto-scaling enabled. Use Amazon EC2 Spot instances for the EKS cluster to further reduce costs. Use On-Demand instances for the Amazon RDS database and its read replicas. Create an encrypted Amazon S3 bucket to store the company documents. Create a bucket lifecycle policy that will move the documents to Amazon S3 Glacier after three months and will delete objects older than five years.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>A <strong>Spot Instance</strong> is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone is set by Amazon EC2, and adjusted gradually based on the long-term supply of and demand for Spot Instances. You can register Spot Instances to your Amazon ECS clusters. Amazon EC2 terminates, stops, or hibernates your Spot Instance when the Spot price exceeds the maximum price for your request or capacity is no longer available. Amazon EC2 provides a Spot Instance interruption notice, which gives the instance a two-minute warning before it is interrupted. If Amazon ECS Spot Instance draining is enabled on the instance, ECS receives the Spot Instance interruption notice and places the instance in DRAINING status.</p><p>When a container instance is set to <code>DRAINING</code>, Amazon ECS prevents new tasks from being scheduled for placement on the container instance. Service tasks on the draining container instance that are in the PENDING state are stopped immediately. If there are container instances in the cluster that are available, replacement service tasks are started on them. Spot Instance draining is disabled by default and must be manually enabled by adding the line <code>ECS_ENABLE_SPOT_INSTANCE_DRAINING=true</code> on your <code>/etc/ecs/ecs.config</code> file.</p><p>Within the Spot provisioning model, you can provide an allocation strategy of either “Diversified” or “Lowest Price” which will define how the EC2 Spot Instances are provisioned. The recommended best practice is to select the “<strong>Diversified</strong>” strategy, to maximize provisioning choices, while reducing the costs. When this is combined with Spot Instance draining, you can allow your Spot instances to drain connections gracefully while having enough time for the cluster to spawn other Spot instance types to handle the load. When configured correctly, you can significantly reduce downtime of Spot instances or eliminate downtime entirely.</p><p><img src=\"https://media.tutorialsdojo.com/sap_asg_mixed_instances.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_asg_mixed_instances.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Deploy the Docker containers using Amazon Elastic Container Service (ECS) with Amazon EC2 Spot Instances. Ensure that Spot Instance draining is enabled on the ECS agent config. Use Reserved instance for the Amazon RDS database and its read replicas. Create an encrypted Amazon S3 bucket to store the company documents. Create a bucket lifecycle policy that will move the documents to Amazon S3 Glacier after three months and will delete objects older than five years.</strong> With a diversified Spot instance type and Spot instance draining, you can allow your ECS cluster to spawn other EC2 instance types automatically to handle the load at a very low cost. Reserved instances are recommended cost-saving to RDS instances that will be running continuously for years.</p><p>The option that says: <strong>Deploy the Docker containers using Amazon Elastic Container Service (ECS) with Amazon EC2 Spot Instances. Use Spot instances for the Amazon RDS database and its read replicas. Create an encrypted ECS volume on the EC2 hosts that is shared with the containers to store the company documents. Set up a cron job that will delete the files after five years</strong> is incorrect. Storing company documents on the EC2 instances will require more disk space on instances, which is unnecessary and expensive. Using Spot instances for RDS instances is not recommended as this will cause major downtime or data loss in case AWS terminates your spot instance.</p><p>The option that says: <strong>Deploy the Docker containers using Amazon Elastic Container Service (ECS) with Amazon EC2 On-Demand instances. Use On-Demand instances as well for the Amazon RDS database and its read replicas. Create an Amazon EFS volume that is mounted on the EC2 instances to store the company documents. Create a cron job that will copy the documents to Amazon S3 Glacier after three months and then create a bucket lifecycle policy that will delete objects older than five years</strong> is incorrect. This is possible, however, using EFS volumes is more expensive than just storing the files on Amazon S3 in the first place.</p><p>The option that says: <strong>Deploy the Docker containers using Amazon Elastic Kubernetes Service (EKS)with auto-scaling enabled. Use Amazon EC2 Spot instances for the EKS cluster to further reduce costs. Use On-Demand instances for the Amazon RDS database and its read replicas. Create an encrypted Amazon S3 bucket to store the company documents. Create a bucket lifecycle policy that will move the documents to Amazon S3 Glacier after three months and will delete objects older than five years</strong> is incorrect. This option is also possible, however, using on-demand instances for continuously running RDS instances is expensive. You can save costs by using Reserved instances for Amazon RDS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-spot.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-spot.html</a></p><p><a href=\"https://aws.amazon.com/ec2/spot/containers-for-less/get-started/\">https://aws.amazon.com/ec2/spot/containers-for-less/get-started/</a></p><p><a href=\"https://aws.amazon.com/ec2/spot/getting-started/\">https://aws.amazon.com/ec2/spot/getting-started/</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-spot.html",
      "https://aws.amazon.com/ec2/spot/containers-for-less/get-started/",
      "https://aws.amazon.com/ec2/spot/getting-started/",
      "https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy"
    ]
  },
  {
    "id": 4,
    "question": "<p>A multinational financial firm plans to do a multi-regional deployment of its cryptocurrency trading application that’s being heavily used in the US and in Europe. The containerized application uses Kubernetes and has Amazon DynamoDB Global Tables as a centralized database to store and sync the data from two regions.</p><p>The architecture has distributed computing resources with several public-facing Application Load Balancers (ALBs). The Network team of the firm manages the public DNS internally and wishes to make the application available through an apex domain for easier access. S3 Multi-Region Access Points are also used for object storage workloads and hosting static assets.</p><p>Which is the MOST operationally efficient solution that the Solutions Architect should implement to meet the above requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an AWS Transit Gateway with a multicast domain that targets specific ALBs on the required AWS Regions. Create a public record in Amazon Route 53 using the static IP address of the AWS Transit Gateway.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an AWS Global Accelerator, which has several endpoint groups that target specific endpoints and ALBs on the required AWS Regions. Create a public alias record in Amazon Route 53 that points your custom domain name to the DNS name assigned to your accelerator.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Launch an AWS Transit Gateway that targets specific ALBs on the required AWS Regions. Create a CNAME record in Amazon Route 53 that directly points your custom domain name to the DNS name assigned to the AWS Transit Gateway.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Launch an AWS Global Accelerator with several endpoint groups that target the ALBs in all the relevant AWS Regions. Create an Amazon Route 53 Resolver Inbound Endpoint that points your custom domain name to the CNAME assigned to your accelerator.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>AWS Global Accelerator is a service in which you create <em>accelerators</em> to improve the performance of your applications for local and global users. Depending on the type of accelerator you choose, you can gain additional benefits:</p><p>-With a standard accelerator, you can improve the availability of your internet applications that are used by a global audience. With a standard accelerator, Global Accelerator directs traffic over the AWS global network to endpoints in the nearest Region to the client.</p><p>-With a custom routing accelerator, you can map one or more users to a specific destination among many destinations.</p><p>For standard accelerators, Global Accelerator uses the AWS global network to route traffic to the optimal regional endpoint based on health, client location, and policies that you configure, which increases the availability of your applications. Endpoints for standard accelerators can be Network Load Balancers, Application Load Balancers, Amazon EC2 instances, or Elastic IP addresses that are located in one AWS Region or multiple Regions. The service reacts instantly to changes in health or configuration to ensure that internet traffic from clients is always directed to healthy endpoints.</p><p>Custom routing accelerators only support virtual private cloud (VPC) subnet endpoint types and route traffic to private IP addresses in that subnet.</p><p><img src=\"https://media.tutorialsdojo.com/aws-global-accelerator-image-SAP-C02.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/aws-global-accelerator-image-SAP-C02.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In most scenarios, you can configure DNS to use your custom domain name (such as <code>www.tutorialsdojo.com</code>) with your accelerator instead of using the assigned static IP addresses or the default DNS name. First, using Amazon Route 53 or another DNS provider, create a domain name, and then add or update DNS records with your Global Accelerator IP addresses. Or you can associate your custom domain name with the DNS name for your accelerator. Complete the DNS configuration and wait for the changes to propagate over the internet. Now when a client makes a request using your custom domain name, the DNS server resolves it to the IP addresses in random order or to the DNS name for your accelerator.</p><p>To use your custom domain name with Global Accelerator when you use Route 53 as your DNS service, you create an alias record that points your custom domain name to the DNS name assigned to your accelerator. An alias record is a Route 53 extension to DNS. It's similar to a CNAME record, but you can create an alias record both for the root domain, such as <code>example.com</code>, and for subdomains, such as <code>www.tutorialsdojo.com</code>.</p><p>Hence, the correct answer is: <strong>Set up an AWS Global Accelerator, which has several endpoint groups that target specific endpoints and ALBs on the required AWS Regions. Create a public alias record in Amazon Route 53 that points your custom domain name to the DNS name assigned to your accelerator.</strong></p><p>The option that says: <strong>Set up an AWS Transit Gateway with a multicast domain that targets specific ALBs on the required AWS Regions. Create a public record in Amazon Route 53 using the static IP address of the AWS Transit Gateway</strong> is incorrect because an AWS Transit Gateway is not meant to be used to distribute traffic to multiple ALBs. In addition, a multicast domain is primarily used in delivering a single stream of data to multiple receiving computers simultaneously. Transit Gateway supports routing multicast traffic between subnets of attached VPCs and not ALBs. You have to use an AWS Global Accelerator instead since you can configure an accelerator to have several endpoint groups that point to multiple ALBs.</p><p>The option that says: <strong>Launch an AWS Transit Gateway that targets specific ALBs on the required AWS Regions. Create a CNAME record in Amazon Route 53 that directly points your custom domain name to the DNS name assigned to the AWS Transit Gateway</strong> is incorrect. As mentioned in the above rationale, an AWS Transit Gateway is not suitable to be used to integrate all the ALBs. Creating a CNAME record is also not right since the scenario explicitly mentioned that you have to use the apex domain. Remember that a CNAME record cannot be used for apex domain configuration in Amazon Route 53. You have to create a public alias record instead.</p><p>The option that says: <strong>Launch an AWS Global Accelerator with several endpoint groups that target the ALBs in all the relevant AWS Regions. Create an Amazon Route 53 Resolver Inbound Endpoint that points your custom domain name to the CNAME assigned to your accelerator</strong> is incorrect. Although the use of AWS Global Accelerator is a valid solution, creating a Route53 Resolver Inbound Endpoint is irrelevant. An Inbound Resolver endpoint simply allows DNS queries to your Amazon VPCs from your on-premises network or another VPC.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p><p><a href=\"https://docs.aws.amazon.com/global-accelerator/latest/dg/dns-addressing-custom-domains.mapping-your-custom-domain.html\">https://docs.aws.amazon.com/global-accelerator/latest/dg/dns-addressing-custom-domains.mapping-your-custom-domain.html</a></p><p><a href=\"https://docs.aws.amazon.com/global-accelerator/latest/dg/dns-addressing-custom-domains.dns-addressing.html\">https://docs.aws.amazon.com/global-accelerator/latest/dg/dns-addressing-custom-domains.dns-addressing.html</a></p><p><br></p><p><strong>Check out this AWS Global Accelerator Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-global-accelerator/?src=udemy\">https://tutorialsdojo.com/aws-global-accelerator/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/global-accelerator/",
      "https://docs.aws.amazon.com/global-accelerator/latest/dg/dns-addressing-custom-domains.mapping-your-custom-domain.html",
      "https://docs.aws.amazon.com/global-accelerator/latest/dg/dns-addressing-custom-domains.dns-addressing.html",
      "https://tutorialsdojo.com/aws-global-accelerator/?src=udemy"
    ]
  },
  {
    "id": 5,
    "question": "<p>A company is hosting its production environment in AWS Fargate. To save costs, the Chief Information Officer (CIO) wants to deploy its new development environment workloads on its on-premises servers as this leverages existing capital investments. As the Solutions Architect, you have been tasked by the CIO to provide a solution that will:</p><ul><li><p>have both on-premises and Fargate managed in the same cluster</p></li><li><p>easily migrate development environment workloads running on-premises to production environment running in AWS Fargate</p></li><li><p>ensure consistent tooling and API experience across container-based workloads</p></li></ul><p>Which of the following is the MOST operationally efficient solution that meets these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use EKS Amazon Anywhere to simplify on-premises Kubernetes management with default component configurations and automated cluster management tools. This makes it easy to migrate the development workloads running on-premises to EKS in an AWS region on Fargate.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Install and configure AWS Outposts in your on-premises data center. Run Amazon ECS on AWS Outposts to launch the development environment workloads. Migrate development workloads to production that is running on AWS Fargate.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Install and configure AWS Outposts in your on-premises data center. Run Amazon EKS Anywhere on AWS Outposts to launch container-based workloads. Migrate development workloads to production that is running on AWS Fargate.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Utilize Amazon ECS Anywhere to streamline software management on-premises and on AWS with a standardized container orchestrator. This makes it easy to migrate the development workloads running on-premises to ECS in an AWS region on Fargate.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Elastic Container Service (ECS) Anywhere</strong> is a feature of Amazon ECS that lets you run and manage container workloads on your infrastructure. This feature helps you meet compliance requirements and scale your business without sacrificing your on-premises investments.</p><p>It ensures consistency with the same on-premises Amazon ECS tools when you migrate to AWS.</p><p><strong>ECS Anywhere</strong> extends the reach of Amazon ECS to provide you with a single management interface for all of your container-based applications, irrespective of the environment they’re running in. As a result, you have a simple, consistent experience when it comes to cluster management, workload scheduling, and monitoring for both the cloud and on-premises. With ECS Anywhere, you do not need to install and maintain any container orchestration software, thus removing the need for your team to learn specialized knowledge domains and skillsets for disparate tooling.</p><p><strong>ECS Anywhere</strong> makes it easy for you to run your applications in on-premises environments as long as desired and then migrate to the cloud with a single click at any time.</p><p><img src=\"https://media.tutorialsdojo.com/public/amazon-ecs-anywhere.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/public/amazon-ecs-anywhere.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is:<strong> Utilize Amazon ECS Anywhere to streamline software management on-premises and on AWS with a standardized container orchestrator. This makes it easy to migrate the development workloads running on-premises to ECS in an AWS region on Fargate</strong> as it can have on-premises compute, EC2 instances, and Fargate in the same ECS cluster, making it easy to migrate ECS workloads running on-premises to ECS in an AWS region on Fargate or EC2 in the future if necessary.<strong><br></strong></p><p>The option that says: <strong>Use EKS Amazon Anywhere to simplify on-premises Kubernetes management with default component configurations and automated cluster management tools. This makes it easy to migrate the development workloads running on-premises to EKS in an AWS region on Fargate</strong> is incorrect because Amazon EKS Anywhere is not designed to run in the AWS cloud. It does not integrate with the Kubernetes Cluster API Provider for AWS. <strong><br></strong></p><p>The following sets of options are incorrect because AWS Fargate is not available on AWS Outposts, and Amazon EKS Anywhere isn't designed to run on AWS Outposts:</p><p><strong>- Install and configure AWS Outposts in your on-premises data center. Run Amazon ECS on AWS Outposts to launch the development environment workloads. Migrate development workloads to production that is running on AWS Fargate.</strong></p><p><strong>- Install and configure AWS Outposts in your on-premises data center. Run Amazon EKS Anywhere on AWS Outposts to launch container-based workloads. Migrate development workloads to production that is running on AWS Fargate.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ecs/anywhere/\">https://aws.amazon.com/ecs/anywhere/</a></p><p><a href=\"https://aws.amazon.com/ecs/anywhere/faqs/\">https://aws.amazon.com/ecs/anywhere/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-on-outposts.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-on-outposts.html</a></p><p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/eks-deployment-options.html\">https://docs.aws.amazon.com/eks/latest/userguide/eks-deployment-options.html</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/ecs/anywhere/",
      "https://aws.amazon.com/ecs/anywhere/faqs/",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-on-outposts.html",
      "https://docs.aws.amazon.com/eks/latest/userguide/eks-deployment-options.html"
    ]
  },
  {
    "id": 6,
    "question": "<p>A tech company will soon launch a new smartwatch that will collect statistics and usage information from its users. The solutions architect was tasked to design a data storage and retrieval solution for the receiving application. The application is expected to ingest millions of records per minute from its worldwide user base. For the storage requirements:</p><ul><li><p>Each record is less than 4KB in size.</p></li></ul><ul><li><p>Data must be stored durably.</p></li></ul><ul><li><p>Data must be stored for 120 days only, then it can be deleted.</p></li></ul><ul><li><p>Data must have low latency retrieval time.</p></li></ul><p>For running the application for a year, the estimated storage requirement is around 10-15 TB.</p><p>Which of the following options is the recommended storage solution while being the most cost-effective?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the application to receive the records and store the records to Amazon Aurora Serverless. Write an AWS Lambda function that runs a query to delete records older than 120 days. Schedule the function to run every night.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure the application to receive the records and set the storage to a DynamoDB table. Configure proper scaling on the DynamoDB table and enable the DynamoDB table Time to Live (TTL) setting to delete records after 120 days.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Kinesis Data Stream to ingest and store the records. Set a custom data retention period of 120 days for the data stream. Send the streamed data to an Amazon S3 bucket for added durability.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the application to ingest the records and store each record on a dedicated Amazon S3 bucket. Ensure that a unique filename is set for each object. Create an S3 bucket lifecycle policy to expire objects that are older than 120 days.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon DynamoDB Time to Live (TTL)</strong> allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. TTL is provided at no extra cost as a means to reduce stored data volumes by retaining only the items that remain current for your workload’s needs.</p><p>TTL is useful if you store items that lose relevance after a specific time. The following are example TTL use cases:</p><p>-Remove user or sensor data after one year of inactivity in an application.</p><p>-Archive expired items to an Amazon S3 data lake via Amazon DynamoDB Streams and AWS Lambda.</p><p>-Retain sensitive data for a certain amount of time according to contractual or regulatory obligations.</p><p>When enabling TTL on a DynamoDB table, you must identify a specific attribute name that the service will look for when determining if an item is eligible for expiration. After you enable TTL on a table, a per-partition scanner background process automatically and continuously evaluates the expiry status of items in the table.</p><p>The scanner background process compares the current time, in Unix epoch time format in seconds, to the value stored in the user-defined attribute of an item. If the attribute is a Number data type, the attribute’s value is a timestamp in Unix epoch time format in seconds, and the timestamp value is older than the current time but not five years older or more (in order to avoid a possible accidental deletion due to a malformed TTL value), then the item is set to expire.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_ttl_setting.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_ttl_setting.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Configure the application to receive the records and set the storage to a DynamoDB table. Configure proper scaling on the DynamoDB table and enable the DynamoDB table Time to Live (TTL) setting to delete records after 120 days.</strong> DynamoDB has a feature to delete items after a defined timestamp. This is cost-effective because it does not consume any write throughput.</p><p>The option that says: <strong>Use Amazon Kinesis Data Stream to ingest and store the records. Set a custom data retention period of 120 days for the data stream. Send the streamed data to an Amazon S3 bucket for added durability</strong> is incorrect. This may be possible; however, you don't need to store the records on Amazon S3. Amazon Kinesis already has durable storage which is backed by S3, so this solution is redundant. Additionally, each retrieval, write, and delete on the Kinesis stream will incur additional costs.</p><p>The option that says: <strong>Configure the application to receive the records and store the records to Amazon Aurora Serverless. Write an AWS Lambda function that runs a query to delete records older than 120 days. Schedule the function to run every night</strong> is incorrect. Since the application will constantly receive records, you won't get the cost-effectiveness benefit of using Amazon Aurora Serverless. The DB will constantly be running anyway.</p><p>The option that says: <strong>Configure the application to ingest the records and store each record on a dedicated Amazon S3 bucket. Ensure that a unique filename is set for each object. Create an S3 bucket lifecycle policy to expire objects that are older than 120 days</strong> is incorrect. This is possible; however, it does not meet the low-latency retrieval time for the records.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/time-to-live-ttl-how-to.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/time-to-live-ttl-how-to.html</a></p><p><br></p><p><strong>Check out these Amazon DynamoDB and Amazon Kinesis Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/time-to-live-ttl-how-to.html",
      "https://tutorialsdojo.com/amazon-dynamodb/?src=udemy",
      "https://tutorialsdojo.com/amazon-kinesis/?src=udemy"
    ]
  },
  {
    "id": 7,
    "question": "<p>An innovative Business Process Outsourcing (BPO) startup is planning to launch a scalable and cost-effective call center system using AWS. The system should be able to receive inbound calls from thousands of customers and generate user contact flows. Callers must have the capability to perform basic tasks such as changing their password or checking their balance without them having to speak to a call center agent. It should also have advanced deep learning functionalities such as automatic speech recognition (ASR) to achieve highly engaging user experiences and lifelike conversational interactions. A feature that allows the solution to query other business applications and send relevant data back to callers must also be implemented. </p><p>Which of the following is the MOST suitable solution that the Solutions Architect should implement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a cloud-based contact center using the Amazon Connect service. Create a conversational chatbot using Amazon Lex with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with Amazon Connect. Connect the solution to various business applications and other internal systems using AWS Lambda functions.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up a cloud-based contact center using the AWS Ground Station service. Create a conversational chatbot using Amazon Alexa for Business with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with AWS Ground Station. Connect the solution to various business applications and other internal systems using AWS Lambda functions.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up a cloud-based contact center using the Amazon Connect service. Create a conversational chatbot using Amazon Comprehend with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with Amazon Connect. Connect the solution to various business applications and other internal systems using AWS Lambda functions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a cloud-based contact center using the AWS Elemental MediaConnect service. Create a conversational chatbot using Amazon Polly with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with AWS Elemental MediaConnect. Connect the solution to various business applications and other internal systems using AWS Lambda functions.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon Connect</strong> provides a seamless omnichannel experience through a single unified contact center for voice and chat. Contact center agents and managers don’t have to learn multiple tools because Amazon Connect has the same contact routing, queuing, analytics, and management tools in a single UI across voice, web chat, and mobile chat.</p><p><strong>Amazon Lex</strong> is a service for building conversational interfaces into any application using voice and text. Amazon Lex provides the advanced deep learning functionalities of automatic speech recognition (ASR) for converting speech to text and natural language understanding (NLU) to recognize the intent of the text, enabling you to build applications with highly engaging user experiences and lifelike conversational interactions. With Amazon Lex, the same deep learning technologies that power Amazon Alexa are now available to any developer, enabling you to quickly and easily build sophisticated, natural language conversational bots (\"chatbots\").</p><p><img src=\"https://media.tutorialsdojo.com/sap_connect_lex.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_connect_lex.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Contact flows define the experience your customers have when they interact with your contact center. These are similar in concept to Interactive Voice Response (IVR). Contact flows are comprised of blocks, with each block defining a step or interaction in your contact center. For example, there are blocks to play a prompt, get input from a customer, branch based on customer input, or invoke an AWS Lambda function or an Amazon Lex bot.</p><p>By using an Amazon Lex chatbot in your Amazon Connect call center, callers can perform tasks such as changing a password, requesting a balance on an account, or scheduling an appointment without needing to speak to an agent. These chatbots use automatic speech recognition and natural language understanding to recognize the intent of the caller. They are able to recognize human speech at an optimal (8 kHz) telephony audio sampling rate and understand the caller’s intent without requiring the caller to speak in specific phrases. Amazon Lex uses AWS Lambda functions to query your business applications, provide information back to callers, and make updates as requested. Amazon Lex chatbots also maintain context and manage the dialogue, dynamically adjusting responses based on the conversation.</p><p>Hence, the correct answer is: <strong>Set up a cloud-based contact center using the Amazon Connect service. Create a conversational chatbot using Amazon Lex with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with Amazon Connect. Connect the solution to various business applications and other internal systems using AWS Lambda functions.</strong></p><p>The option that says: <strong>Set up a cloud-based contact center using the AWS Ground Station service. Create a conversational chatbot using Amazon Alexa for Business with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with AWS Ground Station. Connect the solution to various business applications and other internal systems using AWS Lambda functions</strong> is incorrect. AWS Ground Station is a fully managed service that lets you control satellite communications, process data, and scale your operations without having to worry about building or managing your own ground station infrastructure. This service is not suitable as a cloud-based contact center. Moreover, Alexa for Business simply empowers companies to use Alexa devices as their intelligent assistant to be more productive in meeting rooms, at their desks, and even with the Alexa devices they already use at home or on the go. You should use a different machine learning service, such as Amazon Lex, to create a conversational chatbot.</p><p>The option that says: <strong>Set up a cloud-based contact center using the AWS Elemental MediaConnect service. Create a conversational chatbot using Amazon Polly with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with AWS Elemental MediaConnect. Connect the solution to various business applications and other internal systems using AWS Lambda functions </strong>is incorrect because AWS Elemental MediaConnect is just a high-quality transport service for live video. You have to use Amazon Connect instead to set up your cloud-based contact center. And although Amazon Polly is a machine learning service, it is quite limited as it just turns text into lifelike speech that allows you to create applications that talk and build entirely new categories of speech-enabled products. A more suitable service to use here is Amazon Lex.</p><p>The option that says: <strong>Set up a cloud-based contact center using the Amazon Connect service. Create a conversational chatbot using Amazon Comprehend with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with Amazon Connect. Connect the solution to various business applications and other internal systems using AWS Lambda functions</strong> is incorrect because Amazon Comprehend is used to derive and understand valuable insights from text within documents. It doesn't have automatic speech recognition (ASR) for converting speech to text nor natural language understanding (NLU). You have to use Amazon Lex for this scenario</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/connect/\">https://aws.amazon.com/connect/</a></p><p><a href=\"https://aws.amazon.com/lex/\">https://aws.amazon.com/lex/</a></p><p><a href=\"https://aws.amazon.com/blogs/contact-center/easily-set-up-interactive-messages-for-your-amazon-connect-chatbot/\">https://aws.amazon.com/blogs/contact-center/easily-set-up-interactive-messages-for-your-amazon-connect-chatbot/</a></p><p><br></p><p><strong>Check out this Amazon Lex Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-lex/?src=udemy\">https://tutorialsdojo.com/amazon-lex/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/connect/",
      "https://aws.amazon.com/lex/",
      "https://aws.amazon.com/blogs/contact-center/easily-set-up-interactive-messages-for-your-amazon-connect-chatbot/",
      "https://tutorialsdojo.com/amazon-lex/?src=udemy"
    ]
  },
  {
    "id": 8,
    "question": "<p>A leading media company has a hybrid architecture where its on-premises data center is connected to AWS via a Direct Connect connection. They also have a repository of over 50-TB digital videos and media files. These files are stored on their on-premises tape library and are used by their Media Asset Management (MAM) system. Due to the sheer size of their data, they want to implement an automated catalog system that will enable them to search their files using facial recognition. A catalog will store the faces of the people who are present in these videos including a still image of each person. Eventually, the media company would like to migrate these media files to AWS including the MAM video contents. </p><p>Which of the following options provides a solution which uses the LEAST amount of ongoing management overhead and will cause MINIMAL disruption to the existing system?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Integrate the file system of your local data center to AWS Storage Gateway by setting up a file gateway appliance on-premises. Utilize the MAM solution to extract the media files from the current data store and send them into the file gateway. Build a collection using Amazon Rekognition by populating a catalog of faces from the processed media files. Use an AWS Lambda function to invoke Amazon Rekognition Javascript SDK to have it fetch the media file from the S3 bucket which is backing the file gateway, retrieve the needed metadata, and finally, persist the information into the MAM solution.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Kinesis Video Streams to set up a video ingestion stream and with Amazon Rekognition, build a collection of faces. Stream the media files from the MAM solution into Kinesis Video Streams and configure the Amazon Rekognition to process the streamed files. Launch a stream consumer to retrieve the required metadata, and push the metadata into the MAM solution. Finally, configure the stream to store the files in an S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Request for an AWS Snowball Storage Optimized device to migrate all of the media files from the on-premises library into Amazon S3. Provision a large EC2 instance and allow it to access the S3 bucket. Install an open-source facial recognition tool on the instance like OpenFace or OpenCV. Process the media files to retrieve the metadata and push this information into the MAM solution. Lastly, copy the media files to another S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a tape gateway appliance on-premises and connect it to your AWS Storage Gateway. Configure the MAM solution to fetch the media files from the current archive and push them into the tape gateway to be stored in Amazon Glacier. Using Amazon Rekognition, build a collection from the catalog of faces. Utilize a Lambda function which invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video directly from the tape gateway in real-time, retrieve the required metadata, and push the metadata into the MAM solution.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon Rekognition</strong> can store information about detected faces in server-side containers known as collections. You can use the facial information that's stored in a collection to search for known faces in images, stored videos, and streaming videos. Amazon Rekognition supports the <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_IndexFaces.html\">IndexFaces</a> operation. You can use this operation to detect faces in an image and persist information about facial features that are detected in a collection. This is an example of a <em>storage-based</em> API operation because the service persists information on the server.</p><p>To store facial information, you must first create (<a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_CreateCollection.html\">CreateCollection</a>) a face collection in one of the AWS Regions in your account. You specify this face collection when you call the <code>IndexFaces</code> operation. After you create a face collection and store facial feature information for all faces, you can search the collection for face matches. To search for faces in an image, call <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_SearchFacesByImage.html\">SearchFacesByImage</a>. To search for faces in a stored video, call <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_StartFaceSearch.html\">StartFaceSearch</a>. To search for faces in a streaming video, call <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_CreateStreamProcessor.html\">CreateStreamProcessor</a>.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rekognition_screen.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_rekognition_screen.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>AWS Storage Gateway offers file-based, volume-based, and tape-based storage solutions. With a tape gateway, you can cost-effectively and durably archive backup data in GLACIER or DEEP_ARCHIVE. A tape gateway provides a virtual tape infrastructure that scales seamlessly with your business needs and eliminates the operational burden of provisioning, scaling, and maintaining a physical tape infrastructure.</p><p>You can run AWS Storage Gateway either on-premises as a VM appliance, as a hardware appliance, or in AWS as an Amazon Elastic Compute Cloud (Amazon EC2) instance. You deploy your gateway on an EC2 instance to provision iSCSI storage volumes in AWS. You can use gateways hosted on EC2 instances for disaster recovery, data mirroring, and providing storage for applications hosted on Amazon EC2.</p><p>Hence, the correct answer is: <strong>Integrate the file system of your local data center to AWS Storage Gateway by setting up a file gateway appliance on-premises. Utilize the MAM solution to extract the media files from the current data store and send them into the file gateway. Build a collection using Amazon Rekognition by populating a catalog of faces from the processed media files. Use an AWS Lambda function to invoke Amazon Rekognition Javascript SDK to have it fetch the media file from the S3 bucket which is backing the file gateway, retrieve the needed metadata, and finally, persist the information into the MAM solution.</strong></p><p>The option that says: <strong>Request for an AWS Snowball Storage Optimized device to migrate all of the media files from the on-premises library into Amazon S3. Provision a large EC2 instance and allow it to access the S3 bucket. Install an open-source facial recognition tool on the instance like OpenFace or OpenCV. Process the media files to retrieve the metadata and push this information into the MAM solution. Lastly, copy the media files to another S3 bucket</strong> is incorrect. This entails a lot of ongoing management overhead instead of just using Amazon Rekognition. Moreover, it is more suitable to use the AWS Storage Gateway service rather than an EBS Volume.</p><p>The option that says: <strong>Set up a tape gateway appliance on-premises and connect it to your AWS Storage Gateway. Configure the MAM solution to fetch the media files from the current archive and push them into the tape gateway to be stored in Amazon Glacier. Using Amazon Rekognition, build a collection from the catalog of faces. Utilize a Lambda function which invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video directly from the tape gateway in real-time, retrieve the required metadata, and push the metadata into the MAM solution</strong> is incorrect. Although this solution uses the right combination of AWS Storage Gateway and Amazon Rekognition, take note that you can't directly fetch the media files from your tape gateway in real time since this is backed up using Glacier. Although the on-premises data center is using a tape gateway, you can still set up a solution to use a file gateway in order to properly process the videos using Amazon Rekognition. Keep in mind that the tape gateway in the AWS Storage Gateway service is primarily used as an archive solution.</p><p>The option that says: <strong>Use Amazon Kinesis Video Streams to set up a video ingestion stream and with Amazon Rekognition, build a collection of faces. Stream the media files from the MAM solution into Kinesis Video Streams and configure the Amazon Rekognition to process the streamed files. Launch a stream consumer to retrieve the required metadata, and push the metadata into the MAM solution. Finally, configure the stream to store the files in an S3 bucket </strong>is incorrect. You won't be able to connect your tape gateway directly to your Kinesis Video Streams service. You need to use AWS Storage Gateway first.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/collections.html\">https://docs.aws.amazon.com/rekognition/latest/dg/collections.html</a></p><p><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p><p><br></p><p><strong>Check out this Amazon Rekognition Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-rekognition/?src=udemy\"><strong>https://tutorialsdojo.com/amazon-rekognition/</strong></a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/rekognition/latest/dg/API_IndexFaces.html",
      "https://docs.aws.amazon.com/rekognition/latest/dg/API_CreateCollection.html",
      "https://docs.aws.amazon.com/rekognition/latest/dg/API_SearchFacesByImage.html",
      "https://docs.aws.amazon.com/rekognition/latest/dg/API_StartFaceSearch.html",
      "https://docs.aws.amazon.com/rekognition/latest/dg/API_CreateStreamProcessor.html",
      "https://docs.aws.amazon.com/rekognition/latest/dg/collections.html",
      "https://aws.amazon.com/storagegateway/file/",
      "https://tutorialsdojo.com/amazon-rekognition/?src=udemy",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 9,
    "question": "<p>A company develops new android and iOS mobile apps. The company is considering storing user customization data in AWS. This would provide a more uniform cross-platform experience to their users using multiple mobile devices to access their apps. The preference data for each user is estimated to be 4 KB in size. Additionally, 3 million customers are expected to use the application on a regular basis, using their social login accounts for easier user authentication.</p><p>How should the Solutions Architect design a highly available, cost-effective, scalable, and secure solution to meet the above requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch an RDS MySQL instance in 2 availability zones to contain the user preference data. Deploy a public-facing application on a server in front of the database which will manage authentication and access controls.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Provision a table in DynamoDB containing an item for each user having the necessary attributes to hold the user preferences. The mobile app will query the user preferences directly from the table. Use STS, Web Identity Federation, and DynamoDB's Fine-Grained Access Control for authentication and authorization.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Have the user preference data stored in S3, and set up a DynamoDB table with an item for each user and an item attribute referencing the user's S3 object. The mobile app will retrieve the S3 URL from DynamoDB and then access the S3 object directly utilizing STS, Web identity Federation, and S3 Access Points.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an RDS MySQL instance with multiple read replicas in 2 availability zones to store the user preference data. The mobile application will then query the user preferences from the read replicas. Finally, utilize MySQL's user management and access privilege system to handle the security and access credentials of the users.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>Take note that the question mentioned the use of social media logins. With web identity federation, you don't need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known identity provider (IdP) —such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP, receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. Using an IdP helps you keep your AWS account secure, because you don't have to embed and distribute long-term security credentials with your application.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_items.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_items.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The option that says: <strong>Provision a table in DynamoDB containing an item for each user having the necessary attributes to hold the user preferences. The mobile app will query the user preferences directly from the table. Use STS, Web Identity Federation, and DynamoDB's Fine Grained Access Control for authentication and authorization</strong> is correct because it uses DynamoDB for scalability and cost-efficiency. It uses federated access using Web Identity Provider, and uses fine-grained access privileges for authenticating the access.</p><p>The option that says: <strong>Have the user preference data stored in S3, and set up a DynamoDB table with an item for each user and an item attribute referencing the user's S3 object. The mobile app will retrieve the S3 URL from DynamoDB and then access the S3 object directly utilizing STS, Web identity Federation, and S3 Access Points </strong>is incorrect because it doesn’t use DynamoDB's built-in Fine-Grained Access Control for authentication and authorization feature. Additionally, using Amazon S3 bucket with DynamoDB is recommended for storing large data with the metadata stored on DynamoDB. The user preference data is only 4KB in size which can be stored effectively in a DynamoDB table. The use of S3 Access points is not necessary because these are just unique hostnames that enforce distinct permissions and network controls for any request made through the access point.</p><p>The option that says: <strong>Launch an RDS MySQL instance in 2 availability zones to contain the user preference data. Deploy a public-facing application on a server in front of the database which will manage authentication and access controls</strong> is incorrect because RDS MySQL is not as scalable and cost-effective as DynamoDB.</p><p>The option that says: <strong>Create an RDS MySQL instance with multiple read replicas in 2 availability zones to store the user preference data. The mobile application will then query the user preferences from the read replicas. Finally, utilize MySQL's user management and access privilege system to handle the security and access credentials of the users </strong>is incorrect because RDS MySQL is not as scalable and cost-effective as DynamoDB. Additionally, the user management and access privilege system for RDS cannot be used for controlling access.</p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/dynamodb/developer-resources\">https://aws.amazon.com/dynamodb/developer-resources</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-use-s3-too.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-use-s3-too.html</a></p><p><a href=\"https://developer.amazon.com/blogs/appstore/post/TxZR5F5K6QINFQ/storing-user-preference-in-amazon-dynamodb-using-the-aws-sdk-for-android\">https://developer.amazon.com/blogs/appstore/post/TxZR5F5K6QINFQ/storing-user-preference-in-amazon-dynamodb-using-the-aws-sdk-for-android</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/dynamodb/developer-resources",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-use-s3-too.html",
      "https://developer.amazon.com/blogs/appstore/post/TxZR5F5K6QINFQ/storing-user-preference-in-amazon-dynamodb-using-the-aws-sdk-for-android",
      "https://tutorialsdojo.com/amazon-dynamodb/?src=udemy"
    ]
  },
  {
    "id": 10,
    "question": "<p>A company needs a deployment solution for its application that is hosted on the AWS cloud. The company has the following requirements for the application:</p><p>- The instances must have 500GB worth of static dataset that is accessible for the application upon boot up.</p><p>- The instances must be able to scale-out or scale-in depending on the traffic load of the application.</p><p>- The Development team must have a quick and automated way to deploy their code updates several times during the day.</p><p>- Security patches for the vulnerabilities on the operating system (OS) must be installed within 48 hours of release.</p><p>Which of the following solutions should the Solutions Architect implement to meet the company requirements while being cost-effective?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Auto Scaling group of EC2 instances using the Amazon Linux AMI. Install the application on the EC2 instances. Replace the existing instances as soon as AWS releases a new Amazon Linux AMI version. Write a user data script that will download the 500 GB static dataset from an Amazon S3 bucket. Deploy the new version of the application to the instances using AWS CodeDeploy.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Install OS patches and create a new AMI using AWS Systems Manager. Use this new AMI for the Auto Scaling group of EC2 instances and replace the existing instances. Create a scheduled batch job that will run every night to deploy the new application version and install the OS patches. Mount an Amazon EFS volume containing the static dataset on the instances upon boot up.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Install OS patches and create a new AMI using AWS Systems Manager. Use this new AMI for the Auto Scaling group of EC2 instances and replace the existing instances. Deploy the new version of the application to the instances using AWS CodeDeploy. Mount an Amazon EFS volume containing the static dataset on the instances upon boot up.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Auto Scaling group of EC2 instances using the Amazon Linux AMI. Install the application on the EC2 instances. Write a user data script that will download the 500 GB static dataset from an Amazon S3 bucket. Use AWS Systems Manager to install the OS patches as soon as they are released. Deploy the new version of the application to the instances using AWS CodeDeploy.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. You can use Patch Manager to install Service Packs on Windows instances and perform minor version upgrades on Linux instances.</p><p>Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task.</p><p><strong>AWS Systems Manager Automation</strong> simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following:</p><p>- Build automations to configure and manage instances and AWS resources.</p><p>- Create custom runbooks or use pre-defined runbooks maintained by AWS.</p><p>- Receive notifications about Automation tasks and runbooks by using Amazon EventBridge.</p><p>- Monitor Automation progress and details by using the AWS Systems Manager console.</p><p>AWS Systems Manager Automation provides several runbooks with pre-defined steps that you can use to perform common tasks like restarting one or more EC2 instances or creating an Amazon Machine Image (AMI). A Systems Manager Automation runbook defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation runs. A runbook contains one or more steps that run in sequential order. Each step is built around a single action. Output from one step can be used as input in a later step.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_automation.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ssm_automation.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Amazon Elastic File System (Amazon EFS)</strong> provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS has a simple web services interface that allows you to create and configure file systems quickly and easily. With Amazon EFS, you pay only for the storage used by your file system and there is no minimum fee or setup cost. Amazon EFS oﬀers two storage classes, Standard and Infrequent Access. The Standard storage class is used to store frequently accessed files. The Infrequent Access (IA) storage class is a lower-cost storage class that's designed for storing long-lived, infrequently accessed ﬁles cost-eﬀectively.</p><p>Amazon Elastic File System presents a standard file-system interface that supports full file-system access semantics. Using Network File System (NFS) version 4.1 (NFSv4.1), you can mount your Amazon EFS file system on any Amazon Elastic Compute Cloud (Amazon EC2) Linux-based instance. After your system is mounted, you can work with the files and directories just as you do with a local file system.</p><p>Therefore, the correct answer is: <strong>Install OS patches and create a new AMI using AWS Systems Manager. Use this new AMI for the Auto Scaling group of EC2 instances and replace the existing instances. Deploy the new version of the application to the instances using AWS CodeDeploy. Mount an Amazon EFS volume containing the static dataset on the instances upon boot up.</strong></p><p>The option that says: <strong>Install OS patches and create a new AMI using AWS Systems Manager. Use this new AMI for the Auto Scaling group of EC2 instances and replace the existing instances. Create a scheduled batch job that will run every night to deploy the new application version and install the OS patches. Mount an Amazon EFS volume containing the static dataset on the instances upon boot up</strong> is incorrect. The OS patches can be installed every night, but it is not suitable for the application deployment. A batch job is not suitable for the application deployment as the Developers must deploy several times during the day.</p><p>The option that says: <strong>Create an Auto Scaling group of EC2 instances using the Amazon Linux AMI. Install the application on the EC2 instances. Write a user data script that will download the 500 GB static dataset from an Amazon S3 bucket. Use AWS Systems Manager to install the OS patches as soon as they are released. Deploy the new version of the application to the instances using AWS CodeDeploy</strong> is incorrect. Although Amazon S3 may seem more cost-effective than Amazon EFS in storing static contents, the Amazon EC2 instances will have to download the dataset on its local EBS volume. Attaching 500GB EBS volumes on each of the EC2 instances is more expensive compared to just using a single EFS volume mounted on all EC2 instances at boot up.</p><p>The option that says: <strong>Create an Auto Scaling group of EC2 instances using the Amazon Linux AMI. Install the application on the EC2 instances. Replace the existing instances as soon as AWS releases a new Amazon Linux AMI version. Write a user data script that will download the 500 GB static dataset from an Amazon S3 bucket. Deploy the new version of the application to the instances using AWS CodeDeploy</strong> is incorrect. The Amazon Linux AMI is patched for security vulnerabilities and OS minor versions. However, each new version usually takes weeks or months depending on Amazon’s release cycle.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html\">https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html</a></p><p><br></p><p><strong>Check out these AWS Systems Manager and Amazon EFS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-efs/?src=udemy\">https://tutorialsdojo.com/amazon-efs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html",
      "https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html",
      "https://tutorialsdojo.com/aws-systems-manager/?src=udemy",
      "https://tutorialsdojo.com/amazon-efs/?src=udemy"
    ]
  },
  {
    "id": 11,
    "question": "<p>A tech startup is planning to launch a new global mobile marketplace using AWS Amplify and AWS Mobile Hub. To lower the latency, the backend APIs will be launched to multiple AWS regions to process the sales and financial transactions in the region closest to the users. The solutions architect is instructed to design the system architecture to ensure that the transactions made in one region are automatically replicated to other regions. In the coming months ahead, it is expected that the marketplace will have millions of users across North America, South America, Europe, and Asia.</p><p>Which of the following is the most scalable, cost-effective, and highly available architecture that you should implement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a Global DynamoDB table with replica tables across several AWS regions that you prefer. In each local region, store the individual transactions to a DynamoDB replica table in the same region. Any changes made in one of the replica tables will automatically be replicated across all other tables.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>In each local region, store the individual transactions to a DynamoDB table. Set up an AWS Lambda function to read recent writes from the table, and replay the data to DynamoDB tables in all other regions.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use a combination of AWS Control Tower and Amazon Connect to launch and centrally manage multiple DynamoDB tables in various AWS Regions. In each local region, store the individual transactions to a DynamoDB replica table in the same region.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon Aurora Multi-Master database on all required regions. Store the individual transactions to the Amazon Aurora instance in the local region. Replicate the transactions table between regions using Aurora replication. In this set up, any changes made in one of the tables will be automatically replicated across all other tables.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Global Tables</strong> builds upon DynamoDB’s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions.</p><p><strong>Global Tables</strong> eliminates the difficult work of replicating data between regions and resolving update conflicts, enabling you to focus on your application’s business logic. In addition, Global Tables enables your applications to stay highly available even in the unlikely event of isolation or degradation of an entire region.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_global_table.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_global_table.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is:<strong> Create a Global DynamoDB table with replica tables across several AWS regions that you prefer. In each local region, store the individual transactions to a DynamoDB replica table in the same region. Any changes made in one of the replica tables will automatically be replicated across all other tables.</strong></p><p>The option that says: <strong>In each local region, store the individual transactions to a DynamoDB table. Set up an AWS Lambda function to read recent writes from the table, and replay the data to DynamoDB tables in all other regions</strong> is incorrect. Using an AWS Lambda function to replicate all data across regions is not a scalable solution. Remember that there will be millions of customers who will use the mobile app around the world, and this entails a lot of replication and compute capacity for a single Lambda function. In this scenario, the best solution is to use Global DynamoDB tables with DynamoDB Stream option enabled to automatically handle the replication process.</p><p>The option that says:<strong> Use a combination of AWS Control Tower and Amazon Connect to launch and centrally manage multiple DynamoDB tables in various AWS Regions. In each local region, store the individual transactions to a DynamoDB replica table in the same region</strong> is incorrect. Amazon Connect is just an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost, while AWS Control Tower just offers the easiest way to set up and govern a new, secure, multi-account AWS environment. You can't use these two services to set up a Global DynamoDB Table.</p><p>The option that says: <strong>Create an Amazon Aurora Multi-Master database on all required regions. Store the individual transactions to the Amazon Aurora instance in the local region. Replicate the transactions table between regions using Aurora replication. In this setup, any changes made in one of the tables will be automatically replicated across all other tables</strong> is incorrect. By default, all DB instances in a multi-master cluster must be in the same AWS Region and you can't enable cross-region replicas from multi-master clusters. In addition, DynamoDB provides better global scalability for mobile applications compared to Amazon Aurora.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_HowItWorks.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_HowItWorks.html</a></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_HowItWorks.html",
      "https://aws.amazon.com/dynamodb/global-tables/",
      "https://tutorialsdojo.com/amazon-dynamodb/?src=udemy"
    ]
  },
  {
    "id": 12,
    "question": "<p>A retail company hosts its web application on an Auto Scaling group of Amazon EC2 instances deployed across multiple Availability Zones. The Auto Scaling group is configured to maintain a minimum EC2 cluster size and automatically replace unhealthy instances. The EC2 instances are behind an Application Load Balancer so that the load can be spread evenly on all instances. The application target group health check is configured with a fixed HTTP page that queries a dummy item on the database. The web application connects to a Multi-AZ Amazon RDS MySQL instance. A recent outage caused a major loss to the company's revenue. Upon investigation, it was found that the web server metrics are within the normal range but the database CPU usage is very high, causing the EC2 health checks to timeout. Failing the health checks, the Auto Scaling group continuously replaced the unhealthy instances thus causing the downtime.</p><p>Which of the following options should the Solution Architect implement to prevent this from happening again and allow the application to handle more traffic in the future? (Select TWO.)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Reduce the load on the database tier by creating multiple read replicas for the Amazon RDS MySQL Multi-AZ cluster. Configure the web application to use the single reader endpoint of RDS for all read operations.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon CloudWatch alarm to monitor the Amazon RDS MySQL instance if it has a high-load or in impaired status. Set the alarm action to recover the RDS instance. This will automatically reboot the database to reset the queries.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Reduce the load on the database tier by creating an Amazon ElastiCache cluster to cache frequently requested database queries. Configure the application to use this cache when querying the RDS MySQL instance.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Change the target group health check to use a TCP check on the EC2 instances instead of a page that queries the database. Create an Amazon Route 53 health check for the database dummy item web page to ensure that the application works as expected. Set up an Amazon CloudWatch alarm to send a notification to Admins when the health check fails.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Change the target group health check to a simple HTML page instead of a page that queries the database. Create an Amazon Route 53 health check for the database dummy item web page to ensure that the application works as expected. Set up an Amazon CloudWatch alarm to send a notification to Admins when the health check fails.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Route 53 health checks</strong> monitor the health and performance of your web applications, web servers, and other resources. Each health check that you create can monitor one of the following:</p><p><strong>The health of a specified resource, such as a web server</strong> - You can configure a health check that monitors an endpoint that you specify either by IP address or by the domain name. At regular intervals that you specify, Route 53 submits automated requests over the Internet to your application. You can configure the health check to make requests similar to those that your users make, such as requesting a web page from a specific URL.</p><p><strong>The status of other health checks</strong> - You can create a health check that monitors whether Route 53 considers other health checks healthy or unhealthy. One situation where this might be useful is when you have multiple resources that perform the same function, such as multiple web servers, and your chief concern is whether some minimum number of your resources are healthy.</p><p><strong>The status of an Amazon CloudWatch alarm</strong> - You can create CloudWatch alarms that monitor the status of CloudWatch metrics, such as the number of throttled read events for an Amazon DynamoDB database or the number of Elastic Load Balancing hosts that are considered healthy.</p><p>After you create a health check, you can get the status of the health check, get notifications when the status changes, and configure DNS failover. To improve resiliency and availability, Route 53 doesn't wait for the CloudWatch alarm to go into the ALARM state. The status of a health check changes from healthy to unhealthy based on the data stream and on the criteria in the CloudWatch alarm.</p><p><img src=\"https://media.tutorialsdojo.com/sap_route53_failover_sns.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_route53_failover_sns.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Your <strong>Application Load Balancer</strong> periodically sends requests to its registered targets to test their status. These tests are called health checks. Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. After your target is registered, it must pass one health check to be considered healthy. After each health check is completed, the load balancer node closes the connection that was established for the health check. If a target group contains only unhealthy registered targets, the load balancer nodes route requests across its unhealthy targets.</p><p>Each health check will be executed at configured intervals to all the EC2 instances so if the health check query page involves a database query, there will be several simultaneous queries to the database. This can increase the load of your database tier if there are many EC2 instances and the health check interval period is very quick.</p><p><strong>Amazon ElastiCache</strong> is a web service that makes it easy to set up, manage, and scale a distributed in-memory data store or cache environment in the cloud. It provides a high-performance, scalable, and cost-effective caching solution. At the same time, it helps remove the complexity associated with deploying and managing a distributed cache environment.</p><p>ElastiCache for Memcached has multiple features to enhance reliability for critical production deployments:</p><p>- Automatic detection and recovery from cache node failures.</p><p>- Automatic discovery of nodes within a cluster enabled for automatic discovery so that no changes need to be made to your application when you add or remove nodes.</p><p>- Flexible Availability Zone placement of nodes and clusters.</p><p>- Integration with other AWS services such as Amazon EC2, Amazon CloudWatch, AWS CloudTrail, and Amazon SNS to provide a secure, high-performance, managed in-memory caching solution.</p><p>The option that says: <strong>Change the target group health check to a simple HTML page instead of a page that queries the database. Create an Amazon Route 53 health check for the database dummy item web page to ensure that the application works as expected. Set up an Amazon CloudWatch alarm to send a notification to Admins when the health check fails </strong>is correct. Changing the target group health check to a simple HTML page will reduce the queries to the database tier. The Route 53 health check can act as the “external” check on a specific page that queries the database to ensure that the application is working as expected. The Route 53 health check has an overall lower request count compared to using the target group health check.</p><p>The option that says: <strong>Reduce the load on the database tier by creating an Amazon ElastiCache cluster to cache frequently requested database queries. Configure the application to use this cache when querying the RDS MySQL instance </strong>is correct. Since this is a retail web application, most of the queries will be read-intensive as customers are searching for products. ElastiCache is effective at caching frequent requests, which overall improves the application response time and reduces database queries.</p><p>The option that says: <strong>Reduce the load on the database tier by creating multiple read replicas for the Amazon RDS MySQL Multi-AZ cluster. Configure the web application to use the single reader endpoint of RDS for all read operations </strong>is incorrect. This may be possible because creating read replicas is recommended to increase the read performance of an RDS cluster. However, this option does not resolve the original intention to reduce the number of repetitive queries hitting the database.</p><p>The option that says: <strong>Change the target group health check to use a TCP check on the EC2 instances instead of a page that queries the database. Create an Amazon Route 53 health check for the database dummy item web page to ensure that the application works as expected. Set up an Amazon CloudWatch alarm to send a notification to Admins when the health check fails</strong> is incorrect. An Application Load Balancer does not support a TCP health check. ALB only supports HTTP and HTTPS target health checks.</p><p>The option that says: <strong>Create an Amazon CloudWatch alarm to monitor the Amazon RDS MySQL instance if it has a high-load or in impaired status. Set the alarm action to recover the RDS instance. This will automatically reboot the database to reset the queries </strong>is incorrect. Recovering the database instance results in downtime. If you have the Multi-AZ enabled, the standby database will shoulder all the load causing it to crash too. It is better to scale the database by creating read replicas or adding an ElastiCache cluster in front of it.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/WhatIs.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/WhatIs.html</a></p><p><br></p><p><strong>Check out these Amazon ElastiCache and Amazon RDS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/amazon-elasticache/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html",
      "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/WhatIs.html",
      "https://tutorialsdojo.com/amazon-elasticache/?src=udemy",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 13,
    "question": "<p>A company has several IoT enabled devices and sells them to customers around the globe. Every 5 minutes, each IoT device sends back a data file that includes the device status and other information to an Amazon S3 bucket. Every midnight, a Python cron job runs from an Amazon EC2 instance to read and process each data file on the S3 bucket and loads the values on a designated Amazon RDS database. The cron job takes about 10 minutes to process a day’s worth of data. After each data file is processed, it is eventually deleted from the S3 bucket. The company wants to expedite the process and access the processed data on the Amazon RDS as soon as possible.</p><p>Which of the following actions would you implement to achieve this requirement with the LEAST amount of effort?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Convert the Python script cron job to an AWS Lambda function. Configure AWS CloudTrail to log data events of the Amazon S3 bucket. Set up an Amazon EventBridge rule to trigger the Lambda function whenever an upload event on the S3 bucket occurs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Convert the Python script cron job to an AWS Lambda function. Create an Amazon EventBridge rule scheduled at 1-minute intervals and trigger the Lambda function. Create parallel CloudWatch rules that trigger the same Lambda function to further reduce the processing time.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Convert the Python script cron job to an AWS Lambda function. Configure the Amazon S3 bucket event notifications to trigger the Lambda function whenever an object is uploaded to the bucket.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Increase the Amazon EC2 instance size and spawn more instances to speed up the processing of the data files. Set the Python script cron job schedule to a 1-minute interval to further improve the access time.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>The <strong>Amazon S3 notification feature</strong> enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket. Amazon S3 event notifications are designed to be delivered at least once. Typically, event notifications are delivered in seconds but can sometimes take a minute or longer.</p><p><img src=\"https://media.tutorialsdojo.com/S3-events-notification.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/S3-events-notification.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Currently, Amazon S3 can publish notifications for the following events:</p><p><strong>New object created events</strong> — Amazon S3 supports multiple APIs to create objects. You can request a notification when only a specific API is used (for example, s3:ObjectCreated:Put), or you can use a wildcard (for example, s3:ObjectCreated:*) to request a notification when an object is created regardless of the API used.</p><p><strong>Object removal events</strong> — Amazon S3 supports deletes of versioned and unversioned objects. For information about object versioning, see Object Versioning and Using versioning.</p><p><strong>Restore object events</strong> — Amazon S3 supports the restoration of objects archived to the S3 Glacier storage classes. Your request to be notified of object restoration completion by using s3:ObjectRestore:Completed. You use s3:ObjectRestore:Post to request notification of the initiation of a restore.</p><p><strong>Reduced Redundancy Storage (RRS) object lost events</strong> — Amazon S3 sends a notification message when it detects that an object of the RRS storage class has been lost.</p><p><strong>Replication events</strong> — Amazon S3 sends event notifications for replication configurations that have S3 Replication Time Control (S3 RTC) enabled. It sends these notifications when an object fails replication when an object exceeds the 15-minute threshold, when an object is replicated after the 15-minute threshold, and when an object is no longer tracked by replication metrics. It publishes a second event when that object replicates to the destination Region.</p><p>Enabling notifications is a bucket-level operation; that is, you store notification configuration information in the notification subresource associated with a bucket. After creating or changing the bucket notification configuration, typically you need to wait 5 minutes for the changes to take effect. Amazon S3 supports the following destinations where it can publish events - Amazon Simple Notification Service (Amazon SNS) topic, Amazon Simple Queue Service (Amazon SQS) queue, and AWS Lambda.</p><p>Therefore, the correct answer is: <strong>Convert the Python script cron job to an AWS Lambda function. Configure the Amazon S3 bucket event notifications to trigger the Lambda function whenever an object is uploaded to the bucket</strong> because this provides the best processing and access time. Each of the data files will be processed almost immediately once uploaded on the S3 bucket.</p><p>The option that says: <strong>Convert the Python script cron job to an AWS Lambda function. Configure AWS CloudTrail to log data events of the Amazon S3 bucket. Set up an Amazon EventBridge rule to trigger the Lambda function whenever an upload event on the S3 bucket occurs </strong>is incorrect. Although this is possible, you do not have to use CloudTrail and CloudWatch Events to satisfy the given requirement. This solution entails a lot of steps. You can simply use the Amazon S3 event notification feature that can trigger the Lambda function directly.</p><p>The option that says: <strong>Increase the Amazon EC2 instance size and spawn more instances to speed up the processing of the data files. Set the Python script cron job schedule to a 1-minute interval to further improve the access time </strong>is incorrect. This solution is unreliable since the Amazon EC2 instances can process the same data file at the same time, and because of the limitations of <code>cron</code>, the minimum interval for processing is only 1 minute.</p><p>The option that says: <strong>Convert the Python script cron job to an AWS Lambda function. Create an Amazon EventBridge rule scheduled at 1-minute intervals and trigger the Lambda function. Create parallel CloudWatch rules that trigger the same Lambda function to further reduce the processing time </strong>is incorrect. The scheduled CloudWatch events rule can only have a minimum of 1-minute intervals. Using Amazon S3 Event notifications as triggers will result in almost near real-time processing of the data files.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html#with-s3-example-configure-event-source\">https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html#with-s3-example-configure-event-source</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html#with-s3-example-configure-event-source",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 14,
    "question": "<p>A company provides big data services to enterprise clients around the globe. One of the clients has 60 TB of raw data from their on-premises Oracle data warehouse. The data is to be migrated to Amazon Redshift. However, the database receives minor updates on a daily basis while major updates are scheduled every end of the month. The migration process must be completed within approximately 30 days before the next major update on the Redshift database. The company can only allocate 50 Mbps of Internet connection for this activity to avoid impacting business operations.</p><p>Which of the following actions will satisfy the migration requirements of the company while keeping the costs low?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an AWS Snowball import job to request for a Snowball Edge device. Use the AWS Schema Conversion Tool (SCT) to process the on-premises data warehouse and load it to the Snowball Edge device. Install the extraction agent on a separate on-premises server and register it with AWS SCT. Once the Snowball Edge imports data to the S3 bucket, use AWS SCT to migrate the data to Amazon Redshift. Configure a local task and AWS DMS task to replicate the ongoing updates to the data warehouse. Monitor and verify that the data migration is complete.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a new Oracle Database on Amazon RDS. Configure Site-to-Site VPN connection from the on-premises data center to the Amazon VPC. Configure replication from the on-premises database to Amazon RDS. Once replication is complete, create an AWS Schema Conversion Tool (SCT) project with AWS DMS task to migrate the Oracle database to Amazon Redshift. Monitor and verify if the data migration is complete before the cut-over.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Since you have a 30-day window for migration, configure VPN connectivity between AWS and the company's data center by provisioning a 1 Gbps AWS Direct Connect connection. Launch an Oracle Real Application Clusters (RAC) database on an EC2 instance and set it up to fetch and synchronize the data from the on-premises Oracle database. Once replication is complete, create an AWS DMS task on an AWS SCT project to migrate the Oracle database to Amazon Redshift. Monitor and verify if the data migration is complete before the cut-over.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an AWS Snowball Edge job using the AWS Snowball console. Export all data from the Oracle data warehouse to the Snowball Edge device. Once the Snowball device is returned to Amazon and data is imported to an S3 bucket, create an Oracle RDS instance to import the data. Create an AWS Schema Conversion Tool (SCT) project with AWS DMS task to migrate the Oracle database to Amazon Redshift. Copy the missing daily updates from Oracle in the data center to the RDS for Oracle database over the Internet. Monitor and verify if the data migration is complete before the cut-over.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p>You can use an <strong>AWS SCT</strong> agent to extract data from your on-premises data warehouse and migrate it to Amazon Redshift. The agent extracts your data and uploads the data to either Amazon S3 or, for large-scale migrations, an AWS Snowball Edge device. You can then use AWS SCT to copy the data to Amazon Redshift.</p><p>Large-scale data migrations can include many terabytes of information and can be slowed by network performance and by the sheer amount of data that has to be moved. AWS Snowball Edge is an AWS service you can use to transfer data to the cloud at faster-than-network speeds using an AWS-owned appliance. An AWS Snowball Edge device can hold up to 100 TB of data. It uses 256-bit encryption and an industry-standard Trusted Platform Module (TPM) to ensure both security and full chain-of-custody for your data. AWS SCT works with AWS Snowball Edge devices.</p><p>When you use AWS SCT and an AWS Snowball Edge device, you migrate your data in two stages. First, you use the AWS SCT to process the data locally and then move that data to the AWS Snowball Edge device. You then send the device to AWS using the AWS Snowball Edge process, and then AWS automatically loads the data into an Amazon S3 bucket. Next, when the data is available on Amazon S3, you use AWS SCT to migrate the data to Amazon Redshift. Data extraction agents can work in the background while AWS SCT is closed. You manage your extraction agents by using AWS SCT. The extraction agents act as listeners. When they receive instructions from AWS SCT, they extract data from your data warehouse.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_sct.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_sct.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Create an AWS Snowball import job to request for a Snowball Edge device. Use the AWS Schema Conversion Tool (SCT) to process the on-premises data warehouse and load it to the Snowball Edge device. Install the extraction agent on a separate on-premises server and register it with AWS SCT. Once the Snowball Edge imports data to the S3 bucket, use AWS SCT to migrate the data to Amazon Redshift. Configure a local task and AWS DMS task to replicate the ongoing updates to the data warehouse. Monitor and verify that the data migration is complete.</strong></p><p>The option that says: <strong>Create a new Oracle Database on Amazon RDS. Configure Site-to-Site VPN connection from the on-premises data center to the Amazon VPC. Configure replication from the on-premises database to Amazon RDS. Once replication is complete, create an AWS Schema Conversion Tool (SCT) project with AWS DMS task to migrate the Oracle database to Amazon Redshift. Monitor and verify if the data migration is complete before the cut-over</strong> is incorrect. Replicating 60 TB worth of data over the public Internet will take several days over the 30-day migration window. It is also stated in the scenario that the company can only allocate 50 Mbps of Internet connection for the migration activity. Sending the data over the Internet could potentially affect business operations.</p><p>The option that says: <strong>Create an AWS Snowball Edge job using the AWS Snowball console. Export all data from the Oracle data warehouse to the Snowball Edge device. Once the Snowball device is returned to Amazon and data is imported to an S3 bucket, create an Oracle RDS instance to import the data. Create an AWS Schema Conversion Tool (SCT) project with AWS DMS task to migrate the Oracle database to Amazon Redshift. Copy the missing daily updates from Oracle in the data center to the RDS for Oracle database over the internet. Monitor and verify if the data migration is complete before the cut-over</strong> is incorrect. You need to configure the data extraction agent first on your on-premises server. In addition, you don't need the data to be imported and exported via Amazon RDS. AWS DMS can directly migrate the data to Amazon Redshift.</p><p>The option that says: <strong>Since you have a 30-day window for migration, configure VPN connectivity between AWS and the company's data center by provisioning a 1 Gbps AWS Direct Connect connection. Install Oracle database on an EC2 instance that is configured to synchronize with the on-premises Oracle database. Once replication is complete, create an AWS DMS task on an AWS SCT project to migrate the Oracle database to Amazon Redshift. Monitor and verify if the data migration is complete before the cut-over Since you have a 30-day window for migration, configure VPN connectivity between AWS and the company's data center by provisioning a 1 Gbps AWS Direct Connect connection. Launch an Oracle Real Application Clusters (RAC) database on an EC2 instance and set it up to fetch and synchronize the data from the on-premises Oracle database. Once replication is complete, create an AWS DMS task on an AWS SCT project to migrate the Oracle database to Amazon Redshift. Monitor and verify if the data migration is complete before the cut-over</strong> is incorrect. Although this is possible, the company wants to keep the cost low. Using a Direct Connect connection for a one-time migration is not a cost-effective solution.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/getting-started/hands-on/migrate-oracle-to-amazon-redshift/\">https://aws.amazon.com/getting-started/hands-on/migrate-oracle-to-amazon-redshift/</a></p><p><a href=\"https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.dw.html\">https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.dw.html</a></p><p><a href=\"https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.html\">https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.html</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/getting-started/hands-on/migrate-oracle-to-amazon-redshift/",
      "https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.dw.html",
      "https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.html",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 15,
    "question": "<p>A company wants to launch its online shopping website to give customers an easy way to purchase the products they need. The proposed setup is to host the application on an AWS Fargate cluster, utilize a Load Balancer to distribute traffic between the Fargate tasks, and use Amazon CloudFront for caching and content delivery. The company wants to ensure that the website complies with industry best practices and should be able to protect customers from common “man-in-the-middle” attacks for e-commerce websites such as DNS spoofing, HTTPS spoofing, or SSL hijacking.</p><p>Which of the following configurations will provide the MOST secure access to the website?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Register the domain name on Route 53 and enable DNSSEC validation for all public hosted zones to ensure that all DNS requests have not been tampered with during transit. Use AWS Certificate Manager (ACM) to generate a valid TLS/SSL certificate for the domain name. Configure the Application Load Balancer with an HTTPS listener to use the ACM TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Route 53 for domain registration. Use a third-party DNS service that supports DNSSEC for DNS requests that use the customer-managed keys. Use AWS Certificate Manager (ACM) to generate a valid 2048-bit TLS/SSL certificate for the domain name and configure the Application Load Balancer HTTPS listener to use this TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Register the domain name on Route 53. Use a third-party DNS provider that supports the import of the customer-managed keys for DNSSEC. Import a 2048-bit TLS/SSL certificate from a third-party certificate service to AWS Certificate Manager (ACM). Configure the Application Load Balancer with an HTTPS listener to use the imported TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Register the domain name on Route 53. Since Route 53 only supports DNSSEC for registration, host the company DNS root servers on Amazon EC2 instances running the BIND service. Enable DNSSEC for DNS requests to ensure the replies have not been tampered with. Generate a valid certificate for the website domain name on AWS ACM and configure the Application Load Balancers HTTPS listener to use this TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>Amazon now allows you to enable <strong>Domain Name System Security Extensions (DNSSEC)</strong> signing for all existing and new public hosted zones, and enable DNSSEC validation for Amazon Route 53 Resolver. Amazon Route 53 DNSSEC provides data origin authentication and data integrity verification for DNS and can help customers meet compliance mandates, such as FedRAMP.</p><p>When you enable DNSSEC signing on a hosted zone, Route 53 cryptographically signs each record in that hosted zone. Route 53 manages the zone-signing key, and you can manage the key-signing key in AWS Key Management Service (AWS KMS). Amazon’s domain name registrar, Route 53 Domains, already supports DNSSEC, and customers can now register domains and host their DNS on Route 53 with DNSSEC signing enabled. When you enable DNSSEC validation on the Route 53 Resolver in your VPC, it ensures that DNS responses have not been tampered with in transit. This can prevent DNS Spoofing.</p><p><img src=\"https://media.tutorialsdojo.com/Route53_DNSSEC.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/Route53_DNSSEC.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>AWS Certificate Manager</strong> is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Certificate Manager removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates. Using a valid SSL Certificate for your application load balancer ensures that all requests are encrypted on transit as well as protection against SSL hijacking.</p><p>CloudFront supports Server Name Indication (SNI) for custom SSL certificates, along with the ability to take incoming HTTP requests and redirect them to secure HTTPS requests to ensure that clients are always directed to the secure version of your website.</p><p>Therefore, the correct answer is: <strong>Register the domain name on Route 53 and enable DNSSEC validation for all public hosted zones to ensure that all DNS requests have not been tampered with during transit. Use AWS Certificate Manager (ACM) to generate a valid TLS/SSL certificate for the domain name. Configure the Application Load Balancer with an HTTPS listener to use the ACM TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront.</strong></p><p>The option that says: <strong>Register the domain name on Route 53. Use a third-party DNS provider that supports the import of the customer-managed keys for DNSSEC. Import a 2048-bit TLS/SSL certificate from a third-party certificate service to AWS Certificate Manager (ACM). Configure the Application Load Balancer with an HTTPS listener to use the imported TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront</strong> is incorrect. Although this is possible, you don’t have to rely on a third-party DNS provider as Route 53 supports DNSSEC signing. Also, ACM can secure a 2048-bit TLS/SSL Certificate for free so you don't have to buy certificates from other providers.</p><p>The option that says:<strong> Use Route 53 for domain registration. Use a third-party DNS service that supports DNSSEC for DNS requests that use the customer-managed keys. Use AWS Certificate Manager (ACM) to generate a valid 2048-bit TLS/SSL certificate for the domain name and configure the Application Load Balancer HTTPS listener to use this TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront </strong>is incorrect. This is also possible, but you don't have to rely on a third-party DNS provider as Amazon Route 53 already supports DNSSEC signing.</p><p>The option that says:<strong> Register the domain name on Route 53. Since Route 53 only supports DNSSEC for registration, host the company DNS root servers on Amazon EC2 instances running the BIND service. Enable DNSSEC for DNS requests to ensure the replies have not been tampered with. Generate a valid certificate for the website domain name on AWS ACM and configure the Application Load Balancers HTTPS listener to use this TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront</strong> is incorrect as this solution is no longer recommended. This setup was previously used as a workaround when DNSSEC signing was not supported natively yet in Amazon Route 53.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-configure-dnssec.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-configure-dnssec.html</a></p><p><a href=\"https://docs.aws.amazon.com/acm/latest/userguide/acm-services.html\">https://docs.aws.amazon.com/acm/latest/userguide/acm-services.html</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2020/12/announcing-amazon-route-53-support-dnssec/\">https://aws.amazon.com/about-aws/whats-new/2020/12/announcing-amazon-route-53-support-dnssec/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><br></p><p><strong>Check out these AWS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><a href=\"https://tutorialsdojo.com/aws-certificate-manager/?src=udemy\">https://tutorialsdojo.com/aws-certificate-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-configure-dnssec.html",
      "https://docs.aws.amazon.com/acm/latest/userguide/acm-services.html",
      "https://aws.amazon.com/about-aws/whats-new/2020/12/announcing-amazon-route-53-support-dnssec/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html",
      "https://tutorialsdojo.com/amazon-route-53/?src=udemy",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy",
      "https://tutorialsdojo.com/aws-certificate-manager/?src=udemy"
    ]
  },
  {
    "id": 16,
    "question": "<p>A media company processes and converts its video collection using the AWS Cloud. The videos are processed by an Auto Scaling group of Amazon EC2 instances which scales based on the number of videos on the Amazon Simple Queue Service (SQS) queue. Each video takes about 20-40 minutes to be processed.</p><p>To ensure videos are processed, the management has set a redrive policy on the SQS queue to be used as a dead-letter queue. The visibility timeout has been set to 1 hour and the <code>maxReceiveCount</code> has been set to 1. When there are messages on the dead-letter queue, an Amazon CloudWatch alarm has been set up to notify the development team.</p><p>Within a few days of operation, the dead-letter queue received several videos that failed to process. The development received notifications of messages on the dead-letter queue but they did not find any operational errors based on the application logs.</p><p>Which of the following options should the solutions architect implement to help solve the above problem?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure a higher delivery delay setting on the Amazon SQS queue. This will give time for the consumers more time to pick up the messages on the SQS queue.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The videos were not processed because the Amazon EC2 scale-up process takes too long. Set a minimum number of EC2 instances on the Auto Scaling group to solve this.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Reconfigure the SQS redrive policy and set <code>maxReceiveCount</code> to 10. This will allow the consumers to retry the messages before sending them to the dead-letter queue.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Some of the videos took longer than 1 hour to process. Update the visibility timeout for the Amazon SQS queue to 2 hours to solve this problem.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon SQS</strong> supports <em>dead-letter queues</em> (DLQ), which other queues (<em>source queues</em>) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.</p><p>Occasionally, producers and consumers might fail to interpret aspects of the protocol that they use to communicate, causing message corruption or loss. Also, the consumer's hardware errors might corrupt message payload. If a message can't be consumed successfully, you can send it to a dead-letter queue (DLQ). Dead-letter queues let you isolate problematic messages to determine why they are failing.</p><p>The <strong>Maximum receives</strong> value determines when a message will be sent to the DLQ. If the <strong>ReceiveCount</strong> for a message exceeds the maximum receive count for the queue, Amazon SQS moves the message to the associated DLQ (with its original message ID).</p><p>As you redrive your messages, the redrive status will show you the most recent message redrive status for your dead-letter queue.</p><p>The <em>redrive policy</em> specifies the <em>source queue</em>, the <em>dead-letter queue</em>, and the conditions under which Amazon SQS moves messages from the former to the latter if the consumer of the source queue fails to process a message a specified number of times. The maxReceiveCount is the number of times a consumer tries receiving a message from a queue without deleting it before being moved to the dead-letter queue. Setting the maxReceiveCount to a low value, such as 1 would result in any failure to receive a message to cause the message to be moved to the dead-letter queue. Such failures include network errors and client dependency errors.</p><p>The <em>redrive allow policy</em> specifies which source queues can access the dead-letter queue. This policy applies to a potential dead-letter queue. You can choose whether to allow all source queues, allow specific source queues, or deny all source queues. The default is to allow all source queues to use the dead-letter queue.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_redrive_maxreceivecount.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_sqs_redrive_maxreceivecount.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Reconfigure the SQS redrive policy and set </strong><code><strong>maxReceiveCount</strong></code><strong> to 10. This will allow the consumers to retry the message from the dead-letter queue.</strong> This setting ensures that any message that failed to be processed will be sent back to the queue to be picked up by other consumers and re-processed.</p><p>The option that says: <strong>The videos were not processed because the Amazon EC2 scale-up process takes too long. Set a minimum number of EC2 instances on the Auto Scaling group to solve this</strong> is incorrect. The Auto Scaling group responds to the number of messages on the queue, setting a fixed minimum number of instances is not cost-effective when there are no messages on the SQS queue.</p><p>The option that says: <strong>Some of the videos took longer than 1 hour to process. Update the visibility timeout for the Amazon SQS queue to 2 hours to solve this problem</strong> is incorrect. Even though the visibility timeout is set for longer, the failed videos to be processed should be retried again by other consumers. That's why a <code>maxReceiveCount</code> setting is a much better option.</p><p>The option that says: <strong>Configure a higher delivery delay setting on the Amazon SQS queue. This will give time for the consumers more time to pick up the messages on the SQS queue</strong> is incorrect. This setting does not affect the videos that were already on the queue, picked up for processing, but failed to process completely.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/introducing-amazon-simple-queue-service-dead-letter-queue-redrive-to-source-queues/\">https://aws.amazon.com/blogs/compute/introducing-amazon-simple-queue-service-dead-letter-queue-redrive-to-source-queues/</a></p><p><br></p><p><strong>Check out these Amazon SQS and AWS Auto Scaling Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html",
      "https://aws.amazon.com/blogs/compute/introducing-amazon-simple-queue-service-dead-letter-queue-redrive-to-source-queues/",
      "https://tutorialsdojo.com/amazon-sqs/?src=udemy",
      "https://tutorialsdojo.com/aws-auto-scaling/?src=udemy"
    ]
  },
  {
    "id": 17,
    "question": "<p>A company runs a Flight Deals web application which is currently hosted on their on-premises data center. The website hosts high-resolution photos of top tourist destinations in the world and uses a third-party payment platform to accept payments. Recently, the company heavily invested in their global marketing campaign and there is a high probability that the incoming traffic to their Flight Deals website will increase in the coming days. Due to a tight deadline, the company does not have the time to fully migrate the website to the AWS cloud. A set of security rules that block common attack patterns, such as SQL injection and cross-site scripting should also be implemented to improve website security.</p><p>Which of the following options will maintain the website's functionality despite the massive amount of incoming traffic?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AWS Server Migration Service to easily migrate the website from your on-premises data center to your VPC. Create an Auto Scaling group to automatically scale the web tier based on the incoming traffic. Deploy AWS WAF on the Amazon CloudFront distribution to protect the website from common web attacks.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Generate an AMI based on the existing Flight Deals website. Launch the AMI to a fleet of EC2 instances with Auto Scaling group enabled, for it to automatically scale up or scale down based on the incoming traffic. Place these EC2 instances behind an ALB which can balance traffic between the web servers in the on-premises data center and the web servers hosted in AWS.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use CloudFront to cache and distribute the high resolution images and other static assets of the website. Deploy AWS WAF on the Amazon CloudFront distribution to protect the website from common web attacks.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create and configure an S3 bucket as a static website hosting. Move the web domain of the website from your on-premises data center to Route 53 then route the newly created S3 bucket as the origin. Enable Amazon S3 server-side encryption with AWS Key Management Service managed keys.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p><strong>Amazon CloudFront</strong> is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the user is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.</p><p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define. You can get started quickly using Managed Rules for AWS WAF, a pre-configured set of rules managed by AWS or AWS Marketplace Sellers. The Managed Rules for WAF address issues like the OWASP Top 10 security risks. These rules are regularly updated as new issues emerge. AWS WAF includes a full-featured API that you can use to automate the creation, deployment, and maintenance of security rules.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_waf.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_waf.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>AWS WAF is easy to deploy and protect applications deployed on either Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts all your origin servers, or Amazon API Gateway for your APIs. There is no additional software to deploy, DNS configuration, SSL/TLS certificate to manage, or need for a reverse proxy setup. With AWS Firewall Manager integration, you can centrally define and manage your rules, and reuse them across all the web applications that you need to protect.</p><p>Hence, the option that says: <strong>Use CloudFront to cache and distribute the high resolution images and other static assets of the website. Deploy AWS WAF on the Amazon CloudFront distribution to protect the website from common web attacks</strong> is correct because CloudFront will provide the scalability the website needs without doing major infrastructure changes. Take note that the website has a lot of high-resolution images which can easily be cached using CloudFront to alleviate the massive incoming traffic going to the on-premises web server and also provide a faster page load time for the web visitors.</p><p>The option that says: <strong>Use the AWS Server Migration Service to easily migrate the website from your on-premises data center to your VPC. Create an Auto Scaling group to automatically scale the web tier based on the incoming traffic. Deploy AWS WAF on the Amazon CloudFront distribution to protect the website from common web attacks</strong> is incorrect as migrating to AWS would be time-consuming compared with simply using CloudFront. Although this option can provide a more scalable solution, the scenario says that the company does not have ample time to do the migration.</p><p>The option that says:<strong><em> </em>Create and configure an S3 bucket as a static website hosting. Move the web domain of the website from your on-premises data center to Route53 then route the newly created S3 bucket as the origin. Enable Amazon S3 server-side encryption with AWS Key Management Service managed keys</strong> is incorrect because the website is a dynamic website that accepts payments and bookings. Migrating your web domain to Route 53 may also take some time.</p><p>The option that says: <strong>Generate an AMI based on the existing Flight Deals website. Launch the AMI to a fleet of EC2 instances with Auto Scaling group enabled, for it to automatically scale up or scale down based on the incoming traffic. Place these EC2 instances behind an ALB which can balance traffic between the web servers in the on-premises data center and the web servers hosted in AWS</strong> is incorrect because it didn't mention any existing AWS Direct Connect or VPN connection. Although an Application Load Balancer can load balance the traffic between the EC2 instances in AWS Cloud and web servers located in the on-premises data center, your systems should be connected via Direct Connect or VPN connection first. In addition, the application seems to be used around the world because the company launched a global marketing campaign. Hence, CloudFront is a more suitable option for this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy"
    ]
  },
  {
    "id": 18,
    "question": "<p>A startup develops Internet-Of-Things (IoT) devices that provide health monitoring for dogs and cats which is integrated into their collars. The startup has an engineering team to build a smart pet collar that collects biometric information of the pet every second and then sends it to a web portal through a POST API request. The Solutions Architect has been tasked to set up the API services and the web portal that will accept and process the biometric data as well as provide complete trends and health reports to pet owners around the globe. The portal should be highly durable, available, and scalable with an additional feature for showing real-time biometric data analytics and monitoring.</p><p>Which of the following is the best architecture that the Solutions Architect should implement to meet the above requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>1. Create an Amazon SQS queue to collect the incoming biometric data.</p><p>2. Analyze the data from SQS with Amazon Kinesis.</p><p>3. Store the results to an Amazon RDS for MySQL database.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>1. Create an Amazon S3 bucket to collect the incoming biometric data from the smart pet collar.</p><p>2. Use Amazon Data Pipeline to run a data analysis task in the S3 bucket every day.</p><p>3. Use Amazon Redshift as the online analytic processing (OLAP) database for the web portal.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>1. Use Amazon Kinesis Data Streams to collect the incoming biometric data.</p><p>2. Analyze the data using Amazon Kinesis and show the results in a real-time dashboard.</p><p>3. Set up a simple data aggregation process and pass the results to Amazon S3.</p><p>4. Store the data to Amazon Redshift, configured with automated backups, to handle complex analytics.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>1. Launch an Amazon Elastic MapReduce instance to collect the incoming biometrics data.</p><p>2. Use Amazon Kinesis to analyze the data.</p><p>3. Save the results to an Amazon DynamoDB table.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon Kinesis Data Streams</strong> enable you to build custom applications that process or analyze streaming data for specialized needs. Kinesis Data Streams can continuously capture and store terabytes of data per hour from hundreds of thousands of sources such as website clickstreams, financial transactions, social media feeds, IT logs, and location-tracking events. With the Kinesis Client Library (KCL), you can build Kinesis Applications and use streaming data to power real-time dashboards, generate alerts, implement dynamic pricing and advertising, and more. You can also emit data from Kinesis Data Streams to other AWS services such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon EMR, and AWS Lambda.</p><p><img src=\"https://media.tutorialsdojo.com/sap_kinesis_arch.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_kinesis_arch.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Kinesis Data Streams</strong> can be used to collect log and event data from sources such as servers, desktops, and mobile devices. You can then build Kinesis Applications to continuously process the data, generate metrics, power live dashboards, and emit aggregated data into stores such as Amazon S3.</p><p>You can have your Kinesis Applications run real-time analytics on high frequency event data such as sensor data collected by Kinesis Data Streams, which enables you to gain insights from your data at a frequency of minutes instead of hours or days.</p><p>Hence, the following option is the correct answer as Amazon Kinesis is the one used here to collect the streaming data:</p><p><strong>1. Use Amazon Kinesis Data Streams to collect the incoming biometric data.</strong></p><p><strong>2. Analyze the data using Amazon Kinesis and show the results in a real-time dashboard.</strong></p><p><strong>3. Set up a simple data aggregation process and pass the results to Amazon S3.</strong></p><p><strong>4. Store the data to Amazon Redshift, configured with automated backups, to handle complex analytics.</strong></p><p>The following option is incorrect because S3, Data Pipeline, and Redshift do not provide real-time data analytics:</p><p><strong>1. Create an Amazon S3 bucket to collect the incoming biometric data from the smart pet collar.</strong></p><p><strong>2. Use Amazon Data Pipeline to run a data analysis task in the S3 bucket every day.</strong></p><p><strong>3. Use Amazon Redshift as the online analytic processing (OLAP) database for the web portal.</strong></p><p>The following option is incorrect because an SQS queue is not appropriate to use to accept all of the incoming biometric data. You should use Amazon Kinesis instead:</p><p><strong>1. Create an Amazon SQS queue to collect the incoming biometric data.</strong></p><p><strong>2. Analyze the data from SQS with Amazon Kinesis.</strong></p><p><strong>3. Store the results to an Amazon RDS for MySQL database<em>.</em></strong></p><p>The following option is incorrect because just like in the above, it does not use Amazon Kinesis to accept the incoming data:</p><p><strong>1. Launch an Amazon Elastic MapReduce instance to collect the incoming biometrics data.</strong></p><p><strong>2. Use Amazon Kinesis to analyze the data.</strong></p><p><strong>3. Save the results to an Amazon DynamoDB table.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/details/\">https://aws.amazon.com/kinesis/data-streams/details/</a></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/introduction.html\">https://docs.aws.amazon.com/streams/latest/dev/introduction.html</a></p><p><br></p><p><strong>Check out this Amazon Kinesis Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/details/",
      "https://docs.aws.amazon.com/streams/latest/dev/introduction.html",
      "https://tutorialsdojo.com/amazon-kinesis/?src=udemy"
    ]
  },
  {
    "id": 19,
    "question": "<p>A company runs hundreds of Windows-based Amazon EC2 instances on AWS. The Solutions Architect has been assigned to develop a workflow to ensure that the required patches of all Windows EC2 instances are properly identified and applied automatically. To maintain their system uptime requirements, it is of utmost importance to ensure that the EC2 instance reboots do not occur at the same time on all of their Windows instances. This is to avoid any loss of revenue that could be caused by any unavailability issues of their systems.</p><p>Which of the following will meet the above requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create two Patch Groups with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined <code>AWS-DefaultPatchBaseline</code> baseline on both patch groups. Set up two non-overlapping maintenance windows and associate each with a different patch group. Using Patch Group tags, register targets with specific maintenance windows and lastly, assign the <code>AWS-RunPatchBaseline</code> document as a task within each maintenance window which has a different processing start time.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a Patch Group with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined <code>AWS-DefaultPatchBaseline</code><strong> </strong>baseline on both patch groups. Create a CloudWatch Events rule configured to use a cron expression to automate the execution of patching in a given schedule using the AWS Systems Manager Run command. Set up an AWS Systems Manager State Manager document to define custom commands which will be executed during patch execution.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create two Patch Groups with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined <code>AWS-DefaultPatchBaseline</code><strong> </strong>baseline on both patch groups. Create two CloudWatch Events rules which are configured to use a cron expression to automate the execution of patching for the two Patch Groups using the AWS Systems Manager Run command. Set up an AWS Systems Manager State Manager document to define custom commands which will be executed during patch execution.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a Patch Group with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined <code>AWS-DefaultPatchBaseline</code><strong> </strong>baseline on your patch group. Set up a maintenance window and associate it with your patch group. Assign the <code>AWS-RunPatchBaseline</code><strong> </strong>document as a task within your maintenance window.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications.</p><p>You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Patch Manager uses <em>patch baselines</em>, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p>You can use a <em>patch group</em> to associate instances with a specific patch baseline. Patch groups help ensure that you are deploying the appropriate patches, based on the associated patch baseline rules, to the correct set of instances. Patch groups can also help you avoid deploying patches before they have been adequately tested. For example, you can create patch groups for different environments (such as Development, Test, and Production) and register each patch group to an appropriate patch baseline.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_patch_group.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ssm_patch_group.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>When you run <code>AWS-RunPatchBaseline</code>, you can target managed instances using their instance ID or tags. SSM Agent and Patch Manager will then evaluate which patch baseline to use based on the patch group value that you added to the instance.</p><p>You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group <em>must</em> be defined with the tag key: <strong>Patch Group</strong>. Note that the key is case-sensitive. You can specify any value, for example, \"web servers,\" but the key must be <strong>Patch Group</strong>.</p><p>The <code>AWS-DefaultPatchBaseline</code> baseline is primarily used to approve all Windows Server operating system patches that are classified as \"CriticalUpdates\" or \"SecurityUpdates\" and that have an MSRC severity of \"Critical\" or \"Important\". Patches are auto-approved seven days after release.</p><p>Hence, the option that says: <strong>Create two Patch Groups with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined </strong><code><strong>AWS-DefaultPatchBaseline</strong></code><strong> baseline on both patch groups. Set up two non-overlapping maintenance windows and associate each with a different patch group. Using Patch Group tags, register targets with specific maintenance windows and lastly, assign the </strong><code><strong>AWS-RunPatchBaseline</strong></code><strong> document as a task within each maintenance window which has a different processing start time</strong> is the correct answer as it properly uses two Patch Groups, non-overlapping maintenance windows and the <code>AWS-DefaultPatchBaseline</code> baseline to ensure that the EC2 instance reboots do not occur at the same time.</p><p>The option that says: <strong>Create a Patch Group with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined </strong><code><strong>AWS-DefaultPatchBaseline</strong></code><strong> baseline on your patch group. Set up a maintenance window and associate it with your patch group. Assign the </strong><code><strong>AWS-RunPatchBaseline</strong></code><strong> document as a task within your maintenance window</strong> is incorrect. Although it is correct to use a Patch Group, you must create another Patch Group to avoid any unavailability issues. Having two non-overlapping maintenance windows will ensure that there will be another set of running Windows EC2 instances while the other set is being patched.</p><p>The option that says: <strong>Create two Patch Groups with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined </strong><code><strong>AWS-DefaultPatchBaseline</strong></code><strong> baseline on both patch groups. Create two CloudWatch Events rules which are configured to use a cron expression to automate the execution of patching for the two Patch Groups using the AWS Systems Manager Run command. Set up an AWS Systems Manager State Manager document to define custom commands which will be executed during patch execution</strong> is incorrect. The AWS Systems Manager Run Command is primarily used to remotely manage the configuration of your managed instances while AWS Systems Manager State Manager is just a configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. These two services, including CloudWatch Events, are not suitable to be used in this scenario. The better solution would be to use AWS Systems Manager Maintenance Windows which lets you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches.</p><p>The option that says: <strong>Create a Patch Group with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined </strong><code><strong>AWS-DefaultPatchBaseline</strong></code><strong> baseline on both patch groups. Create a CloudWatch Events rule configured to use a cron expression to automate the execution of patching in a given schedule using the AWS Systems Manager Run command. Set up an AWS Systems Manager State Manager document to define custom commands which will be executed during patch execution</strong> is incorrect. Just as what is mentioned in the above, you have to use Maintenance Windows for scheduling the patches and you also need to set up two Patch Groups in this scenario instead of one.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/\">https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-ssm-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-ssm-documents.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-scheduletasks.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-scheduletasks.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-ssm-documents.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-scheduletasks.html",
      "https://tutorialsdojo.com/aws-systems-manager/?src=udemy"
    ]
  },
  {
    "id": 20,
    "question": "<p>A multinational consumer goods corporation structured their AWS accounts to use AWS Organizations, which consolidates payment of their multiple AWS accounts for their various Business Units (BU’s) namely Beauty products, Baby products, Health products, and Home Care products unit. One of their Solutions Architects for the Baby products business unit has purchased 10 Reserved Instances for their new Supply Chain application which will go live 3 months from now. However, they do not want their Reserved Instance (RI) discounts to be shared by the other business units. </p><p>Which of the following options is the most suitable solution for this scenario?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Since the Baby product business unit is part of an AWS Organization, the Reserved Instances will always be shared across other member accounts. There is no way to disable this setting.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set the Reserved Instance (RI) sharing to private on the AWS account of the Baby products business unit.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Turn off the Reserved Instance (RI) sharing on the master account for all of the member accounts in the Baby products business unit.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Remove the AWS account of the Baby products business unit out of the AWS Organization.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>For billing purposes, the consolidated billing feature of <strong>AWS Organizations</strong> treats all the accounts in the organization as one account. This means that all accounts in the organization can receive the hourly cost-benefit of Reserved Instances that are purchased by any other account. In the payer account, you can turn off Reserved Instance discount sharing on the Preferences page on the Billing and Cost Management console.</p><p>The master account of an organization can turn off Reserved Instance (RI) sharing for member accounts in that organization. This means that Reserved Instances are not shared between that member account and other member accounts. You can change this preference multiple times. Each estimated bill is computed using the last set of preferences. However, take note that turning off Reserved Instance sharing can result in a higher monthly bill.</p><p><img src=\"https://media.tutorialsdojo.com/sap_consolidated_billing_ri.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_consolidated_billing_ri.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Hence, the correct answer is: <strong>Turn off the Reserved Instance (RI) sharing on the master account for all of the member accounts in the Baby products business unit.</strong></p><p>The option that says: <strong>Set the Reserved Instance (RI) sharing to private on the AWS account of the Baby products business unit</strong> is incorrect because there is no \"private\" option in the RI and Savings Plan discount sharing settings in the Billing Management Console. By default, the member account doesn't have the capability to turn off RI sharing on their account.</p><p>The option that says: <strong>Remove the AWS account of the Baby products business unit out of the AWS Organization</strong> is incorrect because removing the Baby products business unit account from the AWS Organization is not the optimal solution to prevent the other account from sharing its RI discounts. You can simply turn off the Reserved Instance discount sharing in the payer account.</p><p>The option that says: <strong>Since the Baby product business unit is part of an AWS Organization, the Reserved Instances will always be shared across other member accounts. There is no way to disable this setting is</strong> incorrect because this statement is false. There is certainly a way to disable the current setting by simply turning off RI sharing.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off.html</a></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off-process.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off-process.html</a></p><p><br></p><p><strong>Check out this AWS Billing and Cost Management Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy\">https://tutorialsdojo.com/aws-billing-and-cost-management/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off.html",
      "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off-process.html",
      "https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy"
    ]
  },
  {
    "id": 21,
    "question": "<p>An IT consultancy company has multiple offices located in San Francisco, Frankfurt, Tokyo, and Manila. The company is using AWS Organizations to easily manage its several AWS accounts which are being used by its regional offices and subsidiaries. A new AWS account was recently added to a specific organizational unit (OU) which is responsible for the overall systems administration. The solutions architect noticed that the account is using a root-created Amazon ECS Cluster with an attached service-linked role. For regulatory purposes, the solutions architect created a custom SCP that would deny the new account from performing certain actions in relation to using ECS. However, after applying the policy, the new account could still perform the actions that it was supposed to be restricted from doing.</p><p>Which of the following is the most likely reason for this problem?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SCPs do not affect any service-linked role. Service-linked roles enable other AWS services to integrate with AWS Organizations and can't be restricted by SCPs.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>The default SCP grants all permissions attached to every root, OU, and account. To apply stricter permissions, this policy is required to be modified.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>There is an SCP attached to a higher-level OU that permits the actions of the service-linked role. This permission would therefore be inherited by the current OU, and override the SCP placed by the administrator.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The ECS service is being run outside the jurisdiction of the organization. SCPs affect only the principals that are managed by accounts that are part of the organization.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>Users and roles must still be granted permissions using IAM permission policies attached to them or to groups. The SCPs filter the permissions granted by such policies, and the user can't perform any actions that the applicable SCPs don't allow. Actions allowed by the SCPs can be used if they are granted to the user or role by one or more IAM permission policies.</p><p><img src=\"https://media.tutorialsdojo.com/sap_scp_how_it_works.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_scp_how_it_works.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>When you attach SCPs to the root, OUs, or directly to accounts, all policies that affect a given account are evaluated together using the same rules that govern IAM permission policies:</p><p>- Any action that has an explicit <code>Deny</code> in an SCP can't be delegated to users or roles in the affected accounts. An explicit <code>Deny</code> statement overrides any <code>Allow</code> that other SCPs might grant.</p><p>- Any action that has an explicit <code>Allow</code> in an SCP (such as the default \"*\" SCP or by any other SCP that calls out a specific service or action) can be delegated to users and roles in the affected accounts.</p><p>- Any action that isn't explicitly allowed by an SCP is implicitly denied and can't be delegated to users or roles in the affected accounts.</p><p>By default, an SCP named <code>FullAWSAccess</code> is attached to every root, OU, and account. This default SCP allows all actions and all services. So in a new organization, until you start creating or manipulating the SCPs, all of your existing IAM permissions continue to operate as they did. As soon as you apply a new or modified SCP to a root or OU that contains an account, the permissions that your users have in that account become filtered by the SCP. Permissions that used to work might now be denied if they're not allowed by the SCP at every level of the hierarchy down to the specified account.</p><p>As stated in the documentation of AWS Organizations, <strong>SCPs DO NOT affect any service-linked role. Service-linked roles enable other AWS services to integrate with AWS Organizations and can't be restricted by SCPs.</strong></p><p><img src=\"https://media.tutorialsdojo.com/sap_scp_org_attached.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_scp_org_attached.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The option that says: <strong>The default SCP grants all permissions attached to every root, OU, and account. To apply stricter permissions, this policy is required to be modified</strong> is incorrect. The scenario already implied that the administrator created a <strong>Deny</strong> policy. By default, an SCP named <em>FullAWSAccess</em> is attached to every root, OU, and account. This default SCP allows all actions and all services. However, you specify a <strong>Deny</strong> policy if you want to create a blacklist that blocks all access to the specified services and actions. The explicit <strong>Deny</strong> on specific actions in the blacklist policy overrides the <strong>Allow</strong> in any other policy, such as the one in the default SCP.</p><p>The option that says: <strong>There is an SCP attached to a higher-level OU that permits the actions of the service-linked role. This permission would therefore be inherited by the current OU, and override the SCP placed by the administrator</strong> is incorrect because even if a higher-level OU has an SCP attached with an <strong>Allow</strong> policy for the service, the current set up should still have restricted access to the service. Creating and attaching a new <strong>Deny</strong> SCP to the new account's OU will not be affected by the pre-existing Allow policy in the same OU.</p><p>The option that says: <strong>The ECS service is being run outside the jurisdiction of the organization. SCPs affect only the principals that are managed by accounts that are part of the organization </strong>is incorrect because the service-linked role must have been created within the organization, most notably by the root account of the organization. It also does not make sense if we make the assumption that the service is indeed outside of the organization's jurisdiction because the <em>Principal</em> element of a policy specifies which entity will have limited permissions. But the scenario tells us that it should be the new account that is denied certain actions, not the service itself.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html",
      "https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy",
      "https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy"
    ]
  },
  {
    "id": 22,
    "question": "<p>A company uses Lightweight Directory Access Protocol (LDAP) for its employee authentication and authorization. The company plans to release a mobile app that can be installed on employee’s smartphones. The mobile application will allow users to have federated access to AWS resources. Due to strict security and compliance requirements, the mobile application must use a custom-built solution for user authentication. It must also use IAM roles for granting user permissions to AWS resources. The Solutions Architect was tasked to create a solution that meets these requirements.</p><p>Which of the following options should the Solutions Architect implement to enable authentication and authorization for the application? (Select TWO.)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Build a custom OpenID Connect-compatible solution in combination with AWS IAM Identity Center to create authentication and authorization functionality for the application.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Build a custom LDAP connector using Amazon API Gateway with AWS Lambda function for user authentication. Use Amazon DynamoDB to store user authorization tokens. Write another Lambda function that will validate user authorization requests based on the token stored on DynamoDB.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Build a custom SAML-compatible solution to handle authentication and authorization. Configure the solution to use LDAP for user authentication and use SAML assertion to perform authorization to the IAM identity provider.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Build a custom OpenID Connect-compatible solution for the user authentication functionality. Use Amazon Cognito Identity Pools for authorizing access to AWS resources.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Build a custom SAML-compatible solution for user authentication. Leverage AWS IAM Identity Center for authorizing access to AWS resources.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p>AWS supports <strong>identity federation with SAML 2.0</strong> (Security Assertion Markup Language 2.0), an open standard that many identity providers (IdPs) use. This feature enables federated single sign-on (SSO), so users can log in to the AWS Management Console or call the AWS API operations without you having to create an IAM user for everyone in your organization. By using SAML, you can simplify the process of configuring federation with AWS because you can use the IdP's service instead of writing custom identity proxy code.</p><p>You can use a role to configure your SAML 2.0-compliant identity provider (IdP) and AWS to permit your federated users to access the AWS Management Console. The role grants the user permissions to carry out tasks in the console. The following diagram illustrates the flow for SAML-enabled single sign-on.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sso_ldap.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_sso_ldap.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The diagram illustrates the following steps:</p><ol><li><p>The user browses your organization's portal and selects the option to go to the AWS Management Console. In your organization, the portal is typically a function of your IdP that handles the exchange of trust between your organization and AWS.</p></li><li><p>The portal verifies the user's identity in your organization.</p></li><li><p>The portal generates a SAML authentication response that includes assertions that identify the user and include attributes about the user. The portal sends this response to the client's browser.</p></li><li><p>The client browser is redirected to the AWS single sign-on endpoint and posts the SAML assertion.</p></li><li><p>The endpoint requests temporary security credentials on behalf of the user and creates a console sign-in URL that uses those credentials.</p></li><li><p>AWS sends the sign-in URL back to the client as a redirect.</p></li><li><p>The client browser is redirected to the AWS Management Console. If the SAML authentication response includes attributes that map to multiple IAM roles, the user is first prompted to select the role for accessing the console.</p></li></ol><p><strong>Amazon Cognito</strong> provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a username and password or through a third party such as Facebook, Amazon, Google, or Apple. The two main components of Amazon Cognito are user pools and identity pools. User pools are user directories that provide sign-up and sign-in options for your app users. Identity pools enable you to grant your users access to other AWS services. You can use identity pools and user pools separately or together.</p><p><strong>Amazon Cognito identity pools</strong> provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and have received a token.</p><p><strong>OpenID Connect</strong> is an open standard for authentication that is supported by a number of login providers. Amazon Cognito supports the linking of identities with OpenID Connect providers that are configured through AWS Identity and Access Management. Once you've created an OpenID Connect provider in the IAM Console, you can associate it with an identity pool.</p><p>The option that says: <strong>Build a custom SAML-compatible solution to handle authentication and authorization. Configure the solution to use LDAP for user authentication and use SAML assertion to perform authorization to the IAM identity provider</strong> is correct. The requirement is to use a custom-built solution for user authentication, and this can use the company LDAP system for authentication. The SAML assertion is also needed to get authorization tokens from the IAM identity provider that will grant IAM roles to users that wish to access AWS resources.</p><p>The option that says: <strong>Build a custom OpenID Connect-compatible solution for the user authentication functionality. Use Amazon Cognito Identity Pools for authorizing access to AWS resources </strong>is correct. The custom OpenID Connect-compatible solution will allow users to log in from their mobile application much like a single sign-on functionality. Amazon Cognito Identity Pool will provide temporary tokens to federated users for accessing AWS resources.</p><p>The option that says: <strong>Build a custom SAML-compatible solution for user authentication. Leverage AWS IAM Identity Center for authorizing access to AWS resources</strong> is incorrect. The requirement is to grant federated access from the mobile application. AWS IAM Identity Center supports single sign-on to business applications through web browsers only.</p><p>The option that says: <strong>Build a custom LDAP connector using Amazon API Gateway with AWS Lambda function for user authentication. Use Amazon DynamoDB to store user authorization tokens. Write another Lambda function that will validate user authorization requests based on the token stored on DynamoDB </strong>is incorrect. It is not recommended to store authorization tokens permanently on DynamoDB tables. These tokens should be generated upon user authentication and then temporarily saved on a DynamoDB for a fixed session length.</p><p>The option that says: <strong>Build a custom OpenID Connect-compatible solution in combination with AWS IAM Identity Center to create authentication and authorization functionality for the application</strong> is incorrect. AWS IAM Identity Center supports only SAML 2.0–based applications so an OpenID Connect-compatible solution will not work for this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/open-id.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/open-id.html</a></p><p><a href=\"https://aws.amazon.com/iam/identity-center/faqs/\">https://aws.amazon.com/iam/identity-center/faqs/</a></p><p><br></p><p><strong>AWS Identity Services Overview:</strong></p><p><a href=\"https://youtu.be/AIdUw0i8rr0\">https://youtu.be/AIdUw0i8rr0</a></p><p><strong>Check out these Amazon Cognito Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cognito/?src=udemy\">https://tutorialsdojo.com/amazon-cognito/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/?src=udemy\">https://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/open-id.html",
      "https://aws.amazon.com/iam/identity-center/faqs/",
      "https://youtu.be/AIdUw0i8rr0",
      "https://tutorialsdojo.com/amazon-cognito/?src=udemy",
      "https://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/?src=udemy"
    ]
  },
  {
    "id": 23,
    "question": "<p>A stocks brokerage firm hosts its legacy application on Amazon EC2 in a private subnet of its Amazon VPC. The application is accessed by the employees from their corporate laptops through a proprietary desktop program. The company network is peered with the AWS Direct Connect (DX) connection to provide a fast and reliable connection to the private EC2 instances inside the VPC. To comply with the strict security requirements of financial institutions, the firm is required to encrypt its network traffic that flows from the employees' laptops to the resources inside the VPC.</p><p>Which of the following solution will comply with this requirement while maintaining the consistent network performance of Direct Connect?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Using the current Direct Connect connection, create a new private virtual interface and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC over the Internet. Configure the employees’ laptops to connect to this VPN.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Using the current Direct Connect connection, create a new public virtual interface and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC over the Internet. Configure the employees’ laptops to connect to this VPN.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Using the current Direct Connect connection, create a new public virtual interface and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC with the BGP protocol using the DX connection. Configure the company network to route employee traffic to this VPN.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Using the current Direct Connect connection, create a new private virtual interface and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC with the BGP protocol using the DX connection. Configure the company network to route employee traffic to this VPN.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>To connect to services such as EC2 using just Direct Connect you need to create a private virtual interface. However, if you want to encrypt the traffic flowing through Direct Connect, you will need to use the public virtual interface of DX to create a VPN connection that will allow access to AWS services such as S3, EC2, and other services.</p><p>To connect to AWS resources that are reachable by a public IP address (such as an Amazon Simple Storage Service bucket) or AWS public endpoints, use a <strong>public virtual interface</strong>. With a public virtual interface, you can:</p><p>- Connect to all AWS public IP addresses globally.</p><p>- Create public virtual interfaces in any DX location to receive Amazon’s global IP routes.</p><p>- Access publicly routable Amazon services in any AWS Region (except for the AWS China Region).</p><p><img src=\"https://media.tutorialsdojo.com/sap_dc_public_vif.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_dc_public_vif.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>To connect to your resources hosted in an Amazon Virtual Private Cloud (Amazon VPC) using their private IP addresses, use a private virtual interface. With a <strong>private virtual interface</strong>, you can:</p><p>- Connect VPC resources (such as Amazon Elastic Compute Cloud (Amazon EC2) instances or load balancers) on your private IP address or endpoint.</p><p>- Connect a private virtual interface to a DX gateway. Then, associate the DX gateway with one or more virtual private gateways in any AWS Region (except the AWS China Region).</p><p>- Connect to multiple VPCs in any AWS Region (except the AWS China Region), because a virtual private gateway is associated with a single VPC.</p><p>If you want to establish a virtual private network (VPN) connection from your company network to an Amazon Virtual Private Cloud (Amazon VPC) over an AWS Direct Connect (DX) connection, you must use a public virtual interface for your DX connection.</p><p>Therefore, the correct answer is: <strong>Using the current Direct Connect connection, create a new public virtual interface, and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC with the BGP protocol using the DX connection. Configure the company network to route employee traffic to this VPN.</strong></p><p>The option that says: <strong>Using the current Direct Connect connection, create a new private virtual interface, and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC with the BGP protocol using the DX connection. Configure the employees’ laptops to connect to this VPN</strong> is incorrect because you must use a public virtual interface for your AWS Direct Connect (DX) connection and not a private one. You won't be able to establish an encrypted VPN along with your DX connection if you create a private virtual interface.</p><p>The following options are incorrect because you need to establish the VPN connection through the DX connection, and not over the Internet.</p><p><strong>- Using the current Direct Connect connection, create a new public virtual interface, and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC over the internet. Configure the employees’ laptops to connect to this VPN.</strong></p><p><strong>- Using the current Direct Connect connection, create a new private virtual interface, and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC over the internet. Configure the company network to route employee traffic to this VPN.</strong></p><p><br></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/\">https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/create-vpn-direct-connect/\">https://aws.amazon.com/premiumsupport/knowledge-center/create-vpn-direct-connect/</a></p><p><br></p><p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/",
      "https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/create-vpn-direct-connect/",
      "https://tutorialsdojo.com/aws-direct-connect/?src=udemy"
    ]
  },
  {
    "id": 24,
    "question": "<p>A company is developing an online voting application for a photo competition. The infrastructure is deployed in AWS using CloudFormation. The application accepts high-quality images of each contestant and stores them in S3 then records the information about the image as well as the contestant's profile in RDS. After the competition, the CloudFormation stack is not used anymore, and to save costs, the stack can be terminated. The manager instructed the solutions architect to back up the RDS database and the S3 bucket so the data can still be used even after the CloudFormation template is deleted.</p><p>Which of the following options is the MOST suitable solution to fulfill this requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set the DeletionPolicy to <code>retain</code> on both the RDS and S3 resource types on the CloudFormation template.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set the DeletionPolicy on the RDS resource to <code>snapshot</code> and set the S3 bucket to <code>retain</code>.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Set the DeletionPolicy for the RDS instance to snapshot and then enable S3 bucket replication on the source bucket to a destination bucket to maintain a copy of all the S3 objects.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set the DeletionPolicy on the S3 bucket to snapshot.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>With the <strong>DeletionPolicy</strong> attribute you can preserve, and in some cases, backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default.</p><p>This capability also applies to stack update operations that lead to resources being deleted from stacks. For example, if you remove the resource from the stack template, and then update the stack with the template. This capability doesn't apply to resources whose physical instance is replaced during stack update operations. For example, if you edit a resource's properties such that CloudFormation replaces that resource during a stack update.</p><p>To keep a resource when its stack is deleted, specify <code>Retain</code> for that resource. You can use retain for any resource. For example, you can retain a nested stack, Amazon S3 bucket, or EC2 instance so that you can continue to use or modify those resources after you delete their stacks.</p><p>There are 3 types of DeletionPolicy Options:</p><p>Delete</p><p>Retain</p><p>Snapshot</p><p>For <code>Delete</code>, CloudFormation deletes the resource and all its contents if applicable during stack deletion. For <code>Retain</code>, CloudFormation keeps the resource without deleting the resource or its contents when its stack is deleted. For <code>Snapshot</code>, CloudFormation creates a snapshot of the resource before deleting it.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_deletionpolicy.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_deletionpolicy.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore the correct answer is: <strong>Set the DeletionPolicy on the RDS resource to </strong><code><strong>snapshot</strong></code><strong> and set the S3 bucket to </strong><code><strong>retain</strong></code>. It correctly sets the DeletionPolicy of retain on S3 bucket and snapshot on RDS instance.</p><p>The option that says:<strong> Set the DeletionPolicy for the RDS instance to snapshot and then enable S3 bucket replication on the source bucket to a destination bucket to maintain a copy of all the S3 objects</strong> is incorrect. Even if the images are backed up to another bucket, the original bucket would be deleted if the CloudFormation stack is deleted. Although this option is valid, it is certainly not the most suitable solution because you don't need to back up the data to another region in the first place. You simply have to set the DeletionPolicy of the S3 bucket to <code>retain.</code></p><p>The option that says: <strong>Set the DeletionPolicy to </strong><code><strong>retain</strong></code><strong> on both the RDS and S3 resource types on the CloudFormation template</strong> is incorrect. The DeletionPolicy attribute for RDS should be set to <code>Snapshot</code> and not <code>Retain</code> because with the snapshot option, the backup of the RDS instance would be stored in the form of snapshots. With the retain option, CloudFormation will keep the RDS instance running.</p><p>The option that says: <strong>Set the DeletionPolicy on the S3 bucket to snapshot</strong> is incorrect because the DeletionPolicy of the S3 bucket should be retained, not snapshot.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/delete-cf-stack-retain-resources/\">https://aws.amazon.com/premiumsupport/knowledge-center/delete-cf-stack-retain-resources/</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/delete-cf-stack-retain-resources/",
      "https://tutorialsdojo.com/aws-cloudformation/?src=udemy"
    ]
  },
  {
    "id": 25,
    "question": "<p>A company runs several clusters of Amazon EC2 instances in AWS. An unusual API activity and port scanning in the VPC have been identified by the security team. They noticed that there are multiple port scans being triggered to the EC2 instances from a specific IP address. To fix the issue immediately, the solutions architect has decided to simply block the offending IP address. The solutions architect is also instructed to fortify their existing cloud infrastructure security from the most frequently occurring network and transport layer DDoS attacks.</p><p>Which of the following is the most suitable method to satisfy the above requirement in AWS?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Change the Windows Firewall settings to deny access from the IP address block. Use Amazon GuardDuty to detect potentially compromised instances or reconnaissance by attackers, and AWS Systems Manager Patch Manager to properly apply the latest security patches to all of your instances.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deny access from the IP Address block in the Network ACL. Use AWS Shield Advanced to protect your cloud resources.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Block the offending IP address using Route 53. Use Amazon Macie to automatically discover, classify, and protect sensitive data in AWS, including DDoS attacks.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deny access from the IP Address block by adding a specific rule to all of the Security Groups. Use a combination of AWS WAF and AWS Config to protect your cloud resources against common web attacks.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Shield</strong> is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. There are two tiers of AWS Shield - Standard and Advanced.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_shield.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_shield.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your web site or applications. When you use AWS Shield Standard with Amazon CloudFront and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks.</p><p>A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC. A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. Network ACLs are stateless; responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa).</p><p>Therefore the correct answer is: <strong>Deny access from the IP Address block in the Network ACL. Use AWS Shield Advanced to protect your cloud resources.</strong></p><p>The option that says: <strong>Block the offending IP address using Route 53. Use Amazon Macie to automatically discover, classify, and protect sensitive data in AWS, including DDoS attacks</strong> is incorrect because Amazon Macie is just a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. It does not provide security against DDoS attacks. In addition, you cannot block the offending IP address using Route 53. You should use Network ACL for this scenario.</p><p>The option that says: <strong>Change the Windows Firewall settings to deny access from the IP address block. Use Amazon GuardDuty to detect potentially compromised instances or reconnaissance by attackers, and AWS Systems Manager Patch Manager to properly apply the latest security patches to all of your instances</strong> is incorrect. You have to use Network ACL to block the specific IP address to your network and not just change the firewall of your Windows server. Amazon GuardDuty and AWS Systems Manager Patch Manager are not suitable to fortify your AWS Cloud against DDoS attacks.</p><p>The option that says:<strong> Deny access from the IP Address block by adding a specific rule to all of the Security Groups. Use a combination of AWS WAF and AWS Config to protect your cloud resources against common web attacks</strong> is incorrect because it is still better to block the offending IP address on the Network ACL level as you cannot directly deny an IP address in your Security Group. AWS WAF and AWS Config are helpful to improve the security of your cloud infrastructure in AWS but these services are not enough to protect your infrastructure against DDoS attacks. You have to use AWS Shield Advanced in this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><a href=\"https://aws.amazon.com/shield/\">https://aws.amazon.com/shield/</a></p><p><br></p><p><strong>Check out these AWS WAF and AWS Shield Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p><p><a href=\"https://tutorialsdojo.com/aws-shield/?src=udemy\">https://tutorialsdojo.com/aws-shield/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html",
      "https://aws.amazon.com/shield/",
      "https://tutorialsdojo.com/aws-waf/?src=udemy",
      "https://tutorialsdojo.com/aws-shield/?src=udemy"
    ]
  },
  {
    "id": 26,
    "question": "<p>A multinational financial company has a suite of web applications hosted in multiple VPCs in various AWS regions. As part of their security compliance, the company’s Solutions Architect has been tasked to set up a logging solution to track all of the changes made to their AWS resources in all regions, which host their enterprise accounting systems. The company is using different AWS services such as Amazon EC2 instances, Amazon S3 buckets, CloudFront web distributions, and AWS IAM. The logging solution must ensure the security, integrity, and durability of your log data in order to pass the compliance requirements. In addition, it should provide an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and API calls.</p><p>In this scenario, which of the following options is the best solution to use?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a new Amazon CloudWatch trail in a new S3 bucket using the AWS CLI and also pass both the --is-multi-region-trail and --include-global-service-events parameters then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a new Amazon CloudWatch trail in a new S3 bucket using the AWS CLI and also pass the --include-global-service-events parameter then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a new AWS CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the --is-multi-region-trail and --include-global-service-events parameters then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create a new AWS CloudTrail trail in a new S3 bucket using the AWS CLI and also pass the --no-include-global-service-events and --is-multi-region-trail parameter then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>The accounting firm requires a secure and durable logging solution that will track all of the activities of all AWS resources (such as EC2, S3, CloudFront, and IAM) on all regions. CloudTrail can be used for this case with multi-region trail enabled. However, CloudTrail will only cover the activities of the regional services (EC2, S3, RDS etc.) and not for global services such as IAM, CloudFront, AWS WAF, and Route 53.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_all_regions.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_all_regions.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The option that says: <strong>Create a new AWS CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the --is-multi-region-trail and --include-global-service-events parameters then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies</strong> is correct because it provides security, integrity, and durability to your log data. In addition, it has the -include-global-service-events parameter enabled which will also include activity from global services such as IAM, Route 53, AWS WAF, and CloudFront.</p><p>The option that says: <strong>Create a new Amazon CloudWatch trail in a new S3 bucket using the AWS CLI and also pass both the --is-multi-region-trail and --include-global-service-events parameters then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies</strong> is incorrect because you need to use CloudTrail instead of CloudWatch.</p><p>The option that says: <strong>Create a new Amazon CloudWatch trail in a new S3 bucket using the AWS CLI and also pass the --include-global-service-events parameter then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies</strong> is incorrect because you need to use CloudTrail instead of CloudWatch. In addition, the --is-multi-region-trail parameter is also missing in this setup.</p><p>The option that says: <strong>Create a new AWS CloudTrail trail in a new S3 bucket using the AWS CLI and also pass the --no-include-global-service-events and --is-multi-region-trail parameter then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies</strong> is incorrect. The --is-multi-region-trail is not enough as you also need to add the --include-global-service-events parameter to track the global service events. The --no-include-global-service-events parameter actually prevents CloudTrail from publishing events from global services such as IAM to the log files.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-and-update-a-trail-by-using-the-aws-cli.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-and-update-a-trail-by-using-the-aws-cli.html</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudtrail/?src=udemy\">https://tutorialsdojo.com/aws-cloudtrail/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-and-update-a-trail-by-using-the-aws-cli.html",
      "https://tutorialsdojo.com/aws-cloudtrail/?src=udemy"
    ]
  },
  {
    "id": 27,
    "question": "<p>A company has just launched a new central employee registry application that contains all of the public employee registration information of each staff of the company. The application has a microservices architecture running in Docker in a single AWS Region. The management teams from other departments who have their servers located in different VPCs need to connect to the central repository application to continue their work. The Solutions Architect must ensure that the traffic to the application does not traverse the public Internet. The IT Security team must also be notified of any denied requests and be able to view the corresponding source IP.</p><p>How will the Architect implement the architecture of the new application given these circumstances?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Link each of the teams' VPCs to the central VPC using VPC Peering. Create VPC Flow Logs on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to an Amazon CloudWatch Logs group. Set up a CloudWatch Logs subscription that streams the log data to the IT Security account.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up a Transit VPC by using third-party marketplace VPN appliances running on an On-Demand Amazon EC2 instance that dynamically routes the VPN connections to the virtual private gateways (VGWs) attached to each VPC. Set up an AWS Config rule on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to an Amazon CloudWatch Logs group. Set up a CloudWatch Logs subscription that streams the log data to the IT Security account.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up an IPSec Tunnel between the central VPC and each of the teams' VPCs. Create VPC Flow Logs on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to an Amazon CloudWatch Logs group. Create a CloudWatch Logs subscription that streams the log data to the IT Security account.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Direct Connect to create a dedicated connection between the central VPC and each of the teams' VPCs. Enable the VPC Flow Logs on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to a CloudWatch Logs group. Set up an Amazon CloudWatch Logs subscription that streams the log data to the IT Security account.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>A <strong>VPC peering connection</strong> is a networking connection between two VPCs that enables you to route traffic between them privately. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.</p><p><img src=\"https://media.tutorialsdojo.com/sap_vpc_peering.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_vpc_peering.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. After you've created a flow log, you can retrieve and view its data in the chosen destination.</p><p>Flow logs can help you with a number of tasks, such as:</p><p>- Diagnosing overly restrictive security group rules</p><p>- Monitoring the traffic that is reaching your instance</p><p>- Determining the direction of the traffic to and from the network interfaces</p><p>Flow log data is collected outside of the path of your network traffic, and therefore does not affect network throughput or latency. You can create or delete flow logs without any risk of impact to network performance.</p><p>Hence, the correct answer is: <strong>Link each of the teams' VPCs to the central VPC using VPC Peering. Create VPC Flow Logs on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to an Amazon CloudWatch Logs group. Set up a CloudWatch Logs subscription that streams the log data to the IT Security account.</strong></p><p>The option that says: <strong>Set up an IPSec Tunnel between the central VPC and each of the teams' VPCs. Create VPC Flow Logs on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to an Amazon CloudWatch Logs group. Create a CloudWatch Logs subscription that streams the log data to the IT Security account</strong> is incorrect. It is mentiond in the scenario that the traffic to the application must not traverse the public Internet. Since an IPSec tunnel uses the Internet to transfer data from your VPC to a specified destination, this solution is definitely incorrect.</p><p>The option that says: <strong>Use AWS Direct Connect to create a dedicated connection between the central VPC and each of the teams' VPCs. Enable the VPC Flow Logs on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to a CloudWatch Logs group. Set up an Amazon CloudWatch Logs subscription that streams the log data to the IT Security account</strong> is incorrect. You cannot set up Direct Connect between different VPCs. AWS Direct Connect is primarily used to set up a dedicated connection between your on-premises data center and your Amazon VPC.</p><p>The option that says: <strong>Set up a Transit VPC by using third-party marketplace VPN appliances running on an On-Demand Amazon EC2 instance that dynamically routes the VPN connections to the virtual private gateways (VGWs) attached to each VPC. Set up an AWS Config rule on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to an Amazon CloudWatch Logs group. Set up a CloudWatch Logs subscription that streams the log data to the IT Security account</strong> is incorrect. An AWS Config rule is not capable of capturing the source IP of the incoming requests. A VPN appliance is using the public Internet to transfer data. Thus, it violates the requirement of ensuring that the data is securely within the AWS network.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a></p><p><br></p><p><strong>Check out these Amazon VPC and VPC Peering Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><a href=\"https://tutorialsdojo.com/vpc-peering/?src=udemy\">https://tutorialsdojo.com/vpc-peering/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy",
      "https://tutorialsdojo.com/vpc-peering/?src=udemy"
    ]
  },
  {
    "id": 28,
    "question": "<p>A company has a fitness tracking app that accompanies its smartwatch. The primary customers are North American and Asian users. The application is read-heavy as it pings the servers at regular intervals for user-authorization. The company wants the infrastructure to have the following capabilities:</p><p>- The application must be fault-tolerant to problems in any Region.</p><p>- The database writes must be highly-available in a single Region.</p><p>- The application tier must be able to read the database on multiple Regions.</p><p>- The application tier must be resilient in each Region.</p><p>- Relational database semantics must be reflected in the application.</p><p>Which of the following options must the Solutions Architect implement to meet the company requirements? (Select TWO.)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the application tier on an Auto Scaling group of EC2 instances for each Region. Create an RDS for MySQL database on each region. Configure the application to perform read/write operations on the local RDS. Enable Multi-AZ failover support for the EC2 Auto Scaling group and the RDS database. Enable cross-Region replication for the database servers.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy the application tier on an Auto Scaling group of EC2 instances for each Region. Create an RDS for MySQL database on each region. Configure the application to perform read/write operations on the local RDS. Enable cross-Region replication for the database servers. Create snapshots of the application and database servers regularly. Store the snapshots in Amazon S3 buckets in both regions.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a geoproximity routing policy on Amazon Route 53 to control traffic and direct users to their closest regional endpoint. Combine this with a multivalue answer routing policy with health checks to direct users to a healthy region at any given time.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a geolocation routing policy on Amazon Route 53 to point the global users to their designated regions. Combine this with a failover answer routing policy with health checks to direct users to a healthy region at any given time.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Deploy the application tier on an Auto Scaling group of EC2 instances for each Region in an active-active configuration. Create a cluster of Amazon Aurora global database in both Regions. Configure the application to use the in-Region Aurora database endpoint for the read/write operations. Create snapshots of the application servers regularly. Store the snapshots in Amazon S3 buckets in both regions.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p><strong>Amazon Aurora Global Database</strong> is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.</p><p>By using an Amazon Aurora global database, you can have a single Aurora database that spans multiple AWS Regions to support your globally distributed applications.</p><p>An Aurora global database consists of one primary AWS Region where your data is mastered, and up to five read-only secondary AWS Regions. You issue write operations directly to the primary DB cluster in the primary AWS Region. Aurora replicates data to the secondary AWS Regions using dedicated infrastructure, with latency typically under a second.</p><p>You can also change the configuration of your Aurora global database while it's running to support various use cases. For example, you might want the read/write capabilities to move from one Region to another, say, in different time zones, to 'follow the sun.' Or, you might need to respond to an outage in one Region. With Aurora global database, you can promote one of the secondary Regions to the primary role to take full read/write workloads in under a minute.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aurora_architecture.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aurora_architecture.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>On <strong>Amazon Route 53</strong>, after you create a hosted zone for your domain, such as tutorialsdojo.com, you can create records to tell the Domain Name System (DNS) how you want traffic to be routed for that domain. You can create a record that points to the DNS name of your Application Load Balancer on AWS.</p><p>When you create a record, you choose a routing policy, which determines how Amazon Route 53 responds to queries:</p><p><strong>Simple routing policy</strong> – Use for a single resource that performs a given function for your domain, for example, a web server that serves content for the example.com website.</p><p><strong>Failover routing policy</strong> – Use when you want to configure active-passive failover.</p><p><strong>Geolocation routing policy</strong> – Use when you want to route traffic based on the location of your users.</p><p><strong>Geoproximity routing policy</strong> – Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.</p><p><strong>Latency routing policy </strong>– Use when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the best latency.</p><p><strong>Multivalue answer routing policy</strong> – Use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.</p><p><strong>Weighted routing policy</strong> – Use to route traffic to multiple resources in proportions that you specify.</p><p>You can use Route 53 health checks to configure active-active and active-passive failover configurations. You configure active-active failover using any routing policy (or combination of routing policies) other than failover, and you configure active-passive failover using the failover routing policy.</p><p>The option that says: <strong>Create a geolocation routing policy on Amazon Route 53 to point the global users to their designated regions. Combine this with a failover answer routing policy with health checks to direct users to a healthy region at any given time</strong> is correct. You can use geolocation routing policy to direct the North American users to your servers on the North America region and configure failover routing to the Asia region in case the North America region fails. You can configure the same for the Asian users pointed to the Asia region servers and have the North America region as its backup.</p><p>The option that says: <strong>Deploy the application tier on an Auto Scaling group of EC2 instances for each Region in an active-active configuration. Create a cluster of Amazon Aurora global database in both Regions. Configure the application to use the in-Region Aurora database endpoint for the read/write operations. Create snapshots of the application servers regularly. Store the snapshots in Amazon S3 buckets in both regions</strong> is correct. The Amazon Aurora global database solves the problem on read/write as well as syncing the data across all the regions. With both regions in an active-active configuration, each region can accept the traffic from users around the world.</p><p>The option that says: <strong>Create a geoproximity routing policy on Amazon Route 53 to control traffic and direct users to their closest regional endpoint. Combine this with a multivalue answer routing policy with health checks to direct users to a healthy region at any given time</strong> is incorrect. Geoproximity routing policy is good to control the user traffic to specific regions. However, a multivalue answer routing policy may cause the users to be randomly sent to other healthy regions that may be far away from the user’s location.</p><p>The option that says: <strong>Deploy the application tier on an Auto Scaling group of EC2 instances for each Region. Create an RDS for MySQL database on each region. Configure the application to perform read/write operations on the local RDS. Enable cross-Region replication for the database servers. Create snapshots of the application and database servers regularly. Store the snapshots in Amazon S3 buckets in both regions</strong> is incorrect. You can’t sync two MySQL master databases that both accept writes on their respective regions.</p><p>The option that says: <strong>Deploy the application tier on an Auto Scaling group of EC2 instances for each Region. Create an RDS for MySQL database on each region. Configure the application to perform read/write operations on the local RDS. Enable Multi-AZ failover support for the EC2 Auto Scaling group and the RDS database. Enable cross-Region replication for the database servers</strong> is incorrect. You can’t sync two MySQL master databases that both accept writes on their respective regions. Enabling Multi-AZ on the RDS MySQL server does not protect you from AWS Regional failures.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html#aurora-global-database-overview\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html#aurora-global-database-overview</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-connecting.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-connecting.html</a></p><p><br></p><p><strong>Check out these Amazon Aurora and Route 53 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora/?src=udemy\">https://tutorialsdojo.com/amazon-aurora/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html#aurora-global-database-overview",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-connecting.html",
      "https://tutorialsdojo.com/amazon-aurora/?src=udemy",
      "https://tutorialsdojo.com/amazon-route-53/?src=udemy"
    ]
  },
  {
    "id": 29,
    "question": "<p>A graphics design startup is using multiple Amazon S3 buckets to store high-resolution media files for their various digital artworks. After securing a partnership deal with a leading media company, the two parties shall be sharing digital resources with one another as part of the contract. The media company frequently performs multiple object retrievals from the S3 buckets every day, which increased the startup's data transfer costs.</p><p>As the Solutions Architect, what should you do to help the startup lower their operational costs?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Provide cross-account access for the media company, which has permissions to access contents in the S3 bucket. Cross-account retrieval of S3 objects is charged to the account that made the request.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a new billing account for the social media company by using AWS Organizations. Apply SCPs on the organization to ensure that each account has access only to its own resources and each other's S3 buckets.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Advise the media company to create their own S3 bucket. Then run the <code>aws s3 sync s3://sourcebucket s3://destinationbucket</code> command to copy the objects from their S3 bucket to the other party's S3 bucket. In this way, future retrievals can be made on the media company's S3 bucket instead.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable the Requester Pays feature in all of the startup's S3 buckets to make the media company pay the cost of the data transfer from the buckets.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In general, bucket owners pay for all Amazon S3 storage and data transfer costs associated with their bucket. A bucket owner, however, can configure a bucket to be a <strong>Requester Pays</strong> bucket. With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket. The bucket owner always pays the cost of storing data.</p><p><img src=\"https://media.tutorialsdojo.com/S3_Requester_Pays.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/S3_Requester_Pays.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You must authenticate all requests involving Requester Pays buckets. The request authentication enables Amazon S3 to identify and charge the requester for their use of the Requester Pays bucket. After you configure a bucket to be a Requester Pays bucket, requesters must include x-amz-request-payer in their requests either in the header, for POST, GET and HEAD requests, or as a parameter in a REST request to show that they understand that they will be charged for the request and the data download.</p><p>Hence, the correct answer is to <strong>enable the Requester Pays feature in all of the startup's S3 buckets to make the media company pay the cost of the data transfer from the buckets.</strong></p><p>The option that says: <strong>Advise the media company to create their own S3 bucket. Then run the </strong><code><strong>aws s3 sync s3://sourcebucket s3://destinationbucket</strong></code><strong> command to copy the objects from their S3 bucket to the other party's S3 bucket. In this way, future retrievals can be made on the media company's S3 bucket instead</strong> is incorrect because sharing all the assets of the startup to the media entails a lot of costs considering that you will be charged for the data transfer charges made during the sync process.</p><p><strong>Creating a new billing account for the social media company by using AWS Organizations, then applying SCPs on the organization to ensure that each account has access only to its own resources and each other's S3 buckets</strong> is incorrect because AWS Organizations does not create a separate billing account for every account under it. Instead, what AWS Organizations has is consolidated billing. You can use the consolidated billing feature in AWS Organizations to consolidate billing and payment for multiple AWS accounts. Every organization in AWS Organizations has a master account that pays the charges of all the member accounts.</p><p>The option that says: <strong>Provide cross-account access for the media company, which has permissions to access contents in the S3 bucket. Cross-account retrieval of S3 objects is charged to the account that made the request</strong> is incorrect because cross-account access does not shoulder the charges that are made during S3 object requests. Unless Requester Pays is enabled on the bucket, the bucket owner is still the one that is charged.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 30,
    "question": "<p>A company hosts its multi-tiered web application on a fleet of Auto Scaling EC2 instances spread across two Availability Zones. The Application Load Balancer is in the public subnets and the Amazon EC2 instances are in the private subnets. After a few weeks of operations, the users are reporting that the web application is not working properly. Upon testing, the Solutions Architect found that the website is accessible and the login is successful. However, when the “find a nearby store” function is clicked on the website, the map loads only about 50% of the time when the page is refreshed. This function involves a third-party RESTful API call to a maps provider. Amazon EC2 NAT instances are used for these outbound API calls.</p><p>Which of the following options are the MOST likely reason for this failure and the recommended solution?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>One of the subnets in the VPC has a misconfigured Network ACL that blocks outbound traffic to the third-party provider. Update the network ACL to allow this connection and configure IAM permissions to restrict these changes in the future.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>This error is caused by failed NAT instance in one of the public subnets. Use NAT Gateways instead of EC2 NAT instances to ensure availability and scalability.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>This error is caused by an overloaded NAT instance in one of the subnets. Scale the EC2 NAT instances to larger-sized instances to ensure that they can handle the growing traffic.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The error is caused by a failure in one of their availability zones in the VPC of the third-party provider. Contact the third-party provider support hotline and request for them to fix it.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>You can use a NAT device to enable instances in a private subnet to connect to the Internet (for example, for software updates) or other AWS services, but prevent the Internet from initiating connections with the instances. A NAT device forwards traffic from the instances in the private subnet to the Internet or other AWS services, and then sends the response back to the instances. When traffic goes to the Internet, the source IPv4 address is replaced with the NAT device’s address, and similarly, when the response traffic goes to those instances, the NAT device translates the address back to those instances’ private IPv4 addresses.</p><p>You can either use a managed NAT device offered by AWS called a NAT gateway, or you can create your own NAT device in an EC2 instance, referred to here as a <strong>NAT instance</strong>. The bandwidth of NAT instances depends on the instance size – higher instances size will have high bandwidth capacity. NAT instances are managed by the customer so if the instance goes down, there could be a potential impact on the availability of your application. You have to manually check and fix the NAT instances.</p><p>AWS recommends <strong>NAT gateways</strong> because they provide better availability and bandwidth over NAT instances. The NAT gateway service is also a managed service that does not require your administration efforts. NAT Gateways are highly available. In each Availability Zone, they are implemented with redundancy. It is managed by AWS so you do not have to perform maintenance or monitoring if it is UP. NAT gateways automatically scale in bandwidth so you don’t have to choose instance types.</p><p>Check out the full comparison in the table below:</p><p><img src=\"https://media.tutorialsdojo.com/public/Natcomparison_25AUG2023.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/public/Natcomparison_25AUG2023.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The correct answer is: <strong>This error is caused by failed NAT instance in one of the public subnets. Use NAT Gateways instead of EC2 NAT instances to ensure availability and scalability. </strong>This is very likely as we have two subnets in the scenario and NAT instances reside in only one AZ. With a failure rate of 50%, one of the NAT instances must have been down. AWS does not automatically recover the failed NAT instances. AWS recommends using NAT gateways because they provide better availability and bandwidth. Even if NAT gateway is deployed on a single AZ, AWS implements redundancy to ensure that it is always available on that AZ.</p><p>The option that says: <strong>One of the subnets in the VPC has a misconfigured Network ACL that blocks outbound traffic to the third-party provider. Update the network ACL to allow this connection and configure IAM permissions to restrict these changes in the future</strong> is incorrect. Network ACLs affect all the subnets associated with it. If there is a misconfigured rule, the other subnets will be affected too, which could result in a 100% failure of requests to the third-party provider.</p><p>The option that says: <strong>The error is caused by a failure in one of their availability zones in the VPC of the third-party provider. Contact the third-party provider support hotline and request for them to fix it </strong>is incorrect. If there is a failure on one availability zone of the third-party provider, the traffic should have stopped sending to that AZ so this failure is most likely caused by a local failure in your VPC.</p><p>The option that says: <strong>This error is caused by an overloaded NAT instance in one of the subnets. Scale the EC2 NAT instances to larger-sized instances to ensure that they can handle the growing traffic</strong> is incorrect. If the NAT instances are overloaded, you will notice inconsistent performance or slowdown for the third-party requests. And this failure should have been gone during off-peak hours. If the failure rate is 50% of the requests, it is most likely that one of the NAT instances is down.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy"
    ]
  },
  {
    "id": 31,
    "question": "<p>A print media company has a popular web application hosted on their on-premises network which allows anyone around the globe to search its back catalog and retrieve individual newspaper pages on their web portal. They scanned the old newspapers into PNG image format and used Optical Character Recognition (OCR) software to automatically convert images to a text file. The license of their OCR software will expire soon, and the news organization decided to move to AWS and produce a scalable, durable, and highly available architecture.</p><p>Which is the best option to achieve this requirement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a new S3 bucket to store and serve the scanned image files using a CloudFront web distribution. Launch a new Elastic Beanstalk environment to host the website across multiple Availability Zones and set up a CloudSearch for query processing, which the website can use. Use Amazon Textract to detect and recognize text from the scanned old newspapers.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Store the images in an S3 bucket and prepare a separate bucket to host the static website. Utilize Amazon Kendra to intelligently search and select the images stored in S3. Set up a lifecycle policy to move the selected images to Glacier after 3 months and if needed, use Glacier Select to query the archives.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a new CloudFormation template which has EBS-backed EC2 instances with an Application Load Balancer in front. Install and run an NGINX web server and an open source search application. Store the images to EBS volumes with Amazon Data Lifecycle Manager configured, and which automatically attach new volumes to the EC2 instances as required.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use S3 Intelligent-Tiering storage class to store and serve the scanned files. Migrate the on-premises web application as well as the Optical Character Recognition (OCR) software to an Auto Scaling group of Spot EC2 Instances across multiple Availability Zones with an Application Load Balancer to balance the incoming load. Use Amazon Rekognition to detect and recognize text from the scanned old newspapers.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon CloudSearch</strong> is a managed service in the AWS Cloud that makes it simple and cost-effective to set up, manage, and scale a search solution for your website or application.</p><p>With Amazon CloudSearch, you can quickly add rich search capabilities to your website or application. You don't need to become a search expert or worry about hardware provisioning, setup, and maintenance. With a few clicks in the AWS Management Console, you can create a search domain and upload the data that you want to make searchable, and Amazon CloudSearch will automatically provision the required resources and deploy a highly-tuned search index.</p><p>You can easily change your search parameters, fine-tune search relevance, and apply new settings at any time. As your volume of data and traffic fluctuates, Amazon CloudSearch seamlessly scales to meet your needs.</p><p><strong>Amazon Textract</strong> is a machine learning (ML) service that automatically extracts text, handwriting, and data from scanned documents. It goes beyond simple optical character recognition (OCR) to identify, understand, and extract data from forms and tables.</p><p>Amazon Textract makes it easy to add document text detection and analysis to your applications. Using Amazon Textract, customers can:</p><p>- Detect typed and handwritten text in a variety of documents, including financial reports, medical records, and tax forms.</p><p>- Extract text, forms, and tables from documents with structured data, using the Amazon Textract Document Analysis API.</p><p>- Specify and extract information from documents using the Queries feature within the Amazon Textract Analyze Document API.</p><p>- Process invoices and receipts with the AnalyzeExpense API.</p><p>- Process ID documents such as driver's licenses and passports issued by the U.S. government using the AnalyzeID API.</p><p><img src=\"https://media.tutorialsdojo.com/saa_amazon_textract.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/saa_amazon_textract.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Create a new S3 bucket to store and serve the scanned image files using a CloudFront web distribution. Launch a new Elastic Beanstalk environment to host the website across multiple Availability Zones and set up a CloudSearch for query processing, which the website can use. Use Amazon Textract to detect and recognize text from the scanned old newspapers.</strong> It satisfies the requirement given in the scenario, i.e., it uses S3 to store the images instead of the commercial product, which will be decommissioned soon. More importantly, it uses CloudSearch for query processing, and in addition, it uses Multi-AZ implementation, which provides high availability. It is also correct to use Amazon Textract to detect and recognize text from scanned old newspapers.</p><p>The option that says:<strong><em> </em>Create a new CloudFormation template which has EBS-backed EC2 instances with an Application Load Balancer in front. Install and run an NGINX web server and an open source search application. Store the images to EBS volumes with Amazon Data Lifecycle Manager configured, and which automatically attach new volumes to the EC2 instances as required</strong> is incorrect. An EBS volume is not a scalable nor a durable solution compared with S3. In addition, it is not as cost-effective compared to S3 since it entails maintenance overhead, unlike the fully managed storage service provided by S3.</p><p>The option that says:<strong> Store the images in an S3 bucket and prepare a separate bucket to host the static website. Utilize Amazon Kendra to intelligently search and select the images stored in S3. Set up a lifecycle policy to move the selected images to Glacier after 3 months and if needed, use Glacier Select to query the archives </strong>is incorrect. Amazon Kendra is an intelligent enterprise search service that allows users to search across different content repositories. It can't recognize text inside from scanned documents, which Amazon Textract is designed for. Storing your data to Amazon Glacier will also affect the retrieval time of your data.</p><p>The option that says: <strong>Use S3 Intelligent-Tiering storage class to store and serve the scanned files. Migrate the on-premises web application as well as the Optical Character Recognition (OCR) software to an Auto Scaling group of Spot EC2 Instances across multiple Availability Zones with an Application Load Balancer to balance the incoming load. Use Amazon Rekognition to detect and recognize text from the scanned old newspapers</strong> is incorrect. Even though it properly uses S3 for durable and scalable storage, it still uses the Optical Character Recognition (OCR) software which will be decommissioned soon. It is better to use CloudSearch instead.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/cloudsearch/\">https://aws.amazon.com/cloudsearch/</a></p><p><a href=\"https://docs.aws.amazon.com/textract/latest/dg/what-is.html\">https://docs.aws.amazon.com/textract/latest/dg/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/textract/latest/dg/how-it-works-documents.html\">https://docs.aws.amazon.com/textract/latest/dg/how-it-works-documents.html</a></p><p><br></p><p><strong>Check out these Amazon CloudSearch and Amazon Textract Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudsearch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudsearch/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-textract/?src=udemy\">https://tutorialsdojo.com/amazon-textract/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/cloudsearch/",
      "https://docs.aws.amazon.com/textract/latest/dg/what-is.html",
      "https://docs.aws.amazon.com/textract/latest/dg/how-it-works-documents.html",
      "https://tutorialsdojo.com/amazon-cloudsearch/?src=udemy",
      "https://tutorialsdojo.com/amazon-textract/?src=udemy"
    ]
  },
  {
    "id": 32,
    "question": "<p>A telecommunications company is planning to host a WordPress website on an Amazon ECS Cluster which uses the Fargate launch type. For security purposes, the database credentials should be provided to the WordPress image by using environment variables. Your manager instructed you to ensure that the credentials are secure when passed to the image and that they cannot be viewed on the cluster itself. The credentials must be kept in a dedicated storage with lifecycle management and key rotation. </p><p>Which of the following is the most suitable solution in this scenario that you can implement with the least effort?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Migrate the container cluster to Amazon Elastic Kubernetes Service (EKS). Create manifest files for deployment and use the Kubernetes Secrets objects to store the database credentials. Reference the secrets on the manifest file using the <code>secretKeyRef</code> to use them a environment variables. Configure EKS to rotate the secret values automatically.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>In the ECS task definition file of the ECS Cluster, store the database credentials and encrypt with KMS. Store the task definition JSON file in a private S3 bucket and ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Create an IAM role to the ECS task definiton script that allows access to the specific S3 bucket and then pass the <code>--cli-input-json</code> parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Store the database credentials using the AWS Systems Manager Parameter Store and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the database credentials using the AWS Secrets Manager and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret which contains the sensitive data, to present to the container.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon ECS</strong> enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.</p><p>Within your container definition, specify <code>secrets</code> with the name of the environment variable to set in the container and the full ARN of either the Secrets Manager secret or Systems Manager Parameter Store parameter containing the sensitive data to present to the container. The parameter that you reference can be from a different Region than the container using it, but must be from within the same account.</p><p><img src=\"https://media.tutorialsdojo.com/aws-secrets-manager-console-q2.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/aws-secrets-manager-console-q2.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Using Secrets Manager, you can secure and manage secrets used to access resources in the AWS Cloud, on third-party services, and on-premises.</p><p>If you want a single store for configuration and secrets, you can use Parameter Store. If you want a dedicated secrets store with lifecycle management, use Secrets Manager.</p><p>Hence, the correct answer is the option that says: <strong>Store the database credentials using the AWS Secrets Manager and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret which contains the sensitive data, to present to the container.</strong></p><p>The option that says: <strong>Store the database credentials using the AWS Systems Manager Parameter Store and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container</strong> is incorrect. Although the use of Systems Manager Parameter Store in securing sensitive data in ECS is valid, this service doesn't provide dedicated storage with lifecycle management and key rotation, unlike Secrets Manager.</p><p>The option that says: <strong>In the ECS task definition file of the ECS Cluster, store the database credentials and encrypt with KMS. Store the task definition JSON file in a private S3 bucket and ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Create an IAM role to the ECS task definiton script that allows access to the specific S3 bucket and then pass the </strong><code><strong>--cli-input-json</strong></code><strong> parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials</strong> is incorrect. Although the solution may work, it is not recommended to store sensitive credentials in S3. This entails a lot of overhead and manual configuration steps which can be simplified by using the Secrets Manager or Systems Manager Parameter Store.</p><p>The option that says:<strong> Migrate the container cluster to Amazon Elastic Kubernetes Service (EKS). Create manifest files for deployment and use the Kubernetes Secrets objects to store the database credentials. Reference the secrets on the manifest file using the </strong><code><strong>secretKeyRef</strong></code><strong> to use them a environment variables. Configure EKS to rotate the secret values automatically </strong>is incorrect. It is possible to use EKS to host the container clusters and use the Secrets object to store secret values. However, this approach will entail more effort for the migration from ECS to EKS. Additionally, Kubernetes doesn't natively support the automatic rotation of secrets.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/\">https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p><p><br></p><p><strong>Check out this AWS Secrets Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-secrets-manager/?src=udemy\">https://tutorialsdojo.com/aws-secrets-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html",
      "https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/",
      "https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy",
      "https://tutorialsdojo.com/aws-secrets-manager/?src=udemy"
    ]
  },
  {
    "id": 33,
    "question": "<p>A company is planning to build its new customer relationship management (CRM) portal in AWS. The application architecture will be using a containerized microservices hosted on an Amazon ECS cluster. A Solutions Architect has been tasked to set up the architecture and comply with the AWS security best practice of granting the least privilege. The architecture should also support the use of security groups and standard network monitoring tools at the container level to comply with the company’s strict IT security policies. </p><p>Which of the following provides the MOST secure configuration for the CRM portal?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS App Runner to run the containerized application instead to improve security and reduce operational overhead. Select VPC and security groups accordingly for deployment. Add IAM credentials to the environment variables when launching the service.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the <code>awsvpc</code> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to the ECS tasks then pass IAM credentials into the container at launch time to access other AWS resources.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the <code>bridge</code> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to Amazon EC2 instances then use IAM roles for EC2 instances to access other resources.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the <code>awsvpc</code> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to the ECS tasks then use IAM roles for tasks to access other resources.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Migration Planning",
    "explanation": "<p><strong>Task definitions</strong> are split into separate parts: the task family, the IAM task role, the network mode, container definitions, volumes, task placement constraints, and launch types. The family and container definitions are required in a task definition, while task role, network mode, volumes, task placement constraints, and launch type are optional.</p><p>You can configure various Docker networking modes that will be used by containers in your ECS task. The valid values are <code>none</code>, <code>bridge</code>, <code>awsvpc</code>, and <code>host</code>. The default Docker network mode is <code>bridge</code>.</p><p>With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. Applications must sign their AWS API requests with AWS credentials, and this feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances. Instead of creating and distributing your AWS credentials to the containers or using the EC2 instance’s role, you can associate an IAM role with an ECS task definition or <code>RunTask</code> API operation. The applications in the task’s containers can then use the AWS SDK or CLI to make API requests to authorized AWS services.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ecs_task_definition.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ecs_task_definition.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>If the network mode is set to <code><strong>none</strong></code>, the task's containers do not have external connectivity, and port mappings can't be specified in the container definition.</p><p>If the network mode is <code><strong>bridge</strong></code>, the task utilizes Docker's built-in virtual network which runs inside each container instance.</p><p>If the network mode is <code><strong>host</strong></code>, the task bypasses Docker's built-in virtual network and maps container ports directly to the EC2 instance's network interface directly. In this mode, you can't run multiple instantiations of the same task on a single container instance when port mappings are used.</p><p>If the network mode is <code><strong>awsvpc</strong></code>, the task is allocated an elastic network interface, and you must specify a <code>NetworkConfiguration</code> when you create a service or run a task with the task definition. When you use this network mode in your task definitions, every task that is launched from that task definition gets its own elastic network interface (ENI) and a primary private IP address. The task networking feature simplifies container networking and gives you more control over how containerized applications communicate with each other and other services within your VPCs.</p><p>Task networking also provides greater security for your containers by allowing you to use security groups and network monitoring tools at a more granular level within your tasks. Because each task gets its own ENI, you can also take advantage of other Amazon EC2 networking features like VPC Flow Logs so that you can monitor traffic to and from your tasks. Additionally, containers that belong to the same task can communicate over the <code>localhost</code> interface. A task can only have one ENI associated with it at a given time.</p><p>Hence, the correct answer is: <strong>Use the </strong><code><strong>awsvpc</strong></code><strong> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to the ECS tasks then use IAM roles for tasks to access other resources.</strong></p><p>The option that says: <strong>Use the </strong><code><strong>bridge</strong></code><strong> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to Amazon EC2 instances then use IAM roles for EC2 instances to access other resources</strong> is incorrect because you won't be able to attach security groups to your ECS tasks using this network mode type. This will only use the Docker's built-in virtual network which runs inside each container instance. You have to use the <code>awsvpc</code> network mode instead to allow you to use security groups and network monitoring tools at a more granular level within your tasks. Moreover, if you are using the <code>awsvpc</code> network mode, you should attach the security group to the ECS task and not to the EC2 instance.</p><p>The option that says: <strong>Use AWS App Runner to run the containerized application instead to improve security and reduce operational overhead. Select VPC and security groups accordingly for deployment. Add IAM credentials to the environment variables when launching the service</strong> is incorrect. This is possible as you can deploy containerized workloads on AWS App Runner. However, it is not recommended to put sensitive IAM credentials on the environment variables when running the service.</p><p>The option that says: <strong>Use the </strong><code><strong>awsvpc</strong></code><strong> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to the ECS tasks then pass IAM credentials into the container at launch time to access other AWS resources</strong> is incorrect. Although it uses the correct network mode, you have to use an IAM Role instead. It is a security risk to pass the IAM credentials into the container as it could be potentially exposed.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#network_mode\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#network_mode</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p><p><br></p><p><strong>AWS Container Services Overview:</strong></p><p><a href=\"https://youtu.be/5QBgDX7O7pw\">https://youtu.be/5QBgDX7O7pw</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#network_mode",
      "https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy",
      "https://youtu.be/5QBgDX7O7pw"
    ]
  },
  {
    "id": 34,
    "question": "<p>A company is migrating an interactive car registration web system hosted on its on-premises network to AWS Cloud. The current architecture of the system consists of a single NGINX web server and a MySQL database running on a Fedora server, which both reside in their on-premises data center. For the new cloud architecture, a load balancer must be used to evenly distribute the incoming traffic to the application servers. Route 53 must be used for both domain registration and domain management.</p><p>In this scenario, what would be the most efficient way to transfer the web application to AWS?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>1. Launch two NGINX EC2 instances in two Availability Zones. </p><p>2. Copy the web files from the on-premises web server to each Amazon EC2 web server, using Amazon S3 as the repository. </p><p>3. Migrate the database using the AWS Database Migration Service. </p><p>4. Create an ELB to front your web servers. </p><p>5. Use Route 53 and create an alias A record<strong> </strong>pointing to the ELB.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>1. Use the AWS Application Migration Service (MGN) to create an EC2 AMI of the NGINX web server.</p><p>2. Configure auto-scaling to launch in two Availability Zones.</p><p>3. Launch a multi-AZ MySQL Amazon RDS instance in one availability zone only.</p><p>4. Import the data into Amazon RDS from the latest MySQL backup.</p><p>5. Create an ELB to front your web servers</p><p>6. Use Amazon Route 53 and create an A record pointing to the elastic load balancer.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>1. Export web files to an Amazon S3 bucket in one Availability Zone using AWS Migration Hub. </p><p>2. Run the website directly out of Amazon S3. </p><p>3. Migrate the database using the AWS Database Migration Service and AWS Schema Conversion Tool (AWS SCT). </p><p>4. Use Route 53 and create an alias record pointing to the ELB.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>1. Use the AWS Application Discovery Service to migrate the NGINX web server. </p><p>2. Configure Auto Scaling to launch two web servers in two Availability Zones. </p><p>3. Launch a Multi-AZ MySQL Amazon Relational Database Service (RDS) instance in one Availability Zone only. </p><p>4. Import the data into Amazon RDS from the latest MySQL backup. </p><p>5. Use Amazon Route 53 to create a private hosted zone and point a non-alias A<strong> </strong>record to the ELB.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p>This is a trick question that contains a lot of information to confuse you, especially if you don’t know the fundamental concepts of AWS. All options seem to be correct except for their last steps in setting up Route 53. The options that have a step to launch a multi-AZ MySQL Amazon RDS instance in one availability zone only are wrong since a Multi-AZ deployment configuration uses multiple Availability Zones.</p><p>To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias record that points to your load balancer. An alias record is a Route 53 extension to DNS. It's similar to a CNAME record, but you can create an alias record both for the root domain, such as example.com, and for subdomains, such as www.example.com. (You can create CNAME records only for subdomains). For EC2 instances, always use a Type A Record without an Alias. For ELB, Cloudfront, and S3, always use a Type A Record with an Alias, and finally, for RDS, always use the CNAME Record with no Alias.</p><p>Hence, the following option is the correct answer:</p><p><strong>1. Launch two NGINX EC2 instances in two Availability Zones.</strong></p><p><strong>2. Copy the web files from the on-premises web server to each Amazon EC2 web server, using Amazon S3 as the repository.</strong></p><p><strong>3. Migrate the database using the AWS Database Migration Service.</strong></p><p><strong>4. Create an ELB to front your web servers.</strong></p><p><strong>5. Use Route 53 and create an alias A record pointing to the ELB.</strong></p><p><img src=\"https://media.tutorialsdojo.com/sap_db_migration.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_db_migration.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>AWS Database Migration Service</strong> helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. The AWS Database Migration Service can migrate your data to and from the most widely used commercial and open-source databases.</p><p>The following sets of options are incorrect because they are just using an A record without an Alias:</p><p><strong>1. Use the AWS Application Migration Service (MGN) to create an EC2 AMI of the NGINX web server.</strong></p><p><strong>2. Configure auto-scaling to launch in two Availability Zones.</strong></p><p><strong>3. Launch a multi-AZ MySQL Amazon RDS instance in one availability zone only.</strong></p><p><strong>4. Import the data into Amazon RDS from the latest MySQL backup.</strong></p><p><strong>5. Create an ELB to front your web servers.</strong></p><p><strong>6. Use Amazon Route 53 and create an A record pointing to the elastic load balancer.</strong></p><p><em>--</em></p><p><strong>1. Use the AWS Application Discovery Service to migrate the NGINX web server.</strong></p><p><strong>2. Configure Auto Scaling to launch two web servers in two Availability Zones.</strong></p><p><strong>3. Launch a Multi-AZ MySQL Amazon Relational Database Service (RDS) instance in one Availability Zone only.</strong></p><p><strong>4. Import the data into Amazon RDS from the latest MySQL backup.</strong></p><p><strong>5. Use Amazon Route 53 to create a private hosted zone and point a non-alias A record to the ELB.</strong></p><p>Take note as well that the AWS Application Migration Service (MGN) is primarily used to migrate virtual machines only, which can be from VMware vSphere and Windows Hyper-V to your AWS cloud. In addition, the AWS Application Discovery Service simply helps you to plan migration projects by gathering information about your on-premises data centers, but this service is not a suitable migration service.</p><p>The following option is also incorrect because the web system that is being migrated is a non-static (dynamic) website, which cannot be hosted in S3:</p><p><strong>1. Export web files to an Amazon S3 bucket in one Availability Zone using AWS Migration Hub.</strong></p><p><strong>2. Run the website directly out of Amazon S3.</strong></p><p><strong>3. Migrate the database using the AWS Database Migration Service and AWS Schema Conversion Tool (AWS SCT).</strong></p><p><strong>4. Use Route 53 and create an alias record pointing to the ELB.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloud-migration/\">https://aws.amazon.com/cloud-migration/</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p><p><br></p><p><strong>Check out this AWS Database Migration Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-database-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-database-migration-service/</a></p><p><br></p><p><strong>AWS Migration Services Overview:</strong></p><p><a href=\"https://youtu.be/yqNBkFMnsL8\">https://youtu.be/yqNBkFMnsL8</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/cloud-migration/",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html",
      "https://tutorialsdojo.com/amazon-route-53/?src=udemy",
      "https://tutorialsdojo.com/aws-database-migration-service/?src=udemy",
      "https://youtu.be/yqNBkFMnsL8"
    ]
  },
  {
    "id": 35,
    "question": "<p>A company wants to implement a multi-account strategy that will be distributed across its several research facilities. There will be approximately 50 teams in total that will need their own AWS accounts. A solution is needed to simplify the DNS management as there is only one team that manages all the domains and subdomains for the whole organization. This means that the solution should allow private DNS to be shared among virtual private clouds (VPCs) in different AWS accounts.</p><p>Which of the following solutions has the LEAST complex DNS architecture and allows all VPCs to resolve the needed domain names?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up Direct Connect connections among the VPCs of each account using private virtual interfaces. Ensure that each VPC has the attributes <code>enableDnsHostnames</code> and <code>enableDnsSupport</code> set to “FALSE”. On Amazon Route 53, create a private hosted zone associated with the central account’s VPC. Manage all domains and subdomains on this hosted zone. Programmatically associate the VPCs from other accounts with this hosted zone.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>On AWS Resource Access Manager (RAM), set up a shared services VPC on your central account. Set up VPC peering from this VPC to each VPC on the other accounts. On Amazon Route 53, create a private hosted zone associated with the shared services VPC. Manage all domains and subdomains on this zone. Programmatically associate the VPCs from other accounts with this hosted zone.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up a VPC peering connection among the VPC of each account. Ensure that the each VPC has the attributes <code>enableDnsHostnames</code> and <code>enableDnsSupport</code> set to “TRUE”. On Amazon Route 53, create a private hosted zone associated with the central account’s VPC. Manage all domains and subdomains on this hosted zone. On each of the other AWS Accounts, create a Route 53 private hosted zone and configure the Name Server entry to use the DNS of the central account.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>On AWS Resource Access Manager (RAM), set up a shared services VPC on your central account. Create a peering from this VPC to each VPC on the other accounts. On Amazon Route 53, create a private hosted zone associated with the shared services VPC. Manage all domains and subdomains on this hosted zone. On each of the other AWS Accounts, create a Route 53 private hosted zone and configure the Name Server entry to use the DNS of the central account.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>When you create a VPC using Amazon VPC, Route 53 Resolver automatically answers DNS queries for local VPC domain names for EC2 instances (ec2-192-0-2-44.compute-1.amazonaws.com) and records in private hosted zones (acme.example.com). For all other domain names, Resolver performs recursive lookups against public name servers.</p><p>You also can integrate DNS resolution between Resolver and DNS resolvers on your network by configuring forwarding rules. Your network can include any network that is reachable from your VPC, such as the following:</p><p>- The VPC itself</p><p>- Another peered VPC</p><p>- An on-premises network that is connected to AWS with AWS Direct Connect, a VPN, or a network address translation (NAT) gateway</p><p><img src=\"https://media.tutorialsdojo.com/sap_route53_resolver_multi_account.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_route53_resolver_multi_account.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>VPC sharing allows customers to share subnets with other AWS accounts within the same AWS Organization. This is a very powerful concept that allows for a number of benefits:</p><p>- Separation of duties: centrally controlled VPC structure, routing, IP address allocation.</p><p>- Application owners continue to own resources, accounts, and security groups.</p><p>- VPC sharing participants can reference security group IDs of each other.</p><p>- Efficiencies: higher density in subnets, efficient use of VPNs and AWS Direct Connect.</p><p>- Hard limits can be avoided, for example, 50 VIFs per AWS Direct Connect connection through simplified network architecture.</p><p>- Costs can be optimized through reuse of NAT gateways, VPC interface endpoints, and intra-Availability Zone traffic.</p><p>Essentially, we can decouple accounts and networks. In this model, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner. You can simplify network topologies by interconnecting shared Amazon VPCs using connectivity features, such as AWS PrivateLink, AWS Transit Gateway, and Amazon VPC peering.</p><p>Therefore, the correct answer is: <strong>On AWS Resource Access Manager (RAM), set up a shared services VPC on your central account. Set up VPC peering from this VPC to each VPC on the other accounts. On Amazon Route 53, create a private hosted zone associated with the shared services VPC. Manage all domains and subdomains on this zone. Programmatically associate the VPCs from other accounts with this hosted zone.</strong></p><p>The option that says: <strong>Set up Direct Connect connections among the VPCs of each account using private virtual interfaces. Ensure that each VPC has the attributes </strong><code><strong>enableDnsHostnames</strong></code><strong> and </strong><code><strong>enableDnsSupport</strong></code><strong> set to “FALSE”. On Amazon Route 53, create a private hosted zone associated with the central account’s VPC. Manage all domains and subdomains on this hosted zone. Programmatically associate the VPCs from other accounts with this hosted zone</strong> is incorrect. Using AWS Direct Connect is not a suitable service to connect the various VPCs. In addition, attributes <code>enableDnsHostnames</code> and <code>enableDnsSupport</code> are set to “TRUE” by default and are needed for VPC resources to query Route 53 zone entries.</p><p>The option that says: <strong>Set up a VPC peering connection among the VPC of each account. Ensure that each VPC has the attributes </strong><code><strong>enableDnsHostnames</strong></code><strong> and </strong><code><strong>enableDnsSupport</strong></code><strong> set to “TRUE”. On Amazon Route 53, create a private hosted zone associated with the central account’s VPC. Manage all domains and subdomains on this hosted zone. On each of the other AWS Accounts, create a Route 53 private hosted zone and configure the Name Server entry to use the DNS of the central account</strong> is incorrect. You won't be able to resolve the hosted private zone entries even if you configure your Route 53 zone NS entry to use the central accounts' DNS servers.</p><p>The option that says: <strong>On AWS Resource Access Manager (RAM), set up a shared services VPC on your central account. Create a peering from this VPC to each VPC on the other accounts. On Amazon Route 53, create a private hosted zone associated with the shared services VPC. Manage all domains and subdomains on this hosted zone. On each of the other AWS Accounts, create a Route 53 private hosted zone and configure the Name Server entry to use the DNS of the central account</strong> is incorrect. Although creating the shared services VPC is a good solution, configuring Route 53 Name Server (NS) records to point to the shared services VPC’s Route 53 is not enough. You need to associate the VPCs from other accounts to the hosted zone on the central account.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/\">https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/simplify-dns-management-in-a-multiaccount-environment-with-route-53-resolver/\">https://aws.amazon.com/blogs/security/simplify-dns-management-in-a-multiaccount-environment-with-route-53-resolver/</a></p><p><br></p><p><strong>Check out these Amazon VPC and Route 53 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html",
      "https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/",
      "https://aws.amazon.com/blogs/security/simplify-dns-management-in-a-multiaccount-environment-with-route-53-resolver/",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy",
      "https://tutorialsdojo.com/amazon-route-53/?src=udemy"
    ]
  },
  {
    "id": 36,
    "question": "<p>A company has recently adopted a hybrid cloud architecture which requires them to migrate their databases from their on-premises data center to AWS. One of their applications requires a heterogeneous database migration in which they need to transform their on-premises Oracle database to PostgreSQL. A schema and code transformation should be done first in order to successfully migrate the data. </p><p>Which of the following options is the most suitable approach to migrate the database in AWS?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AWS Serverless Application Model (SAM) service to transform your database to PostgreSQL using AWS Lambda functions. Migrate the database to RDS using the AWS Database Migration Service (DMS).</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use a combination of AWS Data Pipeline service and CodeCommit to convert the source schema and code to match that of the target PostgreSQL database in RDS. Use AWS Batch with Spot EC2 instances to cost-effectively migrate the data from the source database to the target database in a batch process.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Migrate the database from your on-premises data center using the AWS Server Migration Service (SMS). Afterward, use the AWS Database Migration Service to convert and migrate your data to Amazon RDS for PostgreSQL database.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the AWS Schema Conversion Tool (SCT) to convert the source schema to match that of the target database. Migrate the data using the AWS Database Migration Service (DMS) from the source database to an Amazon RDS for PostgreSQL database.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p><strong>AWS Database Migration Service</strong> helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. The AWS Database Migration Service can migrate your data to and from the most widely used commercial and open-source databases.</p><p>AWS Database Migration Service can migrate your data to and from most of the widely used commercial and open source databases. It supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle to Amazon Aurora. Migrations can be from on-premises databases to Amazon RDS or Amazon EC2, databases running on EC2 to RDS, or vice versa, as well as from one RDS database to another RDS database. It can also move data between SQL, NoSQL, and text-based targets.</p><p><img src=\"https://media.tutorialsdojo.com/sap_db_migration.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_db_migration.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In heterogeneous database migrations, the source and target databases engines are different, like in the case of Oracle to Amazon Aurora, Oracle to PostgreSQL, or Microsoft SQL Server to MySQL migrations. In this case, the schema structure, data types, and database code of source and target databases can be quite different, requiring a schema and code transformation before the data migration starts. That makes heterogeneous migrations a two-step process.</p><p><strong>First, use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database.</strong> All the required data type conversions will automatically be done by the AWS Database Migration Service during the migration. The source database can be located in your own premises outside of AWS, running on an Amazon EC2 instance, or it can be an Amazon RDS database. The target can be a database in Amazon EC2 or Amazon RDS.</p><p>The option that says: <strong>Migrate the database from your on-premises data center using the AWS Server Migration Service (SMS). Afterwards, use the AWS Database Migration Service to convert and migrate your data to Amazon RDS for PostgreSQL database</strong> is incorrect because the AWS Server Migration Service (SMS) is primarily used to migrate virtual machines such as VMware vSphere and Windows Hyper-V. Although it is correct to use AWS Database Migration Service (DMS) to migrate the database, this option is still wrong because you should use the AWS Schema Conversion Tool to convert the source schema.</p><p>The option that says: <strong>Use a combination of AWS Data Pipeline service and CodeCommit to convert the source schema and code to match that of the target PostgreSQL database in RDS. Use AWS Batch with Spot EC2 instances to cost-effectively migrate the data from the source database to the target database in a batch process</strong> is incorrect. AWS Data Pipeline is primarily used to quickly and easily provision pipelines that remove the development and maintenance effort required to manage your daily data operations which lets you focus on generating insights from that data. Although you can use this to connect your data on your on-premises data center, it is not the most suitable service to use, compared with AWS DMS.</p><p>The option that says: <strong>Use the AWS Serverless Application Model (SAM) service to transform your database to PostgreSQL using AWS Lambda functions. Migrate the database to RDS using the AWS Database Migration Service (DMS)</strong> is incorrect. The Serverless Application Model (SAM) is an open-source framework that is primarily used to build serverless applications on AWS, and not for database migration.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p><p><a href=\"https://aws.amazon.com/cloud-migration/\">https://aws.amazon.com/cloud-migration/</a></p><p><br></p><p><strong>Check out these AWS Migration Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-database-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-database-migration-service/</a></p><p><a href=\"https://tutorialsdojo.com/aws-migration-strategies-the-6-rs/?src=udemy\">https://tutorialsdojo.com/aws-migration-strategies-the-6-rs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/dms/",
      "https://aws.amazon.com/cloud-migration/",
      "https://tutorialsdojo.com/aws-database-migration-service/?src=udemy",
      "https://tutorialsdojo.com/aws-migration-strategies-the-6-rs/?src=udemy"
    ]
  },
  {
    "id": 37,
    "question": "<p>A fintech startup has developed a cloud-based payment processing system that accepts credit card payments as well as cryptocurrencies such as Bitcoin, Ripple, and the likes. The system is deployed in AWS which uses EC2, DynamoDB, S3, and CloudFront to process the payments. Since they are accepting credit card information from the users, they are required to be compliant with the Payment Card Industry Data Security Standard (PCI DSS). On the recent 3rd-party audit, it was found that the credit card numbers are not properly encrypted and hence, their system failed the PCI DSS compliance test. You were hired by the fintech startup to solve this issue so they can release the product in the market as soon as possible. In addition, you also have to improve performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content.</p><p>In this scenario, what is the best option to protect and encrypt the sensitive credit card information of the users and to improve the cache hit ratio of your CloudFront distribution?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add a custom SSL in the CloudFront distribution. Configure your origin to add <code>User-Agent</code> and <code>Host</code> headers to your objects to increase your cache hit ratio.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an origin access control (OAC) and add it to the CloudFront distribution. Configure your origin to add <code>User-Agent</code> and <code>Host</code> headers to your objects to increase your cache hit ratio.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure the CloudFront distribution to use Signed URLs. Configure your origin to add a <code>Cache-Control max-age</code> directive to your objects, and specify the longest practical value for <code>max-age</code> to increase your cache hit ratio.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the CloudFront distribution to enforce secure end-to-end connections to origin servers by using HTTPS and field-level encryption. Configure your origin to add a <code>Cache-Control max-age</code> directive to your objects, and specify the longest practical value for <code>max-age</code> to increase your cache hit ratio.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Field-level encryption</strong> adds an additional layer of security, along with HTTPS, that lets you protect specific data throughout system processing so that only certain applications can see it. Field-level encryption allows you to securely upload user-submitted sensitive information to your web servers. The sensitive information provided by your clients is encrypted at the edge closer to the user and remains encrypted throughout your entire application stack, ensuring that only applications that need the data—and have the credentials to decrypt it—are able to do so.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_encryption.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_encryption.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>To use field-level encryption, you configure your CloudFront distribution to specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request. Hence, the correct answer for this scenario is the option that says: <strong>Configure the CloudFront distribution to enforce secure end-to-end connections to origin servers by using HTTPS and field-level encryption. Configure your origin to add a </strong><code><strong>Cache-Control max-age</strong></code><strong> directive to your objects, and specify the longest practical value for </strong><code><strong>max-age</strong></code><strong> to increase your cache hit ratio.</strong></p><p>You can improve performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content; that is, by improving the cache hit ratio for your distribution. To increase your cache hit ratio, you can configure your origin to add a <code>Cache-Control max-age</code> directive to your objects, and specify the longest practical value for <code>max-age</code>. The shorter the cache duration, the more frequently CloudFront forwards another request to your origin to determine whether the object has changed and, if so, to get the latest version.</p><p>The option that says: <strong>Add a custom SSL in the CloudFront distribution. Configure your origin to add </strong><code><strong>User-Agent</strong></code><strong> and </strong><code><strong>Host</strong></code><strong> headers to your objects to increase your cache hit ratio</strong> is incorrect. Although it provides secure end-to-end connections to origin servers, it is better to add field-level encryption to protect the credit card information.</p><p>The option that says: <strong>Configure the CloudFront distribution to use Signed URLs. Configure your origin to add a </strong><code><strong>Cache-Control max-age</strong></code><strong> directive to your objects, and specify the longest practical value for </strong><code><strong>max-age</strong></code><strong> to increase your cache hit ratio</strong> is incorrect because a Signed URL provides a way to distribute private content but it doesn't encrypt the sensitive credit card information.</p><p>The option that says: <strong>Create an Origin Access Control (OAC) and add it to the CloudFront distribution. Configure your origin to add </strong><code><strong><em>User-Agent</em></strong></code><strong><em> </em>and </strong><code><strong><em>Host</em></strong></code><strong> headers to your objects to increase your cache hit ratio</strong> is incorrect because OAC is mainly used to restrict access to objects in S3 bucket, but not provide encryption to specific fields.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html#field-level-encryption-setting-up\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html#field-level-encryption-setting-up</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html#field-level-encryption-setting-up",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 38,
    "question": "<p>A logistics company plans to host its web application on AWS to allow customers to track their shipping worldwide. The web application will have a multi-tier setup – Amazon EC2 instances for running the web and application layer, Amazon S3 bucket for hosting the static content, and a NoSQL database. The company plans to provision the resources in the us-east-1 region. The company also wants to have a second site hosted on us-west-1 region for disaster recovery. The second site must have the same copy of data from the primary site and the failover should be as quick as possible when the primary region becomes unavailable. Failing back to the primary region should be done automatically once it becomes available again.</p><p>Which of the following solutions should the Solutions Architect implement to meet the company requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create the same resources of Auto Scaling group of EC2 instances for web and application tiers on both regions using AWS CloudFormation StackSets. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Create Amazon Route 53 DNS zone entries with a failover routing policy and set the us-west-1 region as the secondary site. For the database tier, create a DynamoDB global table spanning both regions.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Provision the same Auto Scaling group of EC2 instances for web and application tiers in both regions using AWS Service Catalog. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Ensure that Amazon Route 53 health check is enabled on the primary region and update the public DNS zone entry with the secondary region in case of an outage. For the database tier, create an Amazon RDS for MySQL and enable cross-region replication to create a read-replica on the secondary region.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create the same resources of Auto Scaling group of EC2 instances for web and application tiers on both regions using AWS CloudFormation StackSets. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Create Amazon Route 53 DNS zone entries with a failover routing policy and set the us-west-1 region as the secondary site. For the database tier, create an Amazon Aurora global database spanning the two regions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create the same resources of Auto Scaling group of EC2 instances for web and application tiers on both regions using AWS CloudFormation StackSets. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Create an Amazon CloudFront distribution. Set the S3 bucket as the origin for static files and multi-origins for the web and application tiers. For the database tier, create an Amazon DynamoDB table in each region and regularly backup to an Amazon S3 bucket.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p><strong>AWS CloudFormation</strong> helps AWS customers implement an Infrastructure as Code model. Instead of setting up their environments and applications by hand, they build a template and use it to create all of the necessary resources, collectively known as a CloudFormation stack. This model removes opportunities for manual error, increases efficiency, and ensures consistent configurations over time.</p><p>With <strong>Amazon CloudFormation StackSets</strong> you can define an AWS resource configuration in a CloudFormation template and then roll it out across multiple AWS accounts and/or Regions with a couple of clicks. You can use this to set up a baseline level of AWS functionality that addresses the cross-account and cross-region scenarios. Once you have set this up, you can easily expand coverage to additional accounts and regions.</p><p>Amazon S3 Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. The objects may be replicated to a single destination bucket or multiple destination buckets. Destination buckets can be in different AWS Regions or within the same Region as the source bucket. <strong>Amazon S3 Cross-Region Replication (CRR)</strong> is used to copy objects across Amazon S3 buckets in different AWS Regions. CRR is helpful if you want to meet compliance requirements such as the need to have a copy of your data on another location.</p><p><strong>Amazon DynamoDB global tables</strong> provide you with a fully managed, multi-region and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions. Global tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application's business logic. In addition, global tables enable your applications to stay highly available even in the unlikely event of isolation or degradation of an entire Region.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_global_table.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_global_table.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Create the same resources of Auto Scaling group of EC2 instances for web and application tiers on both regions using AWS CloudFormation StackSets. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Create Amazon Route 53 DNS zone entries with a failover routing policy and set the us-west-1 region as the secondary site. For the database tier, create a DynamoDB global table spanning both regions.</strong></p><p>The option that says: <strong>Create the same resources of Auto Scaling group of EC2 instances for web and application tiers on both regions using AWS CloudFormation StackSets. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Create Amazon Route 53 DNS zone entries with a failover routing policy and set the us-west-1 region as the secondary site. For the database tier, create an Amazon Aurora global database spanning the two regions</strong> is incorrect. The application is designed for a NoSQL database so a DynamoDB global table is recommended for this, not an Amazon Aurora global database.</p><p>The option that says: <strong>Provision the same Auto Scaling group of EC2 instances for web and application tiers in both regions using AWS Service Catalog. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Ensure that Amazon Route 53 health check is enabled on the primary region and update the public DNS zone entry with the secondary region in case of an outage. For the database tier, create an Amazon RDS for MySQL and enable cross-region replication to create a read-replica on the secondary region</strong> is incorrect. You don’t have to manually update the public DNS zone entry with the secondary region, you just have to configure the failover routing policy in Route 53 to automatically failover to the secondary site and vice versa. MySQL is not recommended as the application is designed for a NoSQL database.</p><p>The option that says: <strong>Create the same resources of Auto Scaling group of EC2 instances for web and application tiers on both regions using AWS CloudFormation StackSets. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Create an Amazon CloudFront distribution. Set the S3 bucket as the origin for static files and multi-origins for the web and application tiers. For the database tier, create an Amazon DynamoDB table in each region and regularly backup to an Amazon S3 bucket</strong> is incorrect. You can’t reliably sync DynamoDB tables on two regions with just backups from S3. There will be a delay on the backup and restore. You should use DynamoDB global tables instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/\">https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html</a></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><br></p><p><strong>Check out these Amazon S3, Amazon DynamoDB, and CloudFormation StackSet Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation-stacksets-and-nested-stacks/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation-stacksets-and-nested-stacks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html",
      "https://aws.amazon.com/dynamodb/global-tables/",
      "https://tutorialsdojo.com/amazon-dynamodb/?src=udemy",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy",
      "https://tutorialsdojo.com/aws-cloudformation-stacksets-and-nested-stacks/?src=udemy"
    ]
  },
  {
    "id": 39,
    "question": "<p>Four large banks in the country have collaborated to create a secure, simple-to-use, mobile payment app that enables users to easily transfer money and pay bills without much hassle. With the new mobile payment app, anyone can easily pay another person, split the bill with their friends, or pay for their coffee in an instant with just a few taps in the app. The payment app is available on both Android and iOS devices, including a web portal that is deployed in AWS using OpsWorks Stacks and EC2 instances. It was a big success with over 5 million users nationwide and has over 1000 transactions every hour. After one year, a new feature that will enable the users to store their credit card information in the app is ready to be added to the existing web portal. However, due to PCI-DSS compliance, the new version of the APIs and web portal cannot be deployed to the existing application stack.</p><p>How would the solutions architect deploy the new web portal for the mobile app without having any impact on 5 million users?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Deploy the new web portal using a Blue/Green deployment strategy with AWS CodeDeploy and Lambda in which the green environment represents the current web portal version serving production traffic while the blue environment is staged in running a different version of the web portal.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a new stack that contains the latest version of the web portal. Using Route 53 service, direct all the incoming traffic to the new stack at once so that all the customers get to access new features.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy a new OpsWorks stack that contains a new layer with the latest web portal version. Shift traffic between existing stack and new stack, running different versions of the web portal using Blue/Green deployment strategy by using Route53. Route only a small portion of incoming production traffic to use the new application stack while maintaining the old application stack. Check the features of the new portal; once it's 100% validated, slowly increase incoming production traffic to the new stack. If there are issues on the new stack, change Route53 to revert to old stack.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Forcibly upgrade the existing application stack in Production to be PCI-DSS compliant. Once done, deploy the new version of the web portal on the existing application stack.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Blue/green deployments</strong> provide near zero-downtime release and rollback capabilities. The fundamental idea behind blue/green deployment is to shift traffic between two identical environments that are running different versions of your application. The blue environment represents the current application version serving production traffic. In parallel, the green environment is staged running a different version of your application. After the green environment is ready and tested, production traffic is redirected from blue to green. If any problems are identified, you can roll back by reverting traffic back to the blue environment.</p><p><strong>AWS OpsWorks</strong> has the concept of stacks, which are logical groupings of AWS resources (EC2 instances, Amazon RDS, Elastic Load Balancing, and so on) that have a common purpose and should be logically managed together. Stacks are made of one or more layers. A layer represents a set of EC2 instances that serve a particular purpose, such as serving applications or hosting a database server. When a data store is part of the stack, you should be aware of certain data management challenges.</p><p>Next, create the green environment/stack with the newer version of the application. At this point, the green environment is not receiving any traffic. If Elastic Load Balancing needs to be pre-warmed, you can do it at this time.</p><p>When it’s time to promote the green environment/stack into production, update DNS records to point to the green environment/stack’s load balancer. You can also do this DNS flip gradually by using the Amazon Route 53 weighted routing policy.</p><p>To implement this technique in AWS OpsWorks, bring up the blue environment/stack with the current version of the application.</p><p><img src=\"https://media.tutorialsdojo.com/sap_opsworks_blue_green.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_opsworks_blue_green.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Deploy a new OpsWorks stack that contains a new layer with the latest web portal version. Shift traffic between existing stack and new stack, running different versions of the web portal using Blue/Green deployment strategy by using Route53. Route only a small portion of incoming production traffic to use the new application stack while maintaining the old application stack. Check the features of the new portal; once it's 100% validated, slowly increase incoming production traffic to the new stack. If there are issues on the new stack, change Route53 to revert to the old stack.</strong></p><p>The option that says: <strong>Forcibly upgrade the existing application stack in Production to be PCI-DSS compliant. Once done, deploy the new version of the web portal on the existing application stack</strong> is incorrect because if you forcibly deploy the new web portal to the existing application stack and there is an issue on the deployment, then there would be some inevitable downtime in order for you to fix the system and revert back to the original version. It is better to use blue/green deployments instead.</p><p>The option that says:<strong> Create a new stack that contains the latest version of the web portal. Using Route 53 service, direct all the incoming traffic to the new stack at once so that all the customers get to access new features</strong> is incorrect. Although the current application stack can still be used if something goes wrong, the risk of service disruption is quite high when you direct all incoming traffic to the new portal. If something did go wrong, there would be a downtime in order for you to switch back to the old stack.</p><p>The option that says: <strong>Deploy the new web portal using a Blue/Green deployment strategy with AWS CodeDeploy and Lambda in which the green environment represents the current web portal version serving production traffic while the blue environment is staged in running a different version of the web portal </strong>is incorrect. Although it mentioned Blue/Green deployment, the application stack doesn't mention about serverless computing at all. Hence, Lambda is irrelevant in this situation and in addition, the description of the blue and green environments are actually switched. The blue environment represents the current application while the green represents the new one.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\">https://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/clone-a-stack-in-aws-opsworks-and-update-dns.html\">https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/clone-a-stack-in-aws-opsworks-and-update-dns.html</a></p><p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.html</a></p><p><br></p><p><strong>Check out this AWS OpsWOrks Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-opsworks/?src=udemy\">https://tutorialsdojo.com/aws-opsworks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf",
      "https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/clone-a-stack-in-aws-opsworks-and-update-dns.html",
      "https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.html",
      "https://tutorialsdojo.com/aws-opsworks/?src=udemy"
    ]
  },
  {
    "id": 40,
    "question": "<p>A global financial company is launching its new trading platform in AWS which allows people to buy and sell their bitcoin, ethereum, ripple, and other cryptocurrencies, as well as access to various financial reports. To meet the anti-money laundering and counter-terrorist financing (AML/CFT) measures compliance, all report files of the trading platform must not be accessible in certain countries which are listed in the Financial Action Task Force (FATF) list of non-cooperative countries or territories. You were given a task to ensure that the company complies with this requirement to avoid hefty monetary penalties.</p><p>In this scenario, what is the best way to satisfy this security requirement in AWS while still delivering content to users around the globe with lower latency?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Route53 with a Geolocation routing policy that blocks all traffic from the blacklisted countries.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a CloudFront distribution with Geo-Restriction enabled to block all of the blacklisted countries from accessing the trading platform.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Route53 with a Geoproximity routing policy that blocks all traffic from the blacklisted countries.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy the trading platform using Elastic Beanstalk and deny all incoming traffic from the IP addresses of the blacklisted countries in the Network Access Control List (ACL) of the VPC.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>You can use <strong>geo restriction</strong> - also known as geoblocking - to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront web distribution. To use geo restriction, you have two options:</p><p>Use the CloudFront geo restriction feature. Use this option to restrict access to all of the files that are associated with a distribution and to restrict access at the country level.</p><p>Use a third-party geolocation service. Use this option to restrict access to a subset of the files that are associated with a distribution or to restrict access at a finer granularity than the country level.</p><p>When a user requests your content, <strong>CloudFront</strong> typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following:</p><p>Allow your users to access your content only if they're in one of the countries on a whitelist of approved countries.</p><p>Prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries.</p><p>For example, if a request comes from a country where, for copyright reasons, you are not authorized to distribute your content, you can use CloudFront geo restriction to block the request.</p><p>Hence, the option that says: <strong>Create a CloudFront distribution with Geo-Restriction enabled to block all of the blacklisted countries from accessing the trading platform</strong> is correct. CloudFront can provide the users low-latency access to the files as well as block certain countries on the FTAF list.</p><p>The option that says:<strong> Deploy the trading platform using Elastic Beanstalk and deny all incoming traffic from the IP addresses of the blacklisted countries in the Network Access Control List (ACL) of the VPC</strong> is incorrect. Blocking all of the IP addresses of each blacklisted country in the Network Access Control List entails a lot of work and is not a recommended way to accomplish the task. Using CloudFront geo restriction feature is a better solution for this.</p><p>The following options are incorrect because Route 53 only provides Domain Name Resolution and sends the requests based on the configured entries. It does not provide low-latency access to users around the globe, unlike CloudFront.</p><p><strong>Use Route 53 with a Geolocation routing policy that blocks all traffic from the blacklisted countries.</strong></p><p><strong>Use Route 53 with a Geoproximity routing policy that blocks all traffic from the blacklisted countries.</strong></p><p>Geolocation routing policy is used when you want to route traffic based on the location of your users while Geoproximity routing policy is for scenarios where you want to route traffic based on the location of your resources and, optionally, shift traffic from resources on one location to resources in another.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Latency Routing vs Geoproximity Routing vs Geolocation Routing:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-latency-routing-vs-geoproximity-routing-vs-geolocation-routing/?src=udemy\">https://tutorialsdojo.com/aws-cheat-sheet-latency-routing-vs-geoproximity-routing-vs-geolocation-routing/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy",
      "https://tutorialsdojo.com/aws-cheat-sheet-latency-routing-vs-geoproximity-routing-vs-geolocation-routing/?src=udemy",
      "https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/?src=udemy"
    ]
  },
  {
    "id": 41,
    "question": "<p>A media company hosts its entire infrastructure on the AWS cloud. There is a requirement to copy information to or from the shared resources from another AWS account. The solutions architect has to provide the other account access to several AWS resources such as Amazon S3, AWS KMS, and Amazon ES in the form of a list of AWS account ID numbers. In addition, the user in the other account should still work in the trusted account and there is no need to give up his or her user permissions in place of the role permissions. The solutions architect must also set up a solution that continuously assesses, audits, and monitors the policy configurations.</p><p>Which of the following is the MOST suitable type of policy that you should use in this scenario?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a service-linked role with an identity-based policy. Use AWS Systems Manager rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up cross-account access with a resource-based Policy. Use AWS Config rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up a service-linked role with a service control policy. Use AWS Systems Manager rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up cross-account access with a user-based policy configuration. Use AWS Config rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>For some AWS services, you can grant cross-account access to your resources. To do this, you attach a policy directly to the resource that you want to share, instead of using a role as a proxy. The resource that you want to share must support resource-based policies. Unlike a user-based policy, a resource-based policy specifies who (in the form of a list of AWS account ID numbers) can access that resource.</p><p>Cross-account access with a resource-based policy has some advantages over a role. With a resource that is accessed through a resource-based policy, the user still works in the trusted account and does not have to give up his or her user permissions in place of the role permissions. In other words, the user continues to have access to resources in the trusted account at the same time as he or she has access to the resource in the trusting account. This is useful for tasks such as copying information to or from the shared resource in the other account.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_config_timeline.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_config_timeline.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.</p><p>Hence, the option that says: <strong>Set up cross-account access with a resource-based Policy. Use AWS Config rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration</strong> is correct.</p><p>The option that says: <strong>Set up cross-account access with a user-based policy. configuration. Use AWS Config rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration</strong> is incorrect because a user-based policy maps the access to a certain IAM user and not to a certain AWS resource.</p><p>The option that says: <strong>Set up a service-linked role with an identity-based policy. Use AWS Systems Manager rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration</strong> is incorrect because a service-linked role is just a unique type of IAM role that is linked directly to an AWS service. In addition, it is the AWS Config service, and not the AWS Systems Manager, that enables you to assess, audit, and evaluate the configurations of your AWS resources.</p><p>The option that says: <strong>Set up a service-linked role with a service control policy. Use AWS Systems Manager rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration</strong> is incorrect because a service control policy is primarily used in AWS Organizations and not for cross-account access. Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf. This is not suitable for providing access to your resources to other AWS accounts, unlike cross-account access. You should also use AWS Config, and not AWS Systems Manager, to periodically audit changes to the IAM policy.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html</a></p><p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html",
      "https://aws.amazon.com/config/",
      "https://tutorialsdojo.com/aws-config/?src=udemy"
    ]
  },
  {
    "id": 42,
    "question": "<p>A company uses computer simulations for modeling weather patterns in a certain country. The simulations generate terabytes of data, which is stored in a MySQL 8.0 database that runs in an Amazon EC2 instance. A Ruby on Rails application is hosted on a separate EC2 instance to process the data. The current database size is 16 TiB and is expected to grow as more complex simulations are created continuously. The facility wants to re-architect its infrastructure to be highly scalable and highly available as they need to run the application reliably 24x7.</p><p>Which of the following is the MOST cost-effective solution that can satisfy the above requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure your application tier to run on an Auto Scaling group of smaller sized EC2 instances behind an Application Load Balancer. Purchase reserved EC2 instances for fixed capacity and let the Auto Scaling instances run on demand. Migrate the MySQL database to an Amazon RDS MySQL instance with Multi-AZ enabled. Adjust the RDS Storage volume manually as demand increases.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Purchase Reserved Amazon EC2 instances for the application instance and database instances to save costs. Create an EC2 Auto Scaling group with Multi-AZ configuration behind a load balancer for the application tier. Implement a “source-replica” setup for your database tier. Use Logical Volume Management on the database instances to easily attach new EBS volumes and expand the file system.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure your application tier to run on an Auto Scaling group of smaller sized EC2 instances behind an Application Load Balancer. Purchase Reserved EC2 instances for fixed capacity and let the Auto Scaling instances run on demand. Migrate the MySQL database to Amazon Aurora. Create a read-replica on another Availability Zone of the Aurora instance for high availability.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Convert the application tier to Lambda@Edge functions for high availability, scalability, and cost-effectiveness. Migrate the MySQL database to an Amazon RDS MySQL instance with Multi-AZ enabled. Enable storage autoscaling on the RDS instance.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>You can launch and automatically scale a fleet of On-Demand Instances and Spot Instances within a single Auto Scaling group. In addition to receiving discounts for using Spot Instances, you can use Reserved Instances or a Savings Plan to receive discounted rates of the regular On-Demand Instance pricing. All of these factors combined help you to optimize your cost savings for Amazon EC2 instances, while making sure that you obtain the desired scale and performance for your application.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aurora_readers.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aurora_readers.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Amazon Aurora storage automatically scales with the data in your cluster volume. As your data grows, your cluster volume storage expands up to a maximum of 128 tebibytes (TiB). Even though an Aurora cluster volume can scale up in size to many tebibytes, you are only charged for the space that you use in the volume. The mechanism for determining billed storage space depends on the version of your Aurora cluster. Aurora stores copies of the data in a DB cluster across multiple Availability Zones in a single AWS Region. Aurora stores these copies regardless of whether the instances in the DB cluster span multiple Availability Zones. However, you still need to create a read-replica for the high availability of the Aurora DB instance. A single read-replica is enough to quickly recover the database in case the primary instance fails.</p><p>Therefore, the correct answer is:<strong> Configure your application tier to run on an Auto Scaling group of smaller sized EC2 instances behind an Application Load Balancer. Purchase Reserved EC2 instances for fixed capacity and let the Auto Scaling instances to run on demand. Migrate the MySQL database to Amazon Aurora. Create a read-replica on another Availability Zone of the Aurora instance for high-availability.</strong></p><p>The option that says: <strong>Purchase Reserved Amazon EC2 instances for the application instance and database instances to save costs. Create an EC2 Auto Scaling group with Multi-AZ configuration behind a load balancer for the application tier. Implement a \"source-replica\" setup for your database tier. Use Logical Volume Management on the database instances to easily attach new EBS volumes and expand the file system</strong> is incorrect because running your own MySQL server on an EC2 instance entails significant management overhead. This is not scalable because you need to manually resize and configure the EC2 instance as you will eventually need more compute or storage capacity in the future. A better solution is to use Amazon Aurora instead.</p><p>The option that says: <strong>Convert the application tier to Lambda@Edge functions for high availability, scalability, and cost-effectiveness. Migrate the MySQL database to an Amazon RDS MySQL instance with Multi-AZ enabled. Enable storage autoscaling on the RDS instance</strong> is incorrect. Although it is true that MySQL RDS instances can support storage autoscaling for up to 64TiB, using Lambda@Edge to run the application is not warranted. Lambda@Edge is just an extension of AWS Lambda that customizes the content of what Amazon CloudFront delivers. It is not stated in the scenario that the application is used globally. This would be a viable option if you convert your application to Docker-based containers and run them on Amazon ECS.</p><p>The option that says: <strong>Configure your application tier to run on an Auto Scaling group of smaller sized EC2 instances behind an Application Load Balancer. Purchase reserved EC2 instances for fixed capacity and let the Auto Scaling instances to run on demand. Migrate the MySQL database to an Amazon RDS MySQL instance with Multi-AZ enabled. Adjust the RDS Storage volume manually as demand increases</strong> is incorrect. You don't have to manually adjust storage volume because storage auto-scaling is supported by Amazon RDS. You just have to enable the storage auto-scaling option. Amazon Aurora as it automatically scales disk storage by default.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/05/amazon-rds-mysql-mariadb-postgresql-64tib-support/\">https://aws.amazon.com/about-aws/whats-new/2019/05/amazon-rds-mysql-mariadb-postgresql-64tib-support/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html",
      "https://aws.amazon.com/about-aws/whats-new/2019/05/amazon-rds-mysql-mariadb-postgresql-64tib-support/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 43,
    "question": "<p>A top university has launched its serverless online portal using Lambda and API Gateway in AWS that enables its students to enroll, manage their class schedules, and see their grades online. After a few weeks, the portal abruptly stopped working and lost all of its data. The university hired an external cybersecurity consultant and based on the investigation, the outage was due to an SQL injection vulnerability on the portal's login page in which the attacker simply injected the malicious SQL code. You also need to track historical changes to the rules and metrics associated with your firewall.</p><p>Which of the following is the most suitable and cost-effective solution to avoid another SQL Injection attack against their infrastructure in AWS?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a new Application Load Balancer (ALB) and set up AWS WAF in the load balancer. Place the API Gateway behind the ALB and configure a web access control list (web ACL) in front of the ALB to block requests that contain malicious SQL code. Use AWS Firewall Manager to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS WAF to add a web access control list (web ACL) in front of the API Gateway to block requests that contain malicious SQL code. Use AWS Config to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Block the IP address of the attacker in the Network Access Control List of your VPC and then set up a CloudFront distribution. Set up AWS WAF to add a web access control list (web ACL) in front of the CloudFront distribution to block requests that contain malicious SQL code. Use AWS Config to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS WAF to add a web access control list (web ACL) in front of the Lambda functions to block requests that contain malicious SQL code. Use AWS Firewall Manager, to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. With AWS Config, you can track changes to WAF web access control lists (web ACLs). For example, you can record the creation and deletion of rules and rule actions, as well as updates to WAF rule configurations.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_waf.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_waf.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>AWS WAF</strong> gives you control over which traffic to allow or block to your web applications by defining customizable web security rules. You can use AWS WAF to create custom rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that are designed for your specific application. New rules can be deployed within minutes, letting you respond quickly to changing traffic patterns. Also, AWS WAF includes a full-featured API that you can use to automate the creation, deployment, and maintenance of web security rules.</p><p>In this scenario, the best option is to deploy WAF in front of the API Gateway. Hence the correct answer is the option that says: <strong>Use AWS WAF to add a web access control list (web ACL) in front of the API Gateway to block requests that contain malicious SQL code. Use AWS Config to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations</strong>.</p><p>The option that says: <strong>Use AWS WAF to add a web access control list (web ACL) in front of the Lambda functions to block requests that contain malicious SQL code. Use AWS Firewall Manager, to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations </strong>is incorrect because you have to use AWS WAF in front of the API Gateway and not directly to the Lambda functions. AWS Firewall Manager is primarily used to manage your Firewall across multiple AWS accounts under your AWS Organizations and hence, it is not suitable for tracking changes to WAF web access control lists. You should use AWS Config instead.</p><p>The option that says: <strong>Block the IP address of the attacker in the Network Access Control List of your VPC and then set up a CloudFront distribution. Set up AWS WAF to add a web access control list (web ACL) in front of the CloudFront distribution to block requests that contain malicious SQL code. Use AWS Config to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations</strong> is incorrect. Even though it is valid to use AWS WAF with CloudFront, it entails an additional and unnecessary cost to launch a CloudFront distribution for this scenario. There is no requirement that the serverless online portal should be scalable and be accessible around the globe hence, a CloudFront distribution is not necessary.</p><p>The option that says: <strong>Create a new Application Load Balancer (ALB) and set up AWS WAF in the load balancer. Place the API Gateway behind the ALB and configure a web access control list (web ACL) in front of the ALB to block requests that contain malicious SQL code. Use AWS Firewall Manager to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations</strong> is incorrect. Launching a new Application Load Balancer entails additional cost and is not cost-effective. In addition, AWS Firewall manager is primarily used to manage your Firewall across multiple AWS accounts under your AWS Organizations. Using AWS Config is much more suitable for tracking changes to WAF web access control lists.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p><p><br></p><p><strong>Check out this AWS WAF Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p><p><br></p><p><strong>AWS Security Services Overview - WAF, Shield, CloudHSM, KMS:</strong></p><p><a href=\"https://youtu.be/-1S-RdeAmMo\">https://youtu.be/-1S-RdeAmMo</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html",
      "https://tutorialsdojo.com/aws-waf/?src=udemy",
      "https://youtu.be/-1S-RdeAmMo"
    ]
  },
  {
    "id": 44,
    "question": "<p>A company is hosting a multi-tier web application in AWS. It is composed of an Application Load Balancer and EC2 instances across three Availability Zones. During peak load, its stateless web servers operate at 95% utilization. The system is set up to use Reserved Instances to handle the steady-state load and On-Demand Instances to handle the peak load. Your manager instructed you to review the current architecture and do the necessary changes to improve the system.</p><p>Which of the following provides the most cost-effective architecture to allow the application to recover quickly in the event that an Availability Zone is unavailable during peak load?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch a Spot Fleet using a diversified allocation strategy, with Auto Scaling enabled on each AZ to handle the peak load instead of On-Demand instances. Retain the current setup for handling the steady state load.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use a combination of Spot and On-Demand instances on each AZ to handle both the steady state and peak load.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Launch an Auto Scaling group of Reserved instances on each AZ to handle the peak load. Retain the current setup for handling the steady state load.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a combination of Reserved and On-Demand instances on each AZ to handle both the steady state and peak load.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>The scenario requires a cost-effective architecture to allow the application to recover quickly, hence, using an <strong>Auto Scaling group</strong> is a must to handle the peak load and improve both the availability and scalability of the application.</p><p>Setting up a diversified allocation strategy for your<strong> Spot Fleet</strong> is a best practice to increase the chances that a spot request can be fulfilled by EC2 capacity in the event of an outage in one of the Availability Zones. You can include each AZ available to you in the launch specification. And instead of using the same subnet each time, use three unique subnets (each mapping to a different AZ).</p><p><img src=\"https://media.tutorialsdojo.com/sap_asg_spot_ondemand.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_asg_spot_ondemand.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore the correct answer is: <strong>Launch a Spot Fleet using a diversified allocation strategy, with Auto Scaling enabled on each AZ to handle the peak load instead of On-Demand instances. Retain the current setup for handling the steady state load. </strong>The Spot instances are the most cost-effective for handling the temporary peak loads of the application.</p><p>The option that says: <strong>Launch an Auto Scaling group of Reserved instances on each AZ to handle the peak load. Retain the current setup for handling the steady state load</strong> is incorrect. Even though it uses Auto Scaling, Reserved Instances cost more than Spot instances so it is more suitable to use the latter to handle the peak load.</p><p>The following options are incorrect because they did not mention the use of Auto Scaling Groups, which is a requirement for this architecture:</p><p><strong>- Use a combination of Spot and On-Demand instances on each AZ to handle both the steady state and peak load.</strong></p><p><strong>- Use a combination of Reserved and On-Demand instances on each AZ to handle both the steady state and peak load.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet-examples.html#fleet-config5\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet-examples.html#fleet-config5</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11\">https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11</a></p><p><a href=\"https://github.com/awsdocs/amazon-ec2-user-guide/pull/56\">https://github.com/awsdocs/amazon-ec2-user-guide/pull/56</a></p><p><br></p><p><strong>Check out this AWS Billing and Cost Management Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy\">https://tutorialsdojo.com/aws-billing-and-cost-management/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet-examples.html#fleet-config5",
      "https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11",
      "https://github.com/awsdocs/amazon-ec2-user-guide/pull/56",
      "https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy"
    ]
  },
  {
    "id": 45,
    "question": "<p>An enterprise runs its CMS application on an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer. The instances are placed on private subnets while the ALB is placed on public subnets. As part of best practices, the AWS Systems Manager Agent is installed on the instances and the AWS Systems Manager Session Manager is used to login into the instances. The EC2 instances send application logs to Amazon CloudWatch Logs.</p><p>Upon the deployment of the new application version, the new instances are being marked as unhealthy by the ALB and are being replaced by the Auto Scaling group. For troubleshooting, the solutions architect tries to log in on the unhealthy instances but the instances are getting terminated. The collected logs on CloudWatch Logs do not show definitive errors in the application.</p><p>Which of the following options is the quickest way for the solutions architect to troubleshoot the problem?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Go to the Auto Scaling Groups section in the AWS console and suspend the “Terminate” process for the ASG. Log in to one of the unhealthy instances using AWS Systems Manager Session Manager.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a temporary Amazon EC2 instance and deploy the new application version. Login to the EC2 instance using the SSH key. Add the instance to the Application Load Balancer to inspect application logs in real-time.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Update the application log setting to have more verbose logging to capture more application logs. Ensure that the Amazon CloudWatch agent is installed and running on the instances.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Select one of the new Amazon EC2 instances and enable EC2 instance termination protection. Gain access to the unhealthy instance using the AWS Systems Manager Session Manager.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>An <strong><em>Auto Scaling group</em></strong> contains a collection of EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also lets you use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.</p><p>An Auto Scaling group starts by launching enough instances to meet its desired capacity. It maintains this number of instances by performing periodic health checks on the instances in the group. The Auto Scaling group continues to maintain a fixed number of instances even if an instance becomes unhealthy.</p><p>You can suspend and then resume one or more of the processes for your <strong>Auto Scaling group</strong>. You might want to do this, for example, so that you can investigate a configuration issue that is causing the process to fail or to prevent Amazon EC2 Auto Scaling from marking instances unhealthy and replacing them while you are making changes to your Auto Scaling group.</p><p><img src=\"https://media.tutorialsdojo.com/sap_asg_suspend_process.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_asg_suspend_process.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Session Manager</strong> is a fully managed <strong>AWS Systems Manager</strong> capability. With Session Manager, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, and on-premises servers and virtual machines (VMs). You can use either an interactive one-click browser-based shell or the AWS Command Line Interface (AWS CLI). Session Manager provides secure and auditable node management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.</p><p>Therefore, the correct answer is: <strong>Go to the Auto Scaling Groups section in the AWS console and suspend the “Terminate” process for the ASG. Log in to one of the unhealthy instances using AWS Systems Manager Session Manager.</strong> This will stop the Auto Scaling group from terminating the EC2 instances, thus, giving you enough time to log in and troubleshoot the issue.</p><p>The option that says: <strong>Update the application log setting to have more verbose logging to capture more application logs. Ensure that the Amazon CloudWatch agent is installed and running on the instances</strong> is incorrect. This is possible however, this will not solve the problem of terminating EC2 instances. It is possible that the EC2 instances are terminated even before they start sending logs to CloudWatch logs.</p><p>The option that says: <strong>Select one of the new Amazon EC2 instances and enable EC2 instance termination protection. Gain access to the unhealthy instance using the AWS Systems Manager Session Manager</strong> is incorrect. Even if the EC2 instance protection is enabled, the ASG has the ability to terminate an EC2 instance that it has created. You should suspend the \"Terminate\" process of the ASG instead.</p><p>The option that says: <strong>Create a temporary Amazon EC2 instance and deploy the new application version. Login to the EC2 instance using the SSH key. Add the instance to the Application Load Balancer to inspect application logs in real-time</strong> is incorrect. This is possible, but it takes a log of steps and is not the quickest solution.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p><p><br></p><p><strong>Check out these AWS Systems Manager and AWS Auto Scaling Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html",
      "https://tutorialsdojo.com/aws-systems-manager/?src=udemy",
      "https://tutorialsdojo.com/aws-auto-scaling/?src=udemy"
    ]
  },
  {
    "id": 46,
    "question": "<p>A company has created multiple accounts in AWS to support the rapid growth of its cloud services. The multiple accounts are used to separate their various departments such as finance, human resources, engineering, and many others. Each account is managed by a Systems Administrator which has root access for that specific account only. There is a requirement to centrally manage policies across multiple AWS accounts by allowing or denying particular AWS services for individual accounts, or for groups of accounts.</p><p>Which is the most suitable solution that you should implement with the LEAST amount of complexity?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Provide access to externally authenticated users via Identity Federation. Set up an IAM role to specify permissions for users from each department whose identity is federated from your organization or a third-party identity provider.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Organizations and Service Control Policies to control the list of AWS services that can be used by each member account.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up AWS Organizations and Organizational Units (OU) to connect all AWS accounts of each department. Create a custom IAM Policy to allow or deny the use of certain AWS services for each account.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Connect all departments by setting up cross-account access to each of the AWS accounts of the company. Create and attach IAM policies to your resources based on their respective departments to control access.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p><strong>AWS Organizations</strong> offers policy-based management for multiple AWS accounts. With Organizations, you can create groups of accounts, automate account creation, and apply and manage policies for those groups. Organizations enables you to centrally manage policies across multiple accounts, without requiring custom scripts and manual processes. It allows you to create <strong>Service Control Policies (SCPs)</strong> that centrally control AWS service use across multiple AWS accounts.</p><p>Remember that AWS Organizations <strong>does not</strong> replace associating IAM policies with users, groups, and roles within an AWS account. Hence, you still need to set up appropriate IAM policies for your root and member accounts.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_orgganization_nested.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_orgganization_nested.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>IAM policies let you allow or deny access to AWS services (such as Amazon S3), individual AWS resources (such as a specific S3 bucket), or individual API actions (such as <code>s3:CreateBucket</code>). An IAM policy can be applied only to IAM users, groups, or roles, and it can never restrict the root identity of the AWS account.</p><p>By contrast, AWS Organizations lets you use service control policies (SCPs) to allow or deny access to particular AWS services for individual AWS accounts, or for groups of accounts within an organizational unit (OU). The specified actions from an attached SCP affect all IAM users, groups, and roles for an account, including the root account identity.</p><p>When you apply an SCP to an OU or an individual AWS account, you choose to either <strong>enable </strong>(whitelist), or <strong>disable </strong>(blacklist) the specified AWS service. Access to any service that isn’t explicitly allowed by the SCPs associated with an account, its parent OUs, or the master account is <strong>denied </strong>to the AWS accounts or OUs associated with the SCP. When an SCP is applied to an OU, it is inherited by all of the AWS accounts in that OU.</p><p>Therefore, the correct answer is: <strong>Use AWS Organizations and Service Control Policies to control the list of AWS services that can be used by each member account.</strong></p><p>The option that says:<strong> Setting up AWS Organizations and Organizational Units (OU) to connect all AWS accounts of each department and creating a custom IAM Policy to allow or deny the use of certain AWS services for each account</strong> is incorrect. Although it is correct to use AWS Organizations, this option is incorrect about IAM Policy. It is the Service Control Policy (SCP) which enables you to allow or deny the use of certain AWS services for each account, and not the IAM Policy.</p><p>The option that says:<strong> Connecting all departments by setting up cross-account access to each of the AWS accounts of the company, then creating and attaching IAM policies to your resources based on their respective departments to control access</strong> is incorrect. Although you can set up cross-account access to each department, this entails a lot of configuration compared with using AWS Organizations and Service Control Policies (SCPs). Cross-account access would be a more suitable choice if you only have two accounts to manage, but not for multiple accounts.</p><p>The option that says: <strong>Providing access to externally authenticated users via Identity Federation and setting up an IAM role to specify permissions for users from each department whose identity is federated from your organization or a third-party identity provider</strong> is incorrect. This option is focused on the Identity Federation authentication set up for your AWS accounts but not the IAM policy management for multiple AWS accounts. A combination of AWS Organizations and Service Control Policies (SCPs) is a better choice compared to this option.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/organizations/\">https://aws.amazon.com/organizations/</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-service-control-policy/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-service-control-policy/</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/organizations/",
      "https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-service-control-policy/",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html",
      "https://tutorialsdojo.com/aws-organizations/?src=udemy"
    ]
  },
  {
    "id": 47,
    "question": "<p>A company is using AWS Organizations to manage their multi-account and multi-region AWS infrastructure. They are currently doing large-scale automation for their key daily processes to save costs. One of these key processes is sharing specified AWS resources, which an organizational account owns, with other AWS accounts of the company using AWS RAM. There is already an existing service which was previously managed by a separate organization account moderator, who also maintained the specific configuration details. </p><p>In this scenario, what could be a simple and effective solution that would allow the service to perform its tasks on the organization accounts on the moderator's behalf?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable cross-account access with AWS Organizations in the Resource Access Manager Console. Mirror the configuration changes that was performed by the account that previously managed this service.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure a service-linked role for AWS RAM and modify the permissions policy to specify what the role can and cannot do. Lastly, modify the trust policy of the role so that other processes can utilize AWS RAM.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Attach an IAM role on the service detailing all the allowed actions that it will be able to perform. Install an SSM agent in each of the worker VMs. Use AWS Systems Manager to build automation workflows that involve the daily key processes.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use trusted access by running the <code>enable-sharing-with-aws-organization</code> command in the AWS RAM CLI. Mirror the configuration changes that was performed by the account that previously managed this service.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p><strong>AWS Resource Access Manager</strong> (AWS RAM) enables you to share specified AWS resources that you own with other AWS accounts. To enable trusted access with AWS Organizations:</p><p>From the AWS RAM CLI, use the <code>enable-sharing-with-aws-organizations</code> command.</p><p>Name of the IAM service-linked role that can be created in accounts when trusted access is enabled: <em>AWSResourceAccessManagerServiceRolePolicy</em>.</p><p>You can use <strong><em>trusted access</em></strong> to enable an AWS service that you specify, called the <em>trusted service</em>, to perform tasks in your organization and its accounts on your behalf. This involves granting permissions to the trusted service but does not otherwise affect the permissions for IAM users or roles. When you enable access, the trusted service can create an IAM role called a service-linked role in every account in your organization. That role has a permissions policy that allows the trusted service to do the tasks that are described in that service's documentation. This enables you to specify settings and configuration details that you would like the trusted service to maintain in your organization's accounts on your behalf.</p><p><img src=\"https://media.tutorialsdojo.com/sap_resources_access_manager.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_resources_access_manager.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore the correct answer is: <strong>Use trusted access by running the </strong><code><strong>enable-sharing-with-aws-organization</strong></code><strong> command in the AWS RAM CLI. Mirror the configuration changes that was performed by the account that previously managed this service.</strong></p><p>The option that says: <strong>Attach an IAM role on the service detailing all the allowed actions that it will be able to perform. Install an SSM agent in each of the worker VMs. Use AWS Systems Manager to build automation workflows that involve the daily key processes</strong> is incorrect because this is not the simplest way to automate the interaction of AWS RAM with AWS Organizations. AWS Systems Manager is a tool that helps with the automation of EC2 instances, on-premises servers, and other virtual machines. It might not support all the services being used by the key processes.</p><p>The option that says: <strong>Configure a service-linked role for AWS RAM and modify the permissions policy to specify what the role can and cannot do. Lastly, modify the trust policy of the role so that other processes can utilize AWS RAM</strong> is incorrect. This is not the simplest solution for integrating AWS RAM and AWS Organizations since using AWS Organization's trusted access will create the service-linked role for you. Also, the trust policy of a service-linked role cannot be modified. Only the linked AWS service can assume a service-linked role, which is why you cannot modify the trust policy of a service-linked role.</p><p>The option that says: <strong>Enable cross-account access with AWS Organizations in the Resources Access Manager Console. Mirror the configuration changes that was performed by the account that previously managed this service</strong> is incorrect because you should enable trusted access to AWS RAM, not cross-account access.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_integrate_services.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_integrate_services.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-ram.html\">https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-ram.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/introducing-an-easier-way-to-delegate-permissions-to-aws-services-service-linked-roles/\">https://aws.amazon.com/blogs/security/introducing-an-easier-way-to-delegate-permissions-to-aws-services-service-linked-roles/</a></p><p><br></p><p><strong>Check out this AWS Resource Access Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-resource-access-manager/?src=udemy\">https://tutorialsdojo.com/aws-resource-access-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_integrate_services.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-ram.html",
      "https://aws.amazon.com/blogs/security/introducing-an-easier-way-to-delegate-permissions-to-aws-services-service-linked-roles/",
      "https://tutorialsdojo.com/aws-resource-access-manager/?src=udemy"
    ]
  },
  {
    "id": 48,
    "question": "<p>A startup is building a web app that lets users post photos of good deeds in their neighborhood with a 143-character caption/article. The developers decided to write the application in ReactJS, a popular javascript framework so that it would run on the broadest range of browsers, mobile phones, and tablets. The app should provide access to Amazon DynamoDB to store the caption. The initial prototype shows that there aren't large spikes in usage.</p><p>Which option provides the most cost-effective and scalable architecture for this application?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the ReactJS client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) to provide signed credentials to an IAM user. This will allow GET and PUT operations to DynamoDB. Serve your web application from an NGINX server hosted in a fleet of EC2 instances that are load-balanced and auto-scaled. Your EC2 instances are configured with an IAM role that allows GET and PUT operations in DynamoDB.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Register the web application with a Web Identity Provider such as Google, Facebook, Amazon, or from any other popular social sites and use the <code>AssumeRoleWithWebIdentity</code> API of STS to generate temporary credentials. Create an IAM role for that web provider and set up permissions for the IAM role to allow GET and PUT operations in Amazon S3 and DynamoDB. Serve your web app out of an S3 bucket enabled as a website.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Register the web application with a Web Identity Provider such as Google, Facebook, Amazon, or any other popular social site. Create an IAM role for that web provider and set up permissions for the IAM role to allow GET and PUT operations in DynamoDB. Serve your web application from an NGINX server hosted on a fleet of EC2 instances, with a load balancer and auto-scaling. Add an IAM role to the EC2 instance to allow GET and PUT operations to DynamoDB tables.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the ReactJS client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) on an EC2 instance. This will provide signed credentials to an IAM user allowing GET and PUT operations in the DynamoDB table and the S3 bucket. You serve your mobile application out of an S3 bucket enabled as a website.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>ReactJS</strong> is a modern framework for creating front-ends for your web applications. From the users perspective, this website is dynamic since it asks for users to login and users can upload images to it. But since it is written on ReactJS framework, it is basically just a bunch of static files and the web browser renders the page dynamically.</p><p>We can have full-blown web pages like this and be hosted on Amazon S3 (with enabling static website hosting) with no problems. From Amazon S3's perspective, it is hosting a static website, since all files that are on the bucket are static files only. But ReactJS allows the client’s web browser to interpret these static files and render the webpage dynamically for the user.</p><p><strong>Amazon Cognito</strong> follows the OIDC specification to authenticate users of web and mobile apps. Users can sign in directly through the Amazon Cognito hosted UI or through a federated identity provider, such as Amazon, Facebook, Apple, or Google. The hosted UI workflows include sign-in and sign-up, password reset, and MFA. Since not all customer workflows are the same, you can customize Amazon Cognito workflows at key points with AWS Lambda functions, allowing you to run code without provisioning or managing servers. After a user authenticates, Amazon Cognito returns standard OIDC tokens. You can use the user profile information in the ID token to grant your users access to your own resources or you can use the tokens to grant access to APIs hosted by Amazon API Gateway. You can also exchange the tokens for temporary AWS credentials to access other AWS services.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cognito_web_token.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cognito_web_token.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>If you don't use Amazon Cognito, then you choose to write a custom code or app that interacts with a web IdP (Login with Amazon, Facebook, Google, or any other OIDC-compatible IdP) and then call the <code>AssumeRoleWithWebIdentity</code> API to trade the authentication token you get from those IdPs for AWS temporary security credentials. If you have already used this approach for existing apps, you can continue to use it. You can also deploy your app in the S3 bucket.</p><p>The option that says: <strong>Register the web application with a Web Identity Provider such as Google, Facebook, Amazon, or any other popular social sites and use the </strong><code><strong>AssumeRoleWithWebIdentity</strong></code><strong> API of STS to generate temporary credentials. Create an IAM role for that web provider and set up permissions for the IAM role to allow GET and PUT operations in Amazon S3 and DynamoDB. Serve your web app out of an S3 bucket enabled as a website</strong> is correct because it authenticates the application via a federated identity provider such as Google, Facebook, Amazon, or other social sites. It sets up proper permission for DynamoDB access and hosts the website in S3. Plus, it also uses STS and <code>AssumeRoleWithWebIdentity</code> API which provides a better authentication.</p><p>The option that says: <strong>Configure the ReactJS client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) on an EC2 instance. This will provide signed credentials to an IAM user allowing GET and PUT operations in the DynamoDB table and the S3 bucket. You serve your mobile application out of an S3 bucket enabled as a website</strong> is incorrect. The Token Vending Machine (STS Service) is implemented on just a single EC2 instance. This poses a single point of failure which means that the architecture is not highly available and not fault-tolerant. This is not a scalable solution either as the instance can become the performance bottleneck.</p><p>The option that says: <strong>Configure the ReactJS client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) to provide signed credentials to an IAM user. This will allow GET and PUT operations to DynamoDB. Serve your web application from an NGINX server hosted in a fleet of EC2 instances that are load-balanced and auto-scaled. Your EC2 instances are configured with an IAM role that allows GET and PUT operations in DynamoDB</strong> is incorrect. Although this solution is highly-available and scalable, deploying EC2 instances in an auto-scaled environment is not as a cost-effective solution as the S3 website.</p><p>The option that says: <strong>Register the web application with a Web Identity Provider such as Google, Facebook, Amazon, or from any other popular social site. Create an IAM role for that web provider and set up permissions for the IAM role to allow GET and PUT operations in DynamoDB. Serve your web application from an NGINX server hosted on a fleet of EC2 instances, with a load balancer and auto-scaling. Add an IAM role to the EC2 instance to allow GET and PUT operations to DynamoDB tables</strong> is incorrect because it does not mention any security token service that generates temporary credentials. Furthermore, deploying EC2 instances in an auto-scaled environment, albeit scalable, is not as cost-effective as the S3 website.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-add-authentication-single-page-web-application-with-amazon-cognito-oauth2-implementation/\">https://aws.amazon.com/blogs/security/how-to-add-authentication-single-page-web-application-with-amazon-cognito-oauth2-implementation/</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-integrate-apps.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-integrate-apps.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mobile/deploy-a-react-app-to-s3-and-cloudfront-with-aws-mobile-hub/\">https://aws.amazon.com/blogs/mobile/deploy-a-react-app-to-s3-and-cloudfront-with-aws-mobile-hub/</a></p><p><br></p><p><strong>Check out these Amazon S3 and Amazon Cognito Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cognito/?src=udemy\">https://tutorialsdojo.com/amazon-cognito/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html",
      "https://aws.amazon.com/blogs/security/how-to-add-authentication-single-page-web-application-with-amazon-cognito-oauth2-implementation/",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-integrate-apps.html",
      "https://aws.amazon.com/blogs/mobile/deploy-a-react-app-to-s3-and-cloudfront-with-aws-mobile-hub/",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy",
      "https://tutorialsdojo.com/amazon-cognito/?src=udemy"
    ]
  },
  {
    "id": 49,
    "question": "<p>A company has launched a company-wide bug bounty program to find and patch up security vulnerabilities in your web applications as well as the underlying cloud resources. As the solutions architect, you are focused on checking system vulnerabilities on AWS resources for DDoS attacks. Due to budget constraints, the company cannot afford to enable AWS Shield Advanced to prevent higher-level attacks.</p><p>Which of the following are the best techniques to help mitigate Distributed Denial of Service (DDoS) attacks for cloud infrastructure hosted in AWS? (Select TWO.)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use S3 as a POSIX-compliant storage instead of EBS Volumes for storing data. Install the SSM agent to all of your instances and use AWS Systems Manager Patch Manager to automatically patch your instances.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Reserved EC2 instances to ensure that each instance has the maximum performance possible. Use AWS WAF to protect your web applications from common web exploits that could affect application availability.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an Amazon CloudFront distribution for both static and dynamic content of your web applications. Add CloudWatch alerts to automatically look and notify the Operations team for high <code>CPUUtilization</code> and <code>NetworkIn</code> metrics, as well as to trigger Auto Scaling of your EC2 instances.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Add multiple Elastic Network Interfaces to each EC2 instance and use Enhanced Networking to increase the network bandwidth.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use an Application Load Balancer (ALB) to reduce the risk of overloading your application by distributing traffic across many backend instances. Integrate AWS WAF and the ALB to protect your web applications from common web exploits that could affect application availability.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>The following options are the correct answers in this scenario as they can help mitigate the effects of DDoS attacks:</p><p><strong>- Use an Amazon CloudFront distribution for both static and dynamic content of your web applications. Add CloudWatch alerts to automatically look and notify the Operations team for high </strong><code><strong>CPUUtilization</strong></code><strong> and </strong><code><strong>NetworkIn</strong></code><strong> metrics, as well as to trigger Auto Scaling of your EC2 instances</strong>.</p><p><strong>- Use an Application Load Balancer (ALB) to reduce the risk of overloading your application by distributing traffic across many backend instances. Integrate AWS WAF and the ALB to protect your web applications from common web exploits that could affect application availability</strong>.</p><p><strong>Amazon CloudFront</strong> is a content delivery network (CDN) service that can be used to deliver your entire website, including static, dynamic, streaming, and interactive content. Persistent TCP connections and variable time-to-live (TTL) can be used to accelerate delivery of content, even if it cannot be cached at an edge location. This allows you to use Amazon CloudFront to protect your web application, even if you are not serving static content. Amazon CloudFront only accepts well-formed connections to prevent many common DDoS attacks like SYN floods and UDP reflection attacks from reaching your origin.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Larger DDoS attacks can exceed the size of a single Amazon EC2 instance. To mitigate these attacks, you will want to consider options for load balancing excess traffic. With Elastic Load Balancing (ELB), you can reduce the risk of overloading your application by distributing traffic across many backend instances. ELB can scale automatically, allowing you to manage larger volumes of unanticipated traffic, like flash crowds or DDoS attacks.</p><p>Another way to deal with application layer attacks is to operate at scale. In the case of web applications, you can use ELB to distribute traffic to many Amazon EC2 instances that are overprovisioned or configured to auto scale for the purpose of serving surges of traffic, whether it is the result of a flash crowd or an application layer DDoS attack. Amazon CloudWatch alarms are used to initiate Auto Scaling, which automatically scales the size of your Amazon EC2 fleet in response to events that you define. This protects application availability even when dealing with an unexpected volume of requests.</p><p>The option that says: <strong>Use S3 as a POSIX-compliant storage instead of EBS Volumes for storing data. Install the SSM agent to all of your instances and use AWS Systems Manager Patch Manager to automatically patch your instances</strong> is incorrect because using S3 instead of EBS Volumes is mainly for addressing scalability to your storage requirements and not for avoiding DDoS attacks. In addition, Amazon S3 is not a POSIX-compliant storage.</p><p>The option that says:<strong> Add multiple Elastic Network Interfaces to each EC2 instance and use Enhanced Networking to increase the network bandwidth</strong> is incorrect. Even if you add multiple ENIs and are using Enhanced Networking to increase the network throughput of the instances, the CPU of the instance will be saturated with the DDoS requests which will cause the application to be unresponsive.</p><p>The option that says: <strong>Use Reserved EC2 instances to ensure that each instance has the maximum performance possible. Use AWS WAF to protect your web applications from common web exploits that could affect application availability</strong> is incorrect because using Reserved EC2 instances does not provide any additional computing performance compared to other EC2 types.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/shield-chapter.html\">https://docs.aws.amazon.com/waf/latest/developerguide/shield-chapter.html</a></p><p><a href=\"https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf\">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p><p><a href=\"https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf\">https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Best practices on DDoS Attack Mitigation:</strong></p><p><a href=\"https://youtu.be/HnoZS5jj7pk\">https://youtu.be/HnoZS5jj7pk</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/waf/latest/developerguide/shield-chapter.html",
      "https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy",
      "https://youtu.be/HnoZS5jj7pk"
    ]
  },
  {
    "id": 50,
    "question": "<p>A company has several virtual machines on its on-premises data center hosting its three-tier web application. The company wants to migrate the application to AWS to take advantage of the benefits of cloud computing. The following are the company requirements for the migration process:</p><p>- The virtual machine images from the on-premises data center must be imported to AWS.</p><p>- The changes on the on-premises servers must be synchronized to the AWS servers until the production cutover is completed.</p><p>- Have minimal downtime during the production cutover.</p><p>- The root volumes and data volumes (containing Terabytes of data) of the VMs must be migrated to AWS.</p><p>- The migration solution must have minimal operational overhead.</p><p>Which of the following options is the recommended solution to meet the company requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage both AWS Application Discovery Service and AWS Migration Hub to group the on-premises VMs as an application. Write an AWS CLI script that uses VM Import/Export to import the VMs as AMIs. Schedule the script to run at regular intervals to synchronize the changes from the on-premises environment to AWS. Launch Amazon EC2 instances based on the images created from VM Import/Export. After successful testing, perform a final virtual machine import before the cutover. Launch new instances based on the updated AMIs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a job on AWS Application Migration Service (MGN) to migrate the virtual machines to AWS. Install the replication agent on each application tier to sync the changes from the on-premises environment to AWS. Launch Amazon EC2 instances based on the replicated VM from AWS MGN. After successful testing, perform a cutover and launch new instances based on the updated AMIs.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a job on AWS Application Migration Service (MGN) to migrate the root volumes of the virtual machines to AWS. Import the data volumes using the AWS CLI import-snapshot command. Launch Amazon EC2 instances based on the images created from AWS MGN and attach the imported data volumes. After successful testing, perform a final replication before the cutover. Launch new instances based on the updated AMIs and attach the corresponding data volumes.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Write an AWS CLI script that uses VM Import/Export to migrate the virtual machines. Schedule the script to run at regular intervals to synchronize the changes from the on-premises environment to AWS. Launch Amazon EC2 instances based on the images created from VM Import/Export. After successful testing, re-run the script to perform a final replication before the cutover. Launch new instances based on the updated AMIs.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p><strong>AWS Application Migration Service</strong> minimizes time-intensive, error-prone manual processes by automatically converting your source servers to run natively on AWS. It also simplifies application modernization with built-in, post-launch optimization options.</p><p><strong>AWS Application Migration Service (MGN)</strong> is a highly automated lift-and-shift (rehost) solution that simplifies, expedites, and reduces the cost of migrating applications to AWS. It enables companies to lift and shift a large number of physical, virtual, or cloud servers without compatibility issues, performance disruption, or long cutover windows.</p><p>MGN replicates source servers into your AWS account. When you’re ready, it automatically converts and launches your servers on AWS so you can quickly benefit from the cost savings, productivity, resilience, and agility of the Cloud. Once your applications are running on AWS, you can leverage AWS services and capabilities to quickly and easily re-platform or refactor those applications – which makes lift-and-shift a fast route to modernization.</p><p>The first setup step for Application Migration Service is creating the Replication Settings template. Add source servers to Application Migration Service by installing the AWS Replication Agent (also referred to as \"the Agent\") on them. The Agent can be installed on both Linux and Windows servers. After you have added all of your source servers and configured their launch settings, you are ready to launch a Test instance. Once you have finalized the testing of all of your source servers, you are ready for cutover. The cutover will migrate your source servers to the Cutover instances on AWS.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_mgn_overview.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_mgn_overview.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is:<strong> Create a job on AWS Application Migration Service (MGN) to migrate the virtual machines to AWS. Install the replication agent on each application tier to sync the changes from the on-premises environment to AWS. Launch Amazon EC2 instances based on the replicated VM from AWS MGN. After successful testing, perform a cutover and launch new instances based on the updated AMIs.<br></strong></p><p>The option that says: <strong>Write an AWS CLI script that uses VM Import/Export to migrate the virtual machines. Schedule the script to run at regular intervals to synchronize the changes from the on-premises environment to AWS. Launch Amazon EC2 instances based on the images created from VM Import/Export. After successful testing, re-run the script to perform a final replication before the cutover. Launch new instances based on the updated AMIs</strong> is incorrect. AWS VM Import/Export does not support synching incremental changes from the on-premises environment to AWS. You will need to import the VM again as a whole after you make changes to the on-premises environment. This requires a lot of time and adds more operational overhead.</p><p>The option that says: <strong>Create a job on AWS Application Migration Service (MGN) to migrate the root volumes of the virtual machines to AWS. Import the data volumes using the AWS CLI import-snapshot command. Launch Amazon EC2 instances based on the images created from AWS MGN and attach the imported data volumes. After successful testing, perform a final replication before the cutover. Launch new instances based on the updated AMIs and attach the corresponding data volumes</strong> is incorrect. This may be possible but creating manual snapshots of the data volumes requires more operational overhead. AWS MGN supports syncing of attached volumes as well, so you don't have to migrate the data volumes manually.</p><p>The option that says: <strong>Leverage both AWS Application Discovery Service and AWS Migration Hub to group the on-premises VMs as an application. Write an AWS CLI script that uses VM Import/Export to import the VMs as AMIs. Schedule the script to run at regular intervals to synchronize the changes from the on-premises environment to AWS. Launch Amazon EC2 instances based on the images created from VM Import/Export. After successful testing, perform a final virtual machine import before the cutover. Launch new instances based on the updated AMIs</strong> is incorrect. The AWS Application Discovery Service plans migration projects by gathering information about the on-premises data center, and all discovered data are stored in your AWS Migration Hub. This is similar to the other option for VM Import/Export, as you will need to import the VM again as a whole after you make changes on the on-premises environment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/application-migration-service/\">https://aws.amazon.com/application-migration-service/</a></p><p><a href=\"https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html\">https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html</a></p><p><a href=\"https://docs.aws.amazon.com/mgn/latest/ug/installation-requirements.html\">https://docs.aws.amazon.com/mgn/latest/ug/installation-requirements.html</a></p><p><br></p><p><strong>Check out this AWS Server Migration Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-application-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-application-migration-service/</a></p><p><br></p><p><strong>AWS Migration Services Overview:</strong></p><p><a href=\"https://youtu.be/yqNBkFMnsL8\">https://youtu.be/yqNBkFMnsL8</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/application-migration-service/",
      "https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html",
      "https://docs.aws.amazon.com/mgn/latest/ug/installation-requirements.html",
      "https://tutorialsdojo.com/aws-application-migration-service/?src=udemy",
      "https://youtu.be/yqNBkFMnsL8"
    ]
  },
  {
    "id": 51,
    "question": "<p>A company is hosting its flagship product page on a three-tier web application in its on-premises data center. The popularity of the last product launch attracted a sudden surge of traffic to their site, which caused some downtime that resulted in a significant impact on the product’s sales volume. The management decided to move the application to AWS. The application uses a MySQL database and is written in .NET framework. The Solutions Architect must design a highly available and scalable infrastructure to handle the demand of 300,000 peak users.</p><p>Which of the following design options would satisfy the above requirements while being cost-effective?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch a CloudFormation stack that contains an Amazon ECS cluster that spans multiple Availability Zones using Spot Instances. Create an Application Load Balancer in front of the ECS cluster. Use the stack to launch an Amazon RDS MySQL database in Multi-AZ configuration with a “snapshot” deletion policy. Create a Route 53 zone entry for the company’s domain name with an Alias-record pointed to the ALB.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an AWS Elastic Beanstalk application that contains a web server tier and an Amazon RDS MySQL Multi-AZ database tier. The web server tier should launch a fleet of Amazon EC2 Auto Scaling Group spanning multiple Availability Zones and behind a Network Load Balancer. Create a Route 53 zone entry for the company’s domain name with an Alias-record pointed to the NLB.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Launch a CloudFormation stack that contains an Auto Scaling Group of Amazon EC2 instances spanning multiple Availability Zones that are behind an Application Load Balancer. Use the stack to launch an Amazon Aurora MySQL database cluster in a Multi-AZ configuration with a “retain” deletion policy. Create a Route 53 zone entry for the company’s domain name with an Alias-record pointed to the ALB.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an AWS Elastic Beanstalk application with an Auto Scaling group of EC2 instances as web servers that spans two separate regions. Put the EC2 instances behind an Application Load Balancer in each region. Launch a Multi-AZ Amazon Aurora MySQL database with cross-region read replica to the other region. Create zone entries in Route 53 with <code>geoproximity</code> routing policy to direct the traffic between the two regions.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS CloudFormation</strong> gives you an easy way to model a collection of related AWS resources, provision them quickly and consistently, and manage them throughout their lifecycles, by treating infrastructure as code. A CloudFormation template describes your desired resources and their dependencies so you can launch and configure them together as a stack.</p><p>In CloudFormation, the <code>AWS::AutoScaling::AutoScalingGroup</code> resource defines an Amazon EC2 Auto Scaling group, which is a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management.</p><p>Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_nested_stacks.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_nested_stacks.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>To use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto Scaling group to register the group with the load balancer. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group. An Application Load Balancer is ideal for this scenario as routes and load balances at the application layer (HTTP/HTTPS) and supports path-based routing.</p><p>On CloudFormation you can set the RDS DeletionPolicy as “Retain” which keeps the resource without deleting it or its contents when its stack is deleted. This is helpful in the event that the stack is deleted and you need to quickly provision a new stack. You can quickly use the retained RDS instance from the old stack.</p><p>Therefore, the correct answer is:<strong> Launch a CloudFormation stack that contains an Auto Scaling Group of Amazon EC2 instances spanning multiple Availability Zones that are behind an Application Load Balancer. Use the stack to launch an Amazon Aurora MySQL database cluster in a Multi-AZ configuration with a “retain” deletion policy. Create a Route 53 zone entry for the company’s domain name with an Alias-record pointed to the ALB.</strong></p><p>The option that says: <strong>Create an AWS Elastic Beanstalk application that contains a web server tier and an Amazon RDS MySQL Multi-AZ database tier. The web server tier should launch a fleet of Amazon EC2 Auto Scaling Group spanning multiple Availability Zones and behind a Network Load Balancer. Create a Route 53 zone entry for the company’s domain name with an Alias-record pointed to the NLB </strong>is incorrect. You do not need an expensive Network Load Balancer (NLB) to handle the expected peak traffic. An NLB is a good choice if you expect millions of requests per second.</p><p>The option that says: <strong>Create an AWS Elastic Beanstalk application with an Auto Scaling group of EC2 instances as web servers that spans two separate regions. Put the EC2 instances behind an Application Load Balancer in each region. Launch a Multi-AZ Amazon Aurora MySQL database with cross-region read replica to the other region. Create zone entries in Route 53 with geoproximity routing policy to direct the traffic between the two regions</strong> is incorrect. Creating two Auto Scaling groups on separate regions is unnecessary and expensive. Distributing the EC2 instance in multiple Availability Zones is enough to handle the traffic. In this setup, the database on the second region is a read-replica only so any writes to the database will have to be sent on the main region’s RDS instance.</p><p>The option that says: <strong>Launch a CloudFormation stack that contains an Amazon ECS cluster that spans multiple Availability Zones using Spot Instances. Create an Application Load Balancer in front of the ECS cluster. Use the stack to launch an Amazon RDS MySQL database in Multi-AZ configuration with a “snapshot” deletion policy. Create a Route 53 zone entry for the company’s domain name with an Alias-record pointed to the ALB</strong> is incorrect. Although the Spot instance provides good cost savings for the web tier, the reliability of the site will suffer as the Spot instances are usually reclaimed by AWS based on the supply and demand of its global computing capacity. The “snapshot” deletion policy on the database tier is also not ideal as this will require a significant time to restore if you delete the CloudFormation stack.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/parallel-stack-processing-and-nested-stack-updates-for-aws-cloudformation/\">https://aws.amazon.com/blogs/aws/parallel-stack-processing-and-nested-stack-updates-for-aws-cloudformation/</a></p><p><br></p><p><strong>Check out these Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><br></p><p><strong>AWS CloudFormation - Templates, Stacks, Change Sets:</strong></p><p><a href=\"https://youtu.be/9Xpuprxg7aY\">https://youtu.be/9Xpuprxg7aY </a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/cloudformation/",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html",
      "https://aws.amazon.com/blogs/aws/parallel-stack-processing-and-nested-stack-updates-for-aws-cloudformation/",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy",
      "https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy",
      "https://tutorialsdojo.com/aws-cloudformation/?src=udemy",
      "https://youtu.be/9Xpuprxg7aY"
    ]
  },
  {
    "id": 52,
    "question": "<p>A media company has a suite of internet-facing web applications hosted in US West (N. California) region in AWS. The architecture is composed of several On-Demand Amazon EC2 instances behind an Application Load Balancer, which is configured to use public SSL/TLS certificates. The Application Load Balancer also enables incoming HTTPS traffic through the fully qualified domain names (FQDNs) of the applications for SSL termination. A Solutions Architect has been instructed to upgrade the corporate web applications to a multi-region architecture that uses various AWS Regions such as ap-southeast-2, ca-central-1, eu-west-3, and so forth. </p><p>Which of the following approach should the Architect implement to ensure that all HTTPS services will continue to work without interruption?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>In each new AWS Region, request for SSL/TLS certificates using AWS KMS for each FQDN. Associate the new certificates to the corresponding Application Load Balancer of the same AWS Region.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the AWS Certificate Manager service in the US West (N. California) region to request for SSL/TLS certificates for each FQDN which will be used to all regions. Associate the new certificates to the new Application Load Balancer on each new AWS Region that the Architect will add.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the AWS KMS in the US West (N. California) region to request for SSL/TLS certificates for each FQDN which will be used to all regions. Associate the new certificates to the new Application Load Balancer on each new AWS Region that the Architect will add.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>In each new AWS Region, request for SSL/TLS certificates using the AWS Certificate Manager for each FQDN. Associate the new certificates to the corresponding Application Load Balancer of the same AWS Region.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Certificate Manager</strong> is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Certificate Manager removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates.</p><p>With AWS Certificate Manager, you can quickly request a certificate, deploy it on ACM-integrated AWS resources, such as Elastic Load Balancers, Amazon CloudFront distributions, and APIs on API Gateway, and let AWS Certificate Manager handle certificate renewals. It also enables you to create private certificates for your internal resources and manage the certificate lifecycle centrally. Public and private certificates provisioned through AWS Certificate Manager for use with ACM-integrated services are free. You pay only for the AWS resources you create to run your application. With AWS Certificate Manager Private Certificate Authority, you pay monthly for the operation of the private CA and for the private certificates you issue.</p><p><img src=\"https://media.tutorialsdojo.com/sap_alb_https_acm.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_alb_https_acm.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can use the same SSL certificate from ACM in more than one AWS Region but it depends on whether you’re using Elastic Load Balancing or Amazon CloudFront. To use a certificate with Elastic Load Balancing for the same site (the same fully qualified domain name, or FQDN, or set of FQDNs) in a different Region, you must request a new certificate for each Region in which you plan to use it. To use an ACM certificate with Amazon CloudFront, you must request the certificate in the US East (N. Virginia) region. ACM certificates in this region that are associated with a CloudFront distribution are distributed to all the geographic locations configured for that distribution.</p><p>Hence, the correct answer is the option that says: <strong>In each new AWS Region, request for SSL/TLS certificates using the AWS Certificate Manager for each FQDN. Associate the new certificates to the corresponding Application Load Balancer of the same AWS Region.</strong></p><p>The option that says: <strong>In each new AWS Region, request for SSL/TLS certificates using AWS KMS for each FQDN. Associate the new certificates to the corresponding Application Load Balancer of the same AWS Region</strong> is incorrect because AWS KMS is not the right service to use to generate the SSL/TLS certificates. You have to utilize ACM instead.</p><p>The option that says: <strong>Use the AWS KMS in the US West (N. California) region to request for SSL/TLS certificates for each FQDN which will be used to all regions. Associate the new certificates to the new Application Load Balancer on each new AWS Region that the Architect will add </strong>is incorrect. You have to use the AWS Certificate Manager (ACM) service to generate the certificates and not AWS KMS as this service is primarily used for data encryption. Moreover, you have to associate the certificates that were generated from the same AWS Region where the load balancer is launched.</p><p>The option that says: <strong>Use the AWS Certificate Manager service in the US West (N. California) region to request for SSL/TLS certificates for each FQDN which will be used to all regions. Associate the new certificates to the new Application Load Balancer on each new AWS Region that the Architect will add<em> </em></strong>is incorrect. You can only use the same SSL certificate from ACM in more than one AWS Region if you are attaching it to your CloudFront distribution only, and not to your Application Load Balancer. To use a certificate with Elastic Load Balancing for the same site (the same fully qualified domain name, or FQDN, or set of FQDNs) in a different Region, you must request a new certificate for each Region in which you plan to use it.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/certificate-manager/faqs/\">https://aws.amazon.com/certificate-manager/faqs/</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/\">https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-add-or-delete-listeners.html#add-listener-console\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-add-or-delete-listeners.html#add-listener-console</a></p><p><br></p><p><strong>Check out these AWS Certificate Manager and Elastic Load Balancer Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certificate-manager/?src=udemy\">https://tutorialsdojo.com/aws-certificate-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/certificate-manager/faqs/",
      "https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-add-or-delete-listeners.html#add-listener-console",
      "https://tutorialsdojo.com/aws-certificate-manager/?src=udemy",
      "https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy"
    ]
  },
  {
    "id": 53,
    "question": "<p>A company is hosting its production environment on its on-premises servers. Most of the applications are packed as Docker containers that are manually run on self-managed virtual machines. The web servers are using the latest commercial Oracle Java SE suite which costs the company thousands of dollars in licensing costs. The MySQL databases are installed on separate servers configured on a “source-replica” setup for high availability. The company wants to migrate the whole environment to AWS Cloud to take advantage of its flexibility and agility, as well as use OpenJDK to save licensing costs without major changes in its applications.</p><p>Which of the following application migration strategies meet the above requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Re-factor/re-architect the environment on AWS Cloud by converting the Docker containers to run on AWS Lambda Functions. Convert the MySQL database to Amazon DynamoDB using the AWS Schema Conversion Tool (AWS SCT) to save on costs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Re-platform the environment on the AWS Cloud platform by deploying the Docker containers on AWS App Runner to reduce operational overhead. Test the new OpenJDK Docker containers and upload them on Amazon Elastic Container Registry (ECR). Convert the MySQL database to Amazon DynamoDB using the AWS Schema Conversion Tool (AWS SCT) to save on costs.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Re-host the environment on the AWS Cloud platform by creating EC2 instances that mirror the current web servers and database servers. Host the Docker instances on Amazon EC2 and test the new OpenJDK Docker containers on these instances. Create a dump of the on-premises MySQL databases and upload it to an Amazon S3 bucket. Launch a new Amazon EC2 instance with a MySQL database and import the data from Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Re-platform the environment on the AWS Cloud platform by running the Docker containers on Amazon ECS. Test the new OpenJDK Docker containers and upload them on Amazon Elastic Container Registry. Migrate the MySQL database to Amazon RDS using AWS Database Migration Service.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p>The six most common application migration strategies are:</p><p><strong>Rehosting</strong> — Otherwise known as “lift-and-shift”. Many early cloud projects gravitate toward net new development using cloud-native capabilities, but in a large legacy migration scenario where the organization is looking to scale its migration quickly to meet a business case, applications can be rehosted. Most rehosting can be automated with tools (e.g., CloudEndure Migration, AWS VM Import/Export), although you can do this manually to apply changes on legacy systems to the new cloud platform.</p><p><strong>Replatforming</strong> — Sometimes, this is called “lift-tinker-and-shift.” Here you might make a few cloud (or other) optimizations in order to achieve some tangible benefit, but you aren’t otherwise changing the core architecture of the application. You may be looking to reduce the amount of time you spend managing database instances by migrating to a database-as-a-service platform like Amazon Relational Database Service (Amazon RDS) or migrating your application to a fully managed platform like Amazon Elastic Beanstalk.</p><p><img src=\"https://media.tutorialsdojo.com/sap_replatform.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_replatform.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Repurchasing</strong> — Moving to a different product. Repurchasing is a move to a SaaS platform. Moving a CRM to Salesforce.com, an HR system to Workday, a CMS to Drupal, etc.</p><p><strong>Refactoring / Re-architecting</strong> — Re-imagining how the application is architected and developed, typically using cloud-native features. This is typically driven by a strong business need to add features, scale, or performance that would otherwise be difficult to achieve in the application’s existing environment. For example, migrating from a monolithic architecture to a service-oriented (or server-less) architecture to boost agility.</p><p><strong>Retire</strong> — This strategy basically means: \"Get rid of.\" Once you’ve discovered everything in your environment, you might ask each functional area who owns each application and see that some of the applications are no longer used. You can save costs by retiring these applications.</p><p><strong>Retain</strong> — Usually this means “revisit” or do nothing (for now). Maybe you aren’t ready to prioritize an application that was recently upgraded or are otherwise not inclined to migrate some applications. You can retain these applications and revisit your migration strategy.</p><p>Therefore, the correct answer is:<strong> Re-platform the environment on the AWS Cloud platform by running the Docker containers on Amazon ECS. Test the new OpenJDK Docker containers and upload them on Amazon Elastic Container Registry. Migrate the MySQL database to Amazon RDS using AWS Database Migration Service.</strong></p><p>The option that says: <strong>Re-host the environment on the AWS Cloud platform by creating EC2 instances that mirror the current web servers and database servers. Host the Docker instances on Amazon EC2 and test the new OpenJDK Docker images on these instances. Create a dump of the on-premises MySQL databases and upload it to an Amazon S3 bucket. Launch a new Amazon EC2 instance with a MySQL database and import the data from Amazon S3 </strong>is incorrect. Although this is possible, simply re-hosting your applications by mirroring your current on-premises setup does not take advantage of the cloud’s elasticity and agility. A better approach is to use Amazon ECS to run the Docker containers and migrate the MySQL database to Amazon RDS.</p><p>The option that says:<strong> Re-factor/re-architect the environment on AWS Cloud by converting the Docker containers to run on AWS Lambda Functions. Convert the MySQL database to Amazon DynamoDB using the AWS Schema Conversion Tool (AWS SCT) to save on costs </strong>is incorrect because this solution requires major changes on the current application to execute successfully. In addition, there is nothing mentioned in the scenario that warrants the conversion of the MySQL database to Amazon DynamoDB.</p><p>The option that says:<strong> Re-platform the environment on the AWS Cloud platform by deploying the Docker containers on AWS App Runner to reduce operational overhead. Test the new OpenJDK Docker containers and upload them on Amazon Elastic Container Registry (ECR). Convert the MySQL database to Amazon DynamoDB using the AWS Schema Conversion Tool (AWS SCT) to save on costs</strong> is incorrect. It is possible to use AWS App Runner to deploy highly available containerized workloads. However, there is nothing mentioned in the scenario that warrants the conversion of the MySQL database to Amazon DynamoDB.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/\">https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/</a></p><p><a href=\"https://aws.amazon.com/blogs/enterprise-strategy/214-2/\">https://aws.amazon.com/blogs/enterprise-strategy/214-2/</a></p><p><a href=\"https://aws.amazon.com/blogs/enterprise-strategy/considering-a-mass-migration-to-the-cloud/\">https://aws.amazon.com/blogs/enterprise-strategy/considering-a-mass-migration-to-the-cloud/</a></p><p><br></p><p><strong>AWS Migration Strategies Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-migration-strategies-the-6-rs/?src=udemy\">https://tutorialsdojo.com/aws-migration-strategies-the-6-rs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/",
      "https://aws.amazon.com/blogs/enterprise-strategy/214-2/",
      "https://aws.amazon.com/blogs/enterprise-strategy/considering-a-mass-migration-to-the-cloud/",
      "https://tutorialsdojo.com/aws-migration-strategies-the-6-rs/?src=udemy"
    ]
  },
  {
    "id": 54,
    "question": "<p>A company has production, development, and test environments in its software development department, and each environment contains tens to hundreds of EC2 instances, along with other AWS services. Recently, Ubuntu released a series of security patches for a critical flaw that was detected in their OS. Although this is an urgent matter, there is no guarantee yet that these patches will be bug-free and production-ready hence, the company must immediately patch all of its affected Amazon EC2 instances in all the environments, except for the production environment. The EC2 instances in the production environment will only be patched after it has been verified that the patches work effectively. Each environment also has different baseline patch requirements that needed to be satisfied.</p><p>Using the AWS Systems Manager service, how should you perform this task with the least amount of effort?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Tag each instance based on its OS. Create a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize EC2 instances based on their tags using Patch Groups and then apply the patches specified in the corresponding patch baseline to each Patch Group. Afterward, verify that the patches have been installed correctly using Patch Compliance. Record the changes to patch and association compliance statuses using AWS Config.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Tag each instance based on its environment and OS. Create a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize EC2 instances based on their tags using Patch Groups and apply the patches specified in the corresponding patch baseline to each Patch Group.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Schedule a maintenance period in AWS Systems Manager Maintenance Windows for each environment, where the period is after business hours so as not to affect daily operations. During the maintenance period, Systems Manager will execute a cron job that will install the required patches for each EC2 instance in each environment. After that, verify in Systems Manager Managed Instances that your environments are fully patched and compliant.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Tag each instance based on its environment and OS. Create various shell scripts for each environment that specifies which patch will serve as its baseline. Using AWS Systems Manager Run Command, place the EC2 instances into Target Groups and execute the script corresponding to each Target Group.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type.</p><p>Patch Manager uses <strong>patch baselines</strong>, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. For each auto-approval rule that you create, you can specify an auto-approval delay. This delay is the number of days of wait after the patch was released, before the patch is automatically approved for patching.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>A <strong>patch group</strong> is an optional means of organizing instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested. You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: <code><strong>Patch Group</strong></code>. After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution.</p><p>Hence, the correct answer is: <strong>Tag each instance based on its environment and OS. Create a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize EC2 instances based on their tags using Patch Groups and apply the patches specified in the corresponding patch baseline to each Patch Group.</strong></p><p>The option that says: <strong>Tag each instance based on its environment and OS. Create various shell scripts for each environment that specifies which patch will serve as its baseline. Using AWS Systems Manager Run Command, place the EC2 instances into Target Groups and execute the script corresponding to each Target Group </strong>is incorrect as this option takes more effort to perform because you are using Systems Manager Run Command instead of Patch Manager. The Run Command service enables you to automate common administrative tasks and perform ad hoc configuration changes at scale, however, it takes a lot of effort to implement this solution. You can use Patch Manager instead to perform the task required by the scenario since you need to perform this task with the least amount of effort.</p><p>The option that says: <strong>Tag each instance based on its OS. Create a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize EC2 instances based on their tags using Patch Groups and then apply the patches specified in the corresponding patch baseline to each Patch Group. Afterward, verify that the patches have been installed correctly using Patch Compliance. Record the changes to patch and association compliance statuses using AWS Config</strong> is incorrect. You should be tagging instances based on the environment and its OS type in which they belong and not just its OS type. This is because the type of patches that will be applied varies between the different environments. With this option, the Ubuntu EC2 instances in all of your environments, including in production, will automatically be patched.</p><p>The option that says: <strong>Schedule a maintenance period in AWS Systems Manager Maintenance Windows for each environment, where the period is after business hours so as not to affect daily operations. During the maintenance period, Systems Manager will execute a cron job that will install the required patches for each EC2 instance in each environment. After that, verify in Systems Manager Managed Instances that your environments are fully patched and compliant</strong> is incorrect because this is not the simplest way to address the issue using AWS Systems Manager. The AWS Systems Manager Maintenance Windows feature lets you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches. Each Maintenance Window has a schedule, a maximum duration, a set of registered targets (the instances that are acted upon), and a set of registered tasks. Although this solution may work, it entails a lot of configuration and effort to implement.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/\">https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html",
      "https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/",
      "https://tutorialsdojo.com/aws-systems-manager/?src=udemy"
    ]
  },
  {
    "id": 55,
    "question": "<p>A government agency has multiple VPCs in various AWS regions across the United States that need to be linked up to an on-premises central office network in Washington, D.C. The central office requires inter-region VPC access over a private network that is dedicated to each region for enhanced security and more predictable data transfer performance. Your team is tasked to quickly build this network mesh and to minimize the management overhead to maintain these connections. </p><p>Which of the following options is the most secure, highly available, and durable solution that you should use to set up this kind of interconnectivity?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement a hub-and-spoke network topology in each region that routes all traffic through a network transit center using AWS Transit Gateway. Route traffic between VPCs and the on-premise network over AWS Site-to-Site VPN.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable inter-region VPC peering which allows peering relationships to be established between VPCs across different AWS regions. This will ensure that the traffic will always stay on the global AWS backbone and will never traverse the public Internet.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a link aggregation group (LAG) in the central office network to aggregate multiple connections at a single AWS Direct Connect endpoint in order to treat them as a single, managed connection. Use AWS Direct Connect Gateway to achieve inter-region VPC access to all of your AWS resources. Create a virtual private gateway in each VPC and then create a public virtual interface for each AWS Direct Connect connection to the Direct Connect Gateway.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Utilize AWS Direct Connect Gateway for inter-region VPC access. Create a virtual private gateway in each VPC, then create a private virtual interface for each AWS Direct Connect connection to the Direct Connect gateway.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p><strong>AWS Direct Connect</strong> is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS to achieve higher privacy benefits, additional data transfer bandwidth, and more predictable data transfer performance. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.</p><p>Using industry standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. Virtual interfaces can be reconfigured at any time to meet your changing needs. You can use an <strong>AWS Direct Connect gateway </strong>to connect your AWS Direct Connect connection over a private virtual interface to one or more VPCs in your account that are located in the same or different Regions. You associate a Direct Connect gateway with the virtual private gateway for the VPC. Then, create a private virtual interface for your AWS Direct Connect connection to the Direct Connect gateway. You can attach multiple private virtual interfaces to your Direct Connect gateway.</p><p>With <strong>Direct Connect Gateway</strong>, you no longer need to establish multiple BGP sessions for each VPC; this reduces your administrative workload as well as the load on your network devices.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dc_private_vif.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_dc_private_vif.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Utilize AWS Direct Connect Gateway for inter-region VPC access. Create a virtual private gateway in each VPC, then create a private virtual interface for each AWS Direct Connect connection to the Direct Connect gateway.</strong></p><p>The option that says: <strong>Create a link aggregation group (LAG) in the central office network to aggregate multiple connections at a single AWS Direct Connect endpoint in order to treat them as a single, managed connection. Use AWS Direct Connect Gateway to achieve inter-region VPC access to all of your AWS resources. Create a virtual private gateway in each VPC and then create a public virtual interface for each AWS Direct Connect connection to the Direct Connect Gateway</strong> is incorrect. You only need to create <strong>private</strong> virtual interfaces to the Direct Connect gateway since you are only connecting to resources inside a VPC. Using a link aggregation group (LAG) is also irrelevant in this scenario because it is just a logical interface that uses the Link Aggregation Control Protocol (LACP) to aggregate multiple connections at a single AWS Direct Connect endpoint, allowing you to treat them as a single, managed connection.</p><p>The option that says:<strong> Implement a hub-and-spoke network topology in each region that routes all traffic through a network transit center using AWS Transit Gateway. Route traffic between VPCs and the on-premise network over AWS Site-to-Site VPN</strong> is incorrect since the scenario requires a service that can provide a dedicated network between the VPCs and the on-premises network, as well as enhanced privacy and predictable data transfer performance. Simply using AWS Transit Gateway will not fulfill the conditions above. This option is best suited for customers who want to leverage AWS-provided, automated high availability network connectivity features and also optimize their investments in third-party product licensing such as VPN software.</p><p>The option that says: <strong>Enable inter-region VPC peering which allows peering relationships to be established between VPCs across different AWS regions. This will ensure that the traffic will always stay on the global AWS backbone and will never traverse the public internet</strong> is incorrect. This solution would require a lot of manual setup and management overhead to successfully build a functional, error-free inter-region VPC network compared with just using a Direct Connect Gateway. Although the Inter-Region VPC Peering provides a cost-effective way to share resources between regions or replicate data for geographic redundancy, its connections are not dedicated and highly available.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/\">https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/</a></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html</a></p><p><a href=\"https://aws.amazon.com/answers/networking/aws-multiple-region-multi-vpc-connectivity/\">https://aws.amazon.com/answers/networking/aws-multiple-region-multi-vpc-connectivity/</a></p><p><br></p><p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/",
      "https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html",
      "https://aws.amazon.com/answers/networking/aws-multiple-region-multi-vpc-connectivity/",
      "https://tutorialsdojo.com/aws-direct-connect/?src=udemy"
    ]
  },
  {
    "id": 56,
    "question": "<p>A multinational investment bank has a hybrid cloud architecture that uses a single 1 Gbps AWS Direct Connect connection to integrate their on-premises network to AWS Cloud. The bank has a total of 10 VPCs which are all connected to their on-premises data center via the same Direct Connect connection that you manage. Based on the recent IT audit, the existing network setup has a single point of failure which needs to be addressed immediately.</p><p>Which of the following is the MOST cost-effective solution that you should implement in order to improve the connection redundancy of your hybrid network?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Establish VPN tunnels from your on-premises data center to each of the 10 VPCs. Terminate each VPN tunnel connection at the virtual private gateway (VGW) of the respective VPC. Configure BGP for route management.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Establish another 1 Gbps AWS Direct Connect connection using a public Virtual Interface (VIF). Prepare a VPN tunnel that will terminate on the virtual private gateway (VGW) of the respective VPC using the public VIF. Handle the failover to the VPN connection through the use of BGP.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Establish another 1 Gbps AWS Direct Connect connection with corresponding private Virtual Interfaces (VIFs) to connect all of the 10 VPCs individually. Set up a Border Gateway Protocol (BGP) peering session for all of the VIFs.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Establish a new point-to-point Multiprotocol Label Switching (MPLS) connection to all of your 10 VPCs. Configure BGP to use this new connection with an active/passive routing.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>With AWS Direct Connect plus VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than Internet-based VPN connections.</p><p>You can use AWS Direct Connect to establish a dedicated network connection between your network and create a logical connection to public AWS resources, such as an Amazon virtual private gateway IPsec endpoint. This solution combines the AWS managed benefits of the VPN solution with low latency, increased bandwidth, more consistent benefits of the AWS Direct Connect solution, and an end-to-end, secure IPsec connection.</p><p><img src=\"https://media.tutorialsdojo.com/aws_direct_conneect_plus_vpn.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/aws_direct_conneect_plus_vpn.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>It costs a lot of money to establish a Direct Connect connection which you rarely use. For a more cost-effective solution, you can configure a backup VPN connection for failover with your AWS Direct Connect connection.</p><p>If you want a short-term or lower-cost solution, you might consider configuring a hardware VPN as a failover option for a Direct Connect connection. VPN connections are not designed to provide the same level of bandwidth available to most Direct Connect connections. Ensure that your use case or application can tolerate a lower bandwidth if you are configuring a VPN as a backup to a Direct Connect connection.</p><p>Hence, the correct answer is: <strong>Establish VPN tunnels from your on-premises data center to each of the 10 VPCs. Terminate each VPN tunnel connection at the virtual private gateway (VGW) of the respective VPC. Configure BGP for route management.</strong></p><p>The following options are incorrect:</p><p><strong>- Establish another 1 Gbps AWS Direct Connect connection using a public Virtual Interface (VIF). Prepare a VPN tunnel that will terminate on the virtual private gateway (VGW) of the respective VPC using the public VIF. Handle the failover to the VPN connection through the use of BGP.</strong></p><p><strong>- Establish another 1 Gbps AWS Direct Connect connection with corresponding private Virtual Interfaces (VIFs) to connect all of the 10 VPCs individually. Set up a Border Gateway Protocol (BGP) peering session for all of the VIFs.</strong></p><p>Establishing yet another 1 Gbps AWS Direct Connect connection is not a cost-effective solution. It is better to establish a VPN connection instead as a backup.</p><p>The option that says: <strong>Establishing a new point-to-point Multiprotocol Label Switching (MPLS) connection to all of your 10 VPCs and Configuring BGP to use this new connection with an active/passive routing</strong> is incorrect because you can't directly connect to your Multiprotocol Label Switching (MPLS) to AWS. To integrate your MPLS infrastructure, you need to set up a colocation with Direct Connect by placing the CGW in the same physical facility as the Direct Connect location, which will facilitate a local cross-connect between the CGW and AWS devices.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/vpn-connection-as-a-backup-to-aws-dx-connection-example.html\">https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/vpn-connection-as-a-backup-to-aws-dx-connection-example.html</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html</a></p><p><a href=\"https://aws.amazon.com/answers/networking/aws-network-connectivity-over-mpls/\">https://aws.amazon.com/answers/networking/aws-network-connectivity-over-mpls/</a></p><p><br></p><p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/vpn-connection-as-a-backup-to-aws-dx-connection-example.html",
      "https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html",
      "https://aws.amazon.com/answers/networking/aws-network-connectivity-over-mpls/",
      "https://tutorialsdojo.com/aws-direct-connect/?src=udemy"
    ]
  },
  {
    "id": 57,
    "question": "<p>A tech company plans to host a website using an Amazon S3 bucket. The solutions architect created a new S3 bucket called “www.tutorialsdojo.com\" in us-west-2 AWS region, enabled static website hosting, and uploaded the static web content files including the index.html file. The custom domain <code>www.tutorialsdojo.com</code> has been registered using Amazon Route 53 to be associated with the S3 bucket. The next day, a new Route 53 Alias record set was created which points to the S3 website endpoint: <code>http://www.tutorialsdojo.com.s3-website-us-west-2.amazonaws.com</code>. Upon testing, users cannot see any content on the bucket. Both the domains <code>tutorialsdojo.com</code> and <code>www.tutorialsdojo.com</code> do not work properly.</p><p>Which of the following is the MOST likely cause of this issue that the Architect should fix?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "The S3 bucket does not have public read access which blocks the website visitors from seeing the content.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "The site does not work because you have not set a value for the error.html file, which is a required step.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Route 53 is still propagating the domain name changes. Wait for another 12 hours and then try again.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The site will not work because the URL does not include a file name at the end. This means that you need to use this URL instead: <code>www.tutorialsdojo.com/index.html</code></p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>You can host a static website on <strong>Amazon Simple Storage Service (Amazon S3)</strong>. On a static website, individual webpages include static content. They might also contain client-side scripts. To host a static website, you configure an Amazon S3 bucket for website hosting, and then upload your website content to the bucket. This bucket must have public read access. It is intentional that everyone in the world will have read access to this bucket.</p><p><img src=\"https://media.tutorialsdojo.com/sap_route53_s3bucket.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_route53_s3bucket.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>When you configure an Amazon S3 bucket for website hosting, you must give the bucket the same name as the record that you want to use to route traffic to the bucket. For example, if you want to route traffic for <code>example.com</code> to an S3 bucket that is configured for website hosting, the name of the bucket must be <code>example.com</code>.</p><p>If you want to route traffic to an S3 bucket that is configured for website hosting but the name of the bucket doesn't appear in the <strong>Alias Target</strong> list in the <strong>Amazon Route 53</strong> console, check the following:</p><p>- The name of the bucket exactly matches the name of the record, such as <code>tutorialsdojo.com</code> or <code>www.tutorialsdojo.com</code>.</p><p>- The S3 bucket is correctly configured for website hosting.</p><p>In this scenario, the static S3 website does not work because the bucket does not have a public read access.</p><p>Therefore, the correct answer is: <strong>The S3 bucket does not have public read access which blocks the website visitors from seeing the content.</strong> This is the root cause why the static S3 website is inaccessible.</p><p>The option that says:<strong><em> </em>The site does not work because you have not set a value for the error.html file, which is a required step</strong> is incorrect as the error.html is not required and won't affect the availability of the static S3 website.</p><p>The option that says: <strong>The site will not work because the URL does not include a file name at the end. This means that you need to use this URL instead: www.tutorialsdojo.com/index.html</strong> is incorrect as it is not required to manually append the exact filename in S3.</p><p>The option that says: <strong>Route 53 is still propagating the domain name changes. Wait for another 12 hours and then try again</strong> is incorrect as the Route 53 domain name propagation does not take that long. Remember that Amazon Route 53 is designed to propagate updates you make to your DNS records to its worldwide network of authoritative DNS servers within 60 seconds under normal conditions.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-custom-domain-walkthrough.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-custom-domain-walkthrough.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-custom-domain-walkthrough.html",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 58,
    "question": "<p>A company develops Docker containers to host web applications on its on-premises data center. The company wants to migrate its workload to the cloud and use AWS Fargate. The solutions architect has created the necessary task definition and service for the Fargate cluster. For security requirements, the cluster is placed on a private subnet in the VPC that has no direct connection outside of the VPC. The following error is received when trying to launch the Fargate task:</p><p><code>CannotPullContainerError: API error (500): Get https://111122223333.dkr.ecr.us-east-1.amazonaws.com/v2/: net/http: request canceled while waiting for connection</code></p><p>Which of the following options should be able to fix this issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Update the AWS Fargate task definition and set the auto-assign public IP option to ENABLED. Create a gateway VPC endpoint for Amazon ECR. Update the route table to allow AWS Fargate to pull images on Amazon ECR via the endpoint.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Update the AWS Fargate task definition and set the auto-assign public IP option to DISABLED. Launch a NAT gateway on the public subnet of the VPC and update the route table of the private subnet to route requests to the Internet.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Update the AWS Fargate task definition and set the auto-assign public IP option to DISABLED. Launch a NAT gateway on the private subnet of the VPC and update the route table of the private subnet to route requests to the Internet.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>This is a limitation of the “<code>awsvpc</code>” network mode. Update the AWS Fargate definition to use the “<code>bridge</code>” network mode instead to allow connections to the Internet.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS Fargate</strong> is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.</p><p><strong>Fargate</strong> allocates the right amount of compute resources, eliminating the need to choose instances and scale cluster capacity. You only pay for the resources required to run your containers, so there is no over-provisioning and paying for additional servers. Fargate runs each task or pod in its own kernel providing the tasks and pods their own isolated compute environment.</p><p><img src=\"https://media.tutorialsdojo.com/sap_fargate_overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_fargate_overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The <code><strong>CannotPullContainer error (500)</strong></code> is caused by the <code>Connection timed out</code> when connecting to Amazon ECR. This indicates that when creating a task, the container image specified could not be retrieved.</p><p>When a Fargate task is launched, its elastic network interface requires a route to the Internet to pull container images. If you receive an error similar to the following when launching a task, it is because a route to the Internet does not exist:</p><p><code>CannotPullContainerError: API error (500): Get https://111122223333.dkr.ecr.us-east-1.amazonaws.com/v2/: net/http: request canceled while waiting for connection\"</code></p><p>To resolve this issue, you can:</p><p>- For tasks in public subnets, specify <strong>ENABLED</strong> for <strong>Auto-assign public IP</strong> when launching the task.</p><p>- For tasks in private subnets, specify <strong>DISABLED</strong> for <strong>Auto-assign public IP</strong> when launching the task, and configure a NAT gateway in your VPC to route requests to the Internet.</p><p>Therefore, the correct answer is: <strong>Update the AWS Fargate task definition and set the auto-assign public IP option to DISABLED. Launch a NAT gateway on the public subnet of the VPC and update the route table of the private subnet to route requests to the internet</strong>. The NAT gateway in the public subnet should have a public IP address and a route to the Intenet Gateway. The tasks in the private subnet will send Internet traffic to the NAT gateway to be able pull the images on Amazon Elastic Container Registry.</p><p>The option that says: <strong>Update the AWS Fargate task definition and set the auto-assign public IP option to ENABLED. Create a gateway VPC endpoint for Amazon ECR. Update the route table to allow AWS Fargate to pull images on Amazon ECR via the endpoint</strong> is incorrect. Since the Fargate tasks are on private subnet, you don't need to enable the auto-assign public IP option. Additionally, you should interface VPC endpoint, not gateway VPC endpoint.</p><p>The option that says: <strong>Update the AWS Fargate task definition and set the auto-assign public IP option to DISABLED. Launch a NAT gateway on the private subnet of the VPC and update the route table of the private subnet to route requests to the Internet</strong> is incorrect. The NAT gateway should be placed in a public subnet because it needs a Public IP address and a direct route to the Internet Gateway (IGW). If it is placed on a private subnet, it will have the same routing limitation as those resources in the private subnet.</p><p>The option that says: <strong>This is a limitation of the “</strong><code><strong>awsvpc</strong></code><strong>” network mode. Update the AWS Fargate definition to use the “</strong><code><strong>bridge</strong></code><strong>” network mode instead to allow connections to the Internet</strong> is incorrect. AWS Fargate only supports the \"<code>awsvpc</code>\" network mode. Each task is allocated its own elastic network interface (ENI) that is used for communication inside the VPC.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_cannot_pull_image.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_cannot_pull_image.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ecs-pull-container-api-error-ecr/\">https://aws.amazon.com/premiumsupport/knowledge-center/ecs-pull-container-api-error-ecr/</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html</a></p><p><br></p><p><strong>Check out this AWS Fargate Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-fargate/?src=udemy\">https://tutorialsdojo.com/aws-fargate/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_cannot_pull_image.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/ecs-pull-container-api-error-ecr/",
      "https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html",
      "https://tutorialsdojo.com/aws-fargate/?src=udemy"
    ]
  },
  {
    "id": 59,
    "question": "<p>A company plans to decommission its legacy web application that is hosted in AWS. It is composed of an Auto Scaling group of EC2 instances and an Application Load Balancer (ALB). The new application is built on a new framework. The solutions architect has been tasked to set up a new serverless architecture that is comprised of AWS Lambda, API Gateway, and DynamoDB. In addition, it is required to build a CI/CD pipeline to automate the build process and to support gradual deployments.</p><p>Which is the most suitable way to build, test, and deploy the new architecture in AWS?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use CloudFormation and OpsWorks for your build, deployment, and configuration management service.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the AWS Serverless Application Repository to organize related components, share configuration such as memory and timeouts between resources, and deploy all related resources together as a single, versioned entity.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Serverless Application Model (AWS SAM) and set up AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline to build a CI/CD pipeline.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up a CI/CD pipeline using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline to build the CI/CD pipeline then use AWS Systems Manager Automation to automate the build process and support gradual deployments.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>The <strong>AWS Serverless Application Model (AWS SAM)</strong> is an open-source framework that you can use to build serverless applications on AWS. It consists of the AWS SAM template specification that you use to define your serverless applications, and the AWS SAM command line interface (AWS SAM CLI) that you use to build, test, and deploy your serverless applications.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sam_cloudformation.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_sam_cloudformation.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Because AWS SAM is an extension of AWS CloudFormation, you get the reliable deployment capabilities of AWS CloudFormation. You can define resources by using AWS CloudFormation in your AWS SAM template. Also, you can use the full suite of resources, intrinsic functions, and other template features that are available in AWS CloudFormation.</p><p>You can use AWS SAM with a suite of AWS tools for building serverless applications. To build a deployment pipeline for your serverless applications, you can use CodeBuild, CodeDeploy, and CodePipeline. You can also use AWS CodeStar to get started with a project structure, code repository, and a CI/CD pipeline that's automatically configured for you. To deploy your serverless application, you can use the Jenkins plugin, and you can use Stackery.io's toolkit to build production-ready applications.</p><p>Therefore the correct answer is: <strong>Use AWS Serverless Application Model (AWS SAM) and set up AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline to build a CI/CD pipeline.</strong></p><p>The options that says: <strong>Use CloudFormation and OpsWorks for your build, deployment, and configuration management service</strong> is incorrect. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. It can't deploy AWS Lambda applications.</p><p>The options that says: <strong>Set up a CI/CD pipeline using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline to build the CI/CD pipeline then use AWS Systems Manager Automation to automate the build process and support gradual deployments</strong> is incorrect. Systems Manager Automation is designed to configure and manage instances with custom runbooks or pre-defined runbooks maintained by AWS, not for building and deploying serverless applications on AWS Lambda.</p><p>The options that says: <strong>Use the AWS Serverless Application Repository to organize related components, share configuration such as memory and timeouts between resources, and deploy all related resources together as a single, versioned entity</strong> is incorrect. AWS Serverless Application Repository is just a managed repository for serverless applications. This solution is incomplete as you will need other AWS tools to build and deploy your application.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html</a></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><br></p><p><strong>Check out this AWS Serverless Application Model Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-serverless-application-model-sam/?src=udemy\">https://tutorialsdojo.com/aws-serverless-application-model-sam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html",
      "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
      "https://tutorialsdojo.com/aws-serverless-application-model-sam/?src=udemy"
    ]
  },
  {
    "id": 60,
    "question": "<p>A small company has several AWS accounts that are used by multiple teams. To centralize DNS record keeping, the company has created a private hosted zone in Amazon Route 53 on the main Account A. The new application and database servers are hosted on a VPC in Account B. The CNAME record set <code>db.turotialsdojo.com</code> has been created for the Amazon RDS endpoint on the private hosted zone in Amazon Route 53. Upon deployment, the application on the Amazon EC2 instances failed to start. The application logs indicate that the database endpoint <code>db.turotialsdojo.com</code> is not resolvable. However, the solutions architect can confirm that the Route 53 entry is configured correctly.</p><p>Which of the following options is the recommended solution for this issue? (Select TWO.)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>On Account A, create an authorization to associate its private hosted zone to the new VPC in Account B.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>On Account B, associate the VPC to the private hosted zone in Account A. Delete the association authorization after the association is created.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>On Account B, create a new private hosted zone in Amazon Route 53. Associate this zone to the private hosted zone in Account A to allow replication between the AWS accounts.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create custom AMI for the Amazon EC2 instances that have an updated /etc/resolv.conf file containing the Amazon RDS endpoint to private IP address mapping.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Create a VPC peering between the Account A VPC and Account B VPC. Configure the Amazon EC2 instances on Account B to use the DNS resolver IPs in Account A to resolve the Amazon RDS endpoint.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>You can use the <strong>Amazon Route 53</strong> console to associate more VPCs with a private hosted zone if you created the hosted zone and the VPCs by using the same AWS account. Additionally, you can associate a VPC from one account with a private hosted zone in a different account.</p><p>If you want to associate VPCs that you created by using one account with a private hosted zone that you created by using a different account, you first must authorize the association. In addition, you can't use the AWS console either to authorize the association or associate the VPCs with the hosted zone.</p><p>To associate an Amazon VPC and a private hosted zone that you created with different AWS accounts, perform the following procedure:</p><ol><li><p>Using the account that created the hosted zone, authorize the association of the VPC with the private hosted zone by using one of the following methods:</p></li></ol><p>-AWS CLI – using the <code>create-vpc-association-authorization</code> in the AWS CLI</p><p>-AWS SDK or AWS Tools for Windows PowerShell</p><p>-Amazon Route 53 API – Using the <code>CreateVPCAssociationAuthorization</code> API</p><p>Note the following:</p><p>- If you want to associate multiple VPCs that you created with one account with a hosted zone that you created with a different account, you must submit one authorization request for each VPC.</p><p>- When you authorize the association, you must specify the hosted zone ID, so the private hosted zone must already exist.</p><p>- You can't use the Route 53 console either to authorize the association of a VPC with a private hosted zone or to make the association.</p><ol><li><p>Using the account that created the VPC, associate the VPC with the hosted zone. As with authorizing the association, you can use the AWS SDK, Tools for Windows PowerShell, the AWS CLI, or the Route 53 API.</p></li><li><p><em>Optional but recommended</em> – Delete the authorization to associate the VPC with the hosted zone. Deleting the authorization does not affect the association, it just prevents you from reassociating the VPC with the hosted zone in the future. If you want to reassociate the VPC with the hosted zone, you'll need to repeat steps 1 and 2 of this procedure.</p></li></ol><p><img src=\"https://media.tutorialsdojo.com/sap_route53_private_zone_share.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_route53_private_zone_share.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answers are:</p><p><strong>-On Account A, create an authorization to associate its private hosted zone to the new VPC in Account B.</strong></p><p><strong>-On Account B, associate the VPC to the private hosted zone in Account A. Delete the association authorization after the association is created.</strong></p><p>The option that says: <strong>Create a VPC peering between the Account A VPC and Account B VPC. Configure the Amazon EC2 instances on Account B to use the DNS resolver IPs in Account A to resolve the Amazon RDS endpoint</strong> is incorrect. This may be possible to configure, however, Route 53 in Account A has no knowledge of any RDS instance which are hosted on Account B. Therefore the EC2 instances still won't be able to resolve the DB endpoint.</p><p>The option that says: <strong>Create custom AMI for the Amazon EC2 instances that have an updated /etc/resolv.conf file containing the Amazon RDS endpoint to private IP address mapping</strong> is incorrect. Creating a static mapping of the RDS endpoint to a private IP address is not recommended. The internal private IPs of RDS instances may change over time.</p><p>The option that says: <strong>On Account B, create a new private hosted zone in Amazon Route 53. Associate this zone to the private hosted zone in Account A to allow replication between the AWS accounts</strong> is incorrect. This is not possible as you can't have replication between Route 53 in separate accounts.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/private-hosted-zone-different-account/\">https://aws.amazon.com/premiumsupport/knowledge-center/private-hosted-zone-different-account/</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html</a></p><p><br></p><p><strong>Check out the Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/private-hosted-zone-different-account/",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html",
      "https://tutorialsdojo.com/amazon-route-53/?src=udemy"
    ]
  },
  {
    "id": 61,
    "question": "<p>A private bank is hosting a secure web application that allows its agents to view highly sensitive information about the clients. The amount of traffic that the web app will receive is known and not expected to fluctuate. An SSL will be used as part of the application's data security. The chief information security officer (CISO) is concerned about the security of the SSL private key. The CISO wants to ensure that the key cannot be accidentally or intentionally moved outside the corporate environment. The solutions architect is also concerned that the application logs might contain some sensitive information. The EBS volumes used to store the data are already encrypted. In this scenario, the application logs must be stored securely and durably so that they can only be decrypted by authorized employees.</p><p>Which of the following is the most suitable and highly available architecture that can meet all of the requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Distribute traffic to a set of web servers using an Elastic Load Balancer. Use TCP load balancing for the load balancer and configure your web servers to retrieve the SSL private key from a private Amazon S3 bucket on boot. Use another private Amazon S3 bucket to store your web server logs using Amazon S3 server-side encryption.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Distribute traffic to a set of web servers using an Elastic Load Balancer that performs TCP load balancing. Use an AWS CloudHSM to perform the SSL transactions and deliver your application logs to a private Amazon S3 bucket using server-side encryption.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Distribute traffic to a set of web servers using an Elastic Load Balancer that performs TCP load balancing. Use CloudHSM deployed to two Availability Zones to perform the SSL transactions and deliver your application logs to a private Amazon S3 bucket using server-side encryption.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Distribute traffic to a set of web servers using an Elastic Load Balancer. To secure the SSL private key, upload the key to the load balancer and configure the load balancer to offload the SSL traffic. Lastly, write your application logs to an instance store volume that has been encrypted using a randomly generated AES key.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS CloudHSM</strong> is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud. With CloudHSM, you can manage your own encryption keys and automate time-consuming administrative tasks for you, such as hardware provisioning, software patching, high-availability, and backups.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudhsm_s3_logs.PNG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudhsm_s3_logs.PNG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The correct answer is the option that says: <strong>Distribute traffic to a set of web servers using an Elastic Load Balancer that performs TCP load balancing. Use CloudHSM deployed to two Availability Zones to perform the SSL transactions and deliver your application logs to a private Amazon S3 bucket using server-side encryption. </strong>It uses CloudHSM for performing the SSL transaction without requiring any additional way of storing or managing the SSL private key. This is the most secure way of ensuring that the key will not be moved outside of the AWS environment. Also, it uses the highly available and durable S3 service for storing the logs. Take note that this option says \"server-side encryption\" and not \"Amazon S3-Managed Encryption Keys\", which are two different things.</p><p>The option that says: <strong>Distribute traffic to a set of web servers using an Elastic Load Balancer. Use TCP load balancing for the load balancer and configure your web servers to retrieve the SSL private key from a private Amazon S3 bucket on boot. Use another private Amazon S3 bucket to store your web server logs using Amazon S3 server-side encryption</strong> is incorrect because it does not use a secure way of managing the SSL private key for the SSL transaction.</p><p>The option that says: <strong>Distribute traffic to a set of web servers using an Elastic Load Balancer. To secure the SSL private key, upload the key to the load balancer and configure the load balancer to offload the SSL traffic. Lastly, write your application logs to an instance store volume that has been encrypted using a randomly generated AES key</strong> is incorrect. The application logs are written to an ephemeral volume, which means that the data will be lost when the EC2 instance is terminated. Hence, this solution is neither durable nor secure.</p><p>The option that says: <strong>Distribute traffic to a set of web servers using an Elastic Load Balancer that performs TCP load balancing. Use an AWS CloudHSM to perform the SSL transactions and deliver your application logs to a private Amazon S3 bucket using server-side encryption</strong> is incorrect. Although it is almost similar to the correct option, the architecture did not explicitly say that the CloudHSM is deployed to multiple Availability Zones, which means that this architecture is not highly available compared with the correct option.</p><p>We deliberately designed the question to have this sort of ambiguity to make it more challenging as we know how hard the actual AWS SA Professional exam is. So in this case, the correct option does not say if it is using SSE-S3 or SSE-C. This is one of the trick part of this question. If it is using SSE-S3, then the current correct answer would definitely be wrong because AWS will be managing the AES-256 key.</p><p>However, if it uses SSE-C, then the AES-256 key would be managed by the authorized government employees only. Therefore, it is implied that the correct option is using SSE-C, instead of SSE-S3.</p><p>Remember that in SSE-C, when you upload an object, Amazon S3 uses the encryption key you provide to apply AES-256 encryption to your data and removes the encryption key from memory. Amazon S3 does not store the encryption key you provide. Instead, they store a randomly salted HMAC value of the encryption key in order to validate future requests. The salted HMAC value cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object. That means, if you lose the encryption key, you lose the object.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudhsm/\">https://aws.amazon.com/cloudhsm/</a></p><p><a href=\"https://docs.aws.amazon.com/cloudhsm/latest/userguide/ssl-offload.html\">https://docs.aws.amazon.com/cloudhsm/latest/userguide/ssl-offload.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/cloudhsm/",
      "https://docs.aws.amazon.com/cloudhsm/latest/userguide/ssl-offload.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 62,
    "question": "<p>A company processes several petabytes of images submitted by users on their photo hosting site every month. Each month, the images are processed in its on-premises data center by a High-Performance Computing (HPC) cluster with a capacity of 5,000 cores and 10 petabytes of data. Processing a month’s worth of images by thousands of jobs running in parallel takes about a week and the processed images are stored on a network file server, which also backups the data to a disaster recovery site.</p><p>The current data center is nearing its capacity so the users are forced to spread the jobs within the course of the month. This is not ideal for the requirement of the jobs, so the Solutions Architect was tasked to design a scalable solution that can exceed the current capacity with the least amount of management overhead while maintaining the current level of durability.</p><p>Which of the following solutions will meet the company's requirements while being cost-effective?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Using a combination of On-demand and Reserved Instances as Task Nodes, create an EMR cluster that will use Spark to pull the raw data from an Amazon S3 bucket. List the jobs that need to be processed by the EMR cluster on a DynamoDB table. Store the processed images on a separate Amazon S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Utilize AWS Batch with Managed Compute Environments to create a fleet using Spot Instances. Store the raw data on an Amazon S3 bucket. Create jobs on AWS Batch Job Queues that will pull objects from the Amazon S3 bucket and temporarily store them to the EC2 EBS volumes for processing. Send the processed images back to another Amazon S3 bucket.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Package the executable file for the job in a Docker image stored on Amazon Elastic Container Registry (Amazon ECR). Run the Docker images on Amazon Elastic Kubernetes Service (Amazon EKS). Auto Scaling can be handled automatically by EKS. Store the raw data temporarily on Amazon EBS SC1 volumes and then send the images to an Amazon S3 bucket after processing.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon SQS queue and submit the list of jobs to be processed. Create an Auto Scaling Group of Amazon EC2 Spot Instances that will process the jobs from the SQS queue. Share the raw data across all the instances using Amazon EFS. Store the processed images in an Amazon S3 bucket for long term storage.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Batch</strong> enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. With AWS Batch, there is no need to install and manage batch computing software or server clusters that you use to run your jobs, allowing you to focus on analyzing results and solving problems.</p><p>There is no additional charge for AWS Batch. You only pay for the AWS resources (e.g. EC2 instances or Fargate jobs) you create to store and run your batch jobs. From the AWS Batch use cases page, we can see an example similar to this scenario wherein Digital Media and Entertainment companies require highly scalable batch computing resources to enable accelerated and automated processing of data as well as the compilation and processing of files, graphics, and visual effects for high-resolution video content. Use AWS Batch to accelerate content creation, dynamically scale media packaging, and automate asynchronous media supply chain workflows.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_batch.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_batch.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In AWS Batch, job queues are mapped to one or more compute environments. Compute environments contain the Amazon ECS container instances that are used to run containerized batch jobs. A specific compute environment can also be mapped to one or many job queues. Within a job queue, the associated compute environments each have an order that's used by the scheduler to determine where jobs that are ready to be run should run.</p><p>Therefore, the correct answer is:<strong> Utilize AWS Batch with Managed Compute Environments to create a fleet using Spot Instances. Store the raw data on an Amazon S3 bucket. Create jobs on AWS Batch Job Queues that will pull objects from the Amazon S3 bucket and temporarily store them to the EC2 EBS volumes for processing. Send the processed images back to another Amazon S3 bucket.</strong></p><p>The option that says: <strong>Package the executable file for the job in a Docker image stored on Amazon Elastic Container Registry (Amazon ECR). Run the Docker images on Amazon Elastic Kubernetes Service (Amazon EKS). Auto Scaling can be handled automatically by EKS. Store the raw data temporarily on Amazon EBS SC1 volumes and then send the images to an Amazon S3 bucket after processing</strong> is incorrect. Although this is possible, converting the application to a container and deploying it to an EKS cluster will entail a lot of changes for the application. Additionally, since you can’t quickly increase/decrease SC1 EBS volumes, creating a large volume to handle petabytes of data is not cost-effective.</p><p>The option that says: <strong>Using a combination of On-demand and Reserved Instances as Task Nodes, create an EMR cluster that will use Apache Spark to pull the raw data from an Amazon S3 bucket. List the jobs that need to be processed by the EMR cluster on a DynamoDB table. Store the processed images on a separate Amazon S3 bucket</strong> is incorrect as managing the EMR cluster and Apache Spark adds significant management overhead for this solution. There is also an additional cost for the EC2 instances that are constantly running even if there are only a few jobs that need to be run.</p><p>The option that says: <strong>Create an Amazon SQS queue and submit the list of jobs to be processed. Create an Auto Scaling Group of Amazon EC2 Spot Instances that will process the jobs from the SQS queue. Share the raw data across all the instances using Amazon EFS. Store the processed images in an Amazon S3 bucket for long term storage</strong> is incorrect as Amazon EFS is more expensive than storing the raw data on S3 buckets. This is also not efficient as listing the jobs on SQS Queue can cause some to be processed twice, depending on the state of your Spot instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html\">https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html</a></p><p><a href=\"https://aws.amazon.com/batch/use-cases/\">https://aws.amazon.com/batch/use-cases/</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/\">https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/</a></p><p><br></p><p><strong>Check out this AWS Batch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-batch/?src=udemy\">https://tutorialsdojo.com/aws-batch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html",
      "https://aws.amazon.com/batch/use-cases/",
      "https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/",
      "https://tutorialsdojo.com/aws-batch/?src=udemy"
    ]
  },
  {
    "id": 63,
    "question": "<p>A multi-national tech company has multiple VPCs assigned for each of its IT departments. VPC peering has been set up whenever intercommunication is needed between the VPCs. The solutions architect has been instructed to launch a new central database server that can be accessed by the other VPCs of the company using the <code>database.tutorialsdojo.com</code> domain name. This server should only be resolvable and accessible within the associated VPCs since only internal applications will be using the database.</p><p>Which of the following options should the solutions architect implement to meet the above requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a private hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create an A record with a value of database.tutorialsdojo.com which maps to the IP address of the EC2 instance of your database server. Modify the <code>enableDnsHostNames</code> attribute of your VPC to <code>true</code> and the <code>enableDnsSupport</code> attribute to <code>true</code></p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up a public hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create an A record with a value of database.tutorialsdojo.com which maps to the IP address of the EC2 instance of your database server. Modify the <code>enableDnsHostNames</code> attribute of your VPC to <code>true</code> and the <code>enableDnsSupport</code> attribute to <code>true</code></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up a public hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create a CNAME record with a value of database.tutorialsdojo.com which maps to the IP address of the EC2 instance of your database server. Modify the <code>enableDnsHostNames</code> attribute of your VPC to <code>false</code> and the <code>enableDnsSupport</code> attribute to <code>false</code></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a private hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create an A record with a value of database.tutorialsdojo.com which maps to the Elastic IP address of the EC2 instance of your database server. Modify the <code>enableDnsHostNames</code> attribute of your VPC to <code>true</code> and the <code>enableDnsSupport</code> attribute to <code>false</code></p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>In AWS, a hosted zone is a container for records, and records contain information about how you want to route traffic for a specific domain, such as tutorialsdojo.com, and its subdomains (portal.tutorialsdojo.com, database.tutorialsdojo.com). A hosted zone and the corresponding domain have the same name. There are two types of hosted zones:</p><p><em>- </em><strong><em>Public hosted zones</em></strong> contain records that specify how you want to route traffic on the internet.</p><p><em>- </em><strong><em>Private hosted zones</em></strong> contain records that specify how you want to route traffic in an Amazon VPC</p><p><img src=\"https://media.tutorialsdojo.com/sap_route53_vpc_association.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_route53_vpc_association.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>A <em>private hosted zone</em> is a container that holds information about how you want Amazon Route 53 to respond to DNS queries for a domain and its subdomains within one or more VPCs that you create with the Amazon VPC service. Your VPC has attributes that determine whether your EC2 instance receives public DNS hostnames, and whether DNS resolution through the Amazon DNS server is supported.</p><p><code><strong>enableDnsHostnames</strong> - </code>Indicates whether the instances launched in the VPC get public DNS hostnames. If this attribute is <code>true</code>, instances in the VPC get public DNS hostnames, but only if the <code>enableDnsSupport</code> attribute is also set to <code>true</code>.</p><p><code><strong>enableDnsSupport</strong> - </code>Indicates whether the DNS resolution is supported for the VPC. If this attribute is <code>false</code>, the Amazon-provided DNS server in the VPC that resolves public DNS hostnames to IP addresses is not enabled. If this attribute is <code>true</code>, queries to the Amazon provided DNS server at the 169.254.169.253 IP address, or the reserved IP address at the base of the VPC IPv4 network range plus two ( *.*.*.2 ) will succeed.</p><p>Hence, the option that says: <strong>Set up a private hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create an A record with a value of database.tutorialsdojo.com which maps to the IP address of the EC2 instance of your database server. Modify the </strong><code><strong>enableDnsHostNames</strong></code><strong> attribute of your VPC to </strong><code><strong>true</strong></code><strong> and the </strong><code><strong>enableDnsSupport</strong></code><strong> attribute to </strong><code><strong>true</strong></code> is the correct answer.</p><p>The options that say:</p><p><strong>1. Set up a public hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create an A record with a value of database.tutorialsdojo.com which maps to the IP address of the EC2 instance of your database server. Modify the </strong><code><strong>enableDnsHostNames</strong></code><strong> attribute of your VPC to </strong><code><strong>true</strong></code><strong> and the </strong><code><strong>enableDnsSupport</strong></code><strong> attribute to </strong><code><strong>true</strong></code></p><p><strong>2. Set up a public hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create a CNAME record with a value of database.tutorialsdojo.com which maps to the IP address of the EC2 instance of your database server. Modify the </strong><code><strong>enableDnsHostNames</strong></code><strong> attribute of your VPC to </strong><code><strong>false</strong></code><strong> and the </strong><code><strong>enableDnsSupport</strong></code><strong> attribute to </strong><code><strong>false</strong></code></p><p>are incorrect because you have to create a <strong><em>private</em></strong> hosted zone and not a public one, since the database server will only be accessed by the associated VPCs and not publicly over the Internet. In addition, you have to create an A record for your database server and then set both the <code><strong>enableDnsHostNames</strong></code> and <code><strong>enableDnsSupport</strong></code> attributes to <code>true</code>.</p><p>The option that says: <strong>Set up a private hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create an A record with a value of database.tutorialsdojo.com which maps to the Elastic IP address of the EC2 instance of your database server. Modify the </strong><code><strong>enableDnsHostNames</strong></code><strong> attribute of your VPC to </strong><code><strong>true</strong></code><strong> and the </strong><code><strong>enableDnsSupport</strong></code><strong> attribute to </strong><code><strong>false</strong></code> is incorrect. Even though it mentions the use of a private hosted zone, the configuration is incorrect since it is required to set both the <code><strong>enableDnsHostNames</strong></code> and <code><strong>enableDnsSupport</strong></code> attributes of your VPC to <code>true</code>. In addition, an Elastic IP address is a public IPv4 address, which is reachable from the Internet and hence, it violates the requirement that the database server should only be accessible within your associated VPCs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#vpc-dns-support\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#vpc-dns-support</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zones-private.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zones-private.html</a></p><p><br></p><p><strong>Check out these Amazon VPC and Route 53 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#vpc-dns-support",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zones-private.html",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy",
      "https://tutorialsdojo.com/amazon-route-53/?src=udemy"
    ]
  },
  {
    "id": 64,
    "question": "<p>A leading financial company is planning to launch its MERN (MongoDB, Express, React, Node.js) application with an Amazon RDS MariaDB database to serve its clients worldwide. The application will run on both on-premises servers as well as Reserved EC2 instances. To comply with the company's strict security policy, the database credentials must be encrypted both at rest and in transit. These credentials will be used by the application servers to connect to the database. The Solutions Architect is tasked to manage all of the aspects of the application architecture and production deployment. </p><p>How should the Architect automate the deployment process of the application in the MOST secure manner?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Upload the database credentials with a Secure String data type in AWS Systems Manager Parameter Store. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Associate this role<strong> </strong>to all on-premises servers and EC2 instances. Use Elastic Beanstalk to host and manage the application on both on-premises servers and EC2 instances. Deploy the succeeding application revisions to AWS and on-premises servers using Elastic Beanstalk.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Upload the database credentials with a Secure String data type in AWS Systems Manager Parameter Store. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Attach this IAM policy to the instance profile for CodeDeploy-managed EC2 instances. Associate the same policy as well to the on-premises instances. Using AWS CodeDeploy, launch the application packages to the Amazon EC2 instances and on-premises servers.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Upload the database credentials with key rotation in AWS Secrets Manager. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Associate this role to all on-premises servers and EC2 instances. Use Elastic Beanstalk to host and manage the application on both on-premises servers and EC2 instances. Deploy the succeeding application revisions to AWS and on-premises servers using Elastic Beanstalk.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Upload the database credentials with a Secure String data type in AWS Systems Manager Parameter Store. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Associate this role to the EC2 instances. Create an IAM Service Role that will be associated with the on-premises servers. Deploy the application packages to the EC2 instances and on-premises servers using AWS CodeDeploy.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Systems Manager Parameter Store</strong> provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter. Highly scalable, available, and durable, Parameter Store is backed by the AWS Cloud.</p><p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants <code>AssumeRole</code> trust to the Systems Manager service. You only need to create the service role for a hybrid environment once for each AWS account.</p><p>Users in your company or organization who will use Systems Manager on your hybrid machines must be granted permission in IAM to call the SSM API.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_parameter_store.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ssm_parameter_store.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Service role</strong>: A service role is an AWS Identity and Access Management (IAM) that grants permissions to an AWS service so that the service can access AWS resources. Only a few Systems Manager scenarios require a service role. When you create a service role for Systems Manager, you choose the permissions to grant in order for it to access or interact with other AWS resources.</p><p><strong>Service-linked role</strong>: A service-linked role is predefined by Systems Manager and includes all the permissions that the service requires to call other AWS services on your behalf.</p><p>If you plan to use Systems Manager to manage on-premises servers and virtual machines (VMs) in what is called a <strong>hybrid environment</strong>, you must create an IAM role for those resources to communicate with the Systems Manager service.</p><p>Hence, the correct answer is: <strong>Upload the database credentials with a Secure String data type in AWS Systems Manager Parameter Store. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Associate this role to the EC2 instances. Create an IAM Service Role that will be associated with the on-premises servers. Deploy the application packages to the EC2 instances and on-premises servers using AWS CodeDeploy.</strong></p><p>The option that says: <strong>Upload the database credentials with a Secure String data type in AWS Systems Manager Parameter Store. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Associate this role to all on-premises servers and EC2 instances. Use Elastic Beanstalk to host and manage the application on both on-premises servers and EC2 instances. Deploy the succeeding application revisions to AWS and on-premises servers using Elastic Beanstalk</strong> is incorrect. You can't deploy an application to your on-premises servers using Elastic Beanstalk. This is only applicable to your Amazon EC2 instances.</p><p>The option that says: <strong>Upload the database credentials with a Secure String data type in AWS Systems Manager Parameter Store. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Attach this IAM policy to the instance profile for CodeDeploy-managed EC2 instances. Associate the same policy as well to the on-premises instances. Using AWS CodeDeploy, launch the application packages to the Amazon EC2 instances and on-premises servers</strong> is incorrect. You have to use an IAM Role and not an IAM Policy to grant access to AWS Systems Manager Parameter Store.</p><p>The option that says: <strong>Upload the database credentials with key rotation in AWS Secrets Manager. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Associate this role to all on-premises servers and EC2 instances. Use Elastic Beanstalk to host and manage the application on both on-premises servers and EC2 instances. Deploy the succeeding application revisions to AWS and on-premises servers using Elastic Beanstalk</strong> is incorrect. Although you can store the database credentials to AWS Secrets Manager, you still can't deploy an application to your on-premises servers using Elastic Beanstalk.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-service-role.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-service-role.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-service-role.html",
      "https://tutorialsdojo.com/aws-systems-manager/?src=udemy"
    ]
  },
  {
    "id": 65,
    "question": "<p>An accounting firm hosts a mix of Windows and Linux Amazon EC2 instances in its AWS account. The solutions architect has been tasked to conduct a monthly performance check on all production instances. There are more than 200 On-Demand EC2 instances running in their production environment and it is required to ensure that each instance has a logging feature that collects various system details such as memory usage, disk space, and other metrics. The system logs will be analyzed using AWS Analytics tools and the results will be stored in an S3 bucket.</p><p>Which of the following is the most efficient way to collect and analyze logs from the instances with minimal effort?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up and configure a unified CloudWatch Logs agent in each On-Demand EC2 instance which will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up and install AWS Inspector Agent on each On-Demand EC2 instance which will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up and install the AWS Systems Manager Agent (SSM Agent) on each On-Demand EC2 instance which will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable the Traffic Mirroring feature and install AWS CDK on each On-Demand EC2 instance. Create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Set up CloudWatch detailed monitoring and use CloudWatch Logs Insights to analyze the log data of all instances.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>To collect logs from your Amazon EC2 instances and on-premises servers into CloudWatch Logs, AWS offers both a new unified CloudWatch agent, and an older CloudWatch Logs agent. It is recommended to use the unified CloudWatch agent which has the following advantages:</p><p>- You can collect both logs and advanced metrics with the installation and configuration of just one agent.</p><p>- The unified agent enables the collection of logs from servers running Windows Server.</p><p>- If you are using the agent to collect CloudWatch metrics, the unified agent also enables the collection of additional system metrics, for in-guest visibility.</p><p>- The unified agent provides better performance.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_cloudwatch.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_cloudwatch.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CloudWatch Logs Insights</strong> enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you quickly and effectively respond to operational issues. If an issue occurs, you can use CloudWatch Logs Insights to identify potential causes and validate deployed fixes.</p><p>CloudWatch Logs Insights includes a purpose-built query language with a few simple but powerful commands. CloudWatch Logs Insights provides sample queries, command descriptions, query autocompletion, and log field discovery to help you get started quickly. Sample queries are included for several types of AWS service logs.</p><p>Therefore, the correct answer is: <strong>Set up and configure a unified CloudWatch Logs agent in each On-Demand EC2 instance which will automatically collect and push data to CloudWatch Logs, then analyze the log data with CloudWatch Logs Insights.</strong></p><p>The option that says:<strong> Enable the Traffic Mirroring feature and install AWS CDK on each On-Demand EC2 instance. Create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Set up CloudWatch detailed monitoring and use CloudWatch Logs Insights to analyze the log data of all instances</strong> is incorrect. Although this is a valid solution, this entails a lot of effort to implement as you have to allocate time to install the AWS CDK to each instance and develop a custom monitoring solution. Traffic Mirroring is simply an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of Amazon EC2 instances. As per the scenario, you are specifically looking for a solution that can be implemented with minimal effort. In addition, it is unnecessary and not cost-efficient to enable detailed monitoring in CloudWatch in order to meet the requirements since this can be done using CloudWatch Logs.</p><p>The option that says:<strong> Setting up and installing the AWS Systems Manager Agent (SSM Agent) on each On-Demand EC2 instance which will automatically collect and push data to CloudWatch Logs, then analyzing the log data with CloudWatch Logs Insights</strong> is incorrect. Although this is also a valid solution, it is more efficient to use a CloudWatch agent than an SSM agent. Manually connecting to an instance to view log files and troubleshoot an issue with SSM Agent is time-consuming hence, for more efficient instance monitoring, you can use the CloudWatch Agent instead to send the log data to Amazon CloudWatch Logs.</p><p>The option that says:<strong> Setting up and installing AWS Inspector Agent on each On-Demand EC2 instance which will collect and push data to CloudWatch Logs periodically, then setting up a CloudWatch dashboard to properly analyze the log data of all instances</strong> is incorrect. AWS Inspector is simply a security assessments service that only helps you in checking for unintended network accessibility of your EC2 instances and for vulnerabilities on those EC2 instances. Furthermore, setting up an Amazon CloudWatch dashboard is not suitable since it's primarily used for scenarios where you have to monitor your resources in a single view, even those resources that are spread across different AWS Regions. It is better to use CloudWatch Logs Insights instead since it enables you to interactively search and analyze your log data.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>CloudWatch Agent vs SSM Agent vs Custom Daemon Scripts</strong></p><p><a href=\"https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/?src=udemy\">https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html",
      "https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy",
      "https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/?src=udemy"
    ]
  },
  {
    "id": 66,
    "question": "<p>A data analytics startup has been chosen to develop a data analytics system that will track all statistics in the Fédération Internationale de Football Association (FIFA) World Cup, which will also be used by other 3rd-party analytics sites. The system will record, store and provide statistical data reports about the top scorers, goal scores for each team, average goals, average passes, average yellow/red cards per match, and many other details. FIFA fans all over the world will frequently access the statistics reports every day and thus, it should be durably stored, highly available, and highly scalable. In addition, the data analytics system will allow the users to vote for the best male and female FIFA player as well as the best male and female coach. Due to the popularity of the FIFA World Cup event, it is projected that there will be over 10 million queries on game day and could spike to 30 million queries over the course of time.</p><p>Which of the following is the most cost-effective solution that will meet these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>1. Launch a MySQL database in Multi-AZ RDS deployments configuration with Read Replicas.</p><p>\n2. Generate the FIFA reports by querying the Read Replica.</p><p>\n3. Configure a daily job that performs a daily table cleanup.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>1. Generate the FIFA reports from MySQL database in Multi-AZ RDS deployments configuration with Read Replicas.<br><br> 2. Set up a batch job that puts reports in an S3 bucket.</p><p>\n3. Launch a CloudFront distribution to cache the content with a TTL set to expire objects daily.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>1. Launch a MySQL database in Multi-AZ RDS deployments configuration. </p><p>\n2. Configure the application to generate reports from ElastiCache to improve the read performance of the system. </p><p>\n3. Utilize the default expire parameter for items in ElastiCache.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>1. Launch a Multi-AZ MySQL RDS instance. </p><p>\n2. Query the RDS instance and store the results in a DynamoDB table. </p><p>\n3. Generate reports from DynamoDB table.<br><br>4. Delete the old DynamoDB tables every day.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In this scenario, you are required to have the following:</p><p>A durable storage for the generated reports.</p><p>A database that is highly available and can scale to handle millions of queries.</p><p>A Content Delivery Network that can distribute the report files to users all over the world.</p><p><strong>Amazon S3</strong> is object storage built to store and retrieve any amount of data from anywhere. It’s a simple storage service that offers industry leading durability, availability, performance, security, and virtually unlimited scalability at very low costs.</p><p><strong>Amazon RDS</strong> provides high availability and failover support for DB instances using Multi-AZ deployments. In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups.</p><p>Amazon RDS uses the MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL DB engines' built-in replication functionality to create a special type of DB instance called a <strong>read replica</strong> from a source DB instance. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica.</p><p><img src=\"https://media.tutorialsdojo.com/sap_RDSreadreplica_async.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_RDSreadreplica_async.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Amazon CloudFront</strong> is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.</p><p>Hence, the following option is the best solution that satisfies all of these requirements:</p><p><strong>1. Generate the FIFA reports from MySQL database in Multi-AZ RDS deployments configuration with Read Replicas.</strong></p><p><strong>2. Set up a batch job that puts reports in an S3 bucket.</strong></p><p><strong>3. Launch a CloudFront distribution to cache the content with a TTL set to expire objects daily.</strong></p><p>In the above, S3 provides durable storage; Multi-AZ RDS with Read Replicas provide a scalable and highly available database and CloudFront provides the CDN.</p><p>The following option is incorrect:</p><p><strong>1. Launch a MySQL database in Multi-AZ RDS deployments configuration with Read Replicas.</strong></p><p><strong>2. Generate the FIFA reports by querying the Read Replica.</strong></p><p><strong>3. Configure a daily job that performs a daily table cleanup.</strong></p><p>Although the database is scalable and highly available, it neither has any durable data storage nor a CDN.</p><p>The following option is incorrect:</p><p><strong>1. Launch a MySQL database in Multi-AZ RDS deployments configuration.</strong></p><p><strong>2. Configure the application to generate reports from ElastiCache to improve the read performance of the system.</strong></p><p><strong>3. Utilize the default expire parameter for items in ElastiCache.</strong></p><p>Although this option handles and provides a better read capability for the system, it is still lacking a durable storage and a CDN.</p><p>The following option is incorrect:</p><p><strong>1. Launch a Multi-AZ MySQL RDS instance.</strong></p><p><strong>2. Query the RDS instance and store the results in a DynamoDB table.</strong></p><p><strong>3. Generate reports from DynamoDB table.</strong></p><p><strong>4. Delete the old DynamoDB tables every day.</strong></p><p>The above is not a cost-effective solution to maintain both RDS and a DynamoDB instance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/details/multi-az/\">https://aws.amazon.com/rds/details/multi-az/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/rds/details/multi-az/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 67,
    "question": "<p>A company has a hybrid set up for its mobile application. The on-premises data center hosts a 3TB MySQL database server that handles the write-intensive requests from the application. The on-premises network is connected to the AWS VPC with a VPN. On AWS, the serverless application runs on AWS Lambda and API Gateway with an Amazon DynamoDB table used for saving user preferences. The application scales well as more users are using the mobile app. The user traffic is unpredictable but there is an average increase of about 20% each month. A few months into operation, the company noticed the exponential increase of costs for AWS Lambda. The Solutions Architect noticed that the Lambda execution time averages 4.5 minutes and most of that is wait time due to latency when calling the on-premises data MySQL server.</p><p>Which of the following solutions should the Solutions Architect implement to reduce the overall cost?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>1. Provision an AWS Direct Connect connection from the on-premises data<br>&nbsp; &nbsp; center to Amazon VPC instead of a VPN to significantly reduce the<br>&nbsp; &nbsp; network latency to the MySQL server.</p><p>2. Create a CloudFront distribution with the API Gateway as the origin to<br>&nbsp; &nbsp; cache the API responses and reduce the Lambda invocations.</p><p>3. Convert the Lambda functions to run them on Amazon EC2 Reserved<br>&nbsp; &nbsp; Instances. Use Auto Scaling on peak time with a combination of Spot<br>&nbsp; &nbsp; instances to further reduce costs.</p><p>4. Configure Auto Scaling on Amazon DynamoDB to automatically adjust<br>&nbsp; &nbsp; the capacity with user traffic.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>1. Migrate the on-premises MySQL database server to Amazon RDS for MySQL. Enable Multi-AZ to ensure high availability.</p><p>2. Create a CloudFront distribution with the API Gateway as the origin to cache the API responses and reduce the Lambda invocations.</p><p>3. Gradually lower the timeout and memory properties of the Lambda functions without increasing the execution time.</p><p>4. Configure Auto Scaling on Amazon DynamoDB to automatically adjust the capacity with user traffic and enable DynamoDB Accelerator to cache frequently accessed records.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>1. Provision an AWS Direct Connect connection from the on-premises data center to Amazon VPC instead of a VPN to significantly reduce the network latency to the MySQL server.</p><p>2. Configure caching on the mobile application to reduce the overall AWS Lambda function calls.</p><p>3. Gradually lower the timeout and memory properties of the Lambda functions without increasing the execution time.</p><p>4. Add an Amazon Elasticache cluster in front of DynamoDB to cache the frequently accessed records.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>1. Migrate the on-premises MySQL database server to Amazon RDS for MySQL. Enable Multi-AZ to ensure high availability.</p><p>2. Configure API caching on Amazon API Gateway to reduce the overall number of invocations to the Lambda functions.</p><p>3. Gradually lower the timeout and memory properties of the Lambda functions without increasing the execution time.</p><p>4. Configure Auto Scaling on Amazon DynamoDB to automatically adjust the capacity based on user traffic.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon API Gateway</strong> is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud. As an API Gateway API developer, you can create APIs for use in your own client applications.</p><p>You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.</p><p>When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.</p><p>With <strong>AWS Lambda</strong>, you pay only for what you use. You are charged based on the number of requests for your functions and the duration, the time it takes for your code to execute. Lambda counts a request each time it starts executing in response to an event notification or invoke call, including test invokes from the console.</p><p>Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 1ms*. The price depends on the amount of memory you allocate to your function.</p><p><strong>Auto Scaling for DynamoDB</strong> helps automate capacity management for your tables and global secondary indexes. You simply specify the desired target utilization and provide upper and lower bounds for read and write capacity. DynamoDB will then monitor throughput consumption using Amazon CloudWatch alarms and then will adjust provisioned capacity up or down as needed.</p><p>Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you don't pay for unused provisioned capacity.</p><p>Therefore, the following option is correct:</p><p><strong>- Migrate the on-premises MySQL database server to Amazon RDS for MySQL. Enable Multi-AZ to ensure high availability.</strong></p><p><strong>- Configure API caching on Amazon API Gateway to reduce the overall number of invocations to the Lambda functions.</strong></p><p><strong>- Gradually lower the timeout and memory properties of the Lambda functions without increasing the execution time.</strong></p><p><strong>- Configure Auto Scaling on Amazon DynamoDB to automatically adjust the capacity based on user traffic.</strong></p><p>Migrating the on-premises MySQL server to Amazon RDS provides the best latency for the Lambda functions which will significantly reduce the cost for execution time. API Gateway can cache the API request to reduce the Lambda invocation which can reduce the cost further. Auto Scaling for DynamoDB also reduces the cost by provisioning capacity depending on the current user traffic.</p><p><img src=\"https://media.tutorialsdojo.com/sap_apigateway_lambda_dynamodb.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_apigateway_lambda_dynamodb.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The following option is incorrect:</p><p><strong>- Provision an AWS Direct Connect connection from the on-premises data center to Amazon VPC instead of a VPN to significantly reduce the network latency to the MySQL server.</strong></p><p><strong>- Configure caching on the mobile application to reduce the overall AWS Lambda function calls.</strong></p><p><strong>- Gradually lower the timeout and memory properties of the Lambda functions without increasing the execution time.</strong></p><p><strong>- Add an Amazon Elasticache cluster in front of DynamoDB to cache the frequently accessed records.</strong></p><p>Although the Direct Connect connection can reduce the network latency compared to a VPN connection, provisioning a Direct Connection just for a single application is not economical. Having the MySQL server hosted in AWS offers even far better network latency. Provisioning an Elasticache cluster also increases the cost. Caching the API requests should be done on the API Gateway, not on the mobile app itself.</p><p>The following option is incorrect:</p><p><strong>- Provision an AWS Direct Connect connection from the on-premises data center to Amazon VPC instead of a VPN to significantly reduce the network latency to the MySQL server.</strong></p><p><strong>- Create a CloudFront distribution with the API Gateway as the origin to cache the API responses and reduce the Lambda invocations.</strong></p><p><strong>- Convert the Lambda functions to run them on Amazon EC2 Reserved Instances. Use Auto Scaling on peak time with a combination of Spot instances to further reduce costs.</strong></p><p><strong>- Configure Auto Scaling on Amazon DynamoDB to automatically adjust the capacity with user traffic.</strong></p><p>Provisioning a Direct Connection just for the application is not economical even if it offers better latency than a VPN connection. Caching the API requests should be done on the API Gateway, and not on CloudFront. EC2 Reserve instances could be more expensive than Lambda functions when application traffic is low.</p><p>The following option is incorrect:</p><p><strong>- Migrate the on-premises MySQL database server to Amazon RDS for MySQL. Enable Multi-AZ to ensure high availability.</strong></p><p><strong>- Create a CloudFront distribution with the API Gateway as the origin to cache the API responses and reduce the Lambda invocations.</strong></p><p><strong>- Gradually lower the timeout and memory properties of the Lambda functions without increasing the execution time.</strong></p><p><strong>- Configure Auto Scaling on Amazon DynamoDB to automatically adjust the capacity with user traffic and enable DynamoDB Accelerator to cache frequently accessed records.</strong></p><p>Caching the API requests should be done on the API Gateway, and not on CloudFront. DynamoDB Accelerator is used for caching requests if you need response times in microseconds. This is very expensive.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p><p><a href=\"https://aws.amazon.com/api-gateway/faqs/\">https://aws.amazon.com/api-gateway/faqs/</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-auto-scaling-for-amazon-dynamodb/\">https://aws.amazon.com/blogs/aws/new-auto-scaling-for-amazon-dynamodb/</a></p><p><a href=\"https://aws.amazon.com/lambda/pricing/\">https://aws.amazon.com/lambda/pricing/</a></p><p><br></p><p><strong>Check out these Amazon API Gateway, Amazon DynamoDB, and AWS Lambda Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-api-gateway/?src=udemy\">https://tutorialsdojo.com/amazon-api-gateway/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><a href=\"https://tutorialsdojo.com/aws-lambda/?src=udemy\">https://tutorialsdojo.com/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html",
      "https://aws.amazon.com/api-gateway/faqs/",
      "https://aws.amazon.com/blogs/aws/new-auto-scaling-for-amazon-dynamodb/",
      "https://aws.amazon.com/lambda/pricing/",
      "https://tutorialsdojo.com/amazon-api-gateway/?src=udemy",
      "https://tutorialsdojo.com/amazon-dynamodb/?src=udemy",
      "https://tutorialsdojo.com/aws-lambda/?src=udemy"
    ]
  },
  {
    "id": 68,
    "question": "<p>A company currently hosts its online immigration system on one large Amazon EC2 instance with attached EBS volumes to store all of the applicants' data. The registration system accepts the information from the user including documents and photos and then performs automated verification and processing to check if the applicant is eligible for immigration. The immigration system becomes unavailable at times when there is a surge of applicants using the system. The existing architecture needs improvement as it takes a long time for the system to complete the processing and the attached EBS volumes are not enough to store the ever-growing data being uploaded by the users.</p><p>Which of the following options is the recommended option to achieve high availability and more scalable data storage?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Upgrade to EBS with Provisioned IOPS as your main storage service and change your architecture to use an SQS queue to distribute the tasks to a group of EC2 instances. Use Auto Scaling to dynamically increase or decrease the group of EC2 instances depending on the length of the SQS queue.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use EBS with Provisioned IOPS to store files, SNS to distribute tasks to a group of EC2 instances working in parallel, and Auto Scaling to dynamically size the group of EC2 instances depending on the number of SNS notifications. Use CloudFormation to replicate your architecture to another region.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Upgrade your architecture to use an S3 bucket with cross-region replication (CRR) enabled, as the storage service. Set up an SQS queue to distribute the tasks to a group of EC2 instances with Auto Scaling to dynamically increase or decrease the group of EC2 instances depending on the length of the SQS queue. Use CloudFormation to replicate your architecture to another region.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use SNS to distribute the tasks to a group of EC2 instances. Use Auto Scaling to dynamically increase or decrease the group of EC2 instances depending on the length of the SQS queue.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In this scenario, you need to overhaul the existing immigration service to upgrade its storage and computing capacity. Since EBS Volumes can only provide limited storage capacity and are not scalable, you should use S3 instead. The system goes down at times when there is a surge of requests which indicates that the existing large EC2 instance could not handle the requests any longer. In this case, you should implement a highly-available architecture and a queueing system with SQS and Auto Scaling.</p><p><img src=\"https://media.tutorialsdojo.com/sap_disaster_recovery.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_disaster_recovery.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The option that says: <strong>Upgrade your architecture to use an S3 bucket with cross-region replication (CRR) enabled, as the storage service. Set up an SQS queue to distribute the tasks to a group of EC2 instances with Auto Scaling to dynamically increase or decrease the group of EC2 instances depending on the length of the SQS queue. Use CloudFormation to replicate your architecture to another region</strong> is correct. This option provides high availability and scalable data storage with S3. Auto-scaling of EC2 instances reduces the overall processing time and SQS helps in distributing the tasks to a group of EC2 instances.</p><p>The option that says: <strong>Use EBS with Provisioned IOPS to store files, SNS to distribute tasks to a group of EC2 instances working in parallel, and Auto Scaling to dynamically size the group of EC2 instances depending on the number of SNS notifications. Use CloudFormation to replicate your architecture to another region</strong> is incorrect because EBS is not an easily scalable and durable storage solution compared to Amazon S3. Using SQS is more suitable in distributing the tasks to an Auto Scaling group of EC2 instances and not SNS.</p><p>The option that says: <strong>Use SNS to distribute the tasks to a group of EC2 instances. Use Auto Scaling to dynamically increase or decrease the group of EC2 instances depending on the length of the SQS queue </strong>is incorrect because SNS is not a valid choice in this scenario. Using SQS is more suitable in distributing the tasks to an Auto Scaling group of EC2 instances and not SNS.</p><p>The option that says: <strong>Upgrade to EBS with Provisioned IOPS as your main storage service and change your architecture to use an SQS queue to distribute the tasks to a group of EC2 instances. Use Auto Scaling to dynamically increase or decrease the group of EC2 instances depending on the length of the SQS queue</strong> is incorrect. Having a large EBS volume attached to each of the EC2 instance of the auto-scaling group is not economical. And it will be hard to sync the growing data across these EBS volumes. You should use S3 instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 69,
    "question": "<p>The department of education just recently decided to leverage the AWS cloud infrastructure to supplement its current on-premises network. They are building a new learning portal that teaches kids basic computer science concepts and provides innovative gamified courses for teenagers where they can gain higher rankings, power-ups and badges. A Solutions Architect is instructed to build a highly available cloud infrastructure in AWS with multiple Availability Zones. The department wants to increase the application’s reliability and gain actionable insights using application logs. A Solutions Architect needs to aggregate logs, automate log analysis for errors and immediately notify the IT Operations team when errors breached a certain threshold.</p><p>Which of the following is the MOST suitable solution that the Architect should implement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Download and install the Amazon Kinesis agent in the on-premises servers and send the logs to Amazon CloudWatch Logs. Create a metric filter in CloudWatch to turn log data into numerical metrics to identify and measure application errors. Use Amazon QuickSight to monitor the metric filter in CloudWatch and immediately notify the IT Operations team for any issues.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Download and install the Amazon Managed Service for Prometheus in the on-premises servers and send the logs to AWS Lambda to turn log data into numerical metrics that identify and measure application errors. Write the processed metrics back to the time series database in Prometheus. Create a CloudWatch Alarm that monitors the metric and immediately notifies the IT Operations team for any issues.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Download and install the Amazon CloudWatch agent in the on-premises servers and send the logs to Amazon EventBridge. Create a metric filter in CloudWatch to turn log data into numerical metrics to identify and measure application errors. Use Amazon Athena to monitor the metric filter and immediately notify the IT Operations team for any issues.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Download and install the Amazon CloudWatch agent in the on-premises servers and send the logs to Amazon CloudWatch Logs. Create a metric filter in CloudWatch to turn log data into numerical metrics to identify and measure application errors. Create a CloudWatch Alarm that monitors the metric filter and immediately notify the IT Operations team for any issues.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p><strong>Amazon CloudWatch</strong> monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real time. You can use CloudWatch to collect and track metrics, which are variables you can measure for your resources and applications. The CloudWatch home page automatically displays metrics about every AWS service you use. You can additionally create custom dashboards to display metrics about your custom applications and display custom collections of metrics that you choose.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudwatch_overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudwatch_overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>After the <strong>CloudWatch Logs agent</strong> begins publishing log data to Amazon CloudWatch, you can begin searching and filtering the log data by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. You can use any type of CloudWatch statistic, including percentile statistics when viewing these metrics or setting alarms.</p><p>You can create alarms that watch metrics and send notifications or automatically make changes to the resources you are monitoring when a threshold is breached. For example, you can monitor the CPU usage and disk reads and writes of your Amazon EC2 instances and then use this data to determine whether you should launch additional instances to handle the increased load. You can also use this data to stop under-used instances to save money. With CloudWatch, you gain system-wide visibility into resource utilization, application performance, and operational health.</p><p>Hence, the correct answer is: <strong>Download and install the Amazon CloudWatch agent in the on-premises servers and send the logs to Amazon CloudWatch Logs. Create a metric filter in CloudWatch to turn log data into numerical metrics to identify and measure application errors. Create a CloudWatch Alarm that monitors the metric filter and immediately notify the IT Operations team for any issues.</strong></p><p>The option that says:<strong><em> </em>Download and install the Amazon Kinesis agent in the on-premises servers and send the logs to Amazon CloudWatch Logs. Create a metric filter in CloudWatch to turn log data into numerical metrics to identify and measure application errors. Use Amazon QuickSight to monitor the metric filter in CloudWatch and immediately notify the IT Operations team for any issues</strong> is incorrect. You have to use an Amazon CloudWatch agent instead of an Amazon Kinesis agent to send the logs to Amazon CloudWatch Logs. It is also better to use CloudWatch Alarms to monitor the metric filter than to use Amazon QuickSight.</p><p>The option that says:<strong> Download and install the Amazon Managed Service for Prometheus in the on-premises servers and send the logs to AWS Lambda to turn log data into numerical metrics that identify and measure application errors. Write the processed metrics back to the time series database in Prometheus. Create a CloudWatch Alarm that monitors the metric and immediately notifies the IT Operations team for any issues </strong>is incorrect. Amazon Managed Service for Prometheus is a serverless, Prometheus-compatible monitoring service for container metrics. You don't have to create a custom Lambda function to process the logs. Amazon Managed Service for Prometheus is integrated with CloudWatch to monitor metrics and Logs.</p><p>The option that says:<strong><em> </em>Download and install the Amazon CloudWatch agent in the on-premises servers and send the logs to Amazon EventBridge. Create a metric filter in CloudWatch to turn log data into numerical metrics to identify and measure application errors. Use Amazon Athena to monitor the metric filter and immediately notify the IT Operations team for any issues</strong> is incorrect. You have to send the logs to CloudWatch Logs and not CloudWatch Events. It is also better to use CloudWatch Alarm to monitor the metric filter and immediately notify the IT Operations team for any issues.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringPolicyExamples.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringPolicyExamples.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringPolicyExamples.html",
      "https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy"
    ]
  },
  {
    "id": 70,
    "question": "<p>A company runs a sports web portal that covers the latest cricket news in Australia. The solutions architect manages the main AWS account which has resources in multiple AWS regions. The web portal is hosted on a fleet of on-demand EC2 instances and an RDS database which are also deployed to other AWS regions. The IT Security Compliance Officer has given the solutions architect the task of developing a reliable and durable logging solution to track changes made to all of your EC2, IAM, and RDS resources in all of the AWS regions. The solution must ensure the integrity and confidentiality of the log data.</p><p>Which of the following solutions would be the best option to choose?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a new trail in CloudTrail and assign it a new S3 bucket to store the logs. Configure AWS SNS to send delivery notifications to your management system. Secure the S3 bucket that stores your logs using IAM roles and S3 bucket policies.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a new trail in AWS CloudTrail with the global services option selected, and create one new Amazon S3 bucket to store the logs. Create IAM roles, S3 bucket policies, and enable Multi Factor Authentication (MFA) Delete on the S3 bucket storing your logs.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a new trail in AWS CloudTrail with the global services option selected, and assign it an existing S3 bucket to store the logs. Create S3 ACLs and enable Multi Factor Authentication (MFA) delete on the S3 bucket storing your logs.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create three new CloudTrail trails, each with its own S3 bucket to store the logs: one for the AWS Management console, one for AWS SDKs, and one for command line tools. Then create IAM roles and S3 bucket policies for the S3 buckets storing your logs.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>For most services, events are recorded in the region where the action occurred to its respective AWS CloudTrail. For global services such as AWS Identity and Access Management (IAM), AWS STS, Amazon CloudFront, and Route 53, events are delivered to any trail that includes global services (IncludeGlobalServiceEvents flag). AWS CloudTrail service should be your top choice for the scenarios where the application is tracking the changes made by any AWS service, resource, or API.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_s3_logs.PNG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_s3_logs.PNG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><br>AWS Identity and Access Management (IAM) is integrated with AWS CloudTrail, a service that logs AWS events made by or on behalf of your AWS account. CloudTrail logs authenticate AWS API calls and also AWS sign-in events, and collects this event information in files that are delivered to Amazon S3 buckets.<p></p><p>Therefore, the correct answer is: <strong>Create a new trail in AWS CloudTrail with the global services option selected, and create one new Amazon S3 bucket to store the logs. Create IAM roles, S3 bucket policies, and enable Multi Factor Authentication (MFA) Delete on the S3 bucket storing your logs.</strong> It uses AWS CloudTrail with (includeGlobalServiceEvents flag) Global Option enabled, a single new S3 bucket and IAM Roles so that it has the confidentiality, and MFA on Delete on S3 bucket so that it maintains the data integrity.</p><p>The option that says: <strong>Create a new trail in AWS CloudTrail with the global services option selected, and assign it an existing S3 bucket to store the logs. Create S3 ACLs and enable Multi Factor Authentication (MFA) delete on the S3 bucket storing your logs</strong> is incorrect. As an existing S3 bucket is used, it may already be accessed by the user, hence not maintaining the confidentiality, and it is not using IAM roles.</p><p>The option that says: <strong>Create three new CloudTrail trails, each with its own S3 bucket to store the logs: one for the AWS Management console, one for AWS SDKs, and one for command line tools. Then create IAM roles and S3 bucket policies for the S3 buckets storing your logs</strong> is incorrect. Although it uses AWS CloudTrail, the Global Option is not enabled, and three S3 buckets are not needed.</p><p>The option that says: <strong>Create a new trail in CloudTrail and assign it a new S3 bucket to store the logs. Configure AWS SNS to send delivery notifications to your management system. Secure the S3 bucket that stores your logs using IAM roles and S3 bucket policies</strong> is incorrect. Although it uses AWS CloudTrail, the Global Option is not enabled.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html</a></p><p><br></p><p><strong>Check out these AWS CloudTrail and IAM Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudtrail/?src=udemy\">https://tutorialsdojo.com/aws-cloudtrail/</a></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html",
      "https://tutorialsdojo.com/aws-cloudtrail/?src=udemy",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy"
    ]
  },
  {
    "id": 71,
    "question": "<p>A company hosts its online delivery system on a fleet of EC2 instances deployed in multiple Availability Zones in the ap-southeast-1 region. The instances are behind an Application Load Balancer that evenly distributes the load. The system is using a MySQL RDS instance to store the deliveries and transactions of the system. To ensure business continuity, you are instructed to set up a disaster recovery system in which the RTO must be less than 3 hours and the RPO is 15 minutes when a system outage occurs. A system should also be implemented that can automatically discover, classify, and protect any personally identifiable information (PII) or intellectual property in your data store.</p><p>As the Solutions Architect, which disaster recovery strategy should you use to achieve the required RTO and RPO targets in the most cost-effective manner?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up asynchronous replication in the database using a Multi-AZ deployments configuration. Use AWS Shield to automatically discover, classify, and protect any personally identifiable information (PII) or intellectual property from your RDS database.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Schedule a database backup to an S3 bucket every hour and store transaction logs to a separate S3 bucket every 5 minutes. Use Amazon Macie to automatically discover, classify, and protect your sensitive data.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Schedule 15-minute DB backups to Amazon Glacier. Store the transaction logs to an S3 bucket every 5 minutes. Use Amazon Macie to automatically discover, classify, and protect your sensitive data.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Schedule a database backup to AWS Storage Gateway every hour and store transaction logs to a separate S3 bucket every 5 minutes. Use AWS Shield to automatically discover, classify, and protect any personally identifiable information (PII) or intellectual property on your Storage Gateway.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Recovery time objective (RTO)</strong> is the time it takes after a disruption to restore a business process to its service level, as defined by the operational level agreement (OLA). For example, if a disaster occurs at 12:00 PM (noon) and the RTO is eight hours, the DR process should restore the business process to the acceptable service level by 8:00 PM.</p><p><strong>Recovery point objective (RPO)</strong> is the acceptable amount of data loss measured in time. For example, if a disaster occurs at 12:00 PM (noon) and the RPO is one hour, the system should recover all data that was in the system before 11:00 AM. Data loss will span only one hour, between 11:00 AM and 12:00 PM (noon).</p><p><strong>Amazon S3</strong> is an ideal destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network, and is therefore accessible from any location.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_mysql_backup_s3.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_rds_mysql_backup_s3.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Amazon Macie</strong> is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Amazon Macie recognizes sensitive data such as personally identifiable information (PII) or intellectual property, and provides you with dashboards and alerts that give visibility into how this data is being accessed or moved. The fully managed service continuously monitors data access activity for anomalies, and generates detailed alerts when it detects risk of unauthorized access or inadvertent data leaks.</p><p>Hence, the option that says:<em> </em><strong>Schedule a database backup to an S3 bucket every hour and store transaction logs to a separate S3 bucket every 5 minutes. Use Amazon Macie to automatically discover, classify, and protect your sensitive data</strong> is correct because by using an S3 bucket, it makes data retrievals of the backups quicker. Since the transaction logs are stored in S3 every 5 minutes, this will help to restore the application to a state that is within the required RPO of 15 minutes.</p><p><strong>Scheduling 15-minute DB Backups to Amazon Glacier and storing the transaction logs to an S3 bucket every 5 minutes, and using Amazon Macie to automatically discover, classify, and protect your sensitive data</strong> is incorrect because retrieving the database backups from Amazon Glacier archives will normally take around 3 - 5 hours using the Standard Retrievals and hence, this will not meet the RTO and RPO. Although you can use expedited retrievals, which can typically retrieve your archive within 1 – 5 minutes, this will entail additional cost and hence, not a cost-effective solution.</p><p><strong>Setting up asynchronous replication in the database using a Multi-AZ deployments configuration and using AWS Shield to automatically discover, classify, and protect any personally identifiable information (PII) or intellectual property from your RDS database</strong> is incorrect because asynchronous replication is only applicable for Read Replicas and not for Multi-AZ deployments configuration. Although this will improve the availability of the RDS database, it won't provide a better RTO or RPO, especially in the event of a regional outage since you can't export a standby instance to another region. In addition, you have to use AWS Macie to protect your sensitive data in Amazon S3 and not AWS Shield.</p><p><strong>Scheduling a database backup to AWS Storage Gateway every hour and storing transaction logs to a separate S3 bucket every 5 minutes and using AWS Shield to automatically discover, classify, and protect any personally identifiable information (PII) or intellectual property on your Storage Gateway<em> </em></strong>is incorrect. AWS Storage Gateway is primarily used for hybrid cloud storage service that connects your existing on-premises environments with the AWS Cloud. Although this can be a valid option, the scenario did not say that their architecture is hybrid or that they are using an on-premises data center. AWS Shield is primarily used to protect your resources from DDoS attacks, and not to automatically discover, classify, and protect any personally identifiable information (PII) or intellectual property.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://www.slideshare.net/AmazonWebServices/disaster-recovery-options-with-aws\">https://www.slideshare.net/AmazonWebServices/disaster-recovery-options-with-aws</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIT.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIT.html</a></p><p><br></p><p><strong>Check out this Amazon Macie Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-macie/?src=udemy\">https://tutorialsdojo.com/amazon-macie/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://www.slideshare.net/AmazonWebServices/disaster-recovery-options-with-aws",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIT.html",
      "https://tutorialsdojo.com/amazon-macie/?src=udemy"
    ]
  },
  {
    "id": 72,
    "question": "<p>A company has several development teams using AWS CodeCommit to store their source code. With the number of code updates every day, the management is having difficulty tracking if the developers are adhering to company security policies. On a recent audit, the security team found several IAM access keys and secret keys in the CodeCommit repository. This is a big security risk so the company wants to have an automated solution that will scan the CodeCommit repositories for committed IAM credentials and delete/disable the IAM keys for those users.</p><p>Which of the following options will meet the company requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Write a custom AWS Lambda function to search for credentials on new code submissions. Set the function trigger as AWS CodeCommit push events. If credentials are found, notify the user of the violation, and disable the IAM keys.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Using a development instance, use the AWS Systems Manager Run Command to scan the AWS CodeCommit repository for IAM credentials on a daily basis. If credentials are found, rotate them using AWS Secrets Manager. Notify the user of the violation.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Scan the CodeCommit repositories for IAM credentials using Amazon Macie. Using machine learning, Amazon Macie can scan your repository for security violations. If violations are found, invoke an AWS Lambda function to notify the user and delete the IAM keys.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Download and scan the source code from AWS CodeCommit using a custom AWS Lambda function. Schedule this Lambda function to run daily. If credentials are found, notify the user of the violation, generate new IAM credentials and store them in AWS KMS for encryption.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS CodeCommit</strong> is a version control service hosted by Amazon Web Services that you can use to privately store and manage assets (such as documents, source code, and binary files) in the cloud.</p><p>You can configure a CodeCommit repository so that code pushes or other events trigger actions, such as sending a notification from Amazon Simple Notification Service (Amazon SNS) or invoking a function in AWS Lambda. You can create up to 10 triggers for each CodeCommit repository.</p><p>Triggers are commonly configured to:</p><p>- Send emails to subscribed users every time someone pushes to the repository.</p><p>- Notify an external build system to start a build after someone pushes to the main branch of the repository.</p><p>Scenarios like notifying an external build system require writing a Lambda function to interact with other applications. The email scenario simply requires creating an Amazon SNS topic. You can create a trigger for a CodeCommit repository so that events in that repository trigger notifications from an Amazon Simple Notification Service (Amazon SNS) topic.</p><p>You can also create an AWS Lambda trigger for a CodeCommit repository so that events in the repository invoke a Lambda function. For example, you can create a Lambda function that will scan the CodeCommit code submissions for IAM credentials, and then send out notifications or perform corrective actions.</p><p>When you use the Lambda console to create the function, you can create a CodeCommit trigger for the Lambda function. Here is an example of the trigger for all push events:</p><p><img src=\"https://docs.aws.amazon.com/codecommit/latest/userguide/images/codecommit-lambda-trigger.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/codecommit/latest/userguide/images/codecommit-lambda-trigger.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Write a custom AWS Lambda function to search for credentials on new code submissions. Set the function trigger as AWS CodeCommit push events. If credentials are found, notify the user of the violation, and disable the IAM keys.</strong></p><p>The option that says: <strong>Using a development instance, use the AWS Systems Manager Run Command to scan the AWS CodeCommit repository for IAM credentials on a daily basis. If credentials are found, rotate them using AWS Secrets Manager. Notify the user of the violation</strong> is incorrect. You cannot rotate IAM keys on AWS Secrets Manager. Using the Run Command on a development instance just for scanning the repository is costly. It is cheaper to just write your own Lambda function to do the scanning.</p><p>The option that says: <strong>Download and scan the source code from AWS CodeCommit using a custom AWS Lambda function. Schedule this Lambda function to run daily. If credentials are found, notify the user of the violation, generate new IAM credentials and store them in AWS KMS for encryption</strong> is incorrect. You store encryption keys on AWS KMS, not IAM keys.</p><p>The option that says: <strong>Scan the CodeCommit repositories for IAM credentials using Amazon Macie. Using machine learning, Amazon Macie can scan your repository for security violations. If violations are found, invoke an AWS Lambda function to notify the user and delete the IAM keys</strong> is incorrect. Amazon Macie is designed to use machine learning and pattern matching to discover and protect your sensitive data in AWS. Macie primarily scans Amazon S3 buckets for data security and data privacy.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda.html</a></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-sns.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-sns.html</a></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html</a></p><p><br></p><p><strong>Check out the AWS CodeCommit Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codecommit/?src=udemy\">https://tutorialsdojo.com/aws-codecommit/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda.html",
      "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-sns.html",
      "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html",
      "https://tutorialsdojo.com/aws-codecommit/?src=udemy"
    ]
  },
  {
    "id": 73,
    "question": "<p>A startup in the fashion industry is building a mobile app that showcases its latest fashion accessories and gadgets. The marketing manager hired a famous model with millions of Instagram followers to promote their new products and hence, it is expected that the app will be a huge hit once it is launched in the market. It must have the ability to automatically scale to handle millions of views of its static contents and to allow users to store their own photos of themselves wearing fashionable accessories with a maximum of 100 characters for captions.</p><p>In this scenario, which of the following solutions would fulfill this requirement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>1. Use Cognito to handle user authentication and management.</p><p>\n2. Launch a DynamoDB table to store user data.</p><p>\n3. Create an S3 bucket to store all of the user photos and other static files.</p><p>\n4. Distribute the static contents using CloudFront to improve scalability.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>1. Set up a SAML 2.0-based Federation that lets the users sign into the app using a third-party identity provider such as Amazon, Google, or Facebook.</p><p>\n2. Set up an RDS database and an S3 bucket to store the photos.</p><p>\n3. Use the AssumeRoleWithWebIdentity API call to assume the IAM role containing the proper permissions to communicate with the RDS database. </p><p>\n4. Distribute the static contents using S3 to improve scalability.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>1. Use Cognito to handle user authentication and management.</p><p>\n2. Use an RDS database to store user data.</p><p>\n3. Create an S3 bucket to store all of the user photos and other static files.</p><p>\n4. Distribute the static contents using CloudFront to improve scalability.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>1. Configure an on-premises Active Directory (AD) server utilizing SAML 2.0 to manage the application users inside of the on-premises AD server.</p><p>\n2. Develop a custom code that authenticates against the LDAP server. </p><p>\n3. Use DynamoDB as the main database of the app and S3 as the scalable object storage.</p><p>\n4. Grant an IAM role assigned to the STS token to allow the end-user to access the required data in the DynamoDB table.</p><p>\n5. Distribute the static contents using CloudFront to improve scalability.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon Cognito</strong> scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0. <strong>Amazon S3</strong> provides a scalable object storage solution. <strong>Amazon CloudFront</strong> is a Content Delivery Network that helps your system to handle the millions of user views and finally, <strong>DynamoDB</strong> is a scalable database that can handle millions of records.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cognito_dynamodb_cloudfront.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cognito_dynamodb_cloudfront.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In this scenario, the top priority is scalability to meet the upcoming surge of users of your mobile app. Since all of these services can provide the scalability that your mobile app needs, the best option that you can choose is the following:</p><p><strong>1. Use Cognito to handle user authentication and management.</strong></p><p><strong>2. Launch a DynamoDB table to store user data.</strong></p><p><strong>3. Create an S3 bucket to store all of the user photos and other static files.</strong></p><p><strong>4. Distribute the static contents using CloudFront to improve scalability.</strong></p><p>The following option is incorrect because a SAML 2.0-based federation doesn't use social media logins and it is better to use CloudFront to distribute static contents compared to an S3 bucket:</p><p><strong>1. Set up a SAML 2.0-based Federation that lets the users sign into the app using a third party identity provider such as Amazon, Google or Facebook.</strong></p><p><strong>2. Set up an RDS database and an S3 bucket to store the photos.</strong></p><p><strong>3. Use the AssumeRoleWithWebIdentity API call to assume the IAM role containing the proper permissions to communicate with the RDS database.</strong></p><p><strong>4. Distribute the static contents using S3 to improve scalability.</strong></p><p>The following option is incorrect because using an on-premise Active Directory server is not a suitable solution for this scenario:</p><p><strong>1. Configure an on-premises Active Directory (AD) server utilizing SAML 2.0 to manage the application users inside of the on-premises AD server.</strong></p><p><strong>2. Develop a custom code that authenticates against the LDAP server.</strong></p><p><strong>3. Use DynamoDB as the main database of the app and S3 as the scalable object storage.</strong></p><p><strong>4. Grant an IAM role assigned to the STS token to allow the end-user to access the required data in the DynamoDB table.</strong></p><p><strong>5. Distribute the static contents using CloudFront to improve scalability.</strong></p><p>Remember that this is a public mobile app and hence, it is better to set up a Web Identity Federation instead</p><p>The following option is incorrect because RDS is not as scalable as DynamoDB:</p><p><strong>1. Use Cognito to handle user authentication and management.</strong></p><p><strong>2. Use an RDS database to store user data.</strong></p><p><strong>3. Create an S3 bucket to store all of the user photos and other static files.</strong></p><p><strong>4. Distribute the static contents using CloudFront to improve scalability.</strong></p><p>Based on the type of data you will be storing, a relational database like RDS is not suitable to store simple data sets such as a 100-character caption which does not need multiple tables or relational data models.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cognito/\">https://aws.amazon.com/cognito/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/protect-public-clients-for-amazon-cognito-by-using-an-amazon-cloudfront-proxy/\">https://aws.amazon.com/blogs/security/protect-public-clients-for-amazon-cognito-by-using-an-amazon-cloudfront-proxy/</a></p><p><a href=\"https://aws.amazon.com/blogs/mobile/building-fine-grained-authorization-using-amazon-cognito-user-pools-groups/\">https://aws.amazon.com/blogs/mobile/building-fine-grained-authorization-using-amazon-cognito-user-pools-groups/</a></p><p><br></p><p><strong>Check out these Amazon Cognito and Amazon S3 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cognito/?src=udemy\">https://tutorialsdojo.com/amazon-cognito/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/cognito/",
      "https://aws.amazon.com/blogs/security/protect-public-clients-for-amazon-cognito-by-using-an-amazon-cloudfront-proxy/",
      "https://aws.amazon.com/blogs/mobile/building-fine-grained-authorization-using-amazon-cognito-user-pools-groups/",
      "https://tutorialsdojo.com/amazon-cognito/?src=udemy",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 74,
    "question": "<p>A clinic runs its medical record system using a fleet of Windows-based Amazon EC2 instances with several EBS volumes attached to it. Since the records that they are storing are confidential health files of their patients, it is a requirement that the latest security patches are installed on the EC2 instances. In addition, there should be a system in the cloud architecture that checks all of the EC2 instances if they are using an approved Amazon Machine Image (AMI). The system that will be implemented should not impede developers from launching instances using an unapproved AMI, but you still have to be notified if there are non-compliant EC2 instances in your VPC.</p><p>Which of the following should the solutions architect implement to protect and monitor all of your instances as required above? (Select TWO.)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up a patch baseline that defines which patches are approved for installation on your instances using AWS Systems Manager Patch Manager.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up Amazon GuardDuty that continuously monitors your instances if the latest security patches are installed and if there is an instance that is using an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Set up CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Use AWS Shield Advanced to automatically patch all of your EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Patch Manager uses <strong>patch baselines</strong>, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p>AWS Config provides <strong>AWS managed rules</strong>, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p>In this scenario, you can use a combination of AWS Config Managed Rules and AWS Systems Manager Patch Manager to meet the requirements.</p><p>Therefore, the correct answers are:</p><p><strong>- Set up a patch baseline that defines which patches are approved for installation on your instances using AWS Systems Manager Patch Manager.</strong></p><p><strong>- Use the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Set up CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong>.</p><p>The option that says: <strong>Creating an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI </strong>is incorrect. Although you can use an IAM policy to prohibit your developers from launching unapproved AMIs, this will impede their work which violates what the scenario requires. Remember, as per the scenario, the system that you will implement should not impede developers from launching instances using an unapproved AMI.</p><p>The option that says: <strong>Set up Amazon GuardDuty that continuously monitors your instances if the latest security patches are installed and if there is an instance that is using an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect. Amazon GuardDuty is primarily used as a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise however, it does not check if your EC2 instances are using an approved AMI or not.</p><p><strong>Using AWS Shield Advanced to automatically patch all of your EC2 instances and detecting uncompliant EC2 instances which do not use approved AMIs </strong>is incorrect. The AWS Shield Advanced service is most suitable to prevent DDoS attacks in your AWS resources. It cannot check the specific AMIs that your EC2 instances are using.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Config and AWS Systems Manager:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html",
      "https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html",
      "https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html",
      "https://tutorialsdojo.com/aws-config/?src=udemy",
      "https://tutorialsdojo.com/aws-systems-manager/?src=udemy"
    ]
  },
  {
    "id": 75,
    "question": "<p>An international foreign exchange company has a serverless forex trading application that was built using AWS SAM and is hosted on AWS Serverless Application Repository. They have millions of users worldwide who use their online portal 24/7 to trade currencies. However, they are receiving a lot of complaints that it takes a few minutes for their users to log in to their portal lately, including occasional HTTP 504 errors. As the Solutions Architect, you are tasked to optimize the system and to significantly reduce the time to log in to improve the customers' satisfaction.</p><p>Which of the following should you implement in order to improve the performance of the application with minimal cost? (Select TWO.)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up multiple and geographically disperse VPCs to various AWS regions then create a transit VPC to connect all of your resources. Deploy the Lambda function in each region using AWS SAM, in order to handle the requests faster.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Increase the cache hit ratio of your CloudFront distribution by configuring your origin to add a <code>Cache-Control max-age</code> directive to your objects, and specify the longest practical value for <code>max-age</code>.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use Lambda@Edge to allow your Lambda functions to customize content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Lambda@Edge</strong> lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:</p><p>- After CloudFront receives a request from a viewer (viewer request)</p><p>- Before CloudFront forwards the request to the origin (origin request)</p><p>- After CloudFront receives the response from the origin (origin response)</p><p>- Before CloudFront forwards the response to the viewer (viewer response)</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_cache.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_cache.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In the given scenario, you can <strong>use Lambda@Edge to allow your Lambda functions to customize the content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users. </strong>In addition, you can<strong> set up an origin failover by creating an origin group with two origins with one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin fails.</strong> This will alleviate the occasional HTTP 504 errors that users are experiencing.</p><p>The option that says: <strong>Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user</strong> is incorrect. Although this may resolve the performance issue, this solution entails a significant implementation cost since you have to deploy your application to multiple AWS regions. Remember that the scenario asks for a solution that will improve the performance of the application with <strong>minimal cost</strong>.</p><p>The option that says: <strong>Increase the cache hit ratio of your CloudFront distribution by configuring your origin to add a </strong><code><strong>Cache-Control max-age</strong></code><strong> directive to your objects, and specify the longest practical value for </strong><code><strong>max-age</strong> </code>is incorrect because improving the cache hit ratio for the CloudFront distribution is irrelevant in this scenario. You can improve your cache performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content. However, take note that the problem in the scenario is the sluggish authentication process of your global users and not just the caching of the static objects.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html</a></p><p><br></p><p><strong>Check out these Amazon CloudFront and AWS Lambda Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><a href=\"https://tutorialsdojo.com/aws-lambda/?src=udemy\">https://tutorialsdojo.com/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy",
      "https://tutorialsdojo.com/aws-lambda/?src=udemy"
    ]
  }
]