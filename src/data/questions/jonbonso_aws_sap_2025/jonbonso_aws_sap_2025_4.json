[
  {
    "id": 1,
    "question": "<p>A company develops cloud-native applications and uses AWS CloudFormation templates for deploying applications in AWS. The application artifacts and templates are stored in an Amazon S3 bucket with versioning enabled. The developers use Amazon EC2 instances that have integrated development (IDE) to download, modify, and re-upload the artifacts on the S3 bucket. The unit testing is done locally on the EC2 instances. The company wants to improve the existing deployment process with a CI/CD pipeline to help the developers be more productive. The following requirements need to be satisfied:</p><p>- Utilize GitHub as the code repository for application and CloudFormation templates.</p><p>- Have automated testing and security scanning for the generated artifacts.</p><p>- Receive a notification when unit testing fails.</p><p>- Ability to turn on/off application features and dynamically customize the deployment as part of CI/CD.</p><p>- The Lead Developer must approve changes before deploying applications to production.</p><p>Which of the following options should the solutions architect implement to meet the company's requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS CodeArtifact to store generated artifacts. AWS scans the artifacts for common vulnerabilities and allows custom actions to run the unit tests. Create an Amazon CloudWatch rule that will send Amazon SNS alerts when unit testing fails. Use different Docker images for choosing different application features. Add a manual approval stage on the pipeline for the Lead Developer’s approval prior to production deployment.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a Jenkins job to run tests and security scans on the generated artifacts. Create an Amazon EventBridge rule that will send Amazon SES alerts when unit testing fails. Use AWS CloudFormation with nested stacks to allow turning on/off of application features. Add an AWS Lambda function to the pipeline to allow approval from the Lead Developer prior to production deployment.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an AWS CodeBuild job to run tests and security scans on the generated artifacts. Create an Amazon EventBridge rule that will send Amazon SNS alerts when unit testing fails. Create AWS Cloud Development Kit (AWS CDK) constructs with a manifest file to turn on/off features of the AWS CDK app. Add a manual approval stage on the pipeline for the Lead Developer’s approval prior to production deployment.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Write an AWS Lambda function to run unit tests and security scans on the generated artifacts. Add another Lambda trigger on the next pipeline stage to notify the developers if the unit testing fails. Create AWS Amplify plugins to allow turning on/off of application features. Add an AWS SES action on the pipeline to send an approval message to the Lead Developer prior to production deployment.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS CodeBuild</strong> is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and builds tools such as Apache Maven, Gradle, and more. You can also customize build environments in CodeBuild to use your own build tools. CodeBuild scales automatically to meet peak build requests. CodeBuild provides these benefits:</p><p><strong>Fully managed</strong> – CodeBuild eliminates the need to set up, patch, update, and manage your own build servers.</p><p><strong>On-demand</strong> – CodeBuild scales on demand to meet your build needs. You pay only for the number of build minutes you consume.</p><p><strong>Out of the box</strong> – CodeBuild provides preconfigured build environments for the most popular programming languages. All you need to do is point to your build script to start your first build.</p><p><img src=\"https://media.tutorialsdojo.com/sap_codepipeline_stages.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_codepipeline_stages.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The <strong>AWS Cloud Development Kit (AWS CDK)</strong> lets you easily define applications in the AWS Cloud using your programming language of choice. But creating an application is just the start of the journey. You also want to make changes to it and deploy them. You can do this through the Code suite of tools: GitHub, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline. Together, they allow you to build what's called a deployment pipeline for your application.</p><p>In <strong>AWS CodePipeline</strong>, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p>If the action is approved, the pipeline execution resumes. If the action is rejected—or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping—the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application or to confirm the integrity of a build artifact before it is released.</p><p>- You want someone to review new or updated text that is published on a company website.</p><p>Therefore, the correct answer is: <strong>Create an AWS CodeBuild job to run tests and security scans on the generated artifacts. Create an Amazon EventBridge rule that will send Amazon SNS alerts when unit testing fails. Create AWS Cloud Development Kit (AWS CDK) constructs with a manifest file to turn on/off features of the AWS CDK app. Add a manual approval stage on the pipeline for the Lead Developer’s approval prior to production deployment. </strong>AWS CodeBuild allows automated build and testing of application artifacts. AWS CDK allows you to control application features using a manifest file, and adding an approval on AWS CodePipeline will allow the Lead Developer to approve deployments.</p><p>The option that says: <strong>Write an AWS Lambda function to run unit tests and security scans on the generated artifacts. Add another Lambda trigger on the next pipeline stage to notify the developers if the unit testing fails. Create AWS Amplify plugins to allow turning on/off of application features. Add an AWS SES action on the pipeline to send an approval message to the Lead Developer prior to production deployment </strong>is incorrect. Unit testing and security scanning may typically take longer than 15 minutes, so AWS Lambda is not recommended here. It would be easier and much simpler to just add an approval stage on the pipeline instead of using AWS SES to send an approval message to the Lead Developer.</p><p>The option that says: <strong>Create a Jenkins job to run tests and security scans on the generated artifacts. Create an Amazon EventBridge rule that will send Amazon SES alerts when unit testing fails. Use AWS CloudFormation with nested stacks to allow turning on/off of application features. Add an AWS Lambda function to the pipeline to allow approval from the Lead Developer prior to production deployment</strong> is incorrect. Instead of creating a Jenkins job, you can use AWS CodeBuild to test your artifacts. CodeBuild is tightly integrated on AWS, so it is very easy to add it to your deployment pipeline.</p><p>The option that says:<strong> Use AWS CodeArtifact to store generated artifacts. AWS scans the artifacts for common vulnerabilities and allows custom actions to run the unit tests. Create an Amazon CloudWatch rule that will send Amazon SNS alerts when unit testing fails. Use different Docker images for choosing different application features. Add a manual approval stage on the pipeline for the Lead Developer’s approval prior to production deployment </strong>is incorrect. AWS CodeArtifact is a fully managed artifact repository service that primarily allows organizations to securely store, publish, and share software packages used in their software development process. It works with common package managers and builds tools like Maven, Gradle, npm, yarn, twine, and pip. It does not allow the user to configure custom actions or scripts to perform unit tests on artifacts. It is recommended to use AWS CodeBuild for this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cdk/latest/guide/codepipeline_example.html\">https://docs.aws.amazon.com/cdk/latest/guide/codepipeline_example.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><br></p><p><strong>Check out these AWS CodeBuild and AWS CodePipeline Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codebuild/?src=udemy\">https://tutorialsdojo.com/aws-codebuild/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/cdk/latest/guide/codepipeline_example.html",
      "https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html",
      "https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html",
      "https://tutorialsdojo.com/aws-codebuild/?src=udemy"
    ]
  },
  {
    "id": 2,
    "question": "<p>A company is running its new web application on a test environment in its on-premises data center. The stateful application is running on a single web server and it connects to a MySQL database that is hosted on a separate server. In a few weeks, the web application is scheduled to be released to the general public and the company is worried about its scalability. The user traffic will be unpredictable so it has been decided to migrate the web application and database to AWS. The company wants to use the Amazon EC2 service for hosting the web application, Amazon Aurora for the database, and Elastic Load Balancing for load distribution.</p><p>Which of the following solutions will allow the web and database tier to scale along with user traffic?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon Aurora MySQL database instance. Create an Aurora Replica and enable Aurora Auto Scaling for the replica. Create an Auto Scaling group of Amazon EC2 instances placed behind a Network Load Balancer with the least outstanding request (LOR) routing algorithm. Ensure that the sticky sessions feature is enabled for the NLB.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon Aurora MySQL database instance and enable Aurora Auto Scaling for the master database. Create an Auto Scaling group of Amazon EC2 instances placed behind an Application Load Balancer with the round-robin routing algorithm. Ensure that the sticky sessions feature is enabled for the ALB.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon Aurora MySQL database instance and enable Aurora Auto Scaling for the master database. Create an Auto Scaling group of Amazon EC2 instances placed behind a Network Load Balancer with the least outstanding request (LOR) routing algorithm. Ensure that the sticky sessions feature is enabled for the NLB.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon Aurora MySQL database instance. Create an Aurora Replica and enable Aurora Auto Scaling for the replica. Create an Auto Scaling group of Amazon EC2 instances placed behind an Application Load Balancer with the round-robin routing algorithm. Ensure that the sticky sessions feature is enabled for the ALB.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Aurora (Aurora)</strong> is a fully managed relational database engine that's compatible with MySQL and PostgreSQL. Aurora includes a high-performance storage subsystem. Its MySQL- and PostgreSQL-compatible database engines are customized to take advantage of that fast distributed storage. To meet your connectivity and workload requirements, Aurora Auto Scaling dynamically adjusts the number of Aurora Replicas provisioned for an Aurora DB cluster using single-master replication. Aurora Auto Scaling is available for both Aurora MySQL and Aurora PostgreSQL. Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances.</p><p>You define and apply a scaling policy to an Aurora DB cluster. The scaling policy defines the minimum and maximum number of Aurora Replicas that Aurora Auto Scaling can manage. Based on the policy, Aurora Auto Scaling adjusts the number of Aurora Replicas up or down in response to actual workloads, determined by using Amazon CloudWatch metrics and target values. Before you can use Aurora Auto Scaling with an Aurora DB cluster, you must first create an Aurora DB cluster with a primary instance and at least one Aurora Replica.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aurora_rds.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aurora_rds.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Aurora Auto Scaling uses a scaling policy to adjust the number of Aurora Replicas in an Aurora DB cluster. Aurora Auto Scaling has the following components:</p><p><strong>A service-linked role</strong> – a service role to allow scaling of the Aurora Replicas</p><p><strong>A target metric</strong> – a predefined or custom metric and a target value for the metric. Aurora Auto Scaling creates and manages CloudWatch alarms that trigger the scaling policy</p><p><strong>Minimum and maximum capacity</strong> - the minimum and maximum number of Aurora Replicas to be managed by Application Auto Scaling</p><p><strong>A cooldown period</strong> - blocks subsequent scale-in or scale-out requests until the specified period expires.</p><p><strong>Sticky sessions</strong> are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies. Sticky sessions are helpful if your stateful application is having problems handling user sessions in an Auto Scaling environment.</p><p>There are two common routing algorithms for Application Load Balancers (ALB):</p><p>- Round Robin</p><p>- Least Outstanding Requests (LOR)</p><p>With the Least outstanding requests (LOR) algorithm, customers can route requests within a target group. As the new request comes in, the load balancer will send it to the target with the least number of outstanding requests. Targets processing long-standing requests or having lower processing capabilities are not burdened with more requests and the load is evenly spread across targets. This also helps the new targets to effectively take the load off of overloaded targets.</p><p>Therefore, the correct answer is: <strong>Create an Amazon Aurora MySQL database instance. Create an Aurora Replica and enable Aurora Auto Scaling for the replica. Create an Auto Scaling group of Amazon EC2 instances placed behind an Application Load Balancer with the round-robin routing algorithm. Ensure that the sticky sessions feature is enabled for the ALB.</strong></p><p>The option that says: <strong>Create an Amazon Aurora MySQL database instance. Create an Aurora Replica and enable Aurora Auto Scaling for the replica. Create an Auto Scaling group of Amazon EC2 instances placed behind a Network Load Balancer with the least outstanding request (LOR) routing algorithm. Ensure that the sticky sessions feature is enabled for the NLB</strong> is incorrect. A Network Load Balancer is only recommended if your application expects millions of requests per second. Since there is no mention of this strict requirement on the question, the Application Load Balancer is the recommended choice as it is less expensive and supports Layer 7 features that are suitable for web applications.</p><p>The option that says: <strong>Create an Amazon Aurora MySQL database instance and enable Aurora Auto Scaling for the master database. Create an Auto Scaling group of Amazon EC2 instances placed behind an Application Load Balancer with the round-robin routing algorithm. Ensure that the sticky sessions feature is enabled for the ALB</strong> is incorrect. You cannot set Auto Scaling for the master database on Amazon Aurora. You can only manually resize the instance size of the master node.</p><p>The option that says: <strong>Create an Amazon Aurora MySQL database instance and enable Aurora Auto Scaling for the master database. Create an Auto Scaling group of Amazon EC2 instances placed behind a Network Load Balancer with the least outstanding request (LOR) routing algorithm. Ensure that the sticky sessions feature is enabled for the NLB </strong>is incorrect because it is not possible to set Auto Scaling for the master database on Amazon Aurora. You can only manually resize the instance size of the master node. Take note that a Network Load Balancer doesn't fully support the least outstanding request (LOR) routing algorithm as this is commonly used in Application Load Balancers instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.Overview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.Overview.html</a></p><p><br></p><p><strong>Check out the Amazon Aurora and AWS Elastic Load Balancing Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora/?src=udemy\">https://tutorialsdojo.com/amazon-aurora/</a></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.Overview.html",
      "https://tutorialsdojo.com/amazon-aurora/?src=udemy",
      "https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy"
    ]
  },
  {
    "id": 3,
    "question": "<p>A multinational investment bank has multiple cloud architectures across the globe. The company has a VPC in the US East region for their East Coast office and another VPC in the US West for their West Coast office. There is a requirement to establish a low latency, high-bandwidth connection between their on-premises data center in Texas and both of their VPCs in AWS.</p><p>Which of the following options should the solutions architect implement to achieve the requirement in a cost-effective manner?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up two separate VPC peering connections for the two VPCs and for the on-premises data center.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Establish a Direct Connect connection between the VPC in US East region and the on-premises data center in Texas, and then establish another Direct Connect connection between the VPC in US West region and the on-premises data center.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up an AWS VPN managed connection between the VPC in US East region and the on-premises data center in Texas.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up an AWS Direct Connect Gateway with two virtual private gateways. Launch and connect the required Private Virtual Interfaces to the Direct Connect Gateway.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>You can use an <strong>AWS Direct Connect gateway</strong> to connect your AWS Direct Connect connection over a private virtual interface to one or more VPCs in your account that are located in the same or different regions. You associate a Direct Connect gateway with the virtual private gateway for the VPC, and then create a private virtual interface for your AWS Direct Connect connection to the Direct Connect gateway. You can attach multiple private virtual interfaces to your Direct Connect gateway. A Direct Connect gateway is a globally available resource. You can create the Direct Connect gateway in any public region and access it from all other public regions.</p><p><img src=\"https://media.tutorialsdojo.com/sap_directconnect_gateway.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_directconnect_gateway.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Hence, the correct answer is:<strong><em> </em>Set up an AWS Direct Connect Gateway with two virtual private gateways. Create the required Private Virtual Interfaces to the Direct Connect Gateway.</strong></p><p>The option that says: <strong>Establish a Direct Connect connection between the VPC in US East region and the on-premises data center in Texas, and then establish another Direct Connect connection between the VPC in US West region and the on-premises data center</strong> is incorrect because establishing two separate Direct Connect connections is expensive and hence, not a cost-effective option. It is better to establish a Direct Connect gateway instead which uses one Direct Connect connection to integrate the 2 VPCs and the on-premises data center.</p><p><strong>Setting up an AWS VPN managed connection between the VPC in US East region and the on-premises data center in Texas</strong> is incorrect because a VPN Connection is a more suitable solution for low to modest bandwidth requirements which can tolerate the inherent variability in Internet-based connectivity.</p><p><strong>Setting up two separate VPC peering connections for the two VPCs and for the on-premises data center</strong> is incorrect because VPC Peering is used to connect 2 VPC’s together and not to connect your on-premises data center.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html</a></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/virtualgateways.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/virtualgateways.html</a></p><p><br></p><p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html",
      "https://docs.aws.amazon.com/directconnect/latest/UserGuide/virtualgateways.html",
      "https://tutorialsdojo.com/aws-direct-connect/?src=udemy"
    ]
  },
  {
    "id": 4,
    "question": "<p>A company has a large collection of user-submitted stock photos. An AWS Lambda function processes and extracts metadata from these photos to make a searchable catalog. The metadata is extracted depending on several rules and the output is sent to an Amazon ElastiCache for Redis cluster. The metadata extraction is done in several batches and the whole process takes about 45 minutes to complete. Whenever there is a change in the metadata extraction rules, the update process is triggered manually before the extraction process starts. As the stock photo submissions are steadily growing, the company wants to reduce the metadata extraction time for its catalog.</p><p>Which of the following options should the Solutions Architect implement to reduce the time for the metadata extraction process?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Associate the Lambda functions to an AWS Batch compute environment. Write another Lambda function that will retrieve the list of photos for processing and send each item to the job queue in the AWS Batch compute environment.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Write another Lambda function that will retrieve the list of photos for processing and send each item to an Amazon SQS queue. Configure all the Lambda extraction functions to subscribe to this SQS queue with higher batch size.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Create a workflow on AWS Step Functions that will run multiple Lambda functions in parallel. Create another workflow that will retrieve the list of photos for processing and execute the metadata extraction workflow for each photo.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Create a workflow on AWS Step Functions that will run multiple Lambda functions in parallel. Write another Lambda function that will retrieve the list of photos for processing and send each item to an Amazon SQS queue. Set this SQS queue as the input for the Step Functions workflow.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Step Functions</strong> can be used to run a serverless workflow that coordinates multiple AWS Lambda functions. AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by your business logic.</p><p><strong>AWS Lambda</strong> is a serverless compute service that lets you run code without provisioning or managing servers, creating workload-aware cluster scaling logic, maintaining event integrations, or managing runtimes. With Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code as a ZIP file or container image, and Lambda automatically and precisely allocates compute execution power and runs your code based on the incoming request or event, for any scale of traffic.</p><p><img src=\"https://media.tutorialsdojo.com/sap_step_function_lambda_function.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_step_function_lambda_function.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In this scenario, you can quickly make changes to the whole process depending on the extraction rules if you have dedicated Lambda functions for each type of metadata. Technically, you can create one Lambda function to call the other Lambda metadata extraction functions. However, it is quite challenging to orchestrate the data flow of the Lambda functions as the number of functions grow. Plus, any change in the flow of the application will require changes in multiple places, and you could end up writing the same code over and over again.</p><p>To solve this challenge, you can use AWS Step Functions. This is a serverless orchestration service that lets you easily coordinate multiple Lambda functions into flexible workflows that are easy to debug and easy to change. Step Functions will keep your Lambda functions free of additional logic by triggering and tracking each step of your application for you.</p><p>Therefore, the correct answer is: <strong>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Create a workflow on AWS Step Functions that will run multiple Lambda functions in parallel. Create another workflow that will retrieve the list of photos for processing and execute the metadata extraction workflow for each photo.</strong></p><p>The option that says: <strong>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Associate the Lambda functions to an AWS Batch compute environment. Write another Lambda function that will retrieve the list of photos for processing and send each item to the job queue in the AWS Batch compute environment</strong> is incorrect. AWS Batch only adds complexity to the solution. AWS Batch is designed to easily and efficiently run hundreds of thousands of batch computing jobs on AWS but not with Lambda functions.</p><p>The option that says: <strong>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Create a workflow on AWS Step Functions that will run multiple Lambda functions in parallel. Write another Lambda function that will retrieve the list of photos for processing and send each item to an Amazon SQS queue. Set this SQS queue as the input for the Step Functions workflow</strong> is incorrect. This might be possible if you have a Lambda Function that consumes the messages on the SQS and feeds them to the Step Function, which is not explicitly stated on this option. An SQS queue cannot be used as a direct input for an AWS Step Function workflow.</p><p>The option that says: <strong>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Write another Lambda function that will retrieve the list of photos for processing and send each item to an Amazon SQS queue. Configure all the Lambda extraction functions to subscribe to this SQS queue with higher batch size</strong> is incorrect. This may be possible but it will not work for this scenario. When a Lambda function processes the photo for a specific type of metadata, it will become invisible on the queue, and other Lambda functions will not be able to process the same photo. This defeats the purpose of multiple Lambda functions that are supposed to process the photo in parallel.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/\">https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/</a></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/connect-lambda.html\">https://docs.aws.amazon.com/step-functions/latest/dg/connect-lambda.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-step-functions-support-for-dynamic-parallelism/\">https://aws.amazon.com/blogs/aws/new-step-functions-support-for-dynamic-parallelism/</a></p><p><a href=\"https://aws.amazon.com/step-functions/use-cases/\">https://aws.amazon.com/step-functions/use-cases/</a></p><p><br></p><p><strong>Check out the AWS Step Functions Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-step-functions/?src=udemy\">https://tutorialsdojo.com/aws-step-functions/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/",
      "https://docs.aws.amazon.com/step-functions/latest/dg/connect-lambda.html",
      "https://aws.amazon.com/blogs/aws/new-step-functions-support-for-dynamic-parallelism/",
      "https://aws.amazon.com/step-functions/use-cases/",
      "https://tutorialsdojo.com/aws-step-functions/?src=udemy"
    ]
  },
  {
    "id": 5,
    "question": "<p>A company recently launched its new e-commerce platform that is hosted on its on-premises data center. The web servers connect to a MySQL database. The e-commerce platform is quickly gaining popularity, and the management is worried that the on-premises servers won’t be able to keep up with user traffic in the coming months. They decided to migrate the entire application to AWS to take advantage of the scalability of the cloud. The following are required for this migration:</p><p>- Improve the security of the application.</p><p>- Increase the reliability and availability of the application.</p><p>- Reduce the latency between the users and the application.</p><p>- Reduce the maintenance overhead after the migration to the cloud.</p><p>Which combination of options should the Solutions Architect implement to meet the company's requirements? (Select TWO.)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Auto Scaling of Amazon EC2 instances spread in two Availability Zones to host the web servers. To reduce the latency when serving content, create an AWS Global Accelerator endpoint. Migrate the database to an Amazon RDS MySQL instance.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Auto Scaling of Amazon EC2 instances spread in two Availability Zones to host the web servers and the highly available MySQL database cluster in a master and slave configuration.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon S3 bucket to store and host the static contents. To reduce the latency when serving content, set this bucket as the origin for an Amazon CloudFront distribution. Create AWS WAF rules to block common web exploits.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon S3 bucket to store the static contents and enable website hosting. To reduce the latency when serving content, enable S3 Transfer Acceleration. Create AWS WAF rules to block common web exploits.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Create an Auto Scaling of Amazon EC2 instances spread in two Availability Zones to host the web servers. Use an Amazon RDS for MySQL with Multi-AZ enabled as the database.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p><strong>Amazon RDS Multi-AZ</strong> deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure and is engineered to be highly reliable.</p><p>Adding <strong>Amazon EC2 Auto Scaling</strong> to your application architecture is one way to maximize the benefits of the AWS Cloud. When you use Amazon EC2 Auto Scaling, your applications gain the following benefits:</p><p>- Better fault tolerance. Amazon EC2 Auto Scaling can detect when an instance is unhealthy, terminate it, and launch an instance to replace it. You can also configure Amazon EC2 Auto Scaling to use multiple Availability Zones. If one Availability Zone becomes unavailable, Amazon EC2 Auto Scaling can launch instances in another one to compensate.</p><p>- Better availability. Amazon EC2 Auto Scaling helps ensure that your application always has the right amount of capacity to handle the current traffic demand.</p><p>- Better cost management. Amazon EC2 Auto Scaling can dynamically increase and decrease capacity as needed. Because you pay for the EC2 instances you use, you save money by launching instances when they are needed and terminating them when they aren't.</p><p><strong>Amazon CloudFront</strong> works seamlessly with <strong>Amazon Simple Storage Service (S3)</strong> to accelerate the delivery of your web content and reduce the load on your origin servers. The CloudFront edge locations will cache and deliver your content closer to your users to reduce latency and offload capacity from your origin. CloudFront will also restrict access to your S3 bucket to only CloudFront endpoints rendering your content and application more secure and performant.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.</p><p>Hence, the correct answers are:</p><p>The option that says: <strong>Create an Auto Scaling of Amazon EC2 instances spread in two Availability Zones to host the web servers. Use an Amazon RDS for MySQL with Multi-AZ enabled as the database </strong>is correct. The Auto Scaling group ensures there are enough web servers to answer user requests. Spreading the instances on at least two AZ and enabling Multi-AZ on the database ensure that your application is protected if one AZ in AWS goes down.</p><p>The option that says: <strong>Create an Amazon S3 bucket to store and host the static contents. To reduce the latency when serving content, set this bucket as the origin for an Amazon CloudFront distribution. Create AWS WAF rules to block common web exploits</strong> is correct. Amazon S3 can serve static content and Amazon CloudFront can cache frequently requested content, which greatly improves latency. AWS WAF has default rules that you can enable to block common web exploits to improve your application security in AWS.</p><p>The option that says: <strong>Create an Auto Scaling of Amazon EC2 instances spread in two Availability Zones to host the web servers and the highly available MySQL database cluster in a master and slave configuration</strong> is incorrect. Spreading EC2 instances across multiple AZs is generally good for availability; however, hosting the database on the EC2 instances requires significant management overhead. It is primarily recommended to use Amazon RDS for the database.</p><p>The option that says: <strong>Create an Amazon S3 bucket to store the static contents and enable website hosting. To reduce the latency when serving content, enable S3 Transfer Acceleration. Create AWS WAF rules to block common web exploits</strong> is incorrect. Amazon S3 can typically serve static content, but S3 Transfer Acceleration is used for accelerating transfers going to and from the S3 bucket. Caching with CloudFront would typically be even faster, as it avoids repeated access to the S3 bucket for cached requests.</p><p>The option that says: <strong>Create an Auto Scaling of Amazon EC2 instances spread in two Availability Zones to host the web servers. To reduce the latency when serving content, create an AWS Global Accelerator endpoint. Migrate the database to an Amazon RDS MySQL instance</strong> is incorrect. Without enabling Multi-AZ, your RDS is not protected in the event of a crash or failure. When you promote read-replicas to become master, a small downtime is required. This reduces the application availability.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p><p><a href=\"https://aws.amazon.com/cloudfront/getting-started/S3/\">https://aws.amazon.com/cloudfront/getting-started/S3/</a></p><p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p><p><br></p><p><strong>Check out these Amazon RDS, Amazon S3, and Amazon CloudFront Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html",
      "https://aws.amazon.com/cloudfront/getting-started/S3/",
      "https://aws.amazon.com/waf/",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy"
    ]
  },
  {
    "id": 6,
    "question": "<p>A company has a suite of IBM products in its on-premises data centers, such as IBM WebSphere, IBM MQ, and IBM DB2 servers. The solutions architect has been tasked to migrate all of the current systems to the AWS Cloud in the most cost-effective way and improve the availability of the cloud infrastructure.</p><p>Which of the following options is the MOST suitable solution that the solutions architect should implement to meet the company's requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AWS Database Migration Service (DMS) and the AWS Schema Conversion Tool (SCT) to convert, re-architect, and migrate the IBM Db2 database to Amazon Aurora. Set up an Auto Scaling group of EC2 instances with an ELB in front to migrate and re-host your IBM WebSphere. Re-host and migrate the IBM MQ service to Amazon SQS FIFO Queue.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the AWS Database Migration Service (DMS) and the AWS Schema Conversion Tool (SCT) to convert, migrate, and re-architect the IBM Db2 database to Amazon Aurora. Set up an Auto Scaling group of EC2 instances with an ELB in front to migrate and re-host your IBM WebSphere. Migrate and re-platform IBM MQ to Amazon MQ in a phased approach.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use the AWS Application Migration Service to migrate your servers to AWS. Set up Amazon EC2 instances to re-host your IBM WebSphere and IBM DB2 servers separately. Re-host and migrate the IBM MQ service to Amazon SQS Standard Queue.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the AWS Application Migration Service to migrate your servers to AWS. Upload the IBM licenses to AWS License Manager and use the licenses when configuring Amazon EC2 instances to re-host your IBM WebSphere and IBM DB2 servers separately. Re-host and migrate the IBM MQ service to Amazon MQ.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p>On Amazon EC2, you can run many of the proven IBM technologies with which you're already familiar. You may be eligible to bring many of your own IBM software and licenses (BYOSL) to run on Amazon EC2 instances.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ibm_byol_infra.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ibm_byol_infra.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>AWS Database Migration Service (DMS)</strong> and the <strong>AWS Schema Conversion Tool (SCT)</strong> can allow you to convert and migrate IBM Db2 databases on Linux, UNIX, and Windows (Db2 LUW) to any DMS-supported target. This can accelerate your move to the cloud by allowing you to migrate more of your legacy databases. The new Db2 LUW source adds to the existing list of the relational database, NoSQL, and object store sources supported by DMS. If the database migration target is Amazon Aurora, Amazon Redshift, or Amazon DynamoDB, you can use DMS free for six months.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ibmmq_infra.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ibmmq_infra.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Amazon MQ</strong> is a managed message broker service from AWS that makes it easy to set up and operate message brokers in the cloud. To migrate and re-platform your on-premises IBM MQ to Amazon MQ, you can opt for a phased approach for the migration process. You can move the producers (senders) and consumers (receivers) in phases from your on-premises to the cloud. This process uses Amazon MQ as the message broker and decommissions IBM MQ once all producers/consumers have been successfully migrated.</p><p>Hence, the correct option is: <strong>Use the AWS Database Migration Service (DMS) and the AWS Schema Conversion Tool (SCT) to convert, migrate and re-architect the IBM Db2 database to Amazon Aurora. Set up an Auto Scaling group of EC2 instances with an ELB in front to migrate and re-host your IBM WebSphere. Migrate and re-platform IBM MQ to Amazon MQ in phased approach.</strong></p><p>The option that says: <strong>Use the AWS Application Migration Service to migrate your servers to AWS. Set up Amazon EC2 instances to re-host your IBM WebSphere and IBM DB2 servers separately. Re-host and migrate the IBM MQ service to Amazon SQS Standard Queue</strong> is incorrect because the AWS Application Migration Service simply automates the migration of your on-premises virtual machines to the AWS Cloud. Moreover, you can't re-host and migrate your IBM MQ service to Amazon SQS. A better solution is to re-platform IBM MQ to Amazon MQ.</p><p>The option that says:<strong> Use the AWS Application Migration Service to migrate your servers to AWS. Upload the IBM licenses to AWS License Manager and use the licenses when configuring Amazon EC2 instances to re-host your IBM WebSphere and IBM DB2 servers separately. Re-host and migrate the IBM MQ service to Amazon MQ </strong>is incorrect because the AWS Application Migration Service simply automates the migration of your on-premises virtual machines to the AWS Cloud. AWS License Manager is used to create customized licensing rules that emulate the terms of their licensing agreements and then enforce these rules. It is not used for storing software licenses. This service is limited to migrating virtual machines (VMs) and in addition, you cannot directly re-host and migrate your IBM MQ to Amazon MQ since these are two completely different systems. Instead, you have to re-platform IBM MQ to Amazon MQ in a phased approach.</p><p>The option that says: <strong>Use the AWS Database Migration Service (DMS) and the AWS Schema Conversion Tool (SCT) to convert, re-architect and migrate the IBM Db2 database to Amazon Aurora. Set up an Auto Scaling group of EC2 instances with an ELB in front to migrate and re-host your IBM WebSphere. Re-host and migrate the IBM MQ service to Amazon SQS FIFO Queue</strong> is incorrect. Although the use of AWS Database Migration Service (DMS) and the AWS Schema Conversion Tool (SCT) is correct, the migration process for your IBM MQ is wrong. Amazon Simple Queue Service (SQS) is just a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. There are a lot of features in IBM MQ that are not available in Amazon SQS, whether it is a Standard or a FIFO Queue. You have to re-platform IBM MQ to Amazon MQ instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/quickstart/architecture/ibm-websphere-liberty/\">https://aws.amazon.com/quickstart/architecture/ibm-websphere-liberty/</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/04/aws-dms-supports-ibm-db2-as-a-source/\">https://aws.amazon.com/about-aws/whats-new/2018/04/aws-dms-supports-ibm-db2-as-a-source/</a></p><p><a href=\"https://aws.amazon.com/quickstart/architecture/ibm-mq/\">https://aws.amazon.com/quickstart/architecture/ibm-mq/</a></p><p><a href=\"https://developer.ibm.com/messaging/2018/09/26/ibm-mq-available-managed-service-aws/\">https://developer.ibm.com/messaging/2018/09/26/ibm-mq-available-managed-service-aws/</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/migrating-from-ibm-mq-to-amazon-mq-using-a-phased-approach\">https://aws.amazon.com/blogs/compute/migrating-from-ibm-mq-to-amazon-mq-using-a-phased-approach</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Database Migration Service and AWS Application Migration Service:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-database-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-database-migration-service/</a></p><p><a href=\"https://tutorialsdojo.com/aws-application-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-application-migration-service/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/quickstart/architecture/ibm-websphere-liberty/",
      "https://aws.amazon.com/about-aws/whats-new/2018/04/aws-dms-supports-ibm-db2-as-a-source/",
      "https://aws.amazon.com/quickstart/architecture/ibm-mq/",
      "https://developer.ibm.com/messaging/2018/09/26/ibm-mq-available-managed-service-aws/",
      "https://aws.amazon.com/blogs/compute/migrating-from-ibm-mq-to-amazon-mq-using-a-phased-approach",
      "https://tutorialsdojo.com/aws-database-migration-service/?src=udemy",
      "https://tutorialsdojo.com/aws-application-migration-service/?src=udemy",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 7,
    "question": "<p>A digital advertising startup runs an ad-supported photo-sharing website that has users around the globe. The startup is using Amazon S3 to serve photos to website users. Several weeks later, the solutions architect found out that third-party sites have been linking to the photos on the company S3 bucket which is causing losses in overall financial ad revenue. Some users are also reporting that the photos are taking too much time to load.</p><p>Which of the following options is an effective method to mitigate this security flaw and to improve the performance of the photo-sharing website?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use CloudFront to distribute static content and block the IP addresses of the other websites that are illegally linking the photos to their own websites.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Store photos on an EBS volume of the web server with data encryption enabled.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Remove public read access from the S3 bucket. Use CloudFront as the global content delivery network (CDN) service for the photos and use Signed URLs with expiry dates.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Block the IPs of the offending websites in Security Groups and use S3 Cross-Region Replication (CRR).",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon CloudFront</strong> is a global content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to your viewers with low latency and high transfer speeds. CloudFront is integrated with AWS - including physical locations that are directly connected to the AWS global infrastructure, as well as software that works seamlessly with services including AWS Shield for DDoS mitigation, Amazon S3, Elastic Load Balancing or Amazon EC2 as origins for your applications, and Lambda@Edge to run custom code close to your viewers.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_oia.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_oia.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content. These additional information appear in a policy statement, which is based on either a canned policy or a custom policy.</p><p>In this scenario, the main issue is that there are other websites that are illegally using the photos hosted from your S3 bucket. The secondary issue is the slow loading times of the photos. The use of CloudFront as a CDN with Signed URLs is the best solution for this situation.</p><p>Therefore, the correct answer is: <strong>Remove public read access from the S3 bucket. Use CloudFront as the global content delivery network (CDN) service for the photos and use Signed URLs with expiry dates</strong>.</p><p>The option that says: <strong>Use CloudFront to distribute static content and block the IP addresses of the other websites that are illegally linking the photos to their own websites</strong> is incorrect. Although you can block the offending IP addresses of the websites that illegally use your photos, other attackers can still use a different IP address and use your photos without your permission.</p><p>The option that says: <strong>Store photos on an EBS volume of the web server with data encryption enabled</strong> is incorrect. An EBS volume is not a scalable storage solution and is not suitable in hosting static assets for a public website.</p><p>The option that says: <strong>Block the IPs of the offending websites in Security Groups and use S3 Cross-Region Replication (CRR)</strong> is incorrect. In a Security Group, you can specify and allow rules, but not deny rules. In addition, S3 Cross-Region Replication (CRR) will not provide the security needed in this case. Take note: although S3 Cross-Region Replication replicates the bucket to another region, it is still better to use CloudFront to cover all of the global regions.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html</a></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professiona</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html",
      "https://aws.amazon.com/cloudfront/",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 8,
    "question": "<p>A media company recently launched a web service that allows users to upload and share short videos. Currently, the web servers are hosted on an Auto Scaling group of Amazon EC2 instances in which the videos are processed and stored in the EBS volumes. Each uploaded video sends a message on the Amazon SQS queue, which is also processed by an Auto Scaling group of Amazon EC2 instances. The company relies on third-party software to analyze and categorize the videos. The website also contains static content that has variable user traffic. The company wants to re-architecture the application to reduce costs, reduce dependency on third-party software, and reduce management overhead by leveraging AWS-managed services.</p><p>Which of the following solutions will meet the company's requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Reduce operational overhead by using AWS Elastic Beanstalk to provision the Auto Scaling group of EC2 instances for the web servers and the Amazon SQS queue consumers. Use Amazon Rekognition to analyze and categorize the videos instead of the third-party software. Store the videos and static contents on Amazon S3 buckets.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon S3 bucket with website hosting enabled to host the web application. Store the videos and static content on a separate S3 bucket. Configure S3 event notification to send messages to an Amazon SQS queue for each video upload event. Have AWS Lambda poll the Amazon SQS queue for messages and invoke a Lambda function that calls the Amazon Rekognition API to analyze and categorize the videos.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon EFS volume to store the videos and static content. Mount the volume on all EC2 instances of the web application. Have AWS Lambda poll the Amazon SQS queue for messages and invoke a Lambda function that calls the Amazon Rekognition API to analyze and categorize the videos.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon ECS Fargate cluster and use containers to host the web application. Create an Auto Scaling group of Amazon EC2 Spot instances to process the SQS queue. Use Amazon Rekognition to analyze and categorize the videos instead of the third-party software. Store the videos and static contents on Amazon S3 buckets.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Fargate</strong> is a technology that you can use with Amazon ECS to run containers without having to manage servers or clusters of Amazon EC2 instances. With AWS Fargate, you no longer have to provision, configure, or scale clusters of virtual machines to run containers. This removes the need to choose server types, decide when to scale your clusters, or optimize cluster packing. Fargate makes it easy for you to focus on building your applications.</p><p>When you run your tasks and services with the Fargate launch type, you package your application in containers, specify the CPU and memory requirements, define networking and IAM policies, and launch the application.</p><p><strong>Amazon Rekognition</strong> makes it easy to add image and video analysis to your applications. You just provide an image or video to the Amazon Rekognition API, and the service can identify objects, people, text, scenes, and activities. It can detect any inappropriate content as well. Amazon Rekognition also provides highly accurate facial analysis, face comparison, and face search capabilities. You can detect, analyze, and compare faces for a wide variety of use cases, including user verification, cataloging, people counting, and public safety.</p><p>Amazon Rekognition is based on the same proven, highly scalable, deep learning technology developed by Amazon’s computer vision scientists to analyze billions of images and videos daily. It requires no machine learning expertise to use. Amazon Rekognition includes a simple, easy-to-use API that can quickly analyze any image or video file that’s stored in Amazon S3.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_rekognition_screen.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_rekognition_screen.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can use <strong>Amazon S3</strong> to host a static website. On a static website, individual webpages include static content. They might also contain client-side scripts. By contrast, a dynamic website relies on server-side processing, including server-side scripts such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting, but AWS has other resources for hosting dynamic websites. This makes Amazon S3 suitable for hosting static website contents.</p><p>A <strong>Spot Instance</strong> is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone is set by Amazon EC2, and is adjusted gradually based on the long-term supply of and demand for Spot Instances. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price.</p><p>Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted. For example, Spot Instances are well-suited for data analysis, batch jobs, background processing, and optional tasks.</p><p>Therefore, the correct answer is: <strong>Create an Amazon ECS Fargate cluster and use containers to host the web application. Create an Auto Scaling group of Amazon EC2 Spot instances to process the SQS queue. Use Amazon Rekognition to analyze and categorize the videos instead of the third-party software. Store the videos and static contents on Amazon S3 buckets. </strong>Using an ECS Fargate cluster reduces the overhead of managing EC2 instances and using Spot instances to process the SQS queue significantly reduces the operating costs. Amazon Rekognition is designed to analyze videos which you can use to categorize them. Amazon S3 bucket serves as durable and cost-effective storage for the uploaded videos.</p><p>The option that says: <strong>Create an Amazon EFS volume to store the videos and static content. Mount the volume on all EC2 instances of the web application. Have AWS Lambda poll the Amazon SQS queue for messages and invoke a Lambda function that calls the Amazon Rekognition API to analyze and categorize the videos</strong> is incorrect. Amazon S3 is more cost-effective than using EFS. This also increases management overhead as you need to configure the EFS mount point on all your EC2 instances.</p><p>The option that says: <strong>Create an Amazon S3 bucket with website hosting enabled to host the web application. Store the videos and static content on a separate S3 bucket. Configure S3 event notification to send messages to an Amazon SQS queue for each video upload event. Have AWS Lambda poll the Amazon SQS queue for messages and invoke a Lambda function that calls the Amazon Rekognition API to analyze and categorize the videos</strong> is incorrect. The scenario states that the videos are processed and then stored in EBS volumes, suggesting that the service is dynamic in nature. Amazon S3's static website hosting does not provide server-side processing, which is necessary for dynamic websites. Therefore, hosting a web application on an Amazon S3 bucket with website hosting enabled may not be a suitable option.</p><p>The option that says: <strong>Reduce operational overhead by using AWS Elastic Beanstalk to provision the Auto Scaling group of EC2 instances for the web servers and the Amazon SQS queue consumers. Use Amazon Rekognition to analyze and categorize the videos instead of the third-party software. Store the videos and static contents on Amazon S3 buckets</strong> is incorrect. Although Elastic Beanstalk can manage the EC2 instances for you, there is no mention to use Spot instances for processing the SQS queue which can further reduce the operating costs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p><p><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html\">https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html</a></p><p><br></p><p><strong>Check out these AWS Fargate and Amazon Rekognition Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-fargate/?src=udemy\">https://tutorialsdojo.com/aws-fargate/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-rekognition/?src=udemy\">https://tutorialsdojo.com/amazon-rekognition/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html",
      "https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html",
      "https://tutorialsdojo.com/aws-fargate/?src=udemy",
      "https://tutorialsdojo.com/amazon-rekognition/?src=udemy"
    ]
  },
  {
    "id": 9,
    "question": "<p>A call center company has recently adopted a hybrid architecture in which they need a predictable network performance and reduced bandwidth costs to connect their data center and their AWS Cloud. You have implemented two AWS Direct Connect connections between your data center and AWS to have a stable and highly available network performance. After a recent IT financial audit, it was decided to review the current implementation and replace it with a more cost-effective option.</p><p>Which of the following connectivity setup would you recommend for this scenario?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A single AWS Direct Connect connection and enable the built-in failover feature</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS VPN CloudHub to connect your data center network to Amazon VPC</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Setup a Hardware VPN on your datacenter and set it to use the Direct Connect for its connection</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A single AWS Direct Connect and an AWS managed VPN connection to connect your data center with Amazon VPC</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>The scenario requires you to revise the current setup of using two Direct Connect connections being used by the company. Since Direct Connect does not provide any redundancy for its connection, it is recommended to set up at least two connections for high availability. However, this setup is expensive as you will be charged for the two Direct Connect connections. Usually, the second connection is used only when the main connection fails which rarely happens.</p><p>To maintain high availability, but reduce the costs, you can use a single <strong><em>AWS Direct Connect</em></strong> to create a dedicated private connection from your data center network to your Amazon VPC, and then combine this connection with an <strong><em>AWS managed VPN connection</em></strong> to create an IPsec-encrypted connection as a backup connection. If the Direct Connect connection fails, you still have a managed VPN to connect to your Amazon VPC, albeit with a slower connection. This will suffice until your Direct Connect connection is restored.</p><p><img src=\"https://media.tutorialsdojo.com/sap_directconnect_vpn.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_directconnect_vpn.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>A single AWS Direct Connect and an AWS managed VPN connection to connect your data center with Amazon VPC.</strong> The AWS Direct Connect connection will provide the high speed bandwidth network required while having the VPN as the slower, backup link in case the main Direct Connect link fails.</p><p>The option that says: <strong>A single AWS Direct Connect connection and enable the built-in failover feature</strong> is incorrect. AWS Direct Connect does not have a built-in failover feature.</p><p>The option that says: <strong>Setup a Hardware VPN on your datacenter and set it to use the Direct Connect for its connection</strong> is incorrect. If you implement this, your VPN will also fail if Direct Connect fails.</p><p>The option that says: <strong>Use AWS VPN CloudHub to connect your data center network to Amazon VPC</strong> is incorrect. You still need a dedicated high bandwidth network provided by Direct Connect.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpn-connections.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpn-connections.html</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/\">https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><br></p><p><strong>S3 Transfer Acceleration vs Direct Connect vs VPN vs Snowball vs Snowmobile:</strong></p><p><a href=\"https://tutorialsdojo.com/s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/?src=udemy\">https://tutorialsdojo.com/s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpn-connections.html",
      "https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy",
      "https://tutorialsdojo.com/s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/?src=udemy",
      "https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy"
    ]
  },
  {
    "id": 10,
    "question": "A supermarket chain is planning to launch an online shopping website to allow its loyal shoppers to buy their groceries online. Since there are a lot of online shoppers at any time of the day, the website should be highly available 24/7 and fault tolerant. \n\nWhich of the following options provides the best architecture that meets the above requirement?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Deploy the website across 2 Availability Zones with Auto Scaled EC2 instances each and an RDS instance deployed with Read Replicas in two separate Availability Zones.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy the website across 3 Availability Zones with Auto Scaled EC2 instances behind an Application Load Balancer and one RDS instance deployed with Read Replicas in the two separate Availability Zones.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy the website across 3 Availability Zones with Auto Scaled EC2 instances behind an Application Load Balancer and a RDS configured with Multi-AZ Deployments.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Deploy the website across 2 Availability Zones with Auto Scaled EC2 instances behind an Application Load Balancer and an Amazon RDS database running in a single Reserved EC2 Instance.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>For high availability, it is best to always choose an RDS instance configured with Multi-AZ deployments. You should also deploy your application across multiple Availability Zones to improve fault tolerance. AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it’s easy to set up application scaling for multiple resources across multiple services in minutes.</p><p><img src=\"https://media.tutorialsdojo.com/sap_multiaz_multiregion_readreplica.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_multiaz_multiregion_readreplica.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/#Availability_Zones\">Availability Zone</a> (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of <a href=\"https://aws.amazon.com/rds/aurora/\">Amazon Aurora</a>), so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.</p><p>Hence, the correct answer is the option that says: <strong>Deploy the website across 3 Availability Zones with Auto Scaled EC2 instances behind an Application Load Balancer and an RDS configured with Multi-AZ Deployments.</strong></p><p>The option that says: <strong>Deploy the website across 2 Availability Zones with Auto Scaled EC2 instances each and an RDS instance deployed with Read Replicas in two separate Availability Zones</strong> is incorrect because a Read Replica is primarily used to improve the scalability of the database. This architecture will neither be highly available 24/7 nor fault-tolerant in the event of an AZ outage.</p><p>The option that says: <strong>Deploy the website across 3 Availability Zones with Auto Scaled EC2 instances behind an Application Load Balancer and one RDS instance deployed with Read Replicas in the two separate Availability Zones</strong> is incorrect. Although the EC2 instances are highly available, the database tier is not. You have to use an Amazon RDS database with Multi-AZ configuration.</p><p>The option that says: <strong>Deploy the website across 2 Availability Zones with Auto Scaled EC2 instances behind an Application Load Balancer and an Amazon RDS database running in a single Reserved EC2 Instance</strong> is incorrect. With this architecture, the single Reserved Instance is only deployed in a single AZ. In the event of an AZ outage, the entire database will be unavailable.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/autoscaling/\">https://aws.amazon.com/autoscaling/</a></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/about-aws/global-infrastructure/regions_az/#Availability_Zones",
      "https://aws.amazon.com/rds/aurora/",
      "https://aws.amazon.com/autoscaling/",
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://tutorialsdojo.com/aws-auto-scaling/?src=udemy"
    ]
  },
  {
    "id": 11,
    "question": "<p>A large media company based in Los Angeles, California, operates a MySQL RDS instance within an AWS VPC. The company has a custom analytics application running in its on-premises data center that requires read-only access to the database. The company aims to replicate the data from the MySQL RDS instance in AWS to a MySQL instance located on-premises to serve as the read-only endpoint for this analytics application.</p><p>Which of the following options is the most secure way of performing this replication?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an IPSec VPN connection using either OpenVPN or VPN/VGW through the Virtual Private Cloud service. Prepare an instance of MySQL running external to Amazon RDS. Configure the MySQL RDS instance to be the replication source. Use <code>mysqldump</code> to transfer the database from the Amazon RDS instance to the on-premises MySQL instance and start the replication from the Amazon RDS Read Replica.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>RDS cannot replicate to an on-premises database server. Instead, configure the RDS instance to replicate to an EC2 instance with core MySQL and then configure replication over a secure VPN/VPG connection.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure the RDS instance as the master and enable replication over the open Internet using an SSL endpoint to the on-premises server. Use&nbsp; <code>mysqldump</code> to transfer the database from the Amazon S3 to the on-premises MySQL instance and start the replication.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a Data Pipeline that exports the MySQL data each night and securely downloads the data from an S3 HTTPS endpoint. Use&nbsp; <code>mysqldump</code> to transfer the database from the Amazon S3 to the on-premises MySQL instance and start the replication.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>Amazon supports <strong>Internet Protocol security (IPsec) VPN</strong> connections. IPsec is a protocol suite for securing Internet Protocol (IP) communications by authenticating and encrypting each IP packet of a data stream. Data transferred between your VPC and datacenter routes over an encrypted VPN connection to help maintain the confidentiality and integrity of data in transit. An Internet gateway is not required to establish a hardware VPN connection.</p><p>You can set up replication between an <strong>Amazon RDS MySQL</strong> (or MariaDB DB instance) that is running in AWS and a MySQL (or MariaDB instance) to your on-premises data center. A read replica is a read-only copy of a MySQL database instance that is kept synchronized with the source instance. Read replicas can be used to offload read traffic from the primary instance, improving performance and scalability.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-RDS-replication-On-premise-05-Aug-2024.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/public/td-RDS-replication-On-premise-05-Aug-2024.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>To allow communication between RDS and to your on-premises network, you must first set up a VPN or an AWS <em>Direct Connect</em> connection. Once that is done, just follow the below steps to perform the replication:</p><ol><li><p>Set up a MySQL database server in the on-premises environment, which will act as the read replica for the RDS instance.</p></li><li><p>Configure the RDS MySQL instance as the replication source (master) for the on-premises MySQL instance.</p></li><li><p>Use the <code><strong>mysqldump</strong></code> utility to transfer the initial data from the Amazon RDS instance to the on-premises MySQL instance. This step is necessary to establish the initial replication state.</p></li><li><p>After the initial data transfer, start the replication process from the on-premises MySQL instance, which will act as a read replica of the Amazon RDS instance.</p></li></ol><p>By following this approach, the data replication traffic between the RDS instance and the on-premises MySQL server is securely transmitted over the IPSec VPN connection, ensuring the confidentiality and integrity of the data during transit, rather than exposing the RDS instance directly to the internet or using less secure methods for data transfer.</p><p>Hence, the correct answer is <strong>Create an IPSec VPN connection using either OpenVPN or VPN/VGW through the Virtual Private Cloud service. Prepare an instance of MySQL running external to Amazon RDS. Configure the MySQL RDS instance to be the replication source. Use </strong><code><strong>mysqldump</strong></code><strong> to transfer the database from the Amazon RDS instance to the on-premises MySQL instance and start the replication from the Amazon RDS Read Replica </strong>because it is feasible to set up the secure IPSec VPN connection between the on-premises server and AWS VPC using the VPN/Gateways.</p><p>The option that says: <strong>Configure the RDS instance as the master and enable replication over the open Internet using an SSL endpoint to the on-premises server. Use </strong><code><strong>mysqldump</strong></code><strong> to transfer the database from the Amazon S3 to the on-premises MySQL instance and start the replication</strong> is incorrect because SSL endpoint cannot be utilized here as it is only used to securely access the database.</p><p>The option that says: <strong>RDS cannot replicate to an on-premises database server. Instead, configure the RDS instance to replicate to an EC2 instance with core MySQL and then configure replication over a secure VPN/VPG connection</strong> is incorrect because you do not need to establish a secure VPN/VPG connection in the first place as EC2 and RDS are both in AWS Cloud.</p><p>The option that says: <strong>Create a Data Pipeline that exports the MySQL data each night and securely downloads the data from an S3 HTTPS endpoint. Use </strong><code><strong>mysqldump</strong></code><strong> to transfer the database from the Amazon S3 to the on-premises MySQL instance and start the replication</strong> is incorrect because Data Pipeline is primarily for batch jobs and is not suitable for this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Exporting.NonRDSRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Exporting.NonRDSRepl.html</a></p><p><br></p><p><strong>Check out these Amazon RDS and Amazon VPC Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Exporting.NonRDSRepl.html",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 12,
    "question": "<p>A company wants to have a secure content management solution that can be accessed by its external custom applications via API calls. The solutions architect has been instructed to create the infrastructure design. The solution should enable users to upload documents as well as download a specific version or the latest version of a document. There is also a requirement to enable customer administrators to simply submit an API call that can roll back changes to existing files sent to the system.</p><p>Which of the following options is the MOST secure and suitable solution that the solutions architect should implement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon S3 with Versioning and Server Access Logging enabled. Set up an IAM role and access policy for each customer application. Encrypt all documents using client-side encryption for enhanced data security. Share the encryption keys to all customers to unlock the documents. Develop a rollback feature to replace the current document version with the previous version from Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon WorkDocs for document storage and utilize its user access management, version control, and built-in encryption. Integrate the Amazon WorkDocs Content Manager to the external custom applications. Develop a rollback feature to replace the current document version with the previous version from Amazon WorkDocs.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use S3 with Server Access Logging enabled. Set up an IAM role and access policy for each customer application. Use client-side encryption to encrypt customer files then share the KMS Key ID and the client-side master key to all customers in order to access the CMS.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon EFS for object storage and enable data encryption in transit with TLS. Store unique customer managed keys in AWS KMS. Set up IAM roles and IAM access policies for EFS to specify separate encryption keys for each customer application. Utilize file locking and file versioning features in EFS to roll back changes to existing files stored in the CMS.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon WorkDocs</strong> is a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content, and because it’s stored centrally on AWS, access it from anywhere on any device. Amazon WorkDocs makes it easy to collaborate with others, and lets you easily share content, provide rich feedback, and collaboratively edit documents. You can use Amazon WorkDocs to retire legacy file share infrastructure by moving file shares to the cloud. Amazon WorkDocs lets you integrate with your existing systems, and offers a rich API so that you can develop your own content-rich applications. Amazon WorkDocs is built on AWS, where your content is secured on the world's largest cloud infrastructure.</p><p><img src=\"https://media.tutorialsdojo.com/sap_amazon_workdocs_overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_amazon_workdocs_overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Amazon WorkDocs Content Manager is a high-level utility tool that uploads content or downloads it from an Amazon WorkDocs site. It can be used for both administrative and user applications. For user applications, a developer must construct the Amazon WorkDocs Content Manager with anonymous AWS credentials and an authentication token. For administrative applications, the Amazon WorkDocs client must be initialized with AWS Identity and Access Management (IAM) credentials. In addition, the authentication token must be omitted in subsequent API calls.</p><p>Hence, the correct answer in this scenario is: <strong>Use Amazon WorkDocs for document storage and utilize its user access management, version control, and built-in encryption. Integrate the Amazon WorkDocs Content Manager to the external custom applications. Develop a rollback feature to replace the current document version with the previous version from Amazon WorkDocs.</strong></p><p>The option that says: <strong>Use Amazon S3 with Versioning and Server Access Logging enabled. Set up an IAM role and access policy for each customer application. Encrypt all documents using client-side encryption for enhanced data security. Share the encryption keys to all customers to unlock the documents. Develop a rollback feature to replace the current document version with the previous version from Amazon S3</strong> is incorrect. Although you can use Amazon S3, sharing the encryption keys to all customers just to unlock the documents is a security risk.</p><p>The option that says: <strong>Use S3 with Server Access Logging enabled. Set up an IAM role and access policy for each customer application. Use client-side encryption to encrypt customer files then share the KMS Key ID and the client-side master key to all customers in order to access the CMS</strong> is incorrect because the S3 bucket being used has only enabled Server Access Logging and not Versioning. In addition, it is also a security risk to share the KMS Key ID and the client-side master key with all customers.</p><p>The option that says: <strong>Use Amazon EFS for object storage and enable data encryption in transit with TLS. Store unique customer managed keys in AWS KMS. Set up IAM roles and IAM access policies for EFS to specify separate encryption keys for each customer application. Utilize file locking and file versioning features in EFS to roll back changes to existing files stored in the CMS</strong> is incorrect because EFS is primarily used as a file system and not for object storage. Although EFS has file locking capabilities, it does not have file versioning features.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/workdocs/\">https://aws.amazon.com/workdocs/</a></p><p><a href=\"https://docs.aws.amazon.com/workdocs/latest/developerguide/content_manager_constructing.html\">https://docs.aws.amazon.com/workdocs/latest/developerguide/content_manager_constructing.html</a></p><p><a href=\"https://docs.aws.amazon.com/workdocs/latest/developerguide/content_manager_downloading.html\">https://docs.aws.amazon.com/workdocs/latest/developerguide/content_manager_downloading.html</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/workdocs/",
      "https://docs.aws.amazon.com/workdocs/latest/developerguide/content_manager_constructing.html",
      "https://docs.aws.amazon.com/workdocs/latest/developerguide/content_manager_downloading.html"
    ]
  },
  {
    "id": 13,
    "question": "A data analytics company is running a Redshift data warehouse for one of its major clients. In compliance with the Business Continuity Program of the client, they need to provide a Recovery Point Objective of 24 hours and a Recovery Time Objective of 1 hour. The data warehouse should be available even in the event that the entire AWS Region is down. \n\nWhich of the following is the most suitable configuration for this scenario?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "No additional configuration needed. Redshift is configured with automatic snapshot by default.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure Redshift to<strong> </strong>use Cross-Region Replication (CRR) and in case of system failure, failover to the backup region and manually copy the snapshot from the primary region to the secondary region.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure Redshift to have automatic snapshots and do a cross-region snapshot copy to automatically replicate the current production cluster to the disaster recovery region.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Enable Redshift replication from the cluster running in the primary region to the cluster running in the secondary region. Change the DNS endpoint to the secondary cluster's primary node in case of system failures in the primary region.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>When automated snapshots are enabled for a cluster, <strong>Amazon Redshift</strong> periodically takes snapshots of that cluster, usually every eight hours or following every 5 GB per node of data changes, or whichever comes first. Automated snapshots are enabled by default when you create a cluster. These snapshots are deleted at the end of a retention period. The default retention period is one day, but you can modify it by using the Amazon Redshift console or programmatically by using the Amazon Redshift API.</p><p>When you enable Amazon Redshift to automatically copy snapshots to another region, you specify the destination region where you want snapshots to be copied. In the case of automated snapshots, you can also specify the retention period that they should be kept in the destination region. After an automated snapshot is copied to the destination region and it reaches the retention time period there, it is deleted from the destination region, keeping your snapshot usage low. You can change this retention period if you need to keep the automated snapshots for a shorter or longer period of time in the destination region.</p><p><img src=\"https://media.tutorialsdojo.com/sap_redshift_snapshot.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_redshift_snapshot.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore the correct answer is: <strong>Configure Redshift to have automatic snapshots and do a cross-region snapshot copy to automatically replicate the current production cluster to the disaster recovery region.</strong> The automatic snapshots feature is automatically enabled by default. You can then configure Amazon Redshift to automatically copy snapshots (automated or manual) for a cluster to another AWS Region.</p><p>The option that says: <strong>Configure Redshift to use Cross-Region Replication (CRR) and in case of system failure, failover to the backup region and manually copy the snapshot from the primary region to the secondary region</strong> is incorrect. Amazon Redshift only has cross-region backup feature (using snapshots), not Cross-Region Replication.</p><p>The option that says: <strong>Enable Redshift replication from the cluster running in the primary region to the cluster running in the secondary region. Change the DNS endpoint to the secondary cluster's primary node in case of system failures in the primary region</strong> is incorrect. Amazon Redshift only has cross-region backup feature (using snapshots); it can't replicate directly to another cluster in another region.</p><p>The option that says: <strong>No additional configuration needed. Redshift is configured with automatic snapshot by default</strong> is incorrect. Even though the automatic snapshots feature is enabled by default, cross-region snapshot copy is not.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html</a></p><p><br></p><p><strong>Check out this Amazon Redshift Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-redshift/?src=udemy\">https://tutorialsdojo.com/amazon-redshift/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html",
      "https://tutorialsdojo.com/amazon-redshift/?src=udemy"
    ]
  },
  {
    "id": 14,
    "question": "<p>A leading e-commerce company plans to launch a donation website for all the victims of the recent super typhoon in South East Asia for its Corporate and Social Responsibility program. The company will advertise its program on TV and on social media, which is why they anticipate incoming traffic on their donation website. Donors can send their donations in cash, which can be transferred electronically, or they can simply post their home address where a team of volunteers can pick up their used clothes, canned goods, and other donations. Donors can optionally write a positive and encouraging message to the victims along with their donations. These features of the donation website will eventually result in a high number of write operations on their database tier considering that there are millions of generous donors around the globe who want to help.</p><p>Which of the following options is the best solution for this scenario?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use an Amazon RDS instance with Provisioned IOPS.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an Oracle database hosted on an extra large Dedicated EC2 instance as your database tier and an SQS queue for buffering the write operations.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use DynamoDB as a database storage and a CloudFront web distribution for hosting static resources.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon DynamoDB with a provisioned write throughput. Use an SQS queue to buffer the large incoming traffic to your Auto Scaled EC2 instances, which processes and writes the data to DynamoDB.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>In this scenario, the application is write-intensive which means that we have to implement a solution to buffer and scale out the services according to the incoming traffic. SQS can be used to handle the large incoming requests and you can use DynamoDB to store large amounts of data.</p><p><strong>Amazon Simple Queue Service (Amazon SQS)</strong> offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. Amazon SQS offers common constructs such as dead-letter queues and cost allocation tags.</p><p>Amazon SQS queues can deliver very high throughput. Standard queues support a nearly unlimited number of transactions per second (TPS) per action. By default, FIFO queues support up to 3,000 messages per second with batching.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_dynamodb.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_sqs_dynamodb.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Amazon DynamoDB with a provisioned write throughput. Use an SQS queue to buffer the large incoming traffic to your Auto Scaled EC2 instances, which processes and writes the data to DynamoDB.</strong> The SQS queue can act as a buffer to temporarily store all the requests while waiting for them to be processed and written to the DynamoDB table.</p><p>The option that says: <strong>Use DynamoDB as a database storage and a CloudFront web distribution for hosting static resources</strong> is incorrect. CloudFront is used for caching static content, not for hosting the static website resources</p><p>The option that says: <strong>Use an Oracle database hosted on an extra large Dedicated EC2 instance as your database tier and an SQS queue for buffering the write operations</strong> is incorrect. AWS does not recommend hosting a database on EC2 instances. RDS instances are designed for hosting databases.</p><p>The option that says: <strong>Use an Amazon RDS instance with Provisioned IOPS</strong> is incorrect. This may be possible, however, this is significantly more expensive than using a DynamoDB table and adding an SQS queue.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-throughput-horizontal-scaling-and-batching.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-throughput-horizontal-scaling-and-batching.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-client-side-buffering-request-batching.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-client-side-buffering-request-batching.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><br></p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-throughput-horizontal-scaling-and-batching.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-client-side-buffering-request-batching.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html",
      "https://tutorialsdojo.com/amazon-sqs/?src=udemy"
    ]
  },
  {
    "id": 15,
    "question": "<p>A privately funded aerospace and sub-orbital spaceflight services company hosts its rapidly evolving applications in AWS. For its deployment process, the company is using CloudFormation templates which are regularly updated to map the latest AMI IDs for its Amazon EC2 instances clusters. It takes a lot of time to execute this on a regular basis which is why the solutions architect has been instructed to automate this process.</p><p>Which of the following options is the most suitable solution that can satisfy the above requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use a combination of AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure your Systems Manager State Manager to store the latest AMI IDs and integrate them with your CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you decide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>You can use the existing <strong>Parameters</strong> section of your <strong>CloudFormation template</strong> to define Systems Manager parameters, along with other parameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the Parameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation.</p><p>If the parameter being referenced in the template does not exist in Systems Manager, a synchronous validation error is thrown. Also, if you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed against SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_parameter_store.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_parameter_store.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Parameters stored in Systems Manager are mutable. Any time you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation. The <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_Parameter.html\">Parameters</a> section in the output for Describe API will show an additional ‘ResolvedValue’ field that contains the resolved value of the Systems Manager parameter that was used for the last stack operation.</p><p>Hence, the correct answer is the option that says: <strong>Use CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you decide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template.</strong></p><p>The option that says: <strong>Configure your Systems Manager State Manager to store the latest AMI IDs and integrate them with your CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template</strong> is incorrect because the Systems Manager State Manager service simply automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. This can't be used as a parameter store that refers to the latest AMI of your application.</p><p>The following options are incorrect because using AWS Service Catalog is not suitable in this scenario. This service just allows organizations to create and manage catalogs of IT services that are approved for use on AWS:</p><p><strong>- Use a combination of AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.</strong></p><p><strong>- Use CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/\">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_Parameter.html",
      "https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html",
      "https://tutorialsdojo.com/aws-systems-manager/?src=udemy"
    ]
  },
  {
    "id": 16,
    "question": "<p>A tech company uses AWS CloudFormation to deploy a three-tier web application that consists of a web tier, application tier, and database tier. The application will utilize an Amazon DynamoDB table for database storage. All resources will be created using a CloudFormation template.</p><p>Which of the following options would allow the application instances access to the DynamoDB tables without exposing the API credentials?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch an IAM Role that has the required permissions to read and write from the required DynamoDB table. Associate the Role to the application instances by referencing it to the <code>AWS::IAM::InstanceRoleName</code> Property.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "In the CloudFormation template, use the Parameter section to have the user input the AWS Access and Secret Keys from an already created IAM user that has the permissions required to interact with the DynamoDB table.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Launch an IAM Role that has the required permissions to read and write from the DynamoDB table. Reference the IAM Role as a property inside the <code>AWS::IAM::InstanceProfile</code> of the application instance.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Launch an IAM user in the CloudFormation template that has permissions to read and write from the DynamoDB table. Use the GetAtt function to retrieve the Access and secret keys and pass them to the web application instance through the use of its instance user-data.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Identity and Access Management</strong> is an AWS service that you can use to manage users and their permissions in AWS. You can use IAM with AWS CloudFormation to specify what AWS CloudFormation actions users can perform, such as viewing stack templates, creating stacks, or deleting stacks. Furthermore, anyone managing AWS CloudFormation stacks will require permissions to resources within those stacks. For example, if users want to use AWS CloudFormation to launch, update, or terminate Amazon EC2 instances, they must have permission to call the relevant Amazon EC2 actions.</p><p>Using roles to grant permissions to applications that run on EC2 instances requires a bit of extra configuration. An application running on an EC2 instance is abstracted from AWS by the virtualized operating system. Because of this extra separation, an additional step is needed to assign an AWS role and its associated permissions to an EC2 instance and make them available to its applications. This extra step is the creation of an <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html\">instance profile</a> that is attached to the instance.</p><p><img src=\"https://media.tutorialsdojo.com/sap_lambda_iam.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_lambda_iam.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The instance profile contains the role and can provide the role's temporary credentials to an application that runs on the instance. Those temporary credentials can then be used in the application's API calls to access resources and to limit access to only those resources that the role specifies. Note that only one role can be assigned to an EC2 instance at a time, and all applications on the instance share the same role and permissions.</p><p>Using roles in this way has several benefits. Because role credentials are temporary and rotated automatically, you don't have to manage credentials, and you don't have to worry about long-term security risks. In addition, if you use a single role for multiple instances, you can make a change to that one role and the change is propagated automatically to all the instances.</p><p>An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. If you use the AWS Management Console to create a role for Amazon EC2, the console automatically creates an instance profile and gives it the same name as the role.</p><p>Hence, the correct answer is: <strong>Launch an IAM Role that has the required permissions to read and write from the DynamoDB table. Reference the IAM Role as a property inside the </strong><code><strong>AWS::IAM::InstanceProfile</strong></code><strong> of the application instance.</strong></p><p>The option that says: <strong>Launch an IAM Role that has the required permissions to read and write from the required DynamoDB table. Associate the Role to the application instances by referencing it to the </strong><code><strong>AWS::IAM::InstanceRoleName</strong></code><strong> property</strong> is incorrect because you have to use the <code>InstanceProfile</code> property instead. Take note that there is no <code>InstanceRoleName</code> property in IAM.</p><p>The option that says: <strong>In the CloudFormation template, use the Parameter section to have the user input the AWS Access and Secret Keys from an already created IAM user that has the permissions required to interact with the DynamoDB table</strong> is incorrect because it is a security risk to include the IAM access keys in a CloudFormation template. You have to use an IAM Role instead.</p><p>The option that says:<strong> Launch an IAM user in the CloudFormation template that has permissions to read and write from the DynamoDB table. Use the GetAtt function to retrieve the Access and secret keys and pass them to the web application instance through the use of its instance user-data</strong> is incorrect because it is inappropriate to use an IAM User to provide an EC2 instance the required access to the DynamoDB table. Attaching an IAM Role to the EC2 instances is the most suitable solution in this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#use-iam-to-control-access\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#use-iam-to-control-access</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#use-iam-to-control-access",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html",
      "https://tutorialsdojo.com/aws-cloudformation/?src=udemy"
    ]
  },
  {
    "id": 17,
    "question": "<p>A company has performed a security audit on its existing application. It was determined that the application retrieves the Amazon RDS for MySQL credentials from an encrypted file in an Amazon S3 bucket. To improve the security of the application the following should be implemented on the next application deployment:</p><ul><li><p>The database credentials must be randomly generated and stored in a secure AWS managed service.</p></li><li><p>The credentials must be rotated every 90 days.</p></li><li><p>Infrastructure-as-code provisioning of application resources using AWS CloudFormation.</p></li><li><p>For the application deployment, the solutions architect will create a CloudFormation template.</p></li></ul><p>Which of the following options should the solutions architect implement to meet the company’s requirement with the LEAST amount of operational overhead?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Using AWS Secrets Manager, create a secret resource and generate a secure database password. Write an AWS Lambda function to rotate the database password. Create a scheduled rule on Amazon EventBridge to trigger the Lambda function to rotate the database password every 90 days.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Secrets Manager, create a secret resource and generate a secure database password. Use Secrets Manager's managed rotation to automatically rotate the database password every 90 days. On AWS CloudFormation, specify the <code>AutomaticallyAfterDays</code> property in <code>RotationRules</code> to set the rotation schedule to 90 days.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>On Systems Manager Parameter Store, create a SecureString parameter and generate a secure database password. On AWS CloudFormation, create an AWS KMS resource to rotate the database password every 90 days.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>On Systems Manager Parameter Store, create a SecureString parameter and generate a secure database password. Write an AWS Lambda function to rotate the database password. On AWS CloudFormation, specify a resource for Parameter Store <code>RotationSchedule</code> to rotate the password every 90 days.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Secrets Manager</strong> enables you to replace hardcoded credentials in your code, including passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can't be compromised by someone examining your code because the secret no longer exists in the code. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a specified schedule. This enables you to replace long-term secrets with short-term ones, significantly reducing the risk of compromise.</p><p>The following diagram illustrates the most basic scenario. It shows that you can store credentials for a database in Secrets Manager and then use those credentials in an application to access the database.</p><p><img src=\"https://media.tutorialsdojo.com/sap_secrets_manager_how.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_secrets_manager_how.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><ol><li><p>The database administrator creates a set of credentials on the Personnel database for use by an application called MyCustomApp. The administrator also configures those credentials with the permissions required for the application to access the Personnel database.</p></li><li><p>The database administrator stores the credentials as a secret in Secrets Manager named MyCustomAppCreds. Then, Secrets Manager encrypts and stores the credentials within the secret as the <em>protected secret text</em>.</p></li><li><p>When MyCustomApp accesses the database, the application queries Secrets Manager for the secret named MyCustomAppCreds.</p></li><li><p>Secrets Manager retrieves the secret, decrypts the protected secret text, and returns the secret to the client app over a secured (HTTPS with TLS) channel.</p></li><li><p>The client application parses the credentials, connection string, and any other required information from the response and then uses the information to access the database server.</p></li></ol><p><img src=\"https://media.tutorialsdojo.com/aws-cloudformation-aws-rotationschedule.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/aws-cloudformation-aws-rotationschedule.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Amazon RDS</strong> offers managed rotation through <strong>AWS Secrets Manager</strong>, which does not require writing a custom Lambda function. On <strong>AWS CloudFormation</strong> template, the <code><strong>AWS::SecretsManager::RotationSchedule</strong></code> can be used to set the rotation schedule for a secret.</p><p>In the given scenario, the correct property to use for setting the rotation schedule in AWS Secrets Manager is <code><strong>AutomaticallyAfterDays</strong></code>. This property specifies the number of days between automatically scheduled rotations of the secret, which aligns with the requirement to rotate the credentials every 90 days.</p><p>Hence, the correct answer is: <strong>Use AWS Secrets Manager, create a secret resource and generate a secure database password. Use Secrets Manager's managed rotation to automatically rotate the database password every 90 days. On AWS CloudFormation, specify the </strong><code><strong>AutomaticallyAfterDays</strong></code><strong> property in </strong><code><strong>RotationRules</strong></code><strong> to set the rotation schedule to 90 days. </strong>This option leverages AWS Secrets Manager's managed rotation, which simplifies the process and reduces operational overhead by not requiring a custom Lambda function for rotation.</p><p>The option that says: <strong>On Systems Manager Parameter Store, create a SecureString parameter and generate a secure database password. Write an AWS Lambda function to rotate the database password. On AWS CloudFormation, specify a resource for Parameter Store </strong><code><strong>RotationSchedule</strong></code><strong> to rotate the password every 90 days</strong> is incorrect. There is no Systems Manager Parameter Store RotationSchedule resource on AWS CloudFormation.</p><p>The option that says: <strong>Use AWS Secrets Manager, create a secret resource and generate a secure database password. Write an AWS Lambda function to rotate the database password. Create a scheduled rule on Amazon EventBridge to trigger the Lambda function to rotate the database password every 90 days</strong> is incorrect. This option may be possible. However, it does not use AWS CloudFormation in its solution. Additionally, for RDS services covered by Secrets Manager's managed rotation, creating a scheduled rule on EventBridge to trigger a Lambda function is just unnecessary.</p><p>The option that says: <strong>On Systems Manager Parameter Store, create a SecureString parameter and generate a secure database password. On AWS CloudFormation, create an AWS KMS resource to rotate the database password every 90 days</strong> is incorrect. AWS KMS can automatically rotate keys that are stored only on KMS, not parameters stored in the Systems Manager Parameter Store.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html</a></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_managed.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_managed.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-secretsmanager-rotationschedule-rotationrules.html#cfn-secretsmanager-rotationschedule-rotationrules-automaticallyafterdays\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-secretsmanager-rotationschedule-rotationrules.html#cfn-secretsmanager-rotationschedule-rotationrules-automaticallyafterdays</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-rotationschedule.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-rotationschedule.html</a></p><p><br></p><p><strong>Check out these AWS Secrets Manager and AWS CloudFormation Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-secrets-manager/\">https://tutorialsdojo.com/aws-secrets-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/\">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><br></p><p><strong>AWS Secrets Manager and Systems Manager Parameter Store comparison:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/?src=udemy\">https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html",
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_managed.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-secretsmanager-rotationschedule-rotationrules.html#cfn-secretsmanager-rotationschedule-rotationrules-automaticallyafterdays",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-rotationschedule.html",
      "https://tutorialsdojo.com/aws-secrets-manager/",
      "https://tutorialsdojo.com/aws-cloudformation/",
      "https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/?src=udemy"
    ]
  },
  {
    "id": 18,
    "question": "<p>A company has adopted cloud-native computing best practices for its infrastructure. The company started using AWS CloudFormation templates for defining its cloud resources, and the templates are hosted in its private GitHub repository. As the developers continuously update the templates, the company has encountered several downtimes caused by misconfigured templates, wrong executions, or the creation of unnecessary environments. The management wants to streamline the process of testing the CloudFormation templates to prevent these errors. The Solutions Architect has been tasked to create an automated solution.</p><p>Which of the following options should be implemented to meet the company's requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Write an AWS Lambda function that syncs the private GitHub repository to another Git provider like GitLab or Bitbucket. Using AWS CodeDeploy, create a change set and execute the AWS CloudFormation template. Add an AWS CodeBuild stage on the deployment to build and run test scripts to verify the new stack.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Write an AWS Lambda function that builds any changes committed on the private GitHub repository. Store the generated artifacts on AWS CodeArtifact. Using AWS CodePipeline, create a change set with the new artifact and execute the AWS CloudFormation template. Add a CodeBuild action to run test scripts that verify the new stack.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a pipeline in AWS CodePipeline that is triggered automatically for commits on the private GitHub repository. Have the pipeline create a change set and execute the CloudFormation template. Add an AWS CodeBuild stage on the pipeline to build and run test scripts to verify the new stack.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create a pipeline in AWS CodePipeline that is triggered automatically for commits on the private GitHub repository. Have the pipeline create a change set from the CloudFormation template and execute it using AWS CodeDeploy. Add an AWS CodeBuild stage on the pipeline to build and run test scripts to verify the new stack.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>You can apply continuous delivery practices to your <strong>AWS CloudFormation</strong> stacks using AWS CodePipeline. <strong>AWS CodePipeline</strong> is a continuous delivery service for fast and reliable application and infrastructure updates. CodePipeline builds, tests, and deploys your code every time there is a code change based on the release process models you define.</p><p>With continuous delivery, you can automatically deploy CloudFormation template updates to your pipeline stages for testing and then promote them to production. For example, you could use CodePipeline to model an automated release process that provisions a test stack whenever an updated template is committed to a source repository (Git repositories managed by GitHub, GitLab, and Atlassian Bitbucket) or uploaded to an Amazon S3 bucket. You can inspect the test stack and then approve it for the production stage, after which CodePipeline can delete the test stack and create a change set for final approval. When the change set is approved, CodePipeline can execute the change set and deploy the change to production.</p><p><strong>AWS CodeBuild</strong> is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. AWS CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.</p><p>Here is an example of a Quick Start from AWS for CI/CD Pipeline for AWS CloudFormation templates:</p><p><img src=\"https://media.tutorialsdojo.com/sap_codebuild_workflow.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_codebuild_workflow.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>- A pipeline created by CodePipeline, which is triggered when a commit is made to the referenced branch of the GitHub repository used in the source stage.</p><p>- A build project in CodeBuild to run TaskCat and launch AWS CloudFormation templates for testing.</p><p>- An AWS Lambda function that merges the source branch of the GitHub repository with the release branch.</p><p>- AWS Identity and Access Management (IAM) roles for the Lambda function and the build project.</p><p>- An Amazon Simple Storage Service (Amazon S3) bucket to stash the build artifacts temporarily and to store the TaskCat report.</p><p>Therefore, the correct answer is: <strong>Create a pipeline in AWS CodePipeline that is triggered automatically for commits on the private GitHub repository. Have the pipeline create a change set and execute the CloudFormation template. Add an AWS CodeBuild stage on the pipeline to build and run test scripts to verify the new stack.</strong> With AWS CodePipeline, you can create change sets and automatically deploy CloudFormation template updates safely. You can have a CodeBuild stage for building artifacts and testing your new infra.</p><p>The option that says: <strong>Create a pipeline in AWS CodePipeline that is triggered automatically for commits on the private GitHub repository. Have the pipeline create a change set from the CloudFormation template and execute it using AWS CodeDeploy. Add an AWS CodeBuild stage on the pipeline to build and run test scripts to verify the new stack</strong> is incorrect. AWS CodeDeploy is only an unnecessary resource as you will not deploy the changes in your production environment. You need a pipeline to execute the CloudFormation change set and CodeBuild project to test your changes.</p><p>The option that says: <strong>Write an AWS Lambda function that syncs the private GitHub repository to another Git provider like GitLab or Bitbucket. Using AWS CodeDeploy, create a change set and execute the AWS CloudFormation template. Add an AWS CodeBuild stage on the deployment to build and run test scripts to verify the new stack</strong> is incorrect. You don't need a custom Lambda function as AWS CodePipiline supports executions from third-party Git sources.</p><p>The option that says: <strong>Write an AWS Lambda function that builds any changes committed on the private GitHub repository. Store the generated artifacts on AWS CodeArtifact. Using AWS CodePipeline, create a change set with the new artifact and execute the AWS CloudFormation template. Add a CodeBuild action to run test scripts that verify the new stack</strong> is incorrect. Building artifacts may take longer than the 15-minute maximum execution time of Lambda functions. AWS CodeArtifact is primarily used to automatically fetch software packages and dependencies from public artifact repositories. AWS CodeBuild can be used to build and test artifacts before application deployments.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2016/11/continuously-deliver-changes-to-aws-cloudformation-stacks-with-aws-codepipeline/\">https://aws.amazon.com/about-aws/whats-new/2016/11/continuously-deliver-changes-to-aws-cloudformation-stacks-with-aws-codepipeline/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-basic-walkthrough.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-basic-walkthrough.html</a></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html</a></p><p><a href=\"https://aws.amazon.com/quickstart/architecture/cicd-taskcat/\">https://aws.amazon.com/quickstart/architecture/cicd-taskcat/</a></p><p><br></p><p><strong>Check out these AWS CodePipeline and AWS CloudFormation Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/about-aws/whats-new/2016/11/continuously-deliver-changes-to-aws-cloudformation-stacks-with-aws-codepipeline/",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-basic-walkthrough.html",
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html",
      "https://aws.amazon.com/quickstart/architecture/cicd-taskcat/",
      "https://tutorialsdojo.com/aws-codepipeline/?src=udemy",
      "https://tutorialsdojo.com/aws-cloudformation/?src=udemy"
    ]
  },
  {
    "id": 19,
    "question": "<p>A company requires regular processing of a massive amount of product catalogs that need to be handled per batch. The data needs to be processed regularly by on-demand workers. The company instructed its solutions architect to design a workflow orchestration system that will enable to reprocess failures and handle multiple concurrent operations.</p><p>What is the MOST suitable solution that the solutions architect should implement in order to manage the state of every workflow?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement Step Functions to orchestrate batch processing workflows. Use the AWS Management Console to monitor workflow status and manage failure reprocessing.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up a workflow using AWS Config and AWS Step Functions in order to orchestrate multiple concurrent workflows. Visualize the status of each workflow using the AWS Management Console. Store the historical data in an Amazon S3 bucket and visualize the data using Amazon QuickSight.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon MQ to set up a batch process workflow that handles the processing for a single batch. Develop worker jobs using AWS Lambda functions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store workflow data in an Amazon RDS with AWS Lambda functions polling the RDS database instance for status changes. Set up worker Lambda functions to process the next workflow steps, then use Amazon QuickSight to visualize workflow states directly out of RDS.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>AWS Step Functions lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker AI, into feature-rich applications.</p><p><img src=\"https://media.tutorialsdojo.com/public/codepipeline_statemachine.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/public/codepipeline_statemachine.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In this scenario, the company can leverage AWS Step Functions to design and manage the workflows required for processing massive amounts of product catalogs. Step Functions can orchestrate these workflows by defining them as a series of steps or states, where each state can represent a specific task in the process, such as data validation, transformation, or loading. Furthermore, AWS Step Functions offers built-in mechanisms for error handling and retry policies, which are essential for managing failures in any of the workflow steps. Through the AWS Management Console, you can actively monitor each workflow's status, seeing real-time updates on successes, failures, and ongoing tasks. This setup makes it much easier for your solutions architect and operations team to quickly spot and troubleshoot any issues.</p><p>Hence, the correct answer is: <strong>Implement Step Functions to orchestrate batch processing workflows. Use the AWS Management Console to monitor workflow status and manage failure reprocessing.</strong></p><p>The option that says: <strong>Store workflow data in an Amazon RDS with AWS Lambda functions polling the RDS database instance for status changes. Set up worker Lambda functions to process the next workflow steps, then use Amazon QuickSight to visualize workflow states directly out of RDS </strong>is incorrect. While this could work, it involves more overhead in managing database connections and polling mechanisms, making it less efficient for orchestrating complex workflows compared to using a dedicated service like AWS Step Functions.</p><p>The option that says: <strong>Set up a workflow using AWS Config and AWS Step Functions in order to orchestrate multiple concurrent workflows. Visualize the status of each workflow using the AWS Management Console. Store the historical data in an Amazon S3 bucket and visualize the data using Amazon QuickSight </strong>is incorrect. AWS Config is primarily a service for auditing and evaluating the configurations of your AWS resources, not for workflow orchestration.</p><p>The option that says: <strong>Use Amazon MQ to set up a batch process workflow that handles the processing for a single batch. Develop worker jobs using AWS Lambda functions </strong>is incorrect. Amazon MQ is simply a message broker service for messaging between software components but does not offer the same level of workflow orchestration or state management capabilities as AWS Step Functions. It's more suited for decoupling applications and integrating different systems rather than managing complex workflows.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html</a></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html\">https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html</a></p><p><br></p><p><strong>Check out this AWS Step Functions Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-step-functions/?src=udemy\">https://tutorialsdojo.com/aws-step-functions/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/step-functions/",
      "https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html",
      "https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html",
      "https://tutorialsdojo.com/aws-step-functions/?src=udemy"
    ]
  },
  {
    "id": 20,
    "question": "<p>An analytics company hosts its data processing application on its on-premises data center. Data scientists upload input files through a web portal which then are then stored in the company NAS. For every uploaded file, the web server sends a message to the processing server over a message queue. It could take up to 30 minutes to process each file on the NAS. During business hours, the number of files awaiting processing is significantly higher and it could take a while for the processing servers to catch up. The number of files significantly declines after business hours. The company has tasked the solutions architect to migrate this workload to the AWS cloud.</p><p>Which of the following options is the recommended solution while being cost-effective?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Reconfigure the web application to publish messages to a new Amazon MQ queue. Create an auto-scaling group of Amazon EC2 instances to pull messages from the queue and process the files. Store the processed files on an Amazon EFS volume. Power off the EC2 when there are no messages left on the queue.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Reconfigure the web application to publish messages to a new Amazon MQ queue. Write an AWS Lambda function to pull messages from the SQS queue and process the files. Trigger the Lambda function for every new message on the queue. Store the processed files in Amazon EFS</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Reconfigure the web application to publish messages to a new Amazon SQS queue. Write an AWS Lambda function to pull messages from the SQS queue and process the files. Trigger the Lambda function for every new message on the queue. Store the processed files on an Amazon S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Reconfigure the web application to publish messages to a new Amazon SQS queue. Create an auto-scaling group of Amazon EC2 instances based on the SQS queue length to pull messages from the queue and process the files. Store the processed files on an Amazon S3 bucket.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p><strong>Amazon Simple Queue Service (Amazon SQS)</strong> offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components.</p><p>There are some scenarios where you might think about scaling in response to activity in an Amazon SQS queue. For example, suppose that you have a web app that lets users upload images and use them online. In this scenario, each image requires resizing and encoding before it can be published. The app runs on EC2 instances in an Auto Scaling group, and it's configured to handle your typical upload rates. Unhealthy instances are terminated and replaced to maintain current instance levels at all times. The app places the raw bitmap data of the images in an SQS queue for processing. It processes the images and then publishes the processed images where they can be viewed by users. The architecture for this scenario works well if the number of image uploads doesn't vary over time. But if the number of uploads changes over time, you might consider using dynamic scaling to scale the capacity of your Auto Scaling group.</p><p>There are three main parts to this configuration:</p><p>-An <strong>Auto Scaling group to manage EC2</strong> instances for the purposes of processing messages from an SQS queue.</p><p>-A custom metric to send to Amazon CloudWatch that measures the number of messages in the queue per EC2 instance in the Auto Scaling group.</p><p>-A target tracking policy that configures your Auto Scaling group to scale based on the custom metric and a set target value. CloudWatch alarms invoke the scaling policy.</p><p>The following diagram illustrates the architecture of this configuration.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_auto_scaling.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_sqs_auto_scaling.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Reconfigure the web application to publish messages to a new Amazon SQS queue. Create an auto-scaling group of Amazon EC2 instances based on the SQS queue length to pull messages from the queue and process the files. Store the processed files on an Amazon S3 bucket.</strong> This is a cost-effective solution as the EC2 instances scale out depending on the length of the SQS queue, and Amazon S3 is cheaper compared to Amazon EFS.</p><p>The option that says: <strong>Reconfigure the web application to publish messages to a new Amazon SQS queue. Write an AWS Lambda function to pull messages from the SQS queue and process the files. Trigger the Lambda function for every new message on the queue. Store the processed files on an Amazon S3 bucket</strong> is incorrect. The files may take up to 30 minutes to be processed. AWS Lambda has an execution limit of 15 minutes.</p><p>The option that says: <strong>Reconfigure the web application to publish messages to a new Amazon MQ queue. Create an auto-scaling group of Amazon EC2 instances to pull messages from the queue and process the files. Store the processed files on an Amazon EFS volume. Power off the EC2 when there are no messages left on the queue</strong> is incorrect. This is possible but not the most cost-effective solution. Amazon EFS is significantly more expensive than Amazon S3.</p><p>The option that says: <strong>Reconfigure the web application to publish messages to a new Amazon MQ queue. Write an AWS Lambda function to pull messages from the SQS queue and process the files. Trigger the Lambda function for every new message on the queue. Store the processed files in Amazon EFS</strong> is incorrect. The files may take up to 30 minutes to be processed. AWS Lambda has an execution limit of 15 minutes. Amazon EFS is significantly more expensive than Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p><p><br></p><p><strong>Check out these Amazon EC2 and Amazon SQS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html",
      "https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy",
      "https://tutorialsdojo.com/amazon-sqs/?src=udemy"
    ]
  },
  {
    "id": 21,
    "question": "<p>A research company hosts its internal applications inside AWS VPCs in multiple AWS Accounts. The internal applications are accessed securely from inside the company network using an AWS Site-to-Site VPN connection. VPC peering connections has been established from the company’s main AWS account to VPCs in other AWS Accounts. The company has recently announced that employees will be allowed to work remotely if they are connected using a VPN. The solutions architect has been tasked to create a scalable and reliable AWS Client VPN solution that employees can use when working remotely.</p><p>Which of the following options is the most cost-effective implementation to meet the company requirements with minimal changes to the current setup?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Install the AWS Client VPN on each employee workstation. Create a Client VPN endpoint in each of the AWS accounts. Update each VPC route configuration to allow communication with the internal applications.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Install the AWS Client VPN on the company data center. Create AWS Transit Gateway to connect the main VPC and other VPCs into a single hub. Update the VPC route configurations to allow communication with the internal applications.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Install the AWS Client VPN on each employee workstation. Create a Client VPN endpoint in the same VPC region in the main AWS account. Update the VPC route configurations to allow communication with the internal applications.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Install the AWS Client VPN on the company data center. Configure connectivity between the existing AWS Site-to-Site VPN and client VPN endpoint.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS Client VPN</strong> is a managed client-based VPN service that enables you to securely access your AWS resources and resources in your on-premises network. With Client VPN, you can access your resources from any location using an OpenVPN-based VPN client.</p><p>The <strong>administrator</strong> is responsible for setting up and configuring the service. This involves creating the Client VPN endpoint, associating the target network, configuring the authorization rules, and setting up additional routes (if required).</p><p>The <strong>client</strong> is the end user. This is the person who connects to the Client VPN endpoint to establish a VPN session. The client establishes the VPN session from their local computer or mobile device using an OpenVPN-based VPN client application.</p><p>The following are the key components for using AWS Client VPN.</p><p><strong>-Client VPN endpoint</strong> — Your Client VPN administrator creates and configures a Client VPN endpoint in AWS. Your administrator controls which networks and resources you can access when establishing a VPN connection.</p><p><strong>-VPN client application</strong> — The software application that you use to connect to the Client VPN endpoint and establish a secure VPN connection.</p><p><strong>-Client VPN endpoint configuration file</strong> — A configuration file that's provided to you by your Client VPN administrator. The file includes information about the Client VPN endpoint and the certificates required to establish a VPN connection. You load this file into your chosen VPN client application.</p><p>The configuration for this scenario includes a target VPC (VPC A) that is peered with an additional VPC (VPC B). We recommend this configuration if you need to give clients access to the resources inside a target VPC and other VPCs that are peered with it (such as VPC B).</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_client_vpn.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_client_vpn.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Install the AWS Client VPN on each employee workstation. Create a Client VPN endpoint in the same VPC region in the main AWS account. Update the VPC route configurations to allow communication with the internal applications.</strong> This solution is cost-effective and requires minimal changes to the current network setup.</p><p>The option that says: <strong>Install the AWS Client VPN on each employee workstation. Create a Client VPN endpoint in each of the AWS accounts. Update each VPC route configuration to allow communication with the internal applications</strong> is incorrect. You only need to create a Client VPN endpoint on the main VPC account that has Site-to-Site VPN.</p><p>The option that says: <strong>Install the AWS Client VPN on the company data center. Create AWS Transit Gateway to connect the main VPC and other VPCs into a single hub. Update the VPC route configurations to allow communication with the internal applications</strong> is incorrect. This may be possible, however, this will require reconfiguring the existing VPC peering between the main VPC and the other VPCs in other AWS accounts. Additionally, the AWS Client VPN should be installed on the employee workstations.</p><p>The option that says: <strong>Install the AWS Client VPN on the company data center. Configure connectivity between the existing AWS Site-to-Site VPN and client VPN endpoint</strong> is incorrect. This configuration is impossible and the AWS Client VPN should be installed on the employee workstations and not in the company data center.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/scenario-peered.html\">https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/scenario-peered.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/what-is.html\">https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html#vpc-peering-limitations\">https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html#vpc-peering-limitations</a></p><p><br></p><p><strong>Check out these Amazon VPC and VPC Peeing Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><a href=\"https://tutorialsdojo.com/vpc-peering/?src=udemy\">https://tutorialsdojo.com/vpc-peering/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/scenario-peered.html",
      "https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/what-is.html",
      "https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html#vpc-peering-limitations",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy",
      "https://tutorialsdojo.com/vpc-peering/?src=udemy"
    ]
  },
  {
    "id": 22,
    "question": "<p>A digital media publishing company hired a solutions architect to manage its online portal, which is deployed on a single Amazon EC2 instance. The architecture uses a combination of Reserved EC2 Instances to handle the steady-state load and On-Demand EC2 Instances to handle the peak load. Currently, the web servers operate at 90% utilization during peak load.</p><p>Which of the following is the most cost-effective option to enable the online portal to quickly recover in the event of a zone outage?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Auto Scaling group of Spot instances on multiple Availability Zones. Attach an Application Load Balancer to the group.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Launch a Spot Fleet of On-demand and Spot instances across multiple Availability Zones. Attach an Application Load Balancer to the fleet.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Launch a Spot Fleet of Spot instances across multiple Availability Zones. Attach an Application Load Balancer to the fleet.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Auto Scaling group On-Demand instances on multiple Availability Zones. Attach an Application Load Balancer to the group.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that are launched based on criteria that you specify. The Spot Fleet selects the Spot capacity pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated. You can submit a Spot Fleet as a one-time request, which does not persist after the instances have been terminated. You can include On-Demand Instance requests in a Spot Fleet request.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ec2_type_fleet_request.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ec2_type_fleet_request.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>To avoid interruption to your Spot instances, you can actually set up a diversified spot fleet allocation strategy in which you are using a range of different EC2 instance types such as c3.2xlarge, m3.xlarge, r3.xlarge et cetera instead of just one type. This will effectively increase the chances of providing a more stable compute capacity to your application. Therefore, in the event that there is a Spot interruption due to the high demand for a specific instance type, say c3.2xlarge, your application could still scale using another instance type, such as m3.xlarge or r3.xlarge.</p><p>In the scenario, using a mix of On-demand and Spot instances within a Spot Fleet across multiple Availability Zones strikes the right balance. The On-demand instances primarily handle the steady-state load, while additional Spot instances are launched to help with the increased load during peak times, keeping costs down. The fleet setup automatically adjusts to maintain service even if an Availability Zone goes down, ensuring that the system is both cost-effective and resilient.</p><p>Hence, the correct answer is: <strong>Launch a Spot Fleet of On-demand and Spot instances across multiple Availability Zones. Attach an Application Load Balancer to the fleet.</strong></p><p>The option that says: <strong>Create an Auto Scaling group of Spot instances on multiple Availability Zones. Attach an Application Load Balancer to the group</strong> is incorrect. While being highly cost-effective due to the use of Spot instances, it could lead to availability issues since Spot instances can be reclaimed by AWS when spot market prices exceed the bid price.</p><p>The option that says: <strong>Create an Auto Scaling group On-Demand instances on multiple Availability Zones. Attach an Application Load Balancer to the group</strong> is incorrect. This approach ensures that the portal can handle peak loads without the risk of losing instances unexpectedly. However, the cost of On-Demand instances is significantly higher than Spot instances, making this option less cost-effective for managing fluctuating or unpredictable workloads.</p><p>The option that says: <strong>Launch a Spot Fleet of Spot instances across multiple Availability Zones. Attach an Application Load Balancer to the fleet</strong> is incorrect. Like the other incorrect option, this solution fully relies on Spot instances and, while being cheaper, is more susceptible to service disruption due to Spot market fluctuations.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11\">https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html#spot-fleet-allocation-strategy\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html#spot-fleet-allocation-strategy</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/08/new-capacity-optimized-allocation-strategy-for-provisioning-amazon-ec2-spot-instances/\">https://aws.amazon.com/about-aws/whats-new/2019/08/new-capacity-optimized-allocation-strategy-for-provisioning-amazon-ec2-spot-instances/</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html#spot-fleet-allocation-strategy",
      "https://aws.amazon.com/about-aws/whats-new/2019/08/new-capacity-optimized-allocation-strategy-for-provisioning-amazon-ec2-spot-instances/",
      "https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy"
    ]
  },
  {
    "id": 23,
    "question": "<p>A retail company runs its two-tier e-commerce website on its on-premises data center. The application runs on a LAMP stack behind a load balancing appliance. The operations team uses SSH to login to the application servers to deploy software updates and install patches on the system. The website has been a target of multiple cyber-attacks recently such as:</p><p> - Distributed Denial of Service (DDoS) attacks</p><p> - SQL Injection attacks</p><p> - Dictionary attacks to SSH accounts on the web servers</p><p>The solutions architect plans to migrate the whole system to AWS to improve its security and availability. The following approaches are laid out to address the company concerns:</p><p> - Fix SQL injection attacks by reviewing existing application code and logic.</p><p> - Use the latest Amazon Linux AMIs to ensure that initial security patches are installed.</p><p> - Install the AWS Systems Manager agent on the instances to manage OS patching.</p><p>Which of the following are additional recommended actions to address the identified attacks while maintaining high availability and security for the application?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Disable remote SSH login to the EC2 instances and use AWS SSM Session Manager instead. Migrate the on-premises MySQL server to an Amazon RDS Multi-AZ instance. Create a CloudFront distribution in front of the application servers, and apply AWS WAF rules for the distribution. Enable AWS Shield Advanced for added protection.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Enable remote SSH login only on a bastion host with limited access from the company IP address. Migrate the on-premises MySQL server to an Amazon RDS Multi-AZ instance. Create a CloudFront distribution in front of the application servers and enable AWS Shield Standard for DDoS protection.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Enable remote SSH login on the EC2 instances but with limited access from the company IP address only. Migrate the on-premises MySQL server to an Amazon RDS Single-AZ instance. Enable AWS Shield Standard to protect the instances from DDoS attacks.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Disable remote SSH login to the EC2 instances and use AWS SSM Session Manager instead. Migrate the on-premises MySQL server to an Amazon RDS Single-AZ instance. Create a CloudFront distribution in front of the application servers, and apply AWS WAF rules for the distribution.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS Systems Manager Session Manager</strong> allows you to manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, on-premises instances, and virtual machines (VMs) through an interactive one-click browser-based shell or through the AWS Command Line Interface (AWS CLI). Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.</p><p>A distributed denial of service (DDoS) attack is an attack in which multiple compromised systems attempt to flood a target, such as a network or web application, with traffic. A DDoS attack can prevent legitimate users from accessing a service and can cause the system to crash due to the overwhelming traffic volume.</p><p>AWS provides two levels of protection against DDoS attacks: <strong>AWS Shield Standard</strong> and <strong>AWS Shield Advanced</strong>. All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against the most common, frequently occurring network and transport layer DDoS attacks that target your website or applications. For higher levels of protection against attacks, you can subscribe to AWS Shield Advanced. When you subscribe to AWS Shield Advanced and add specific resources to be protected, AWS Shield Advanced provides expanded DDoS attack protection for web applications running on the resources.</p><p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting. You can also customize rules that filter out specific traffic patterns.</p><p><img src=\"https://media.tutorialsdojo.com/sap_well_architectured_security.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_well_architectured_security.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Disable remote SSH login to the EC2 instances and use AWS SSM Session Manager instead. Migrate the on-premises MySQL server to an Amazon RDS Multi-AZ instance. Create a CloudFront distribution in front of the application servers, and apply AWS WAF rules for the distribution. Enable AWS Shield Advanced for added protection.</strong> AWS SSM Session manager allows secure remote access to your instances without using SSH login. AWS WAF rules can block common web exploits like SQL injection attacks and AWS Shield Advanced provides added protection for DDoS attacks.</p><p>The option that says: <strong>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Enable remote SSH login only on a bastion host with limited access from the company IP address. Migrate the on-premises MySQL server to an Amazon RDS Multi-AZ instance. Create a CloudFront distribution in front of the application servers and enable AWS Shield Standard for DDoS protection</strong> is incorrect. You don't need to use a bastion host if you have AWS SSM agent installed on the instances. You can use SSM Session Manager to login to the servers. AWS Shield Standard is already enabled by default but you should enable AWS Shield Advanced for a higher level of protection.</p><p>The option that says: <strong>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Enable remote SSH login on the EC2 instances but with limited access from the company IP address only. Migrate the on-premises MySQL server to an Amazon RDS Single-AZ instance. Enable AWS Shield Standard to protect the instances from DDoS attacks</strong> is incorrect. You don't need to use a bastion host if you have AWS SSM agent installed on the instances. Using a Single-AZ RDS instance is not recommended if you require a highly available database.</p><p>The option that says: <strong>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Disable remote SSH login to the EC2 instances and use AWS SSM Session Manager instead. Migrate the on-premises MySQL server to an Amazon RDS Single-AZ instance. Create a CloudFront distribution in front of the application servers, and apply AWS WAF rules for the distribution</strong> is incorrect. AWS WAF may protect you from common web attacks, but you still need to enable AWS Shield Advanced for a higher level of protection against DDoS attacks. Using a Single-AZ RDS instance is not recommended if you require a highly available database.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/ddos-overview.html\">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-overview.html</a></p><p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p><p><br></p><p><strong>Check out these AWS Systems Manager and AWS WAF Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html",
      "https://docs.aws.amazon.com/waf/latest/developerguide/ddos-overview.html",
      "https://aws.amazon.com/waf/",
      "https://tutorialsdojo.com/aws-systems-manager/?src=udemy",
      "https://tutorialsdojo.com/aws-waf/?src=udemy"
    ]
  },
  {
    "id": 24,
    "question": "<p>A company has data centers in Europe, Asia, and North America regions. Each data center has a 10Gbps Direct Connect connection to AWS, and the company uses a custom VPN to encrypt traffic between its data center network and AWS. In total, the data centers have about five hundred physical servers that host a mix of Windows and Linux-based applications and database services. The company plans to decommission these data centers and migrate its entire infrastructure to the AWS cloud instead. Separate accounts for staging and launching VMs must be implemented, as well as the ability to do AWS Region-to-Region Amazon VPC stack creation.</p><p>Which of the following options is the recommended solution for this migration?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage the Application Discovery Service from AWS. Install the Application Discovery Service agents on each physical server and visualize the infrastructure on the AWS Migration Hub console. Trigger the replication to copy your servers to AWS. After the replication is completed, start the cutover to AWS.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Leverage AWS Outposts service for the migration of physical servers to AWS. Install the Outposts server agent on the data center to incrementally replicate the servers into Amazon Machine Images (AMIs). Deploy the AMIs into Amazon EC2 instances to initiate cutover.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Leverage AWS Storage Gateway for the migration. Install the AWS Storage Gateway software appliance on their on-premises servers and use it to transfer their data to Amazon S3. Once the data is in S3, use Amazon EC2 to create virtual machines that replicate their on-premises infrastructure.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage AWS Application Migration Service (AWS MGN) for the migration. Install the AWS Replication agent on each physical machine to start the replication to the AWS Cloud. Once syncing is completed, launch test instances and initiate cutover to the AWS Cloud.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Application Migration Service (MGN)</strong> is a highly automated lift-and-shift (rehost) solution that simplifies, expedites, and reduces the cost of migrating applications to AWS. It enables companies to lift and shift a large number of physical, virtual, or cloud servers without compatibility issues, performance disruption, or long cutover windows. MGN replicates source servers into your AWS account.</p><p>When you’re ready, it automatically converts and launches your servers on AWS so you can quickly benefit from the cost savings, productivity, resilience, and agility of the Cloud. Once your applications are running on AWS, you can leverage AWS services and capabilities to quickly and easily re-platform or refactor those applications – which makes lift-and-shift a fast route to modernization.</p><p>AWS Application Migration Service minimizes time-intensive, error-prone manual processes by automatically converting your source servers from physical, virtual, or cloud infrastructure to run natively on AWS.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_mgn_overview.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_mgn_overview.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Leverage AWS Application Migration Service (AWS MGN) for the migration. Install the AWS Replication agent on each physical machine to start the replication to the AWS Cloud. Once syncing is completed, launch test instances and initiate cutover to the AWS Cloud.</strong> AWS Application Migration Service (AWS MGN) allows you to quickly lift-and-shift physical, virtual, or cloud servers to AWS. When you’re ready to migrate, it automatically converts and launches your servers on AWS.</p><p>The option that says: <strong>Leverage AWS Storage Gateway for the migration. Install the AWS Storage Gateway software appliance on their on-premises servers and use it to transfer their data to S3. Once the data is in Amazon S3, use Amazon EC2 to create virtual machines that replicate their on-premises infrastructure</strong> is incorrect because AWS Storage Gateway is primarily used for connecting an on-premises software appliance with cloud-based storage to provide seamless and secure integration between an organization’s on-premises IT environment and AWS’s storage infrastructure. It is not designed for migrating live applications and databases.</p><p>The option that says: <strong>Leverage AWS Outposts service for the migration of physical servers to AWS. Install the Outposts server agent on the data center to incrementally replicate the servers into Amazon Machine Images (AMIs). Deploy the AMIs into Amazon EC2 instances to initiate cutover </strong>is incorrect. AWS Outposts is a service that extends AWS infrastructure, services, APIs, and tools to customer premises. It allows customers to build and run applications on-premises using the same programming interfaces as in AWS Regions. It is not a migration service.</p><p>The option that says: <strong>Leverage the Application Discovery Service from AWS. Install the Application Discovery Service agents on each physical server and visualize the infrastructure on the AWS Migration Hub console. Trigger the replication to copy your servers to AWS. After the replication is completed, start the cutover to AWS</strong> is incorrect. AWS Application Discovery Service only gathers data for migration. It helps customers plan migration projects by gathering information about their on-premises data centers. It does not perform the actual migration of physical servers to AWS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/application-migration-service/\">https://aws.amazon.com/application-migration-service/</a></p><p><a href=\"https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html\">https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html</a></p><p><a href=\"https://docs.aws.amazon.com/mgn/latest/ug/first-time-setup-gs.html\">https://docs.aws.amazon.com/mgn/latest/ug/first-time-setup-gs.html</a></p><p><br></p><p><strong>Check out this AWS Application Migration Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-application-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-application-migration-service/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/application-migration-service/",
      "https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html",
      "https://docs.aws.amazon.com/mgn/latest/ug/first-time-setup-gs.html",
      "https://tutorialsdojo.com/aws-application-migration-service/?src=udemy"
    ]
  },
  {
    "id": 25,
    "question": "<p>A leading insurance company in South East Asia recently deployed a new web portal that enables your users to log in and manage their accounts, view their insurance plans, and pay their monthly premiums. After a few weeks, the solutions architect noticed that there is a significant amount of incoming traffic from a country in which the insurance company does not operate. Later on, it was determined that the same set of IP addresses coming from the unsupported country is sending out massive amounts of requests to your portal which has caused some performance issues on the application.</p><p>Which of the following options is the recommended solution to block the series of attacks coming from a set of determined IP ranges?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Launch a Security Group with explicit deny rules to block the attacking IP addresses.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Launch the online portal on the private subnet.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an inbound Network Access control list associated with explicit deny rules to block the attacking IP addresses.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create a custom route table associated with the web tier and block the attacking IP addresses from the Internet Gateway.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.</p><p>The following are the basic things that you need to know about network ACLs:</p><p>- Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic.</p><p>- You can create a custom network ACL and associate it with a subnet. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.</p><p>- Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</p><p>- You can associate a network ACL with multiple subnets. However, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.</p><p>- A network ACL contains a numbered list of rules. We evaluate the rules in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL. The highest number that you can use for a rule is 32766. We recommend that you start by creating rules in increments (for example, increments of 10 or 100) so that you can insert new rules where you need to later on.</p><p>- A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic.</p><p>- Network ACLs are stateless, which means that responses from allow inbound traffic are subject to the rules for outbound traffic (and vice versa).</p><p>The required task is to block all of the offending IP addresses from the unsupported country. In order to do this, you have to use Network Access Control List (NACL).</p><p><img src=\"https://media.tutorialsdojo.com/sap_nacls_subnet_example.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_nacls_subnet_example.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Create an inbound Network Access control list associated with explicit deny rules to block the attacking IP addresses.</strong></p><p>The option that says: <strong>Launch the online portal on the private subnet</strong> is incorrect because if you launch the online portal on the private subnet, then it will not be accessible to the users over the public Internet anymore.</p><p>The option that says: <strong>Create a custom route table associated with the web tier and blocking the attacking IP addresses from the Internet Gateway</strong> is incorrect because you do not have to change the route table nor make any changes to the Internet Gateway (IGW).</p><p>The option that says: <strong>Launch a Security Group with explicit deny rules to block the attacking IP addresses</strong> is incorrect because you cannot explicitly deny or block IP addresses using a security group.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><br></p><p><strong>Check out this security group and network access list comparison:</strong></p><p><a href=\"https://tutorialsdojo.com/security-group-vs-nacl/?src=udemy\">https://tutorialsdojo.com/security-group-vs-nacl/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy",
      "https://tutorialsdojo.com/security-group-vs-nacl/?src=udemy"
    ]
  },
  {
    "id": 26,
    "question": "<p>A company plans to migrate its on-premises legacy application to AWS and develop a highly scalable application. Currently, all user requests are sent to the on-premises load balancer which forwards the requests to two Linux servers hosting the legacy application. The database is hosted on two servers in master-master configuration. Since this is an old application, the communication to the database servers is done through static IP addresses and not via DNS names. The license of the application is tied to the MAC address of the network adapter of the Linux server. If the application is to be installed on a new server, it will take about 15 hours for the software vendor to send the new license via email.</p><p>Which combination of actions must be done to meet the company requirements? (Select TWO.)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Install the application on an EC2 instance and configure the needed license file. Update the local configuration with the database IP addresses. Use this instance as the base AMI for all instances in the Auto Scaling group.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an AWS Lambda function to update the database IP addresses on the Systems Manager Parameter Store. Create an Amazon EC2 bootstrap script that will retrieve the database IP address from SSM Parameter Store. Update the local configuration files with the parameters.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon EC2 bootstrap script that will resolve the database DNS names into IP addresses. Update the local configuration files with the resolved values.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Provision a pool of Elastic Network Interfaces (ENIs). Request a license file for each ENI from the software vendor. Store the license files inside an Amazon EC2 instance and create a base AMI from this EC2 instance. Use bootstrap scripts to configure license keys and attach the corresponding ENI when provisioning EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Provision a pool of Elastic Network Interfaces (ENIs). Request a license file for each ENI from the software vendor. Store the license files on an Amazon S3 bucket and use bootstrap scripts to retrieve an unused license file and attach corresponding ENI when provisioning EC2 instances.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Systems Manager Parameter Store</strong> provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter.</p><p>Parameter Store offers these benefits:</p><p>- You can use a secure, scalable, hosted secrets management service with no servers to manage.</p><p>- Improve your security posture by separating your data from your code.</p><p>- Store configuration data and encrypted strings in hierarchies and track versions.</p><p>- Control and audit access at granular levels.</p><p>Parameter Store provides support for three types of parameters: <code>String</code>, <code>StringList</code>, and <code>SecureString</code>.</p><p><img src=\"https://media.tutorialsdojo.com/public/Parameter-Store_29AUG2023.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/public/Parameter-Store_29AUG2023.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>An <strong>elastic network interface (ENI)</strong> is a logical networking component in a VPC that represents a virtual network card. It can include the following attributes:</p><p>- A primary private IPv4 address from the IPv4 address range of your VPC</p><p>- One or more secondary private IPv4 addresses from the IPv4 address range of your VPC</p><p>- One Elastic IP address (IPv4) per private IPv4 address</p><p>- One public IPv4 address</p><p>- One or more IPv6 addresses</p><p>- One or more security groups</p><p>- A MAC address</p><p>- A source/destination check flag</p><p>- A description</p><p>You can create and configure network interfaces in your account and attach them to instances in your VPC. Your account might also have requester-managed network interfaces, which are created and managed by AWS services to enable you to use other resources and services.</p><p>You can create a network interface, attach it to an instance, detach it from an instance, and attach it to another instance. The attributes of a network interface follow it as it's attached or detached from an instance and reattached to another instance. When you move a network interface from one instance to another, network traffic is redirected to the new instance.</p><p>Each instance has a default network interface, called the primary network interface. You cannot detach a primary network interface from an instance. You can create and attach additional network interfaces.</p><p>The option that says: <strong>Provision a pool of Elastic Network Interfaces (ENIs). Request a license file for each ENI from the software vendor. Store the license files on an Amazon S3 bucket and use bootstrap scripts to retrieve an unused license file and attach corresponding ENI when provisioning EC2 instances</strong> is correct. Having the license files on an Amazon S3 bucket reduces the management overhead for the EC2 instances, as you can easily add/remove more license keys if needed.</p><p>The option that says: <strong>Create an AWS Lambda function to update the database IP addresses on the Systems Manager Parameter Store. Create an Amazon EC2 bootstrap script that will retrieve the database IP address from SSM Parameter Store. Update the local configuration files with the parameters</strong> is correct. Having the database IP addresses on Parameter Store ensures that all the EC2 instances will have a central location to retrieve the IP addresses. This also reduces the need to constantly update any script from inside the EC2 instance even if you add/remove more databases in the future.</p><p>The option that says: <strong>Provision a pool of Elastic Network Interfaces (ENIs). Request a license file for each ENI from the software vendor. Store the license files inside an Amazon EC2 instance and create a base AMI from this EC2 instance. Use bootstrap scripts to configure license keys and attach the corresponding ENI when provisioning EC2 instances</strong> is incorrect. Although this is possible, this is not an ideal solution. If you need more EC2 instances and more ENIs, you will have to manually update your base AMI to include all the new license files. This can be a lot of work if you scale your cluster on a regular basis.</p><p>The option that says: <strong>Create an Amazon EC2 bootstrap script that will resolve the database DNS names into IP addresses. Update the local configuration files with the resolved values</strong> is incorrect. Although this is possible, this is not recommended. You will have to update the bootstrap script manually for the new DNS name every time you create a new database such as when you are scaling out your database instances.</p><p>The option that says: <strong>Install the application on an EC2 instance and configure the needed license file. Update the local configuration with the database IP addresses. Use this instance as the base AMI for all instances in the Auto Scaling group</strong> is incorrect. This will not work because the application license is tied to the MAC address on which it was installed. When you provision a new EC2 instance, it will have a new IP address and a newly assigned MAC address for its network adapter.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><a href=\"https://docs.aws.amazon.com/cdk/latest/guide/get_ssm_value.html\">https://docs.aws.amazon.com/cdk/latest/guide/get_ssm_value.html</a></p><p><br></p><p><strong>Check out these AWS SSM Parameter Store and Amazon EC2 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/?src=udemy\">https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html",
      "https://docs.aws.amazon.com/cdk/latest/guide/get_ssm_value.html",
      "https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/?src=udemy",
      "https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy"
    ]
  },
  {
    "id": 27,
    "question": "<p>A software development company based in New Jersey has tasked the solutions architect to design the network architecture for their new enterprise resource planning (ERP) system in AWS. The new system should allow access to business managers and analysts over the Internet, whether they are in their hotel rooms, cafes, or elsewhere. However, the ERP system should not be publicly accessible by anyone over the Internet but only by authorized personnel.</p><p>Which of the following network design meets the above requirements while minimizing deployment and operational costs?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Establish an AWS Direct Connect connection and create a private interface to your VPC. Create a public subnet and place your app servers in it.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Establish an IPsec VPN connection and provide the users with the configuration details. Create a public subnet in your VPC, and place your application servers in it.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Establish an SSL VPN solution in a public subnet of your VPC. Install and configure SSL VPN client software on all the workstations/laptops of the users who need access to the ERP system. Create a private subnet in your VPC and place your application servers behind it.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Deploy the ERP system behind an Elastic Load Balancer with an SSL certificate to allow HTTPS connections.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Secure Sockets Layer (SSL) VPN</strong> is an emerging technology that provides remote-access VPN capability, using the SSL function that is already built into a modern web browser. SSL VPN allows users from any Internet-enabled location to launch a web browser to establish remote-access VPN connections, thus promising productivity enhancements and improved availability, as well as further IT cost reduction for VPN client software and support.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_vpn.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_vpn.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Always keep in mind to install the SSL VPN software in your EC2 instances which are deployed in the public subnet. This will be where your users can connect to the Internet to be able to access the business applications, which is deployed in the private subnet of your VPC.</p><p>Therefore, the correct answer is: <strong>Establish an SSL VPN solution in a public subnet of your VPC. Install and configure SSL VPN client software on all the workstations/laptops of the users who need access to the ERP system. Create a private subnet in your VPC and place your application servers behind it</strong>. Configuring the SSL VPN solution is cost-effective and allows access only for business travelers and remote employees. And since the application servers are in the private subnet, the application is not accessible via the Internet.</p><p>The option that says: <strong>Establish an AWS Direct Connect connection and create a private interface to your VPC. Create a public subnet and place your app servers in it</strong> is incorrect. AWS Direct Connect is not a cost-effective solution compared with a VPN solution.</p><p>The option that says: <strong>Deploy the ERP system behind an Elastic Load Balancer with an SSL certificate to allow HTTPS connections</strong> is incorrect. It does not mention how the application would be accessible only for business travelers and remote employees, and not to the public.</p><p>The option that says: <strong>Establish an IPsec VPN connection and provide the users with the configuration details. Create a public subnet in your VPC, and place your application servers in it</strong> is incorrect. If the application servers are put in the public subnet, they would be publicly accessible via the Internet.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/quickstart/architecture/aviatrix-user-vpn/\">https://aws.amazon.com/quickstart/architecture/aviatrix-user-vpn/</a></p><p><a href=\"https://aws.amazon.com/articles/connecting-multiple-vpcs-with-ec2-instances-ssl\">https://aws.amazon.com/articles/connecting-multiple-vpcs-with-ec2-instances-ssl</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/quickstart/architecture/aviatrix-user-vpn/",
      "https://aws.amazon.com/articles/connecting-multiple-vpcs-with-ec2-instances-ssl",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 28,
    "question": "<p>A global data analytics firm has various data centers from different countries all over the world. The staff are regularly uploading analytics, financial, and regulatory files of each of their respective data centers to a web portal deployed in AWS, which uses an S3 bucket named <code>global-analytics-reports-bucket</code> to durably store the data. The staff download various reports from a CloudFront distribution which uses the <code>global-analytics-reports-bucket</code> S3 bucket as the origin. The security team noticed that the staff are using both the CloudFront link and the direct Amazon S3 URLs to download the reports. The security team sees this as a security risk and they recommended implementing a way to prevent anyone from bypassing CloudFront and using the direct Amazon S3 URLs.</p><p>Which of the following options should the solutions architect implement to meet the above requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "1. In your CloudFront distribution, use a custom SSL instead of the default SSL.\n2. Remove anyone else's permission to use Amazon S3 URLs to read the objects.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>1. Create a special CloudFront user called an origin access identity (OAI) and associate it with your CloudFront distribution.</p><p>2. Give the origin access identity permission to read the objects in your bucket.</p><p>3. Remove anyone else's permission to use Amazon S3 URLs to read the objects.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "1. Set up a field-level encryption configuration in the CloudFront distribution.\n2. Remove anyone else's permission to use Amazon S3 URLs to read the objects.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "1. Configure the distribution to use Signed URLs.\n2. Create a special CloudFront user called an origin access identity (OAI).\n3. Give the origin access identity permission to read the objects in your bucket.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>You can optionally secure the content in your Amazon S3 bucket so that users can access it through CloudFront but cannot access it directly by using Amazon S3 URLs. This prevents someone from bypassing CloudFront and using the Amazon S3 URL to get content that you want to restrict access to. This step isn't required to use signed URLs, but is recommended by AWS. Be aware that this option is only available if you have not set up your Amazon S3 bucket as a website endpoint.</p><p>To require that users access your content through CloudFront URLs, you do the following tasks:</p><p>- Create a special CloudFront user called an origin access control and associate it with your CloudFront distribution.</p><p>- Give the origin access control permission to read the files in your bucket.</p><p>- Remove permission for anyone else to use Amazon S3 URLs to read the files.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_oia.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_oia.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In this scenario, the main objective is to prevent the staff from using the direct Amazon S3 URLs to download the reports. The best solution that you can choose here is to use an Origin Access Control (OAC) and remove anyone else's permission to use the S3 URLs to read the objects. Hence, the correct answer is the following option:</p><p><strong>1. Create a special CloudFront user called an origin access control (OAC).</strong></p><p><strong>2. Give the origin access control permission to read the objects in your bucket.</strong></p><p><strong>3. Remove anyone else's permission to use Amazon S3 URLs to read the objects.</strong></p><p><br></p><p>The following option is incorrect because SSL is not needed in this particular scenario:</p><p><strong>1. In your CloudFront distribution, use a custom SSL instead of the default SSL.</strong></p><p><strong>2. Remove anyone else's permission to use Amazon S3 URLs to read the objects.</strong></p><p>What you need to implement is an OAC.</p><p><br></p><p>The following option is incorrect because the field-level encryption configuration is mainly used for safeguarding sensitive fields in your CloudFront, and not suitable for this scenario:</p><p><strong>1. Set up a field-level encryption configuration in the CloudFront distribution.</strong></p><p><strong>2. Remove anyone else's permission to use Amazon S3 URLs to read the objects.</strong></p><p><br></p><p>The following option is incorrect because although it is recommended to use Signed URLs and OAC in CloudFront, this option is still missing the crucial step of removing anyone else's permission to use the S3 URLs to read the objects:</p><p><strong>1. Configure the distribution to use Signed URLs.</strong></p><p><strong>2. Create a special CloudFront user called an origin access control (OAC).</strong></p><p><strong>3. Give the origin access control permission to read the objects in your bucket.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html#private-content-overview-s3\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html#private-content-overview-s3</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html#private-content-overview-s3",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy"
    ]
  },
  {
    "id": 29,
    "question": "<p>A big fast-food chain in Asia is planning to implement a location-based alert on their existing mobile app. If a user is in proximity to one of its restaurants, an alert will be shown on the user’s mobile phone. The notification needs to happen in less than a minute while the user is still in the vicinity. Currently, the mobile app has 10 million users in the Philippines, China, Korea, and other Asian countries.</p><p>Which one of the following AWS architecture is the most suitable option for this scenario?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Establish connectivity with mobile carriers using AWS Direct Connect. Set up an API on all EC2 instances to receive the location data from the mobile app via the carrier's GPS connection. Use RDS to store the data and fetch relevant offers from the restaurant. The EC2 instances will communicate with mobile carriers to send alerts to the mobile app.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The mobile app will send the real-time location data using Amazon Kinesis. Set up an API which uses an Application Load Balancer and an Auto Scaling group of EC2 instances to retrieve the relevant offers from a DynamoDB table. Use Amazon Lambda and SES to push the notification to the mobile app.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The mobile app will send device location to an SQS endpoint. Set up an API that utilizes an Application Load Balancer and an Auto Scaling group of EC2 instances, which will retrieve the relevant offers from DynamoDB. Use Amazon SNS to send offers to the mobile app.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Set up an API that uses an Application Load Balancer and an Auto Scaling group of EC2 instances. The mobile app will send the user's location data to the API web service. Use DynamoDB to store and retrieve relevant offers on the nearest restaurant. Configure the EC2 instances to push alerts to the mobile app.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Simple Queue Service (SQS)</strong> is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p><p>Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Additionally, SNS can be used to fan out notifications to end users using mobile push, SMS, and email.</p><p>With the <strong>Mobile Push feature of Amazon SNS</strong>, you have the ability to send push notification messages directly to apps on mobile devices. Push notification messages sent to a mobile endpoint can appear in the mobile app as message alerts, badge updates, or even sound alerts.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sns_publisher.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_sns_publisher.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You send push notification messages to both mobile devices and desktops using one of the following supported push notification services:</p><p>- Amazon Device Messaging (ADM)</p><p>- Apple Push Notification Service (APNS) for both iOS and Mac OS X</p><p>- Baidu Cloud Push (Baidu)</p><p>- Google Cloud Messaging for Android (GCM)</p><p>- Microsoft Push Notification Service for Windows Phone (MPNS)</p><p>- Windows Push Notification Services (WNS)</p><p>Therefore, the correct answer is: <strong>The mobile app will send device location to an SQS endpoint. Set up an API that utilizes an Application Load Balancer and an Auto Scaling group of EC2 instances, which will retrieve the relevant offers from DynamoDB. Use Amazon SNS to send offers to the mobile app.</strong></p><p>The option that says: <strong>Set up an API that uses an Application Load Balancer and an Auto Scaling group of EC2 instances. The mobile app will send the user's location data to the API web service. Use DynamoDB to store and retrieve relevant offers on the nearest restaurant. Configure the EC2 instances to push alerts to the mobile app </strong>is incorrect. Using EC2 instances to push alerts to the mobile app is not an appropriate solution. You have to use the AWS Mobile Push feature of SNS.</p><p>The option that says:<strong> Establish connectivity with mobile carriers using AWS Direct Connect. Set up an API on all EC2 instances to receive the location data from the mobile app via the carrier's GPS connection. Use RDS to store the data and fetch relevant offers from the restaurant. The EC2 instances will communicate with mobile carriers to send alerts to the mobile app </strong>is incorrect. AWS Direct Connect is primarily used to establish a dedicated network connection from your premises to AWS.</p><p>The option that says: <strong>The mobile app will send the real-time location data using Amazon Kinesis. Set up an API which uses an Application Load Balancer and an Auto Scaling group of EC2 instances to retrieve the relevant offers from a DynamoDB table. Use Amazon Lambda and SES to push the notification to the mobile app</strong> is incorrect. You can't use SES to send push notifications to mobile phones. You have to use SNS instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html\">https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html</a></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/mobile-push-send.html\">https://docs.aws.amazon.com/sns/latest/dg/mobile-push-send.html</a></p><p><br></p><p><strong>Check out this Amazon SNS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sns/?src=udemy\">https://tutorialsdojo.com/amazon-sns/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html",
      "https://docs.aws.amazon.com/sns/latest/dg/mobile-push-send.html",
      "https://tutorialsdojo.com/amazon-sns/?src=udemy"
    ]
  },
  {
    "id": 30,
    "question": "<p>A consumer goods company runs its e-commerce website entirely on its on-premises data center with high-resolution photos and videos. Due to the unprecedented growth of their popular product, they are expecting an increase in incoming traffic to their website across the globe in the coming days. The CTO requested to urgently do the necessary architectural changes to be able to handle the demand. The solutions architect suggested migrating the application to AWS, but the CTO decided that they need at least 3 months to implement a hybrid cloud architecture.</p><p>What could the solutions architect do with the current on-premises website to help offload some of the traffic and scale out to meet the demand in a cost-effective way?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Transit Gateway to establish a dedicated connection with the on-premises website and to manage and configure the servers with AWS App Runner to meet the demand.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Rehost the website to an S3 bucket with website hosting enabled. Create a CloudFront distribution with the S3 endpoint as the origin. Set up Origin Shield and launch a CloudFront Function to offload the DNS to AWS to handle CloudFront traffic.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Launch a CloudFront web distribution with the URL of the on-premises web application as the origin. Offload the DNS to AWS to handle CloudFront traffic.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Replicate the current web infrastructure of the on-premises website on AWS. Offload the DNS to Route 53 and configure weight-based DNS routing to send 50% of the traffic to AWS.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon CloudFront</strong> is a web service that speeds up the distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay) so that content is delivered with the best possible performance.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>- If the content is already in the edge location with the lowest latency, CloudFront delivers it immediately.</p><p>- If the content is not in that edge location, CloudFront retrieves it from an origin that you've defined—such as an Amazon S3 bucket, a MediaPackage channel, or an HTTP server (for example, a web server) that you have identified as the source for the definitive version of your content.</p><p>You can use CloudFront with the on-premises website as the origin. CloudFront is a highly available, scalable service that can cache frequently accessed files on the website and can significantly make the load times faster.</p><p>Therefore, the correct answer is: <strong>Launch a CloudFront web distribution with the URL of the on-premises web application as the origin. Offload the DNS to AWS to handle CloudFront traffic.</strong></p><p>The option that says:<strong> Use AWS Transit Gateway to establish a dedicated connection with the on-premises website and to manage and configure the servers with AWS App Runner to meet the demand</strong> is incorrect since you cannot create a hybrid dedicated connection to your on-premises network by using an AWS Transit Gateway alone. You have to use AWS Direct Connect for this to work. AWS App Runner is simply a fully managed container application service that lets you build, deploy, and run containerized web applications and API services. The scenario didn't mention any container-based application; thus, AWS App Runner is irrelevant in this case.</p><p>The option that says: <strong>Rehost the website to an S3 bucket with website hosting enabled. Create a CloudFront distribution with the S3 endpoint as the origin. Set up Origin Shield and launch a CloudFront Function to offload the DNS to AWS to handle CloudFront traffic </strong>is incorrect. While S3 can be used for hosting websites, it's only suitable for static websites and simple landing pages, not for full-blown web applications that require dynamic content processing and user interaction. Lastly, the CloudFront function is not capable of DNS offloading, which is typically handled by DNS management services like Route 53.</p><p>The option that says: <strong>Replicate the current web infrastructure of the on-premises website on AWS. Offload the DNS to Route 53 and configure weight-based DNS routing to send 50% of the traffic to AWS</strong> is incorrect as this option is time-consuming and you don't have enough time to replicate the entire architecture of the on-premises website.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-custom-origins/\">https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-custom-origins/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/cloudfront/",
      "https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-custom-origins/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy"
    ]
  },
  {
    "id": 31,
    "question": "<p>A fintech startup has several resources provisioned on the AWS cloud. The majority of the company’s compute clusters are composed of an Application Load Balancer (ALB) in front of an Auto Scaling group of On-Demand Amazon EC2 instances. To lower down the overall cost, the management wants to have one EC2 instance terminated whenever the overall CPU utilization of the cluster is at 15% or lower.</p><p>Which of the following options should the solutions architect implement for a cost-effective and scalable architecture that satisfies the company requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Lambda triggers to send a notification to the Auto Scaling group when the CPU utilization is less than 15% to kick off the scaling in policy to remove the EC2 instance.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use scheduled actions in the Auto Scaling configuration to automatically terminate EC2 instances when the CPU Utilization hits below 15%.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use CloudWatch for the monitoring and configure the scaling in policy of the Auto Scaling group to terminate one EC2 instance when the CPU Utilization is 15% or below.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure a monitoring script that sends out an email using SNS when the CPU utilization is less than 15% so the administrator can manually remove an EC2 instance.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Simple scaling policies</strong> enable you to take specific actions in response to CloudWatch alarms triggered by changes in metrics like CPU utilization. You can configure these policies to add or remove a specified number of instances when a metric crosses a defined threshold. <strong>Amazon EC2 Auto Scaling</strong> is responsible for creating and managing the CloudWatch alarms that trigger the scaling policy and calculate the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as needed to maintain the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy adjusts to fluctuations in the metric due to a fluctuating load pattern and minimizes rapid fluctuations in the capacity of the Auto Scaling group.</p><p><img src=\"https://media.tutorialsdojo.com/sap_autoscaling_group.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_autoscaling_group.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>For example, you could use simple scaling to:</p><p>Configure a simple scaling policy to add instances when your Auto Scaling group's average CPU utilization exceeds 70% and remove instances when it falls below 30%.</p><p>Configure a simple scaling policy to scale up when the request count per target of your Elastic Load Balancing target group exceeds 1200 and scale down when it falls below 800.</p><p>Therefore, the correct answer is: <strong>Use CloudWatch for the monitoring and configure the scaling in policy of the Auto Scaling group to terminate one EC2 instance when the CPU Utilization is 15% or below.</strong></p><p>The option that says: <strong>Use scheduled actions in the Auto Scaling configuration to automatically terminate EC2 instances when the CPU Utilization hits below 15%</strong> is incorrect. You can't use scheduled actions because the CPU usage of the instances can be unpredictable depending on the network traffic.</p><p>The option that says: <strong>Configure a monitoring script that sends out an email using SNS when the CPU utilization is less than 15% so the administrator can manually remove an EC2 instance</strong> is incorrect. You don't have to configure your own monitoring script. Amazon CloudWatch has integration with Amazon SNS to send scaling notifications to you.</p><p>The option that says: <strong>Use AWS Lambda triggers to send a notification to the Auto Scaling group when the CPU utilization is less than 15% to kick off the scaling in policy to remove the EC2 instance</strong> is incorrect. This may be possible, but you don't have to create your own Lambda triggers. The Auto Scaling group has built-in functionality to automatically remove an EC2 instance depending on your CPU threshold setting.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/autoscaling-policy-cloudwatch-alarm/\">https://aws.amazon.com/premiumsupport/knowledge-center/autoscaling-policy-cloudwatch-alarm/</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-monitoring-features.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-monitoring-features.html</a></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/autoscaling-policy-cloudwatch-alarm/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-monitoring-features.html",
      "https://tutorialsdojo.com/aws-auto-scaling/?src=udemy"
    ]
  },
  {
    "id": 32,
    "question": "<p>A company launched a high-performance computing (HPC) application inside the VPC of its AWS account. The application is composed of hundreds of private EC2 instances running in a cluster placement group, which allows the instances to communicate with each other at network speeds of up to 10 Gbps. There is also a custom cluster controller EC2 instance that closely controls and monitors the system performance of each instance. The cluster controller has the same instance type and AMI as the other instances. It is configured with a public IP address and runs outside the placement group. The Solutions Architect has been tasked to improve the network performance between the controller instance and the EC2 instances in the placement group.</p><p>Which option provides the MOST suitable solution that the Architect must implement to satisfy the requirement while maintaining low-latency network performance?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Attach an Elastic IP address to the custom cluster controller instance to increase its network capability to 10 Gbps.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Terminate the custom cluster controller EC2 instance and stop all of the running instances in the existing placement group. Move the cluster controller instance to the existing placement group and restart all of the instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Terminate the custom cluster controller instance and re-launch it to the existing placement group. Attach an Elastic Network Adapter (ENA) to the cluster controller instance to increase its network performance. Change the placement strategy of the placement group to <code>Spread</code>.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Stop the custom cluster controller instance and move it to the existing placement group.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>Cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also recommended when the majority of the network traffic is between the instances in the group. To provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking.</p><p><img src=\"https://media.tutorialsdojo.com/sap_az1_placement_group.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_az1_placement_group.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Launching instances in a spread placement group reduces the risk of simultaneous failures that might occur when instances share the same racks. Spread placement groups provide access to distinct racks, and are therefore suitable for mixing instance types or launching instances over time.</p><p><img src=\"https://media.tutorialsdojo.com/sap_az1_placement_group_instances.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_az1_placement_group_instances.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can change the placement group for an instance in any of the following ways:</p><p>Move an existing instance to a placement group</p><p>Move an instance from one placement group to another</p><p>Remove an instance from a placement group</p><p>Before you move or remove the instance, the instance must be in the stopped state. You can move or remove an instance using the AWS CLI or an AWS SDK.</p><p>Hence, the correct answer is: <strong>Stop the custom cluster controller instance and move it to the existing placement group.</strong></p><p>The option that says: <strong>Terminate the custom cluster controller EC2 instance and stop all of the running instances in the existing placement group. Move the cluster controller instance to the existing placement group and restart all of the instances</strong> is incorrect because you don't need to restart or terminate any instance to move a new EC2 instance to an existing placement group. You just have to stop the cluster controller instance, move it to the placement group, and restart it.</p><p>The option that says:<strong> Attach an Elastic IP address to the custom cluster controller instance to increase its network capability to 10 Gbps</strong> is incorrect because an Elastic IP is simply a static IPv4 address designed for dynamic cloud computing. It doesn't increase the network bandwidth of the instance to 10 Gbps either.</p><p>The option that says: <strong>Terminate the custom cluster controller instance and re-launch it to the existing placement group. Attach an Elastic Network Adapter (ENA) to the cluster controller instance to increase its network performance. Change the placement strategy of the placement group to </strong><code><strong>Spread</strong></code> is incorrect because using a Spread placement group will degrade the existing network performance of the architecture. The use of a Cluster placement group must be maintained. And while it is true that the Elastic Network Adapter (ENA) can increase the instance network performance, the described process of moving the custom cluster controller instance to the placement group is still incorrect. You can just stop the instance and directly include it to the placement group. There is no need to terminate the EC2 instance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html",
      "https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy"
    ]
  },
  {
    "id": 33,
    "question": "<p>An enterprise is in the process of integrating the systems of the smaller companies it has acquired in the past few months. The company wants to create an AWS Landing Zone that will allow hundreds of new employees to use their corporate credentials to log in to the AWS Console. The company is using a Microsoft Active Directory (AD) service for user authentication and has an AWS Direct Connect connection to AWS. The newly acquired companies come from a wide range of engineering fields so it is required that the solution will be able to federate third-party services and providers as well as custom applications.</p><p>Which of the following implementations will meet the company requirements with the LEAST amount of management overhead?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Active Directory Federation Services (AD FA) with SAML 2.0 to connect the company Active Directory to AWS. Configure the AD FS to use Regex with the AD naming convention for the security group. This will allow federation on all AWS accounts. Configure single sign-on integrations for third party applications by adding them to the AD FS server.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Connect the company on-premises Active Directory using the AWS Directory Service AD connector to create a single sign-on experience for users. Configure IAM and service roles to enable federation support. Configure single sign-on integrations for connections with third-party applications.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure AWS IAM Identity Center with AWS Organizations to manage SSO access and permissions on AWS. Set up a two-way forest trust relationship between the AWS Directory service and the company Active Directory to allow users to use their corporate credentials when logging in to AWS. Leverage on the third-party integration support of AWS IAM Identity Center.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Active Directory Federation Services (AD FS) portal page with the company branding. Integrate third-party applications on this portal with SAML 2.0 support. Use single sign-on with the AD FS to connect the company Active Directory to AWS. Configure the Identity Provider (IdP) to use form-based authentication with the portal page.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS IAM Identity Center</strong> (successor to AWS Single Sign-On) expands the capabilities of AWS Identity and Access Management (IAM) to provide a central place that brings together administration of users and their access to AWS accounts and cloud applications. Although the service name AWS Single Sign-On has been retired, the term single sign-on is still used throughout this guide to describe the authentication scheme that allows users to sign in one time to access multiple applications and websites.</p><p>With IAM Identity Center, you can manage sign-in security for your workforce by creating or connecting your users and groups to AWS in one place. With multi-account permissions, you can assign your workforce identities access to AWS accounts. You can use application assignments to assign your users access to software as a service (SaaS) applications. With a single click, IAM Identity Center enabled application admins can assign access to your workforce users and can also use application assignments to assign your users access to the software as a service (SaaS) applications.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_directory_service.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_directory_service.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>AWS IAM Identity Center has integration with Microsoft AD through the <strong>AWS Directory Service</strong>. This means your employees can sign in to your AWS access portal using their corporate Active Directory credentials. To grant Active Directory users access to accounts and applications, you simply add them to the appropriate Active Directory groups. For example, you can grant the DevOps group SSO access to your production AWS accounts. Users added to the DevOps group are then granted SSO access to these AWS accounts automatically. This automation makes it easy to onboard new users and gives existing users access to new accounts and applications quickly.</p><p>You can configure one and two-way external and forest trust relationships between your AWS Directory Service for Microsoft Active Directory and on-premises directories, as well as between multiple AWS Managed Microsoft AD directories in the AWS cloud. AWS Managed Microsoft AD supports all three trust relationship directions: Incoming, Outgoing, and Two-way (Bi-directional). AWS Managed Microsoft AD supports both external and forest trusts.</p><p>Users in your self-managed Active Directory (AD) can also have SSO access to AWS accounts and cloud applications in the AWS access portal. To do that, AWS Directory Service has the following two options available:</p><p><strong>Create a two-way trust relationship</strong> – When two-way trust relationships are created between AWS Managed Microsoft AD and a self-managed AD, users in your self-managed AD can sign in with their corporate credentials to various AWS services and business applications. One-way trusts do not work with AWS IAM Identity Center.</p><p>AWS IAM Identity Center (successor to AWS Single Sign-On) requires a two-way trust so that it has permission to read user and group information from your domain to synchronize user and group metadata. IAM Identity Center uses this metadata when assigning access to permission sets or applications. User and group metadata is also used by applications for collaboration, like when you share a dashboard with another user or group. The trust from AWS Directory Service for Microsoft Active Directory to your domain permits IAM Identity Center to trust your domain for authentication. The trust in the opposite direction grants AWS permissions to read user and group metadata.</p><p><strong>Create an AD Connector</strong> – AD Connector is a directory gateway that can redirect directory requests to your self-managed AD without caching any information in the cloud.</p><p>Therefore, the correct answer is: <strong>Configure AWS IAM Identity Center with AWS Organizations to manage SSO access and permissions on AWS. Set up a two-way forest trust relationship between the AWS Directory service and the company Active Directory to allow users to use their corporate credentials when logging in to AWS. Leverage on the third-party integration support of AWS IAM Identity Center.</strong></p><p>The option that says: <strong>Create an Active Directory Federation Services (AD FS) portal page with the company branding. Integrate third-party applications on this portal with SAML 2.0 support. Use single sign-on with the AD FS to connect the company Active Directory to AWS. Configure the Identity Provider (IdP) to use form-based authentication with the portal page</strong> is incorrect. This may be possible, but creating a form-based authentication defeats the purpose of a single sign-on. Also, this requires more additional management to create the AD FS portal page.</p><p>The option that says: <strong>Connect the company on-premises Active Directory using the AWS Directory Service AD connector to create a single sign-on experience for users. Configure IAM and service roles to enable federation support. Configure single sign-on integrations for connections with third-party applications</strong> is incorrect. This does not address the third-party integrations because when using the AD connector for SSO, you cannot use both on-premises AD and third-party integrations at the same time.</p><p>The option that says: <strong>Create an Active Directory Federation Services (AD FS) with SAML 2.0 to connect the company Active Directory to AWS. Configure the AD FS to use Regex with the AD naming convention for the security group. This will allow federation on all AWS accounts. Configure single sign-on integrations for third party applications by adding them to the AD FS server as a principal (trusted entity)</strong> is incorrect. This is possible but will require more management overhead compared to just using AWS IAM Identity Center service. Using Regex is favorable if you have established standard naming conventions, however, you may encounter some problems if other companies are not using your expected naming convention.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html</a></p><p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_setup_trust.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_setup_trust.html</a></p><p><a href=\"https://docs.microsoft.com/en-us/azure/active-directory-domain-services/concepts-forest-trust\">https://docs.microsoft.com/en-us/azure/active-directory-domain-services/concepts-forest-trust</a></p><p><br></p><p><strong>Check out this AWS Directory Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-directory-service/?src=udemy\">https://tutorialsdojo.com/aws-directory-service/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html",
      "https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_setup_trust.html",
      "https://docs.microsoft.com/en-us/azure/active-directory-domain-services/concepts-forest-trust",
      "https://tutorialsdojo.com/aws-directory-service/?src=udemy"
    ]
  },
  {
    "id": 34,
    "question": "<p>An AWS Partner company hosts all its infrastructure on the AWS cloud. All resources are currently deployed in the us-east-1 region. The company plans to expand its business to include deployments in Europe and Asia. The solutions architect has been tasked to provision the needed resources on multiple regions across multiple AWS accounts under the company’s AWS Organization.</p><p>Which of the following options is the recommended solution to meet the company requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Organizations to centrally manage the deployment of AWS CloudFormation template from the central account. Use AWS Control Tower as an orchestration layer deploying resources across multiple accounts and regions.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Write infrastructure-as-code to maintain consistency. Create nested stacks with AWS CloudFormation templates and use global parameters to specify which target region and accounts to provision the needed resources.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Write infrastructure-as-code to maintain consistency. Create AWS CloudFormation templates and create IAM policies to control multiple accounts. Use regional parameters when deploying CloudFormation templates across multiple regions to provision the needed resources.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Write infrastructure-as-code to maintain consistency. Use AWS Organizations to centrally orchestrate the deployment of AWS CloudFormation template from the central account. Use CloudFormation StackSets to simplify permissions and automatic provisioning of resources across multiple regions and accounts.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p><strong>AWS CloudFormation StackSets</strong> allow you to roll out CloudFormation stacks over multiple AWS accounts and in multiple Regions with just a couple of clicks. With AWS Organizations, you can centrally manage multiple AWS accounts across diverse business needs including billing, access control, compliance, security and resource sharing.</p><p>Using <strong>AWS Organizations</strong> you can centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions. For example, you can deploy your centralized AWS Identity and Access Management (IAM) roles, provision Amazon Elastic Compute Cloud (Amazon EC2) instances or AWS Lambda functions across AWS Regions and accounts in your organization. CloudFormation StackSets simplify the configuration of cross-accounts permissions and allow for automatic creation and deletion of resources when accounts are joining or are removed from your Organization.</p><p>You can get started by enabling data sharing between CloudFormation and Organizations from the StackSets console. Once done, you will be able to use StackSets in the Organizations master account to deploy stacks to all accounts in your organization or in specific organizational units (OUs).</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_aws_organizations.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_aws_organizations.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Write infrastructure-as-code to maintain consistency. Use AWS Organizations to centrally orchestrate the deployment of AWS CloudFormation template from the central account. Use CloudFormation StackSets to simplify permissions and automatic provisioning of resources across multiple regions and accounts.</strong> With AWS Organizations, you can centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions.</p><p>The option that says: <strong>Write infrastructure-as-code to maintain consistency. Create AWS CloudFormation templates and create IAM policies to control multiple accounts. Use regional parameters when deploying CloudFormation templates across multiple regions to provision the needed resources</strong> is incorrect. This is not recommended as it entails more operational overhead. It is recommended to use CloudFormation StackSets to deploy stack across multiple regions and accounts in a single operation.</p><p>The option that says: <strong>Use AWS Organizations to centrally manage the deployment of AWS CloudFormation template from the central account. Use AWS Control Tower as an orchestration layer deploying resources across multiple accounts and regions</strong> is incorrect. It is possible to use AWS Control Tower as an orchestration layer for multi-account environments. However, for this solution to work, you still need to create CloudFormation StackSets for deployment on multiple regions.</p><p>The option that says: <strong>Write infrastructure-as-code to maintain consistency. Create nested stacks with AWS CloudFormation templates and use global parameters to specify which target region and accounts to provision the needed resources</strong> is incorrect. This is not possible, if you want to deploy CloudFormation stacks on multiple regions and accounts in a single operation, you should use CloudFormation StackSets.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/\">https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p><p><br></p><p><strong>Check out these AWS Organizations and AWS CloudFormation Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/",
      "https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html",
      "https://tutorialsdojo.com/aws-organizations/?src=udemy",
      "https://tutorialsdojo.com/aws-cloudformation/?src=udemy"
    ]
  },
  {
    "id": 35,
    "question": "<p>A leading media company in the country is building a voting system for a popular singing competition show on national TV. The viewers who watch the performances can visit the company’s dynamic website to vote for their favorite singer. After the show has finished, it is expected that the site will receive millions of visitors who would like to cast their votes. Web visitors should log in using their social media accounts and then submit their votes. The webpage will display the winner after the show, as well as the vote total for each singer. The solutions architect is tasked to build the voting site and ensure that it can handle the rapid influx of incoming traffic in the most cost-effective way possible.</p><p>Which of the following architecture should you use to meet the requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use a CloudFront web distribution and an Application Load balancer in front of an Auto Scaling group of EC2 instances. The servers will first authenticate the user using IAM and then process the user's vote which will then be stored to RDS.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use a CloudFront web distribution and an Application Load Balancer in front of an Auto Scaling group of EC2 instances. Use Amazon Cognito for user authentication. The web servers will process the user's vote and pass the result in an SQS queue. Set up an IAM Role to grant the EC2 instances permissions to write to the SQS queue. A group of EC2 instances will then retrieve and process the items from the queue. Finally, store the results in a DynamoDB table.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use a CloudFront web distribution and deploy the website using S3 hosting feature. Write a custom NodeJS application to authenticate the user using STS and AssumeRole API. Setup an IAM Role to grant permission to store the user's vote to a DynamoDB table.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use a CloudFront web distribution and an Application Load Balancer in front of an Auto Scaling group of EC2 instances. Develop a custom authentication service using STS and AssumeRoleWithSAML API. The servers will process the user's vote and store the result in a DynamoDB table.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>For User authentication, you can use <strong>Amazon Cognito</strong>. To host the static assets of the website, you can use CloudFront. Considering that there would be millions of voters and data to be stored, it is best to use DynamoDB which can automatically scale.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cognito_identity_pool.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cognito_identity_pool.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0.</p><p>Therefore, the correct answer is: <strong>Use a CloudFront web distribution and an Application Load Balancer in front of an Auto Scaling group of EC2 instances. Use Amazon Cognito for user authentication. The web servers will process the user's vote and pass the result in an SQS queue. Set up an IAM Role to grant the EC2 instances permissions to write to the SQS queue. A group of EC2 instances will then retrieve and process the items from the queue. Finally, store the results in a DynamoDB table.</strong></p><p>The option that says: <strong>Use a CloudFront web distribution and an Application Load balancer in front of an Auto Scaling group of EC2 instances. The servers will first authenticate the user using IAM and then process the user's vote which will then be stored to RDS</strong> is incorrect. By default, you can't use IAM alone to set up social media account registration to your website. You have to use Amazon Cognito. It is also more suitable to use DynamoDB instead of an RDS database since this is only a simple voting application that doesn't warrant a complex table relationship. DynamoDB is a fully managed database that automatically scales, unlike RDS. It can store and accommodate millions of data from the users who cast their votes more effectively than RDS.</p><p>The option that says:<strong> Use a CloudFront web distribution and deploy the website using S3 hosting feature. Write a custom NodeJS application to authenticate the user using STS and AssumeRole API. Setup an IAM Role to grant permission to store the user's vote to a DynamoDB table</strong> is incorrect. The <code>AssumeRole</code> API is not suitable for user authentication that uses a web identity provider such as Amazon Cognito, Login with Amazon, Facebook, Google, or any social media identity provider. You can use the AssumeRoleWithWebIdentity API or better yet, Amazon Cognito instead.</p><p>The option that says: <strong>Use a CloudFront web distribution and an Application Load Balancer in front of an Auto Scaling group of EC2 instances. Develop a custom authentication service using STS and AssumeRoleWithSAML API. The servers will process the user's vote and store the result in a DynamoDB table</strong> is incorrect. The <code>AssumeRoleWithSAML</code> API is primarily used for SAML 2.0-based federation and for social media user registration.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cognito/\">https://aws.amazon.com/cognito/</a></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html</a></p><p><br></p><p><strong>Check out these Amazon Cognito Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cognito/?src=udemy\">https://tutorialsdojo.com/amazon-cognito/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/?src=udemy\">https://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/cognito/",
      "https://aws.amazon.com/cloudfront/",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html",
      "https://tutorialsdojo.com/amazon-cognito/?src=udemy",
      "https://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/?src=udemy"
    ]
  },
  {
    "id": 36,
    "question": "<p>A startup is developing a health-related mobile app for both iOS and Android devices. The co-founder developed a sleep tracking app that collects the user's biometric data then stores them in an Amazon DynamoDB table, which is configured with an on-demand provisioned throughput capacity. Every nine in the morning, a scheduled task scans the DynamoDB table to extract and aggregate last night’s data for each user and stores the results in an Amazon S3 bucket. When the new data is available, the users are then notified via Amazon SNS mobile push notifications. Due to budget constraints, the management wants to optimize the current architecture of the backend system to lower costs and increase the overall revenue.</p><p>Which of the following options can the solutions architect implement to further lower the cost in AWS? (Select TWO.)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Launch a Redshift cluster to replace Amazon DynamoDB. Switch from a Standard S3 bucket to One Zone-Infrequent Access storage class.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use ElastiCache to cache reads and writes from the DynamoDB table.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up a scheduled job to drop the DynamoDB table for the previous day that contains the biometric data after it is successfully stored in the S3 bucket. Create another DynamoDB table for the day and perform the deletion and creation process everyday.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use a RDS instance configured with Multi-AZ deployments and Read Replicas as a replacement to your DynamoDB.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Avail a reserved capacity for provisioned throughput for DynamoDB.",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>You can purchase reserved capacity in advance to lower the costs of running your DynamoDB instance. With reserved capacity, you pay a one-time upfront fee and commit to a minimum usage level over a period of time. By reserving your read and write capacity units ahead of time, you realize significant cost savings compared to on-demand provisioned throughput settings. In addition, you can also drop the DynamoDB table which contains the biometric data right after it is successfully stored in S3.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamoDB_auto_scaling.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_dynamoDB_auto_scaling.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The option that says: <strong>Avail a reserved capacity for provisioned throughput for DynamoDB</strong> is correct. If you can predict your need for Amazon DynamoDB read-and-write throughput, reserved capacity offers significant savings over the normal price of DynamoDB provisioned throughput capacity.</p><p>The option that says: <strong>Set up a scheduled job to drop the DynamoDB table for the previous day that contains the biometric data after it is successfully stored in the S3 bucket. Create another DynamoDB table for the day and perform the deletion and creation process everyday</strong> is correct. This saves costs by not storing too much data on DynamoDB tables.</p><p>The option that says: <strong>Use a RDS instance configured with Multi-AZ deployments and Read Replicas as a replacement to your DynamoDB</strong> is incorrect because using an RDS database with Multi-AZ deployments and Read Replicas will actually cost more than the current architecture. The scenario just wants to lower the cost and it didn't specify the need to shift to a NoSQL database.</p><p>The option that says: <strong>Launch a Redshift cluster to replace Amazon DynamoDB. Switch from a Standard S3 bucket to One Zone-Infrequent Access storage class</strong> is incorrect because Redshift is primarily used for online analytical processing (OLAP) applications and not as a NoSQL database. This change also entails a significant amount of time and resources to execute.</p><p>The option that says: <strong>Use ElastiCache to cache reads and writes from the DynamoDB table</strong> is incorrect. Although an ElastiCache cluster can lower the CPU utilization of your EC2 instances and improve application performance, it doesn't provide significant cost reduction as compared to Options 3 and 5. Take note that this will also increase the cost since you have to pay for your ElastiCache cluster.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html#HowItWorks.ProvisionedThroughput.ReservedCapacity\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html#HowItWorks.ProvisionedThroughput.ReservedCapacity</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-reservation-models/amazon-dynamodb-reservations.html\">https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-reservation-models/amazon-dynamodb-reservations.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html#HowItWorks.ProvisionedThroughput.ReservedCapacity",
      "https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-reservation-models/amazon-dynamodb-reservations.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html",
      "https://tutorialsdojo.com/amazon-dynamodb/?src=udemy"
    ]
  },
  {
    "id": 37,
    "question": "<p>A government technology agency has recently hired a team to build a mobile tax app that allows users to upload their tax deductions and income records using their devices. The app would also allow users to view or download their uploaded files later on. These files are confidential, tax-related documents that need to be stored in a single, secure S3 bucket. The mobile app's design is to allow the users to upload, view, and download their files directly from an Amazon S3 bucket via the mobile app. Since this app will be used by potentially hundreds of thousands of taxpayers in the country, the solutions architect must ensure that proper user authentication and security features are in place.</p><p>Which of the following options should the solutions architect implement in the infrastructure when a new user registers on the app?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon DynamoDB to record the user's information and when the user uses the mobile app, create access credentials using STS with appropriate permissions. Store these credentials in the mobile app's memory and use them to access the S3 bucket every time the user runs the app.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Record the user's information in Amazon RDS and create a role in IAM with appropriate permissions. When the user uses his/her mobile app, create temporary credentials using the 'AssumeRole' function in STS. Store these credentials in the mobile app's memory and use them to access the S3 bucket. Generate new credentials the next time the user runs the mobile app.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a set of long-term credentials using AWS Security Token Service with appropriate permissions. Store these credentials in the mobile app and use them to access Amazon S3.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an IAM user then assign appropriate permissions to the IAM user. Generate an access key and secret key for the IAM user, store them in the mobile app and use these credentials to access Amazon S3.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>This scenario requires the mobile application to have access to the S3 bucket. The mobile app might potentially have millions of tax-paying users that will upload their documents to S3. In this scenario where mobile applications need to access AWS Resources, always think about using STS actions such as <strong>\"AssumeRole\"</strong>, <strong>\"AssumeRoleWithSAML\"</strong>, and <strong>\"AssumeRoleWithWebIdentity\"</strong>.</p><p>You can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences:</p><p>-Temporary security credentials are <em>short-term</em>, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them.</p><p>-Temporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permission to do so.</p><p><img src=\"https://media.tutorialsdojo.com/sap_assumeRole.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_assumeRole.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can let users sign in using a well-known third-party identity provider such as Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC) 2.0 compatible provider. You can exchange the credentials from that provider for temporary permissions to use resources in your AWS account. This is known as the web identity federation approach to temporary access. When you use web identity federation for your mobile or web application, you don't need to create custom sign-in codes or manage your own user identities. Using web identity federation helps you keep your AWS account secure because you don't have to distribute long-term security credentials, such as IAM user access keys, with your application.</p><p>The option that says: <strong>Record the user's information in Amazon RDS and create a role in IAM with appropriate permissions. When the user uses his/her mobile app, create temporary credentials using the 'AssumeRole' function in STS. Store these credentials in the mobile app's memory and use them to access the S3 bucket. Generate new credentials the next time the user runs the mobile app</strong> is correct because it creates an IAM Role with the required permissions and generates temporary security credentials using STS \"AssumeRole\" function. Furthermore, it generates new credentials when the user runs the app the next time around.</p><p>The option that says: <strong>Create a set of long-term credentials using AWS Security Token Service with appropriate permissions. Store these credentials in the mobile app and use them to access Amazon S3</strong> is incorrect because you should never store credentials inside a mobile app for security purposes to avoid the risk of exposing any access keys or passwords. You should instead grant temporary credentials for the mobile app. In addition, you cannot create long-term credentials using AWS STS, as this service can only generate temporary access tokens.</p><p>The option that says: <strong>Use Amazon DynamoDB to record the user's information and when the user uses the mobile app, create access credentials using STS with appropriate permissions. Store these credentials in the mobile app's memory and use them to access the S3 bucket every time the user runs the app</strong> is incorrect. Even though the setup is similar to the previous option and uses DynamoDB, it is still wrong to store long-term credentials in a mobile app as it is a security risk. In addition, it does not create an IAM Role with proper permissions, which is an essential step.</p><p>The option that says:<strong> Create an IAM user then assign appropriate permissions to the IAM user. Generate an access key and secret key for the IAM user, store them in the mobile app and use these credentials to access Amazon S3</strong> is incorrect because it creates an IAM User and not an IAM Role. You should create an IAM Role so that the app can access the AWS Resource via the \"AssumeRole\" action in STS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a></p><p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html",
      "https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy"
    ]
  },
  {
    "id": 38,
    "question": "<p>A company manages more than 50 AWS accounts under its AWS Organization. All AWS accounts deploy resources on a single AWS region only. To enable routing across all accounts, each VPC has a Transit Gateway Attachment to a centralized AWS Transit Gateway. Each VPC also has an internet gateway and NAT gateway to provide outbound internet connectivity for its resources. As a security requirement, the company must have a centrally managed rule-based filtering solution for outbound internet traffic on all AWS accounts under its organization. It is expected that peak outbound traffic for each Availability Zone will not exceed 25 Gbps.</p><p>Which of the following options should the solutions architect implement to fulfill the company requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a dedicated VPC for outbound internet traffic with a NAT gateway on it. Connect this VPC to the existing AWS Transit Gateway. Configure an AWS Network Firewall firewall for the rule-based filtering. Modify all the default routes in each account to point to the Network Firewall endpoint.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a dedicated VPC for outbound internet traffic with a NAT gateway on it. Connect this VPC to the existing AWS Transit Gateway. On this VPC, create an Auto Scaling group of Amazon EC2 instances running with an open-source internet proxy software for rule-based filtering across all AZ in the region. Configure the route tables on each VPC to point to this Auto Scaling group.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Provision an Auto Scaling group of Amazon EC2 instances with network-optimized instance type on each AWS account. Install an open-source internet proxy software for rule-based filtering. Configure the route tables on each VPC to point to the Auto Scaling group.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Network Firewall service to create firewall rule groups and firewall policies for rule-based filtering. Attach the firewall policy to a new Network Firewall firewall on each account. Modify all the default routes in each account to point to their corresponding Network Firewall firewall.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS Network Firewall</strong> is a stateful, managed, network firewall and intrusion detection and prevention service for your virtual private cloud (VPC) that you created in Amazon Virtual Private Cloud (Amazon VPC). With Network Firewall, you can filter traffic at the perimeter of your VPC. This includes filtering traffic going to and coming from an internet gateway, NAT gateway, or over VPN or AWS Direct Connect.</p><p>Once AWS Network Firewall is deployed, you will see a firewall endpoint in each firewall subnet. Firewall endpoint is similar to interface endpoint and it shows up as vpce-id in your VPC route table target selection. You have multiple deployment models for Network Firewall.</p><p>For a centralized egress deployment model, an AWS Transit Gateway is a prerequisite. AWS Transit Gateway acts as a network hub and simplifies the connectivity between VPCs. For this model, we have a dedicated, central egress VPC which has a NAT gateway configured in a public subnet with access to IGW.</p><p><img src=\"https://media.tutorialsdojo.com/sap_network_firewall_central_egress.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_network_firewall_central_egress.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Traffic originating from spoke VPCs is forwarded to inspection VPC for processing. It is then forwarded to central egress VPC using a default route in the Transit Gateway firewall route table. The default route is set to target central egress VPC Attachment (pointing to the AWS Network Firewall endpoint).</p><p>Therefore, the correct answer is: <strong>Create a dedicated VPC for outbound internet traffic with a NAT gateway on it. Connect this VPC to the existing AWS Transit Gateway. Configure an AWS Network Firewall firewall for the rule-based filtering. Modify all the default routes in each account to point to the Network Firewall endpoint. </strong>This solution provides a dedicated VPC for rule-based inspection and controlling of egress traffic. Please check the references section for more details.</p><p>The option that says: <strong>Use AWS Network Firewall service to create firewall rule groups and firewall policies for rule-based filtering. Attach the firewall policy to a new Network Firewall firewall on each account. Modify all the default routes in each account to point to their corresponding Network Firewall firewall</strong> is incorrect. For centralized rule-based filtering with a Network Firewall, you will need an AWS Transit Gateway to act as a network hub and allow the connectivity between VPCs.</p><p>The option that says: <strong>Provision an Auto Scaling group of Amazon EC2 instances with network-optimized instance type on each AWS account. Install an open-source internet proxy software for rule-based filtering. Configure the route tables on each VPC to point to the Auto Scaling group</strong> is incorrect. This will be difficult to manage since you will have a proxy cluster essentially on each AWS account.</p><p>The option that says: <strong>Create a dedicated VPC for outbound internet traffic with a NAT gateway on it. Connect this VPC to the existing AWS Transit Gateway. On this VPC, create an Auto Scaling group of Amazon EC2 instances running with an open-source internet proxy software for rule-based filtering across all AZ in the region. Configure the route tables on each VPC to point to this Auto Scaling group</strong> is incorrect. This may be possible but is not recommended. AWS Network Firewall can offer centralized rule-based traffic filtering and inspection across your VPCs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/deployment-models-for-aws-network-firewall/\">https://aws.amazon.com/blogs/networking-and-content-delivery/deployment-models-for-aws-network-firewall/</a></p><p><a href=\"https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html\">https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html</a></p><p><a href=\"https://docs.aws.amazon.com/network-firewall/latest/developerguide/how-it-works.html\">https://docs.aws.amazon.com/network-firewall/latest/developerguide/how-it-works.html</a></p><p><br></p><p><strong>Check out this AWS Transit Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-transit-gateway/?src=udemy\">https://tutorialsdojo.com/aws-transit-gateway/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/networking-and-content-delivery/deployment-models-for-aws-network-firewall/",
      "https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html",
      "https://docs.aws.amazon.com/network-firewall/latest/developerguide/how-it-works.html",
      "https://tutorialsdojo.com/aws-transit-gateway/?src=udemy"
    ]
  },
  {
    "id": 39,
    "question": "<p>A company has an on-premises data center that is hosting its gaming service. Its primary function is player-matching and is accessible from players around the world. The gaming service prioritizes network speed for the users so all traffic to the servers uses User Datagram Protocol (UDP). As more players join, the company is having difficulty scaling its infrastructure so it plans to migrate the service to the AWS cloud. The Solutions Architect has been tasked with the migration and AWS Shield Advanced has been enabled already to protect all public-facing resources.</p><p>Which of the following actions should the Solutions Architect implement to achieve the company requirements? (Select TWO.)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon CloudFront distribution and set the Load Balancer as the origin. Use only secure protocols on the distribution origin settings.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up network ACL rules on the VPC to deny all non-UDP traffic. Ensure that the NACL is associated with the load balancer subnets.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Place the Auto Scaling of Amazon EC2 instances behind an Internet-facing Application Load Balancer (ALB). For the domain name, create an Amazon Route 53 entry that is Aliased to the FQDN of the ALB.</p><p><br></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Place the Auto Scaling of Amazon EC2 instances behind a Network Load Balancer (NLB). For the domain name, create an Amazon Route 53 entry that points to the Elastic IP address of the NLB.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create an AWS WAF rule that will explicitly block all non-UDP traffic. Ensure that the AWS WAF rule is associated with the load balancer of the EC2 instances.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>A<strong> Network Load Balancer</strong> operates at the connection level (Layer 4), routing connections to targets (Amazon EC2 instances, microservices, and containers) within Amazon VPC based on IP protocol data. Ideal for load balancing of both TCP and UDP traffic, Network Load Balancer is capable of handling millions of requests per second while maintaining ultra-low latencies. Network Load Balancer is optimized to handle sudden and volatile traffic patterns while using a single static IP address per Availability Zone.</p><p>For UDP traffic, the load balancer selects a target using a flow hash algorithm based on the protocol, source IP address, source port, destination IP address, and destination port. A UDP flow has the same source and destination, so it is consistently routed to a single target throughout its lifetime. Different UDP flows have different source IP addresses and ports so that they can be routed to different targets.</p><p><img src=\"https://media.tutorialsdojo.com/sap_nlb_ec2_backend.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_nlb_ec2_backend.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>When you create <strong>Network Access Control Lists (NACLs)</strong>, you can specify both allow and deny rules. This is useful if you want to explicitly deny certain types of traffic to your application. For example, you can define IP addresses (as CIDR ranges), protocols, and destination ports that are denied access to the entire subnet. If your application is used only for TCP traffic, you can create a rule to deny all UDP traffic or vice versa. This option is useful when responding to DDoS attacks because it lets you create your own rules to mitigate the attack when you know the source IPs or other signatures.</p><p><img src=\"https://media.tutorialsdojo.com/sap_nacl_subnet.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_nacl_subnet.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>If you are subscribed to AWS Shield Advanced, you can register Elastic IPs (EIPs) as Protected Resources. DDoS attacks against EIPs that have been registered as Protected Resources are detected more quickly, which can result in a faster time to mitigate.</p><p>The option that says: <strong>Place the Auto Scaling of Amazon EC2 instances behind a Network Load Balancer (NLB). For the domain name, create an Amazon Route 53 entry that points to the Elastic IP address of the NLB</strong> is correct. The service uses UDP traffic and prioritizes network speed, so a Network Load Balancer is ideal for this scenario. You can create a Route 53 entry pointing to the NLB’s Elastic IP.</p><p>The option that says: <strong>Set up network ACL rules on the VPC to deny all non-UDP traffic. Ensure that the NACL is associated with the load balancer subnets</strong> is correct. Since all traffic to the servers is UDP, you can set NACL to block all non-UDP traffic which can help block attacks such as traffic coming from TCP connections.</p><p>The option that says: <strong>Place the Auto Scaling of Amazon EC2 instances behind an internet-facing Application Load Balancer (ALB). For the domain name, create an Amazon Route 53 entry that is Aliased to the FQDN of the ALB </strong>is incorrect. UDP traffic operates at Layer 4 of the OSI model while an ALB operates at Layer 7. A Network Load Balancer is a better fit for this scenario.</p><p>The option that says: <strong>Create an AWS WAF rule that will explicitly block all non-UDP traffic. Ensure that the AWS WAF rule is associated with the load balancer of the EC2 instances</strong> is incorrect. AWS WAF rules cannot protect a Network Load Balancer yet. It is better to use NACL rules to block the non-UDP traffic.</p><p>The option that says: <strong>Create an Amazon CloudFront distribution and set the Load Balancer as the origin. Use only secure protocols on the distribution origin settings</strong> is incorrect. Although CloudFront does provide caching and security settings for the origin, it helps mitigate DDoS attacks to your instances or NLBs because AWS Shield Standard is enabled by default. For more advanced DDoS mitigation AWS Shield Advanced offers integration with NACLs that will block traffic at the edge of the AWS Network.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/security-groups-and-network-access-control-lists-nacls-bp5.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/security-groups-and-network-access-control-lists-nacls-bp5.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/now-you-can-use-aws-shield-advanced-to-protect-your-amazon-ec2-instances-and-network-load-balancers/\">https://aws.amazon.com/blogs/security/now-you-can-use-aws-shield-advanced-to-protect-your-amazon-ec2-instances-and-network-load-balancers/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p><p><br></p><p><strong>Check out these Application Load Balancer and AWS Shield Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/?src=udemy\">https://tutorialsdojo.com/application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/</a></p><p><a href=\"https://tutorialsdojo.com/aws-shield/?src=udemy\">https://tutorialsdojo.com/aws-shield/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/security-groups-and-network-access-control-lists-nacls-bp5.html",
      "https://aws.amazon.com/blogs/security/now-you-can-use-aws-shield-advanced-to-protect-your-amazon-ec2-instances-and-network-load-balancers/",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
      "https://tutorialsdojo.com/application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/?src=udemy",
      "https://tutorialsdojo.com/aws-shield/?src=udemy"
    ]
  },
  {
    "id": 40,
    "question": "<p>A company's cloud governance team is tightening its AWS policies for an upcoming audit. During an initial review, the team found IAM policies attached to Lambda function execution roles, granting full access to S3 buckets and DynamoDB tables. As a best practice, the team recommends implementing the principle of least privilege access for compliance with the security audit.</p><p>What steps should be taken to determine the minimum access needed by each function with the LEAST amount of effort? (Select TWO.)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up AWS Audit Manager to continuously audit AWS usage. Review the findings and create IAM access policies to provide more restrictive permissions for the Lambda functions.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable CloudTrail logging in the AWS account and export the logs to S3. Use Amazon GuardDuty to analyze the data and monitor unauthorized access to Amazon S3 and Amazon DynamoDB. Restrict access in IAM.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Build an inventory of AWS API Calls by applying the CodeGuru Profiler function decorator to Lambda’s handler functions. Export this data to a CSV file. Create a generic access policy for each Lambda function and apply the update to the functions using a Python script.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use IAM Access Analyzer to review AWS CloudTrail logs and generate a policy template with permissions required by the Lambda functions.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create a trail in AWS CloudTrail and turn on CloudTrail logging to capture Amazon S3 and Amazon DynamoDB events.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>AWS CloudTrail is an AWS service that helps enable operational and risk auditing, governance, and compliance of your AWS account. It provides three ways to record events: event history, CloudTrail Lake, and Trails. Trails capture a record of AWS activities and store them in an Amazon S3 bucket. These events can be integrated into security monitoring solutions for further analysis.</p><p>IAM Access Analyzer can analyze your CloudTrail events to identify actions and services used by an IAM entity (user or role). Based on that activity, you can generate an IAM policy. You can refine an entity's permissions when you replace a broad permissions policy attached to the entity with the generated policy.</p><p><img src=\"https://media.tutorialsdojo.com/public/CloudTrailPolicy.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/public/CloudTrailPolicy.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>To start logging API activities for Amazon S3 and DynamoDB, you must first set up a CloudTrail trail. Once API activities are captured in the CloudTrail log, you can use the IAM Access Analyzer to generate IAM access policy templates for a specific date range from the events logged. After creating these policies, you can review the actions and services and customize them based on your needs.</p><p>Therefore, the correct answers are:</p><p><strong>- Create a trail in AWS CloudTrail and turn on CloudTrail logging to capture Amazon S3 and Amazon DynamoDB events.</strong></p><p><strong>- Use IAM Access Analyzer to review AWS CloudTrail logs and generate a policy template with permissions required by the Lambda functions.</strong></p><p>The option that says: <strong>Enable CloudTrail logging in the AWS account and export the logs to S3. Use Amazon GuardDuty to analyze the data and monitor unauthorized access to Amazon S3 and Amazon DynamoDB. Restrict access in IAM</strong> is incorrect. While enabling the use of Amazon GuardDuty can help identify potential security threats, it does not directly help identify the minimum access needed by the Lambda functions. Amazon GuardDuty is more of a threat detection service.</p><p>The option that says: <strong>Set up AWS Audit Manager to continuously audit AWS usage. Review the findings and create IAM access policies to provide more restrictive permissions for the Lambda functions</strong> is incorrect. AWS Audit Manager facilitates continuous AWS usage auditing to simplify risk management and ensure compliance with regulations and industry standards. While it can identify non-compliant resources, it does not provide insights into the minimum access needed by the Lambda functions. It aids more in overall compliance than granular access to AWS services.</p><p>The option that says: <strong>Build an inventory of AWS API Calls by applying the CodeGuru Profiler function decorator to Lambda's handler functions. Export this data to a CSV file. Create a generic access policy for each Lambda function and apply the update to the functions using a Python script </strong>is incorrect. AWS CodeGuru Profiler can be integrated with AWS Lambda, but the insights that it provides is more on improving function performance and reducing costs by identifying expensive lines of code and anomalies. It won't be helpful in determining the minimum access needed by the Lambda functions.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access-analyzer-policy-generation.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access-analyzer-policy-genation.html</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html</a></p><p><strong><br>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access-analyzer-policy-generation.html",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy"
    ]
  },
  {
    "id": 41,
    "question": "<p>A company manually runs its custom scripts when deploying a new version of its application that is hosted on a fleet of Amazon EC2 instances. This method is prone to human errors, such as accidentally running the wrong script or deploying the wrong artifact. The company wants to automate its deployment procedure.</p><p>If errors are encountered after the deployment, the company wants to be able to roll back to the older application version as fast as possible.</p><p>Which of the following options should the Solutions Architect implement to meet the requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Utilize AWS CodeBuild and add a job with Chef recipes for the new application version. Use a “canary” deployment strategy to the new version on a new instance. Delete the canary instance if errors are found on the new version.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create two identical environments of the application on AWS Elastic Beanstalk. Use a blue/green deployment strategy by swapping the environment’s URL. Deploy the custom scripts using Elastic Beanstalk platform hooks.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a new pipeline on AWS CodePipeline and add a stage that will deploy the application on the EC2 instances. Choose a “rolling update with an additional batch” deployment strategy, to allow a quick rollback to the older version in case of errors.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an AWS System Manager automation runbook to manage the deployment process. Set up the runbook to first deploy the new application version to a staging environment. Include automated tests and, upon successful completion, use the runbook to deploy the application to the production environment</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>Blue/Green Deployment involves maintaining two separate, identical environments. The \"Blue\" environment is the current production version, while the \"Green\" is for the new version. . This Green environment is an exact replica of the Blue one but hosts the new version of your application. After deploying and thoroughly testing the new version in the Green environment, you simply switch the environment's URL to redirect traffic from the Blue to the Green environment. This switch makes the new version live for users. If a rollback is needed due to any issues, it's just a matter of switching the URL back to the original Blue environment and instantly reverting to the previous version of the application.</p><p><img src=\"https://media.tutorialsdojo.com/public/elasticbeanstalk-diagram013124-aws-blue-green.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/public/elasticbeanstalk-diagram013124-aws-blue-green.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In Elastic Beanstalk you can perform a blue/green deployment by swapping the CNAMEs of the two environments to redirect traffic to the new version instantly. If there are any custom scripts or executable files that you want to run automatically as part of your deployment process, you may use platform hooks.</p><p>To provide platform hooks that run during an application deployment, place the files under the <code>.platform/hooks</code> directory in your source bundle, in one of the following subdirectories:</p><p><code>prebuild</code> – Files here run after the Elastic Beanstalk platform engine downloads and extracts the application source bundle, and before it sets up and configures the application and web server.</p><p><code>predeploy</code> – Files here run after the Elastic Beanstalk platform engine sets up and configures the application and web server, and before it deploys them to their final runtime location.</p><p><code>postdeploy</code> – Files here run after the Elastic Beanstalk platform engine deploys the application and proxy server.</p><p>Therefore, the correct answer is: <strong>Create two identical environments of the application on AWS Elastic Beanstalk. Use a blue/green deployment strategy by swapping the environment’s URL. Deploy the custom scripts using Elastic Beanstalk platform hooks.</strong></p><p>The option that says: <strong>Create an AWS System Manager automation runbook to manage the deployment process. Set up the runbook to first deploy the new application version to a staging environment. Include automated tests and, upon successful completion, use the runbook to deploy the application to the production environment </strong>is incorrect. While this is technically possible, it does not offer the fastest rollback mechanism in case of immediate issues post-deployment, as the rollback would involve a separate process. Moreover, unlike AWS Elastic Beanstalk, which has built-in features for version tracking, using AWS System Manager for deployment requires a more manual approach to version control. You would need to maintain a system for tracking different application versions, ensuring that you have the correct version deployed in the right environment (staging vs. production). This adds complexity to the deployment process.</p><p>The option that says: <strong>Create a new pipeline on AWS CodePipeline and add a stage that will deploy the application on the EC2 instances. Choose a “rolling update with an additional batch” deployment strategy, to allow a quick rollback to the older version in case of errors</strong> is incorrect. Although the pipeline can primarily deploy the new version on the EC2 instances, rollback for this strategy takes time. You will have to re-deploy the older version if you want to do a rollback.</p><p>The option that says: <strong>Utilize AWS CodeBuild and add a job with Chef recipes for the new application version. Use a “canary” deployment strategy to the new version on a new instance. Delete the canary instance if errors are found on the new version </strong>is incorrect. Although you can detect errors on a canary deployment, AWS CodeBuild cannot deploy the new application version on the EC2 instances. You typically have to use AWS CodeDeploy if you want to go this route.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.platform.upgrade.html#using-features.platform.upgrade.bluegreen\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.platform.upgrade.html#using-features.platform.upgrade.bluegreen</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/platforms-linux-extend.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/platforms-linux-extend.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.platform.upgrade.html#using-features.platform.upgrade.bluegreen",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/platforms-linux-extend.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html",
      "https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy"
    ]
  },
  {
    "id": 42,
    "question": "<p>A top Internet of Things (IoT) company has developed a wrist-worn activity tracker for soldiers deployed in the field. The device acts as a sensor to monitor the health and vital statistics of the wearer. It is expected that there would be thousands of devices that will send data to the server every minute and after 5 years, the number will increase to tens of thousands. One of the requirements is that the application should be able to accept the incoming data, run it through ETL to store in a data warehouse, and archive the old data. The officers in the military headquarters should have a real-time dashboard to view the sensor data.</p><p>Which of the following options is the most suitable architecture to implement in this scenario?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the raw data directly in an Amazon S3 bucket with a lifecycle policy to store in Glacier after a month. Register the S3 bucket as a source on AWS Lake Formation. Launch an EMR cluster access the data lake, runs it through ETL, and then output that data to Amazon Redshift.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Send the raw data directly to Amazon Data Firehose for processing and output the data to an S3 bucket. For archiving, create a lifecycle policy from S3 to Glacier. Use Amazon EMR to process the data stored in S3 and load the processed data into Amazon Redshift.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Store the data directly to DynamoDB. Launch a data pipeline that starts an EMR cluster using data from DynamoDB and sends the data to S3 and Redshift.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage Amazon Athena to accept the incoming data and store them using DynamoDB. Setup a cron job that takes data from the DynamoDB table and sends it to an Amazon EMR cluster for ETL, then outputs the result to Amazon Redshift.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>Whenever there is a requirement for real-time data collection or analysis, always consider Amazon Kinesis.</p><p><strong>Amazon Data Firehose</strong> is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic. Data Firehose is part of the Kinesis streaming data platform, along with Kinesis Data Streams and Kinesis Video Streams.</p><p><img src=\"https://media.tutorialsdojo.com/sap_kinesis_data_firehose_overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_kinesis_data_firehose_overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Send the raw data directly to Amazon Data Firehose for processing and output the data to an S3 bucket. For archiving, create a lifecycle policy from S3 to Glacier. Use Amazon EMR to process the data stored in S3 and load the processed data into Amazon Redshift.</strong> Amazon Kinesis will ingest the data in real-time, transform it, and output it to an Amazon S3 bucket.</p><p>The option that says: <strong>Store the data directly to DynamoDB. Launch a data pipeline that starts an EMR cluster using data from DynamoDB and sends the data to S3 and Redshift</strong> is incorrect. For the collection of real-time data, AWS recommends Amazon Kinesis for ingestion. After ingestion, you can output the data into several AWS services for further processing.</p><p>The option that says: <strong>Store the raw data directly in an Amazon S3 bucket with a lifecycle policy to store in Glacier after a month. Register the S3 bucket as a source on AWS Lake Formation. Launch an EMR cluster access the data lake, runs it through ETL, and then output that data to Amazon Redshift</strong> is incorrect. Amazon S3 can only accept data using HTTP requests, and the data sent by IoT may be in a different format. For ingesting the data, Amazon Kinesis should be the first one to accept, process it, and store it in Amazon S3.</p><p>The option that says: <strong>Leverage Amazon Athena to accept the incoming data and store them using DynamoDB. Setup a cron job that takes data from the DynamoDB table and sends it to an Amazon EMR cluster for ETL, then outputs the result to Amazon Redshift</strong> is incorrect. Amazon Athena is a query service to query data and analyze big data, and not suitable for ingesting real-time IoT data.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p><p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p><p><a href=\"https://docs.aws.amazon.com/kinesisanalytics/latest/java/examples-s3.html\">https://docs.aws.amazon.com/kinesisanalytics/latest/java/examples-s3.html</a></p><p><br></p><p><strong>Check out this Amazon Kinesis Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html",
      "https://docs.aws.amazon.com/kinesisanalytics/latest/java/examples-s3.html",
      "https://tutorialsdojo.com/amazon-kinesis/?src=udemy"
    ]
  },
  {
    "id": 43,
    "question": "<p>A legal consulting firm is running a WordPress website on EC2 instances deployed across multiple Availability Zones with a Multi-AZ RDS MySQL database instance. Their website is designed to use an eventual consistency model and performs a high number of read and write operations. There is a growing number of people who are reporting that the website is slow and after checking, the root cause is due to the slow read processing in your database tier. The current DB instances are already optimized for the firm's operational budget, with considerations for cost-effectiveness and resource utilization.</p><p>Which of the following options could solve this issue? (Select THREE.)</p>",
    "corrects": [
      1,
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Add an RDS MySQL Read Replica in each Availability Zone.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Deploy an Amazon ElastiCache Cluster with nodes running in each Availability Zone.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Integrate Amazon CloudFront to the website to deliver the static media assets to the viewers faster. Consider using AWS Compute Optimizer to rightsize your fleet of Amazon EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Upgrade the instance type of the RDS MySQL database instance to a larger type.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Implement sharding to distribute the incoming load to multiple RDS MySQL instances.",
        "correct": true
      },
      {
        "id": 6,
        "answer": "Upgrade the RDS MySQL instance to use provisioned IOPS.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>To improve the read performance of the application, you can use RDS Read Replicas and ElastiCache.</p><p><strong>Amazon ElastiCache</strong> is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores instead of relying entirely on slower disk-based databases.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_elasticache.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_rds_elasticache.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Amazon RDS Read Replicas</strong> provide enhanced performance and durability for database (DB) instances. This replication feature makes it easy to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput.</p><p><strong>Database sharding</strong> is the process of storing a large database across multiple machines. A single machine, or database server, can store and process only a limited amount of data. Database sharding overcomes this limitation by splitting data into smaller chunks, called shards, and storing them across several database servers. All database servers usually have the same underlying technologies, and they work together to store and process large volumes of data.</p><p>The option that says: <strong>Add an RDS MySQL Read Replica in each Availability Zone. This improves the database retrieval time as there will be more nodes that can serve the database request</strong> is correct. You won't need to increase the master instance size to accommodate all the read loads.</p><p>The option that says: <strong>Deploy an Amazon ElastiCache Cluster with nodes running in each Availability Zone</strong> is correct. Adding a cache will significantly reduce the retrieval time for frequent database queries. This also reduces the load significantly on the database tier.</p><p>The option that says: <strong>Implement sharding to distribute the incoming load to multiple RDS MySQL instances</strong> is correct. You can use Amazon RDS as the building block a sharded architecture. Sharding allows you to split your current instances into smaller ones that could operate more efficiently. It would also provide horizontal scalability, which can be more cost-effective in the long term compared to simply upgrading to a larger DB instance.</p><p>The option that says: <strong>Upgrade the RDS MySQL instance to use provisioned IOPS</strong> is incorrect. This provides an increase in IOPS for the database tier, however, this is also very expensive to implement since provisioned IOPS costs a lot more.</p><p>The option that says: <strong>Integrate Amazon CloudFront to the website to deliver the static media assets to the viewers faster. Consider using AWS Compute Optimizer to rightsize your fleet of Amazon EC2 instances</strong> is incorrect. Amazon CloudFront simply caches content at the edge. It can be a complementary solution for overall performance improvement but would not directly impact the database layer where the bottleneck is occuring.</p><p>The option that says: <strong>Upgrade the instance type of the RDS MySQL database instance to a larger type </strong>is incorrect. While this may improve the overall performance of the database, it's costly and may exceed the operational budget. This approach is an example of vertical scaling, which has limits and may not be as sustainable as horizontal scaling solutions in the long run.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><a href=\"https://aws.amazon.com/rds/details/read-replicas/\">https://aws.amazon.com/rds/details/read-replicas/</a></p><p><a href=\"https://aws.amazon.com/what-is/database-sharding/\">https://aws.amazon.com/what-is/database-sharding/</a></p><p><a href=\"https://aws.amazon.com/blogs/database/sharding-with-amazon-relational-database-service/\">https://aws.amazon.com/blogs/database/sharding-with-amazon-relational-database-service/</a></p><p><br></p><p><strong>Check out these Amazon Elasticache and Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/amazon-elasticache/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/elasticache/",
      "https://aws.amazon.com/rds/details/read-replicas/",
      "https://aws.amazon.com/what-is/database-sharding/",
      "https://aws.amazon.com/blogs/database/sharding-with-amazon-relational-database-service/",
      "https://tutorialsdojo.com/amazon-elasticache/?src=udemy",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 44,
    "question": "<p>A law firm has decided to use Amazon S3 buckets for storage after an extensive Total Cost of ownership (TCO) analysis comparing S3 versus acquiring more storage for its on-premises hardware. The attorneys, paralegals, clerks, and other employees of the law firm will be using Amazon S3 buckets to store their legal documents and other media files. For a better user experience, the management wants to implement a single-sign-on system in which the user can just use their existing Active Directory login to access the S3 storage to avoid having to remember yet another password.</p><p>Which of the following options should the solutions architect implement for the above requirement and also provide a mechanism that restricts access for each user to a designated user folder in a bucket? (Select TWO.)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a federation proxy or a custom identity provider and use AWS Security Token Service to generate temporary tokens. Use an IAM Role to enable access to AWS services.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Connect to integrate the on-premises Active Directory with Amazon S3 and AWS IAM.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure an IAM user that provides access for the user and an IAM Policy that restricts access only to the user-specific folders in the S3 Bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure an IAM Policy that restricts access only to the user-specific folders in the Amazon S3 Bucket.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Set up a matching IAM user and IAM Policy for every user in your corporate directory that needs access to a folder in the bucket.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>Federation enables you to manage access to your AWS Cloud resources centrally. With federation, you can use single sign-on (SSO) to access your AWS accounts using credentials from your corporate directory. Federation uses open standards, such as Security Assertion Markup Language 2.0 (SAML), to exchange identity and security information between an identity provider (IdP) and an application.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sso_ldap_broker.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_sso_ldap_broker.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Your users might already have identities outside of AWS, such as in your corporate directory. If those users need to work with AWS resources (or work with applications that access those resources) then those users also need AWS security credentials. You can use an IAM role to specify permissions for users whose identity is federated from your organization or a third-party identity provider (IdP). Setting up an identity provider for federated access is required for integrating your on-premises Active Directory with AWS.</p><p>Therefore, the following options are the correct answers:</p><p><strong>- Set up a federation proxy or a custom identity provider and use AWS Security Token Service to generate temporary tokens. Use an IAM Role to enable access to AWS services.</strong></p><p><strong>- Configure an IAM Policy that restricts access only to the user-specific folders in the Amazon S3 Bucket.</strong></p><p>The option that says: <strong>Using Amazon Connect to integrate the on-premises Active Directory with Amazon S3 and AWS IAM </strong>is incorrect because Amazon Connect is simply an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost.</p><p>The option that says: <strong>Configure an IAM user that provides access for the user and an IAM Policy that restricts access only to the user-specific folders in the S3 Bucket</strong> is incorrect because you have to use an IAM Role, instead of an IAM user, to provide the access needed to your AWS Resources.</p><p>The option that says: <strong>Set up a matching IAM user and IAM Policy for every user in your corporate directory that needs access to a folder in the bucket</strong> is incorrect because you should be creating IAM Roles rather than IAM Users.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/\">https://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios.html</a></p><p><br></p><p><strong>Check out this AWS Identity &amp; Access Management (IAM) Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios.html",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy"
    ]
  },
  {
    "id": 45,
    "question": "<p>A large company has multiple AWS accounts with multiple IAM Users that launch different types of Amazon EC2 instances and EBS volumes every day. As a result, most accounts quickly hit the service limit and IAM users can no longer create any new instances. When cleaning up the AWS accounts, the solutions architect noticed that the majority of the instances and volumes are untagged. Therefore, it is difficult to pinpoint the owner of these resources and verify if they are safe to terminate. Because of this, the management had issued a new protocol that requires adding a predefined set of tags before anyone can launch their EC2 instances.</p><p>Which of the following options is the simplest way to enforce this new requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure AWS Organizations to group different accounts into separate Organizational Units (OU) depending on the business function. Create a rule using AWS Systems Manager requiring users to tag specific resources and raise an alert whenever the rule is violated. This will allow a user to launch EC2 instances only if certain tags were defined. If the user applies any other tag then the action is denied.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure AWS Organizations to group different accounts into separate Organizational Units (OU) depending on the business function. Create a Service Control Policy that restricts launching any AWS resources without a tag by including the <code>Condition</code> element in the policy which uses the <code>ForAllValues</code> qualifier and the <code>aws:TagKeys</code> condition. This policy will require its principals to tag resources during creation. Apply the SCP to the OU which will automatically cascade the policy to individual member accounts.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure AWS Organizations to group different accounts into separate Organizational Units (OU) depending on the business function. Create a rule in AWS Config requiring users to tag specific resources and raise an alert whenever the rule is violated. The Config Rule should allow a user to launch EC2 instances only if the user adds all the tags defined in the rule. If the user applies any other tag then the action is denied.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Apply an IAM policy to the individual member accounts of the OU that includes a <code>Condition</code> element in the policy containing the <code>ForAllValues</code> qualifier and the <code>aws:TagKeys</code> condition. This policy will require its principals to attach specific tags to their resources during creation.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>You can specify tags for EC2 instances and EBS volumes as part of the API call that creates the resources. Using this principle, you can require users to tag specific resources by applying conditions to their IAM policy. Using AWS Organizations, you can consolidate all of your AWS accounts and group the business units into separate Organizational Units (OUs) with a custom service control policy (SCP).</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_organization_explanations.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_organization_explanations.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can configure your IAM policy to allow a user to launch an EC2 instance and create an EBS volume only if the user applies all the tags that are defined in the policy using the <code><strong>ForAllValues</strong></code><strong> </strong>qualifier. If the user applies any tag that's not included in the policy then the action is denied. To enforce case sensitivity, use the condition <code><strong><em>aws:TagKeys</em></strong></code>.</p><p>You can use organizational units (OUs) to group accounts together to administer as a single unit. This greatly simplifies the management of your accounts. For example, you can attach a policy-based control to an OU, and all accounts within the OU automatically inherit the policy. You can create multiple OUs within a single organization, and you can create OUs within other OUs.</p><p>Therefore, the correct answer is: <strong>Configure AWS Organizations to group different accounts into separate Organizational Units (OU) depending on the business function. Create a Service Control Policy that restricts launching any AWS resources without a tag by including the </strong><code><strong>Condition</strong></code><strong> element in the policy which uses the </strong><code><strong>ForAllValues</strong></code><strong> qualifier and the </strong><code><strong>aws:TagKeys</strong></code><strong> condition. This policy will require its principals to tag resources during creation. Apply the SCP to the OU which will automatically cascade the policy to individual member accounts.</strong></p><p>The option that says: <strong>Apply an IAM policy to the individual member accounts of the OU that includes a </strong><code><strong>Condition</strong></code><strong> element in the policy containing the </strong><code><strong>ForAllValues</strong></code><strong> qualifier and the </strong><code><strong>aws:TagKeys</strong></code><strong> condition. This policy will require its principals to attach specific tags to their resources during creation</strong> is incorrect as it requires a lot of effort to implement the policy in each and every AWS account of the organization. You should set up AWS Organizations, group different accounts into separate Organizational Units (OU), and use SCP instead.</p><p>The option that says: <strong>Configure AWS Organizations to group different accounts into separate Organizational Units (OU) depending on the business function. Create a rule in AWS Config requiring users to tag specific resources and raise an alert whenever the rule is violated. The Config Rule should allow a user to launch EC2 instances only if the user adds all the tags defined in the rule. If the user applies any other tag then the action is denied</strong> is incorrect. AWS Config only audits and evaluates if your instance and volume configurations match the rules you have created. Unlike an IAM policy, it does not permit nor restrict users from performing certain actions.</p><p>The option that says: <strong>Configure AWS Organizations to group different accounts into separate Organizational Units (OU) depending on the business function. Create a rule using AWS Systems Manager requiring users to tag specific resources and raise an alert whenever the rule is violated. This will allow a user to launch EC2 instances only if certain tags were defined. If the user applies any other tag then the action is denied</strong> is incorrect because you cannot create a rule using AWS Systems Manager requiring users to tag specific resources and raise an alert whenever the rule is violated. You have to use an IAM policy in order to do this.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-tags-restrict/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-tags-restrict/</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_multi-value-conditions.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_multi-value-conditions.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-tags-restrict/",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_multi-value-conditions.html",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy",
      "https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy",
      "https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy"
    ]
  },
  {
    "id": 46,
    "question": "<p>A company has deployed a multi-tier web application on AWS that uses Compute Optimized Instances for server-side processing and Storage Optimized EC2 Instances to store various media files. To ensure data durability, there is a scheduled job that replicates the files to each EC2 instance. The current architecture worked for a few months but it started to fail as the number of files grew, which is why the management decided to redesign the system.</p><p>Which of the following options should the solutions architect implement in order to launch a new architecture with improved data durability and cost-efficiency?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Migrate the web application to AWS Elastic Beanstalk and move all media files to Amazon EFS for a durable and scalable storage. Set up an Amazon CloudFront distribution with EFS as the origin. Use a combination of Consolidated Billing and AWS Trusted advisor checks to monitor the operating costs and identify potential savings.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Migrate all media files to an Amazon S3 bucket and use this as the origin for the new CloudFront web distribution. Set up an Elastic Load Balancer with an Auto Scaling of EC2 instances to host the web servers. Use a combination of Cost Explorer and AWS Trusted advisor checks to monitor the operating costs and identify potential savings.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Migrate and host the entire web application to Amazon S3 for a more cost-effective web hosting. Enable cross-region replication to improve data durability. Use a combination of Consolidated Billing and AWS Trusted advisor checks to monitor the operating costs and identify potential savings.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Migrate all media files to Amazon EFS then attach this new drive as a mount point to a new set of Storage Optimized EC2 Instances. For the web servers, set up an Elastic Load Balancer with an Auto Scaling of EC2 instances and use this as the origin for a new Amazon CloudFront web distribution. Use a combination of Cost Explorer and AWS Trusted advisor checks to monitor the operating costs and identify potential savings.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p>Cloud storage is a cloud computing model that stores data on the Internet through a cloud computing provider who manages and operates data storage as a service. It’s delivered on demand with just-in-time capacity and costs, and eliminates buying and managing your own data storage infrastructure. This gives you agility, global scale and durability, with 'anytime, anywhere' data access.</p><p><strong>AWS Trusted Advisor</strong> is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices. Whether establishing new workflows, developing applications, or as part of ongoing improvement, take advantage of the recommendations provided by Trusted Advisor on a regular basis to help keep your solutions provisioned optimally.</p><p><strong>AWS Cost Explorer</strong> has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. Get started quickly by creating custom reports that analyze cost and usage data, both at a high level and for highly-specific requests. Using AWS Cost Explorer, you can dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies.</p><p><img src=\"https://media.tutorialsdojo.com/sap_efs_s3_ebs_table.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_efs_s3_ebs_table.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In this scenario, you can use a combination of Cost Explorer and AWS Trusted advisor checks to monitor the operating costs and identify potential savings. For data storage, you can either use S3 or EFS to store the media files. However, S3 is cheaper than EFS and is more suitable to use in storing static media files.</p><p>Therefore, the correct answer is: <strong>Migrate all media files to an Amazon S3 bucket and use this as the origin for the new CloudFront web distribution. Set up an Elastic Load Balancer with an Auto Scaling of EC2 instances to host the web servers. Use a combination of Cost Explorer and AWS Trusted advisor checks to monitor the operating costs and identify potential savings.</strong></p><p>The option that says: <strong>Migrate and host the entire web application to Amazon S3 for a more cost-effective web hosting. Enable cross-region replication to improve data durability. Use a combination of Consolidated Billing and AWS Trusted advisor checks to monitor the operating costs and identify potential savings</strong> is incorrect. Amazon S3 is not capable to handle server-side processing as this can only be used for static websites. Moreover, you can only use Consolidated Billing if your account is configured to use AWS Organizations.</p><p>The option that says: <strong>Migrate all media files to Amazon EFS then attach this new drive as a mount point to a new set of Storage Optimized EC2 Instances. For the web servers, set up an Elastic Load Balancer with an Auto Scaling of EC2 instances and use this as the origin for a new Amazon CloudFront web distribution. Use a combination of Cost Explorer and AWS Trusted advisor checks to monitor the operating costs and identify potential savings</strong> is incorrect. Although this new setup may work, it entails a higher cost to maintain a new set of Storage Optimized EC2 Instances along with EFS. There is also an added cost of maintaining your web-tier which is comprised of an Elastic Load Balancer with another set of Auto Scaling of EC2 instances. It is also better to use S3 instead of EFS since you are only storing media files and not documents which requires file locking or POSIX-compliant storage.</p><p>The option that says: <strong>Migrate the web application to AWS Elastic Beanstalk and move all media files to Amazon EFS for a durable and scalable storage. Set up an Amazon CloudFront distribution with EFS as the origin. Use a combination of Consolidated Billing and AWS Trusted advisor checks to monitor the operating costs and identify potential savings</strong> is incorrect. You cannot set EFS as the origin of your CloudFront web distribution. The scenario also doesn't mention the use of AWS Organization, which is why Consolidated Billing is not applicable.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/efs/when-to-choose-efs/\">https://aws.amazon.com/efs/when-to-choose-efs/</a></p><p><a href=\"https://aws.amazon.com/what-is-cloud-storage/\">https://aws.amazon.com/what-is-cloud-storage/</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><br></p><p><strong>Check out this Amazon S3, EBS, and EFS comparison:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/?src=udemy\">https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/efs/when-to-choose-efs/",
      "https://aws.amazon.com/what-is-cloud-storage/",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy",
      "https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/?src=udemy"
    ]
  },
  {
    "id": 47,
    "question": "<p>A company runs a mission-critical application on a fixed set of Amazon EC2 instances behind an Application Load Balancer. The application responds to user requests by querying a 120GB dataset. The application requires high throughput and low latency storage so the dataset is stored on Provisioned IOPS (PIOPS) Amazon EBS volumes with 3000 IOPS provisioned. The EC2 launch template has been configured to allocate and attach this 120GB size PIOPS EBS volume for the fleet of EC2 instances. After a few months of operation, the company noticed the high cost of EBS volumes in the billing section. The Solutions Architect has been tasked to design a solution that will reduce the costs without a negative impact on the application performance and data durability.</p><p>Which of the following solutions will meet the company requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon EFS volume and mount it across all the EC2 instances. Use the Provisioned Throughput mode on the EFS volume to ensure that the application can reach the required IOPS.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use the cheaper General Purpose SSD (gp2) EBS volumes instead of PIOPS EBS volumes. Allocating 1TB EBS volumes (gp2) will have a throughput of 3000 IOPS. Update the EC2 launch template to allocate this type of volume.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon EFS volume and mount it across all the EC2 instances. Use Max I/O performance mode on the EFS volume to ensure the application can reach the required IOPS.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Remove the PIOPS EBS volume allocation on the EC2 launch template. Attach a 120GB instance store volume on the EC2 instance to ensure that the application will have enough IOPS for its operation.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Elastic File System (Amazon EFS)</strong> provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth. Amazon EFS has a simple web services interface that allows you to create and configure file systems quickly and easily. The service manages all the file storage infrastructure for you, meaning that you can avoid the complexity of deploying, patching, and maintaining complex file system configurations.</p><p><img src=\"https://media.tutorialsdojo.com/sap_efs_how_it_works.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_efs_how_it_works.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Amazon EFS is designed to provide the throughput, IOPS, and low latency needed for a broad range of workloads. With Amazon EFS, you can choose from two performance modes and two throughput modes:</p><p>- The default<strong> General Purpose performance mode</strong> is ideal for latency-sensitive use cases, like web serving environments, content management systems, home directories, and general file serving. File systems in the <strong>Max I/O mode</strong> can scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies for file metadata operations.</p><p>- Using the default <strong>Bursting Throughput mode</strong>, throughput scales as your file system grows. Using <strong>Provisioned Throughput mode</strong>, you can specify the throughput of your file system independent of the amount of data stored.</p><p>With Bursting Throughput mode, throughput on Amazon EFS scales as the size of your file system in the standard storage class grows. With Provisioned Throughput mode, you can instantly provision the throughput of your file system (in MiB/s) independent of the amount of data stored.</p><p>If your file system is in the Provisioned Throughput mode, you can increase the Provisioned Throughput of your file system as often as you want. You can decrease your file system throughput in Provisioned Throughput mode as long as it's been more than 24 hours since the last decrease.</p><p>Hence, the correct answer is: <strong>Create an Amazon EFS volume and mount it across all the EC2 instances. Use the Provisioned Throughput mode on the EFS volume to ensure that the application can reach the required IOPS.</strong></p><p>The option that says: <strong>Create an Amazon EFS volume and mount it across all the EC2 instances. Use Max I/O performance mode on the EFS volume to ensure the application can reach the required IOPS</strong> is incorrect. File systems in the Max I/O mode can scale to higher levels of aggregate throughput and operations per second. However, this scaling is done with a tradeoff of slightly higher latencies for file metadata operations.</p><p>The option that says: <strong>Use the cheaper General Purpose SSD (gp2) EBS volumes instead of PIOPS EBS volumes. Allocating 1TB EBS volumes (gp2) will have a throughput of 3000 IOPS. Update the EC2 launch template to allocate this type of volume</strong> is incorrect. Although this may look cheaper at first, creating several 1TB volumes for each EC2 instance entails higher costs. The Amazon EFS volume solution will be cheaper for sharing storage across all EC2 instances. Although you can use EBS Multi-Attach to attach EBS volumes to multiple EC2 instances, this is limited only to Provisioned IOPS SSD (io1 or io2 Block Express) volumes that are attached to Nitro-based EC2 instances in the same Availability Zone.</p><p>The option that says: <strong>Remove the PIOPS EBS volume allocation on the EC2 launch template. Attach a 120GB instance store volume on the EC2 instance to ensure that the application will have enough IOPS for its operation</strong> is incorrect. Instance store volumes are ephemeral which means that you will typically lose all data in the volume when you stop/start the instance. This is not recommended for this mission-critical application.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html\">https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><br></p><p><strong>Check out these Amazon EFS and Comparison Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-efs/?src=udemy\">https://tutorialsdojo.com/amazon-efs/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/?src=udemy\">https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/efs/latest/ug/performance.html",
      "https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
      "https://tutorialsdojo.com/amazon-efs/?src=udemy",
      "https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/?src=udemy"
    ]
  },
  {
    "id": 48,
    "question": "<p>A leading electronics company is getting ready to do a major public announcement of its latest smartphone. Their official website uses an Application Load Balancer in front of an Auto Scaling group of On-Demand EC2 instances, which are deployed across multiple Availability Zones with a Multi-AZ RDS MySQL database. In preparation for their new product launch, the solutions architect checked the performance of the company website and found that the database takes a lot of time to retrieve the data when there are over 100,000 simultaneous requests on the server. The static content such as the images and videos are promptly loaded as expected, but not the customer information that is fetched from the database.</p><p>Which of the following options could be done to solve this issue in a cost-effective way? (Select TWO.)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Add Read Replicas in RDS for each Availability Zone.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Configure the database tier to use sharding, which will distribute the incoming load to multiple RDS MySQL instances.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Upgrade the RDS MySQL database instance size and increase the provisioned IOPS for faster processing.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Migrate the database to use Amazon Keyspaces which natively supports sharding to distribute database queries to multiple nodes.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Implement a caching system using ElastiCache in-memory cache on each Availability Zone.",
        "correct": true
      },
      {
        "id": 6,
        "answer": "Launch a CloudFront web distribution to solve the latency issue.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In-memory data caching can be one of the most effective strategies to improve your overall application performance and to <strong>reduce</strong> your database costs. Caching can be applied to any type of database including relational databases such as Amazon RDS or NoSQL databases such as Amazon DynamoDB, MongoDB, and Apache Cassandra. The best part of caching is that it’s minimally invasive to implement and by doing so, your application performance regarding both scale and speed is dramatically improved.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_caching.PNG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_rds_caching.PNG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In this scenario, the issue lies in the database tier of the architecture.</p><p>The option that says: <strong>Implement a caching system using ElastiCache in-memory cache on each Availability Zone</strong> is correct. Adding a cache will significantly reduce the retrieval time for frequent database queries. This also reduces the load significantly on the database tier.</p><p>The option that says: <strong>Add Read Replicas in RDS for each Availability Zone</strong> is correct. This improves the database retrieval time as there will be more nodes that can serve the database request. You won't need to increase the master instance size to accommodate all the read loads.</p><p>The following options are possible answers here, but they are not cost-effective unlike the options mentioned above:</p><p>-<strong>Configure the database tier to use sharding, which will distribute the incoming load to multiple RDS MySQL instances.</strong></p><p><strong>-Upgrade the RDS MySQL database instance size and increase the provisioned IOPS for faster processing</strong></p><p>It is quite expensive to implement load distribution on the RDS instances because you will need to use multiple sets of standby instances and read replicas. Since it will use more instances, this solution will cost more as opposed to using just Read Replicas.</p><p>The option that says: <strong>Migrate the database to use Amazon Keyspaces which natively supports sharding to distribute database queries to multiple nodes</strong> is incorrect. Amazon Keyspaces is designed to be compatible with Apache Cassandra databases. Since the question uses MySQL compatible database, using Amazon Keyspaces is not recommended here.</p><p>The option that says: <strong>Launch a CloudFront web distribution to solve the latency issu<em>e </em></strong>is incorrect. This option tries to improve the performance of the front-end tier. To solve the slow data retrieval times from RDS, it is best to implement caching and adding Read Replicas which are cheaper and simpler to do than upgrading the RDS database instance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/caching/database-caching/\">https://aws.amazon.com/caching/database-caching/</a></p><p><a href=\"https://aws.amazon.com/blogs/database/sharding-with-amazon-relational-database-service/\">https://aws.amazon.com/blogs/database/sharding-with-amazon-relational-database-service/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/caching/database-caching/",
      "https://aws.amazon.com/blogs/database/sharding-with-amazon-relational-database-service/",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 49,
    "question": "<p>A retail company has an online shopping website that provides cheap bargains and discounts on various products. The company has recently moved its infrastructure from its previous hosting provider to AWS. The architecture uses an Application Load Balancer (ALB) in front of an Auto Scaling group of Spot and On-Demand EC2 instances. The solutions architect must set up a CloudFront web distribution that uses a custom domain name and the origin should point to the new ALB.</p><p>Which of the following options is the correct implementation of an end-to-end HTTPS connection from the origin to the CloudFront viewers?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Import a certificate that is signed by a trusted third-party certificate authority, store it to ACM then attach it in your ALB. Set the Viewer Protocol Policy to HTTPS Only in CloudFront and use an SSL/TLS certificate from a third-party certificate authority which was imported to either ACM or the IAM certificate store.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use a certificate that is signed by a trusted third-party certificate authority in the ALB, which is then imported into ACM. Set the Viewer Protocol Policy to HTTPS Only in CloudFront, then use an SSL/TLS certificate from a third-party certificate authority which was imported to S3.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Upload a self-signed certificate in the ALB. Set the Viewer Protocol Policy to <code>HTTPS Only</code> in CloudFront and use an SSL/TLS certificate from a third-party certificate authority which was imported to either ACM or the IAM certificate store.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use a certificate that is signed by a trusted third-party certificate authority in the ALB, which is then imported into ACM. Set the Viewer Protocol Policy to Match Viewer to support both HTTP or HTTPS in CloudFront then use an SSL/TLS certificate from a third-party certificate authority which was imported to either ACM or the IAM certificate store.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>Remember that there are rules on which type of SSL Certificate to use if you are using an EC2 or an ELB as your origin. This question is about setting up an end-to-end HTTPS connection between the Viewers, CloudFront, and your custom origin, which is an ALB instance.</p><p>The certificate issuer you must use depends on whether you want to require HTTPS between viewers and CloudFront or between CloudFront and your origin:</p><p><strong>HTTPS between viewers and CloudFront</strong></p><p>- You can use a certificate that was issued by a trusted certificate authority (CA) such as Comodo, DigiCert, Symantec or other third-party providers.</p><p>- You can use a certificate provided by AWS Certificate Manager (ACM)</p><p><strong>HTTPS between CloudFront and a custom origin</strong></p><p>- If the origin is not an ELB load balancer, such as Amazon EC2, the certificate must be issued by a trusted CA such as Comodo, DigiCert, Symantec or other third-party providers.</p><p>- If your origin is an ELB load balancer, you can also use a certificate provided by ACM.</p><p><br></p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><br></p><p>If you're using your own domain name, such as tutorialsdojo.com, you need to change several CloudFront settings. You also need to use an SSL/TLS certificate provided by AWS Certificate Manager (ACM), or import a certificate from a third-party certificate authority into ACM or the IAM certificate store. Lastly, you should set the Viewer Protocol Policy to HTTPS Only in CloudFront.</p><p>Hence, the option that says: <strong>Import a certificate that is signed by a trusted third-party certificate authority, store it to ACM then attach it in your ALB. Set the Viewer Protocol Policy to HTTPS Only in CloudFront and use an SSL/TLS certificate from a third-party certificate authority which was imported to either ACM or the IAM certificate store</strong> is the correct answer in this scenario.</p><p>The option that says: <strong>Upload a self-signed certificate in the ALB. Set the Viewer Protocol Policy to </strong><code><strong>HTTPS only</strong></code><strong> in CloudFront and use an SSL/TLS certificate from a third-party certificate authority which was imported to either ACM or the IAM certificate store</strong> is incorrect because you cannot directly upload a self-signed certificate in your ALB.</p><p>The option that says: <strong>Use a certificate that is signed by a trusted third-party certificate authority in the ALB, which is then imported into ACM. Set the Viewer Protocol Policy to Match Viewer to support both HTTP or HTTPS in CloudFront then use an SSL/TLS certificate from a third-party certificate authority which was imported to either ACM or the IAM certificate store</strong> is incorrect because you have to set the Viewer Protocol Policy to <code>HTTPS Only</code>.</p><p>The option that says: <strong>Use a certificate that is signed by a trusted third-party certificate authority in the ALB, which is then imported into ACM. Set the Viewer Protocol Policy to HTTPS Only in CloudFront, then use an SSL/TLS certificate from a third-party certificate authority which was imported to S3 </strong>is incorrect because you cannot use an SSL/TLS certificate from a third-party certificate authority which was imported to S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy"
    ]
  },
  {
    "id": 50,
    "question": "<p>A travel and tourism company has multiple AWS accounts that are assigned to various departments. The marketing department stores the images and media files that are used in its marketing campaigns on an encrypted Amazon S3 bucket in its AWS account. The marketing team wants to share this S3 bucket so that the management team can review the files.</p><p>The solutions architect created an IAM role named mgmt_reviewer in the Management AWS account as well as a custom AWS Key Management System (AWS KMS) key on the Marketing AWS account which is associated with the S3 bucket. However, when users from the Management account received an Access Denied error when they assume the IAM role and try to access the objects on the S3 bucket.</p><p>Which of the following options should the solutions architect implement to make sure that the users on the Management AWS account can access the Marketing team's S3 bucket with the minimum required permissions? (Select THREE.)</p>",
    "corrects": [
      3,
      5,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Ensure that the mgmt_reviewer IAM role on the Management account has full permissions to access the S3 bucket. Add a decrypt permission for the custom KMS key on the IAM policy.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add an Amazon S3 bucket policy that includes read permission. Ensure that the Principal is set to the Marketing team’s AWS account ID.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Ensure that the mgmt_reviewer IAM role policy includes read permissions to the Amazon S3 bucket and a decrypt permission to the custom AWS KSM key.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Update the custom AWS KMS key policy in the Marketing account to include decrypt permission for the Management team’s AWS account ID.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Update the custom AWS KMS key policy in the Marketing account to include decrypt permission for the mgmt_reviewer IAM role.</p>",
        "correct": true
      },
      {
        "id": 6,
        "answer": "<p>Add an Amazon S3 bucket policy that includes read permission. Ensure that the Principal is set to the Management team’s AWS account ID.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>An AWS account—for example, Account A—can grant another AWS account, Account B, permission to access its resources, such as buckets and objects. Account B can then delegate those permissions to users in its account. Account A can also directly grant a user in Account B permissions using a bucket policy. But the user will still need permission from the parent account, Account B, to which the user belongs, even if Account B does not have permission from Account A. As long as the user has permission from both the resource owner and the parent account, the user will be able to access the resource.</p><p>In <strong>Amazon S3</strong>, you can grant users in another AWS account (Account B) granular cross-account access to objects owned by your account (Account A).</p><p>Depending on the type of access that you want to provide, use one of the following solutions to grant cross-account access to objects:</p><p><strong>-AWS Identity and Access Management (IAM) policies and resource-based bucket policies</strong> for programmatic-only access to S3 bucket objects</p><p><strong>-IAM policies and resource-based Access Control Lists (ACLs)</strong> for programmatic-only access to S3 bucket objects</p><p><strong>-Cross-account IAM roles</strong> for programmatic and console access to S3 bucket objects</p><p><img src=\"https://media.tutorialsdojo.com/sap_s3_cross_account_access.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_s3_cross_account_access.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>If the requester is an IAM principal, then the AWS account that owns the principal must grant the S3 permissions through an IAM policy. Based on your specific use case, the bucket owner must also grant permissions through a bucket policy or ACL. After access is granted, programmatic access of cross-account buckets is the same as accessing the same account buckets.</p><p>To grant access to an <strong>AWS KMS-encrypted</strong> bucket in Account A to a user in Account B, you must have these permissions in place:</p><p>-The bucket policy in Account A must grant access to Account B.</p><p>-The AWS KMS key policy in Account A must grant access to the user in Account B.</p><p>-The AWS Identity and Access Management (IAM) policy in Account B must grant user access to the bucket and the AWS KMS key in Account A.</p><p>The option that says: <strong>Add an Amazon S3 bucket policy that includes read permission. Ensure that the Principal is set to the Management team’s AWS account ID </strong>is correct. The S3 bucket policy must allow read permission from the Management team. Thus, the Principal on the bucket policy should be set to the Management team's account ID.</p><p>The option that says: <strong>Update the custom AWS KMS key policy in the Marketing account to include decrypt permission for the mgmt_reviewer IAM role </strong>is correct. The decrypt permission is needed by the mgmt_reviewer IAM role to use the KMS key to decrypt the objects on the S3 bucket.</p><p>The option that says: <strong>Ensure that the mgmt_reviewer IAM role policy includes read permissions to the Amazon S3 bucket and a decrypt permission to the custom AWS KSM key </strong>is correct. This is needed so that all users assuming this role will have access permission to access the S3 and use the KMS key to decrypt the S3 objects.</p><p>The option that says: <strong>Ensure that the mgmt_reviewer IAM role on the Management account has full permissions to access the S3 bucket. Add a decrypt permission for the custom KMS key on the IAM policy</strong> is incorrect. The Management team only requires read permissions on the S3 bucket. Unless required, you should not grant full permission access to the S3 bucket.</p><p>The option that says: <strong>Add an Amazon S3 bucket policy that includes read permission. Ensure that the Principal is set to the Marketing team’s AWS account ID</strong> is incorrect. The Marketing team already owns the S3 bucket. The bucket policy Principal should be set to the Management team's account ID.</p><p>The option that says: <strong>Update the custom AWS KMS key policy in the Marketing account to include decrypt permission for the Management team’s AWS account ID</strong> is incorrect. Policy on the KMS key should include the mgmt_reviewer IAM role ARN, not the Management team's account ID.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://repost.aws/knowledge-center/cross-account-access-denied-error-s3\">https://repost.aws/knowledge-center/cross-account-access-denied-error-s3</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html</a></p><p><a href=\"https://repost.aws/knowledge-center/cross-account-access-s3\">https://repost.aws/knowledge-center/cross-account-access-s3</a></p><p><br></p><p><strong>Check out these Amazon S3 and AWS Key Management Service Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/aws-key-management-service-aws-kms/?src=udemy\">https://tutorialsdojo.com/aws-key-management-service-aws-kms/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://repost.aws/knowledge-center/cross-account-access-denied-error-s3",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html",
      "https://repost.aws/knowledge-center/cross-account-access-s3",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy",
      "https://tutorialsdojo.com/aws-key-management-service-aws-kms/?src=udemy"
    ]
  },
  {
    "id": 51,
    "question": "<p>A credit company deployed its online load application system in an Auto Scaling group across multiple Availability Zones in the ap-southeast-2 region. As part of the Disaster Recovery Plan of the company, the target RTO must be less than 2 hours and the target RPO must be 10 minutes. At 12:00 PM, there was a production incident in the main database and the operations team found out that they cannot recover the transactions made from 10:30 AM onwards or 1.5 hours ago.</p><p>How can the solutions architect change the current architecture to achieve the required RTO and RPO in case a similar system failure occurred in the future?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Improve data redundancy by implementing a synchronous database “source-replica” replication between two Availability Zones.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create database backups every hour and store it in an S3 bucket<strong><em> </em></strong>with Cross-Region Replication enabled. Store the transaction logs in the same S3 bucket every 5 minutes.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Perform database backups every hour and store the result to EBS volumes. Backup the transaction logs every 5 minutes to an S3 bucket with Cross-Region Replication enabled.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create database backups every hour and store it to Glacier for archiving. Store the transaction logs in an S3 bucket every 5 minutes.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>Businesses of all sizes are using AWS to enable faster disaster recovery of their critical IT systems without incurring the infrastructure expense of a second physical site. AWS supports many disaster recovery architectures, from those built for smaller workloads to enterprise solutions that enable rapid failover at scale. AWS provides a set of cloud-based disaster recovery services that enable fast recovery of your IT infrastructure and data.</p><p><strong>Recovery time objective (RTO)</strong> - The time it takes after a disruption to restore a business process to its service level, as defined by the operational level agreement (OLA). For example, if a disaster occurs at 12:00 PM (noon) and the RTO is eight hours, the DR process should restore the business process to the acceptable service level by 8:00 PM.</p><p><strong>Recovery point objective (RPO)</strong> - The acceptable amount of data loss measured in time. For example, if a disaster occurs at 12:00 PM (noon) and the RPO is one hour, the system should recover all data that was in the system before 11:00 AM. Data loss will span only one hour, between 11:00 AM and 12:00 PM (noon).</p><p><img src=\"https://media.tutorialsdojo.com/sap_s3_versioning_enable.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_s3_versioning_enable.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>A company typically decides on an acceptable RTO and RPO based on the financial impact to the business when systems are unavailable. The company determines financial impact by considering many factors, such as the loss of business and damage to its reputation due to downtime and the lack of systems availability. IT organizations then plan solutions to provide cost-effective system recovery based on the RPO within the timeline and the service level established by the RTO.</p><p><strong>Cross-region replication (CRR)</strong> enables automatic, asynchronous copying of objects across buckets in different AWS Regions. Buckets configured for cross-region replication can be owned by the same AWS account or by different accounts. This is also helpful to durably store your data and for disaster recovery in the event of a region-wide outage.</p><p>Therefore, the correct answer is: <strong>Create database backups every hour and store it in an S3 bucket with Cross-Region Replication enabled. Store the transaction logs in the same S3 bucket every 5 minutes.</strong></p><p>The option that says: <strong>Improve data redundancy by implementing a synchronous database \"source-replica\" replication between two Availability Zones</strong> is incorrect. Although this is a valid answer, it will not be able to provide the needed data redundancy in the event of a region-wide outage. If one Availability Zone goes down, then there is another Availability Zone that contains the data. However, if the whole AWS region is down, then the data will be totally unreachable. A better solution is to use S3 and enable Cross-Region Replication (CRR).</p><p>The option that says: <strong>Perform database backups every hour and store the result to EBS volumes. Backup the transaction logs every 5 minutes to an S3 bucket with Cross-Region Replication enabled</strong> is incorrect. EBS Volumes are not durable enough to store the database backups every hour for this scenario. If there is an AZ or an AWS Region-wide outage, then the data in the EBS Volume could potentially be unavailable.</p><p>The option that says: <strong>Create database backups every hour and store it to Glacier for archiving. Store the transaction logs in an S3 bucket every 5 minutes</strong> is incorrect because Glacier is primarily used for long term data archiving.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/disaster-recovery/\">https://aws.amazon.com/disaster-recovery/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html</a></p><p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html\">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/disaster-recovery/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html",
      "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 52,
    "question": "<p>A logistics company is running its business application on Amazon EC2 instances. The web application is running on an Auto Scaling group of EC2 instances behind an Application Load Balancer. The self-managed MySQL database is also running on a large EC2 instance to handle the heavy I/O operations needed by the application. The application is able to handle the amount of traffic during normal hours. However, the performance slows down significantly during the last four days of the month as more users run their month-end reports simultaneously. The Solutions Architect was tasked to improve the performance of the application, especially during the peak days.</p><p>Which of the following should the Solutions Architect implement to improve the application performance with the LEAST impact on availability?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create Amazon CloudWatch metrics based on EC2 instance CPU usage or response time on the ALB. Trigger an AWS Lambda function to change the instance size, type, and the allocated IOPS of the EBS volumes based on the breached threshold.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Take a snapshot of the EBS volumes with I/O heavy operations and replace them with Provisioned IOPS volumes during the end of the month. Revert to the old EBS volume type afterward to save on costs.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Convert all EBS volumes of the EC2 instances to GP2 volumes to improve I/O performance. Scale up the EC2 instances into bigger instance types. Pre-warm the Application Load Balancer to handle sudden spikes in traffic.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Migrate the Amazon EC2 database instance to Amazon RDS for MySQL. Add more read replicas to the database cluster during the end of the month to handle the spike in traffic.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon Relational Database Service (Amazon RDS)</strong> is a web service that makes it easier to set up, operate, and scale a relational database in the AWS Cloud. It provides cost-efficient, resizable capacity for an industry-standard relational database and manages common database administration tasks.</p><p>Amazon RDS supports the most demanding database applications. You can choose between two SSD-backed storage options: one optimized for high-performance OLTP applications, and the other for cost-effective general-purpose use. You can scale your database's compute and storage resources with only a few mouse clicks or an API call, often with no downtime.</p><p><strong>Amazon RDS Read Replicas</strong> provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances.</p><p><img src=\"https://media.tutorialsdojo.com/sap_RDSreadreplica_async.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_RDSreadreplica_async.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. Read replicas allow you to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Because read replicas can be promoted to master status, they are useful as part of a sharding implementation.</p><p>In this scenario, the Amazon EC2 instances are in an Auto Scaling group already which means that the database read operations is the possible bottleneck especially during the month-end wherein the reports are generated. This can be solved by creating RDS read replicas.</p><p>Therefore, the correct answer is: <strong>Migrate the Amazon EC2 database instance to Amazon RDS for MySQL. Add more read replicas to the database cluster during the end of the month to handle the spike in traffic.</strong></p><p>The option that says: <strong>Convert all EBS volumes of the EC2 instances to GP2 volumes to improve I/O performance. Scale up the EC2 instances into bigger instance types. Pre-warm the Application Load Balancer to handle sudden spikes in traffic</strong> is incorrect. The Amazon EC2 instances are in an Auto Scaling group already which means that the database read operations is the possible bottleneck so changing to bigger EC2 instances will just increase the costs unnecessarily.</p><p>The option that says: <strong>Create Amazon CloudWatch metrics based on EC2 instance CPU usage or response time on the ALB. Trigger an AWS Lambda function to change the instances size, type, and the allocated IOPS of the EBS volumes based on the breached threshold</strong> is incorrect. This will cause a lot of interruption on the application when you change the EBS volumes and the instance type for the EC2 database instance. You will have to reboot the database instances when you change the instance type.</p><p>The option that says: <strong>Take a snapshot of the EBS volumes with I/O heavy operations and replace them with Provisioned IOPS volumes during the end of the month. Revert to the old EBS volume type afterward to save on costs </strong>is incorrect. Creating snapshots will cause the disk write operations to temporarily stop in which the database will stop responding to write requests. Changing the disk types every month is not ideal as this also causes downtime on the application during the switch. A better solution for heavy read operations is to provision an Amazon RDS database with Read Replicas.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html</a></p><p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/\">https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html",
      "https://aws.amazon.com/rds/features/read-replicas/",
      "https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 53,
    "question": "<p>A digital banking company runs its production workload on the AWS cloud. The company has enabled multi-region support on an AWS CloudTrail trail. As part of the company security policy, the creation of any IAM users must be approved by the security team. When an IAM user is created, all of the permissions from that user must be removed automatically. A notification must then be sent to the security team to approve the user creation.</p><p>Which of the following options should the solutions architect implement to meet the company requirements? (Select THREE.)</p>",
    "corrects": [
      1,
      2,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a rule in Amazon EventBridge that will check for patterns in AWS CloudTrail API calls with the CreateUser eventName.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Send a message to an Amazon Simple Notification Service (Amazon SNS) topic. Have the security team subscribe to the SNS topic.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon AWS Audit Manager to continually audit newly created users and send a notification to the security team.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure an event filter in AWS CloudTrail for the CreateUser event and send a notification to an Amazon Simple Notification Service (Amazon SNS) topic.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use Amazon EventBridge to invoke an AWS Fargate tasks that will remove permissions on the newly created IAM user.</p>",
        "correct": false
      },
      {
        "id": 6,
        "answer": "<p>Use Amazon EventBridge to invoke an AWS Step Function state machine that will remove permissions on the newly created IAM user.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>Using IAM, you can manage access to AWS services and resources securely. You can create and manage AWS users and groups and use permissions to allow and deny those users and groups access to AWS resources.</p><p>You can create an <strong>Amazon EventBridge</strong> rule with an event pattern that matches a specific IAM API call or multiple IAM API calls. Then, associate the rule with an <strong>Amazon Simple Notification Service (Amazon SNS)</strong> topic. When the rule runs, an SNS notification is sent to the corresponding subscriptions.</p><p>Amazon EventBridge works with <strong>AWS CloudTrail</strong>, a service that records actions from AWS services. CloudTrail captures API calls made by or on behalf of your AWS account from the EventBridge console and to EventBridge API operations.</p><p>Using the information collected by CloudTrail, you can determine what request was made to EventBridge, the IP address from which the request was made, who made the request, when it was made, and more. When an event occurs in EventBridge, CloudTrail records the event in Event history. You can view, search, and download recent events in your AWS account.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_event_notification.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_event_notification.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The option that says: <strong>Use Amazon EventBridge to invoke an AWS Step Function state machine that will remove permissions on the newly created IAM user</strong> is correct. When Amazon EventBridge detects a pattern from the specified events, it can trigger an AWS Step Function that has specific actions.</p><p>The option that says: <strong>Send a message to an Amazon Simple Notification Service (Amazon SNS) topic. Have the security team subscribe to the SNS topic</strong> is correct. If you create a pattern in Amazon EventBridge, it can then send a message to an SNS Topic once the event is detected.</p><p>The option that says: <strong>Create a rule in Amazon EventBridge that will check for patterns in AWS CloudTrail API calls with the CreateUser eventName</strong> is correct. With Amazon EventBridge, you define patterns to scan events that happen in your AWS account, such as creating an IAM user.</p><p>The option that says: <strong>Configure an event filter in AWS CloudTrail for the CreateUser event and send a notification to an Amazon Simple Notification Service (Amazon SNS) topic</strong> is incorrect. You can filter events on CloudTrail by searching for keywords, however, you can't configure it to send notifications to Amazon SNS. You should use EventBridge for this scenario.</p><p>The option that says: <strong>Use Amazon AWS Audit Manager to continually audit newly created users and send a notification to the security team</strong> is incorrect. AWS Audit Manager is used to map compliance requirements to AWS usage data with prebuilt and custom frameworks and automated evidence collection, not for scanning IAM user creation events.</p><p>The option that says: <strong>Use Amazon EventBridge to invoke an AWS Fargate tasks that will remove permissions on the newly created IAM user</strong> is incorrect. Amazon EventBridge cannot directly invoke AWS Fargate tasks.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/send-a-notification-when-an-iam-user-is-created.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/send-a-notification-when-an-iam-user-is-created.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-eventbridge-sns-rule/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-eventbridge-sns-rule/</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/sns-email-notifications-eventbridge/\">https://aws.amazon.com/premiumsupport/knowledge-center/sns-email-notifications-eventbridge/</a></p><p><br></p><p><strong>Check out these Amazon CloudWatch and AWS Simple Notification Service Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-sns/?src=udemy\">https://tutorialsdojo.com/amazon-sns/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/send-a-notification-when-an-iam-user-is-created.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/iam-eventbridge-sns-rule/",
      "https://aws.amazon.com/premiumsupport/knowledge-center/sns-email-notifications-eventbridge/",
      "https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy",
      "https://tutorialsdojo.com/amazon-sns/?src=udemy"
    ]
  },
  {
    "id": 54,
    "question": "<p>A leading aerospace engineering company has over 1 TB of aeronautical data stored on the corporate file server of its on-premises network. This data is used by a lot of its in-house analytical and engineering applications. The aeronautical data consists of technical files which can have a file size of a few megabytes to multiple gigabytes. The data scientists typically modify an average of 10 percent of these files every day. Recently, the management decided to adopt a hybrid cloud architecture to serve its clients around the globe better. The management requested to migrate its applications to AWS over the weekend to minimize any business impact and system downtime. The on-premises data center has a 50-Mbps Internet connection, which can be used to transfer all of the 1 TB of data in AWS, but based on the calculations, it will take at least 48 hours to complete this task.</p><p>Which of the following options will allow the solutions architect to move all of the aeronautical data to AWS MOST cost-effectively?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>1. Synchronize the data from your on-premises data center to an S3 bucket using Multipart upload for large files from Saturday morning to Sunday evening. </p><p><br></p><p>2. Configure your application hosted in AWS to use the S3 bucket to serve the aeronautical data files.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>1. At the end of business hours on Friday, start copying the data to a Snowball Edge device.\n</p><p>2. When the Snowball Edge have completely transferred your data to your AWS Cloud, copy all of the data to multiple EBS Volumes.\n</p><p>3. On Sunday afternoon, mount the generated EBS volume to your EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>1. Synchronize the on-premises data to an S3 bucket one week before the migration schedule using the AWS CLI's S3&nbsp; <code>sync</code> command.\n</p><p>2. Perform a final synchronization task on Friday after the end of business hours.\n</p><p>3. Set up your application hosted in a large EC2 instance in your VPC to use the S3 bucket.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>1. Set up a Gateway-Stored volume gateway using the AWS Storage Gateway service. </p><p><br></p><p>2. Establish an iSCSI connection between your on-premises data center and your AWS Cloud then copy the data to the Storage Gateway volume. </p><p><br></p><p>3. After all of your data has been successfully copied, create an EBS snapshot of the volume. </p><p><br></p><p>4. Restore the snapshots as EBS volumes and attach them to your EC2 instances on Sunday.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p>The most effective choice here is to use the S3 <code>sync</code> feature that is available in AWS CLI. In this way, you can comfortably synchronize the data in your on-premises server and in AWS a week before the migration. And on Friday, just do another sync to complete the task. Remember that the sync feature of S3 only uploads the \"delta\" or in other words, the \"difference\" in the subset. Therefore, it will only take just a fraction of the time to complete the data synchronization compared to the other methods.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_s3_cli.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_s3_cli.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is:</p><p><strong>1. Synchronize the on-premises data to an S3 bucket one week before the migration schedule using the AWS CLI's S3 </strong><code><strong>sync</strong></code><strong> command.</strong></p><p><strong>2. Perform a final synchronization task on Friday after the end of business hours.</strong></p><p><strong>3. Set up your application hosted in a large EC2 instance in your VPC to use the S3 bucket.</strong></p><p>The idea is to synchronize the data days before the migration weekend to avoid the risks and possible delays of transferring the data in such a short period of allowable time, i.e., 48 hours.</p><p><br></p><p>The following option is incorrect:</p><p><strong>1. At the end of business hours on Friday, start copying the data to a Snowball Edge device.</strong></p><p><strong>2. When the Snowball Edge have completely transferred your data to your AWS Cloud, copy all of the data to multiple EBS Volumes.</strong></p><p><strong>3. On Sunday afternoon, mount the generated EBS volume to your EC2 instances.</strong></p><p>Although using a Snowball appliance is a valid option, there is a risk that your data won't make it on time on Monday. Remember that you have to consider the time it takes to transfer the data to Snowball, including the process of shipping it back to AWS, which may take a day or two. Factoring all of these considerations, it is clear that this is not the best option as the risk is just too high that the Snowball delivery might not make it in time.</p><p><br></p><p>The following option is incorrect:</p><p><strong>1. Set up a Gateway-Stored volume gateway using the AWS Storage Gateway service.</strong></p><p><strong>2. Establish an iSCSI connection between your on-premises data center and your AWS Cloud then copy the data to the Storage Gateway volume.</strong></p><p><strong>3. After all of your data has been successfully copied, create an EBS snapshot of the volume.</strong></p><p><strong>4. Restore the snapshots as EBS volumes and attach them to your EC2 instances on Sunday.</strong></p><p>In this scenario, you have to create storage volumes in the AWS Cloud that your on-premises applications can access as Internet Small Computer System Interface (iSCSI) targets, which is not mentioned in the option. This is not a cost-effective solution, considering that you only have to migrate 1 TB of data, and the fact that you can just use the S3 <code>sync</code> command via AWS CLI.</p><p><br></p><p>The following option is incorrect:</p><p><strong>1. Synchronize the data from your on-premises data center to an S3 bucket using Multipart upload for large files from Saturday morning to Sunday evening.</strong></p><p><strong>2. Configure your application hosted in AWS to use the S3 bucket to serve the aeronautical data files.</strong></p><p>Although using multi-part upload in S3 is faster than regular S3 upload, this method still carries a risk, considering that the company only has a 50 Mbps internet connection. It is still better to use S3 <code>sync</code> command via AWS CLI days before the actual migration to mitigate the risks.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/s3/sync.html\">https://docs.aws.amazon.com/cli/latest/reference/s3/sync.html</a></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html\">https://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/cli/latest/reference/s3/sync.html",
      "https://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 55,
    "question": "<p>A multinational bank has recently set up AWS Organizations to manage its several AWS accounts from their various business units. The Senior Solutions Architect attached the SCP below to an Organizational Unit (OU) to define the services that its member accounts can use:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"Version\"</span><span class=\"pun\">:</span><span class=\"str\">\"2012-10-17\"</span><span class=\"pun\">,</span><span class=\"pln\"> </span></li><li class=\"L2\"><span class=\"str\">\"Statement\"</span><span class=\"pun\">:[</span><span class=\"pln\"> </span></li><li class=\"L3\"><span class=\"pln\">  </span><span class=\"pun\">{</span><span class=\"pln\"> </span></li><li class=\"L4\"><span class=\"pln\">     </span><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span><span class=\"pln\"> </span></li><li class=\"L5\"><span class=\"pln\">     </span><span class=\"str\">\"Action\"</span><span class=\"pun\">:[</span><span class=\"str\">\"EC2:*\"</span><span class=\"pun\">,</span><span class=\"str\">\"S3:*\"</span><span class=\"pun\">],</span><span class=\"pln\"> </span></li><li class=\"L6\"><span class=\"pln\">     </span><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"str\">\"*\"</span><span class=\"pln\"> </span></li><li class=\"L7\"><span class=\"pln\">   </span><span class=\"pun\">}</span><span class=\"pln\"> </span></li><li class=\"L8\"><span class=\"pln\"> </span><span class=\"pun\">]</span><span class=\"pln\"> </span></li><li class=\"L9\"><span class=\"pun\">}</span><span class=\"pln\"> </span></li></ol></pre></div></div><p>In one of the member accounts under that OU, an IAM user tried to create a new S3 bucket but was getting a permission denied error.</p><p>Which of the following options is the most likely cause of this issue?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>All accounts within the OU does not automatically inherit the policy attached to them. You still have to manually attach the SCP to the individual AWS accounts of the OU.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>You should use the root user of the account to be able to create the new S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The IAM user in the member account does not have IAM policies that explicitly grant EC2 or S3 service actions.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>An IAM policy that allows the use of S3 and EC2 services should be the one attached in the OU instead of an SCP.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>A <strong>service control policy (SCP)</strong> determines what services and actions can be delegated by administrators to the users and roles in the accounts that the SCP is applied to. An SCP <strong><em>does not</em></strong> grant any permissions. Instead, SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). The SCP limits permissions for entities in member accounts, including each AWS account root user.</p><p>If the SCP allows the actions for a service, the administrator of the account can grant permissions for those actions to the users and roles in that account, and the users and roles can perform the actions of the administrators grant those permissions. If the SCP denies actions for a service, the administrators in that account can't effectively grant permissions for those actions, and the users and roles in the account can't perform the actions even if granted by an administrator.</p><p><img src=\"https://media.tutorialsdojo.com/sap_scp_org_attached.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_scp_org_attached.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Users and roles must still be granted permissions using IAM permission policies attached to them or to groups. The SCPs filter the permissions granted by such policies, and the user can't perform any actions that the applicable SCPs don't allow. Actions allowed by the SCPs can be used if they are granted to the user or role by one or more IAM permission policies. Take note that the IAM user being used by the administrator does not have IAM policies that explicitly grant EC2 or S3 service actions.</p><p>Hence, the correct answer is: <strong>The IAM user in the member account does not have IAM policies that explicitly grant EC2 or S3 service actions</strong>.</p><p>The option that says: <strong>All accounts within the OU does not automatically inherit the policy attached to them. You still have to manually attach the SCP to the individual AWS accounts of the OU</strong> is incorrect because an SCP attached to an OU is automatically inherited by all accounts within that same OU. The main cause of this issue is the missing IAM policy in the account, which explicitly grants EC2 or S3 service actions to the IAM user.</p><p>The option that says: <strong>An IAM policy that allows the use of S3 and EC2 services should be the one attached in the OU instead of an SCP</strong> is incorrect because you cannot directly assign an IAM policy to an OU. In addition, there is no attached IAM policy that allows EC2 or S3 service actions to the IAM user.</p><p>The option that says:<strong> Using the root user of the account to be able to create the new S3 bucket</strong> is incorrect because SCPs <strong>do</strong> affect the root user along with all IAM users and standard IAM roles in any affected account. The issue lies in the missing IAM policy of the account and not with the SCP, OU, or its AWS Organizations settings.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html</a></p><p><a href=\"https://aws.amazon.com/organizations/faqs/\">https://aws.amazon.com/organizations/faqs/</a></p><p><br></p><p><strong>Check out these AWS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html",
      "https://aws.amazon.com/organizations/faqs/",
      "https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy"
    ]
  },
  {
    "id": 56,
    "question": "<p>A retail company runs its customer support call system on its in-house data center. The Solutions Architect was tasked to migrate the call system to AWS and leverage the use of managed services to reduce management overhead. The solution must be able to handle the current tasks such as receiving calls and creating contact flows. It must be able to scale to handle more calls as the customer base grows. The company also wants to add deep learning capabilities to the call system to reduce the need to speak to an agent. It must be able to recognize the intent of the caller based on certain keywords and handle basic tasks, as well as provide information to the call center agents.</p><p>Which combination of actions should the Solutions Architect implement to meet the company's requirements? (Select TWO.)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Send incoming customer calls to an Amazon Kinesis stream and process their voice through Amazon Comprehend to determine the customer’s intent.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Build a conversational interface on Amazon Alexa for Business to have AI-based answers to customer queries, thereby reducing the need to speak to an agent.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an Amazon Lex bot to recognize callers' intent.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use the Amazon Connect service to create an omnichannel cloud-based contact center for the agents.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Leverage Amazon Rekognition to identify the caller and process the voice through Amazon Polly to determine the intent based on the customer’s voice.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Connect</strong> is an easy-to-use omnichannel cloud contact center that helps companies provide superior customer service across voice, chat, and tasks at a lower cost than traditional contact center systems. You can set up a contact center in a few steps, add agents who are located anywhere, and start engaging with your customers.</p><p>You can create personalized experiences for your customers using omnichannel communications. For example, you can dynamically offer chat and voice contact based on such factors as customer preference and estimated wait times. Agents, meanwhile, conveniently handle all customers from just one interface. For example, they can chat with customers and create or respond to tasks as they are routed to them.</p><p>The following diagram shows these key characteristics of Amazon Connect.</p><p><img src=\"https://media.tutorialsdojo.com/sap_amazon_connect.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_amazon_connect.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>To help provide a better contact center, you can use Amazon Connect to integrate with several AWS services to provide Machine Learning (ML) and Artificial Intelligence (AI) capabilities.</p><p>Amazon Connect uses the following services for ML/AI:</p><p><strong>Amazon Lex</strong>—Let you create a chatbot to use as an Interactive Voice Response (IVR).</p><p><strong>Amazon Polly</strong>—Provides text-to-speech in all contact flows.</p><p><strong>Amazon Transcribe</strong>—Grabs conversation recordings from Amazon S3 and transcribes them to text so you can review them.</p><p><strong>Amazon Comprehend</strong>—Takes the transcription of recordings and applies speech analytics machine learning to the call to identify sentiment, keywords, adherence to company policies, and more.</p><p><strong>Amazon Lex</strong> is a service for building conversational interfaces into any application using voice and text. Amazon Lex provides the advanced deep learning functionalities of automatic speech recognition (ASR) for converting speech to text and natural language understanding (NLU) to recognize the intent of the text to enable you to build applications with highly engaging user experiences and lifelike conversational interactions.</p><p>By using an Amazon Lex chatbot in your call center, callers can perform tasks such as changing a password, requesting a balance on an account, or scheduling an appointment without needing to speak to an agent. These chatbots use automatic speech recognition and natural language understanding to ascertain a caller’s intent, maintain context and fluidly manage the conversation. Amazon Lex uses AWS Lambda functions to query your business applications, provide information back to callers, and make updates as requested.</p><p>The option that says: <strong>Use the Amazon Connect service to create an omnichannel cloud-based contact center for the agents</strong> is correct. Amazon Connect is the AWS service you will want to use to build a cloud-based call center system.</p><p>The option that says: <strong>Use an Amazon Lex bot to recognize callers' intent </strong>is correct. Amazon Lex provides the deep learning capabilities required to recognize the intent of a caller based on certain keywords and handle basic tasks autonomously. It integrates well with Amazon Connect, allowing the creation of conversational interfaces that can interact with users effectively.</p><p>The option that says: <strong>Leverage Amazon Rekognition to identify the caller and process the voice through Amazon Polly to determine the intent based on the customer’s voice</strong> is incorrect. Amazon Rekognition is used to identify persons on photos or videos, not on voice calls. Amazon Polly is used for text-to-speech services. You should use Amazon Lex for this scenario.</p><p>The option that says: <strong>Build a conversational interface on Amazon Alexa for Business to have AI-based answers to customer queries, thereby reducing the need to speak to an agent. </strong>is incorrect. Alexa for Business gives you the tools you need to manage Alexa devices, enroll your users, and assign skills at scale for your organization. For conversational interfaces such as chatbots, you can use Amazon Lex.</p><p>The option that says: <strong>Send incoming customer calls to an Amazon Kinesis stream and process their voice through Amazon Comprehend to determine the customer’s intent</strong> is incorrect. Amazon Comprehend processes text and applies speech analysis. It can't handle speech from audio sources. The scenario requires understanding the intent of the user based on their speech, so Amazon Lex is better suited for this.</p><p><strong><br>References:</strong></p><p><a href=\"https://aws.amazon.com/connect/\">https://aws.amazon.com/connect/</a></p><p><a href=\"https://docs.aws.amazon.com/lexv2/latest/dg/what-is.html\">https://docs.aws.amazon.com/lexv2/latest/dg/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/connect/latest/adminguide/related-services-amazon-connect.html\">https://docs.aws.amazon.com/connect/latest/adminguide/related-services-amazon-connect.html</a></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/delivering-natural-conversational-experiences-using-amazon-lex-streaming-apis/\">https://aws.amazon.com/blogs/machine-learning/delivering-natural-conversational-experiences-using-amazon-lex-streaming-apis/</a></p><p><br></p><p><strong>Check out the Amazon Lex Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-lex/?src=udemy\">https://tutorialsdojo.com/amazon-lex/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/connect/",
      "https://docs.aws.amazon.com/lexv2/latest/dg/what-is.html",
      "https://docs.aws.amazon.com/connect/latest/adminguide/related-services-amazon-connect.html",
      "https://aws.amazon.com/blogs/machine-learning/delivering-natural-conversational-experiences-using-amazon-lex-streaming-apis/",
      "https://tutorialsdojo.com/amazon-lex/?src=udemy"
    ]
  },
  {
    "id": 57,
    "question": "<p>A cryptocurrency exchange company has recently signed up for a 3rd party online auditing system, which is also using AWS, to perform regulatory compliance audits on their cloud systems. The online auditing system needs to access certain AWS resources in your network to perform the audit.</p><p>In this scenario, which of the following approach is the most secure way of providing access to the 3rd party online auditing system?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a new IAM role for cross-account access which allows the online auditing system account to assume the role. Assign a policy that allows full and unrestricted access to all AWS resources.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a new IAM role for cross-account access which allows the online auditing system account to assume the role. Assign it a policy that allows only the actions required for the compliance audit.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a new IAM user and assign a user policy to the IAM user that allows full and unrestricted access to all AWS resources. Create a new access and secret key for the IAM user and provide these credentials to the 3rd party auditing company.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a new IAM user and assign a user policy to the IAM user that allows only the actions required by the online audit system. Create a new access and secret key for the IAM user and provide these credentials to the 3rd party auditing company.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>An IAM <em>role</em> is an IAM identity that you can create in your account that has specific permissions. An IAM role is similar to an IAM user, in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have standard long-term credentials such as a password or access keys associated with it. Instead, when you assume a role, it provides you with temporary security credentials for your role session.</p><p>You can use roles to delegate access to users, applications, or services that don't normally have access to your AWS resources. For example, you might want to grant users in your AWS account access to resources they don't usually have, or grant users in one AWS account access to resources in another account. Or you might want to allow a mobile app to use AWS resources, but not want to embed AWS keys within the app (where they can be difficult to rotate and where users can potentially extract them). Sometimes you want to give AWS access to users who already have identities defined outside of AWS, such as in your corporate directory. Or, you might want to grant access to your account to third parties so that they can perform an audit on your resources.</p><p><img src=\"https://media.tutorialsdojo.com/sap_iam_assume_role_cross_account.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_iam_assume_role_cross_account.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>To allow users from one AWS account to access resources in another AWS account, create a role that defines who can access it and what permissions it grants to users that switch to it. Use IAM roles to delegate access within or between AWS accounts. By setting up cross-account access, you don't need to create individual IAM users in each account in order to provide access to different AWS accounts.</p><p>The option that says: <strong>Create a new IAM role for cross-account access which allows the online auditing system account to assume the role. Assign it a policy that allows only the actions required for the compliance audit </strong>is correct because it uses an IAM role and only provides the needed access, which adheres to the principle of least privilege.</p><p>The option that says:<strong><em> </em>Create a new IAM role for cross-account access which allows the online auditing system account to assume the role. Assign a policy that allows full and unrestricted access to all AWS resources</strong> is incorrect. Although you are right to use an IAM role, you should provide only the access needed by the 3rd party audit system and not a full, unrestricted access to all AWS resources.</p><p>The option that says: <strong>Create a new IAM user and assign a user policy to the IAM user that allows only the actions required by the online audit system. Create a new access and secret key for the IAM user and provide these credentials to the 3rd party auditing company</strong> is incorrect as you need to use an IAM role instead of an IAM user.</p><p>The option that says: <strong>Create a new IAM user and assign a user policy to the IAM user that allows full and unrestricted access to all AWS resources. Create a new access and secret key for the IAM user and provide these credentials to the 3rd party auditing company</strong> is incorrect as you need to use an IAM role instead of an IAM user. In addition, you should provide only the access needed by the 3rd party audit system and not a full, unrestricted access to all AWS resources.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy"
    ]
  },
  {
    "id": 58,
    "question": "<p>A company runs hundreds of Amazon EC2 instances inside an Amazon VPC. Whenever an EC2 error is encountered, the solutions architect performs manual steps in order to regain access to the impaired instance. The management wants to automatically recover impaired EC2 instances in the VPC. The goal is to automatically fix an instance that has become unreachable due to network misconfigurations, RDP issues, firewall settings, and many others to meet the compliance requirements.</p><p>Which of the following options is the most suitable solution that the solutions architect should implement to meet the above requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the EC2Rescue tool to diagnose and troubleshoot problems on your Amazon EC2 Linux and Windows Server instances. Run the tool automatically by using AWS Lambda and a custom script.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>To meet the compliance requirements, use a combination of AWS Config and the AWS Systems Manager State Manager to self-diagnose and troubleshoot problems on your Amazon EC2 Linux and Windows Server instances. Automate the recovery process by using AWS Systems Manager Maintenance Windows.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>To meet the compliance requirements, use a combination of AWS Config and the AWS Systems Manager Session Manager to self-diagnose and troubleshoot problems on your EC2 Linux and Windows Server instances. Automate the recovery process by setting up a monitoring system using Amazon CloudWatch, AWS Lambda, and the AWS Systems Manager Run Command that will automatically monitor and recover impaired EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the EC2Rescue tool to diagnose and troubleshoot problems on your EC2 Linux and Windows Server instances. Run the tool automatically by using the AWS Systems Manager Automation and the <code>AWSSupport-ExecuteEC2Rescue</code> document.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>EC2Rescue</strong> can help you diagnose and troubleshoot problems on Amazon EC2 Linux and Windows Server instances. You can run the tool manually, or you can run the tool automatically by using Systems Manager Automation and the <em>AWSSupport-ExecuteEC2Rescue</em> document. The <em>AWSSupport-ExecuteEC2Rescue </em>document is designed to perform a combination of Systems Manager actions, AWS CloudFormation actions, and Lambda functions that automate the steps normally required to use EC2Rescue.</p><p><strong>Systems Manager Automation</strong> simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following:</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon EventBridge.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_ssm_overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_ssm_overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Use the EC2Rescue tool to diagnose and troubleshoot problems on your EC2 Linux and Windows Server instances. Run the tool automatically by using the AWS Systems Manager Automation and the </strong><code><strong>AWSSupport-ExecuteEC2Rescue</strong></code><strong> document.</strong></p><p>The option that says: <strong>To meet the compliance requirements, use a combination of AWS Config and the AWS Systems Manager State Manager to self-diagnose and troubleshoot problems on your EC2 Linux and Windows Server instances. Automate the recovery process by using AWS Systems Manager Maintenance Windows</strong> is incorrect. AWS Config is a service which is primarily used to assess, audit, and evaluate the configurations of your AWS resources but not to diagnose and troubleshoot problems in your EC2 instances. In addition, AWS Systems Manager State Manager is primarily used as a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define but does not help you in troubleshooting your EC2 instances.</p><p>The option that says: <strong>To meet the compliance requirements, use a combination of AWS Config and the AWS Systems Manager Session Manager to self-diagnose and troubleshoot problems on your EC2 Linux and Windows Server instances. Automate the recovery process by setting up a monitoring system using Amazon CloudWatch, AWS Lambda, and the AWS Systems Manager Run Command that will automatically monitor and recover impaired EC2 instances</strong> is incorrect. Just like the other option, AWS Config does not help you troubleshoot the problems in your EC2 instances. The AWS Systems Manager Sessions Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys for your EC2 instances, but it does not provide the capability of helping you diagnose and troubleshoot problems in your instance like what the EC2Rescue tool can do. In addition, setting up a CloudWatch, AWS Lambda, and the AWS Systems Manager Run Command that will automatically monitor and recover impaired EC2 instances is an operational overhead that can be easily done by using AWS Systems Manager Automation.</p><p>The option that says: <strong>Use the EC2Rescue tool to diagnose and troubleshoot problems on your EC2 Linux and Windows Server instances. Run the tool automatically by using AWS Lambda and a custom script</strong> is incorrect because while it is technically possible to use AWS Lambda to automate the running of the EC2Rescue tool, this approach typically requires significant development effort to create and maintain the custom script. On the other hand, using the existing <code>AWSSupport-ExecuteEC2Rescue</code> automation document in AWS Systems Manager can be a more straightforward solution. This document, maintained by AWS, can be triggered automatically, reducing the need for custom scripting and the associated development and maintenance effort.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2rescue.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2rescue.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2rescue.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html",
      "https://tutorialsdojo.com/aws-systems-manager/?src=udemy"
    ]
  },
  {
    "id": 59,
    "question": "<p>A pharmaceutical company has a hybrid cloud architecture in AWS. It has three different accounts for its environments: DEV, UAT, and PROD, which are all part of the consolidated billing account. The PROD account has purchased 10 Reserved EC2 Instances in the us-west-2a Availability Zone.</p><p>Currently, there are no running EC2 instances in the PROD account because the application is not live yet. However, in the DEV account, there are 5 EC2 instances running in the us-west-2a Availability Zone. In the UAT account, there are also 5 EC2 instances running in the us-west-1a Availability Zone. All the EC2 instances in the DEV and UAT accounts match the reserved instance type in the PROD account.</p><p>In this scenario, which account benefits the most from the Reserved Instance pricing?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Currently, only the UAT account benefits from the Reserved Pricing.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Since both DEV and UAT accounts are running an EC2 instance type that exactly matches the Reserved Instance type, then the Reserved instance pricing will be applied to all EC2 instances in those two member accounts.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Currently, only the DEV account benefits from the Reserved Pricing.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "None. Considering that the PROD account is the one that purchased the Reserved Instance and it does not have any running EC2 instance, there is currently no other member account that benefits from the Reserved Instance pricing.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In the question, DEV and UAT are running the same instance type, but the main difference is that the former is using us-west-2a Availability Zone while the latter uses us-west-1a. Remember that the PROD account has bought the Reserved Instance in the us-west-2a Availability Zone, which means that only the DEV account exactly matches the criteria.</p><p>As an <strong>Amazon EC2 Reserved Instance</strong>s example, suppose that Bob and Susan each have an account in an organization. Susan has five Reserved Instances of the same type, and Bob has none. During one particular hour, Susan uses three instances, and Bob uses six, for a total of nine instances on the organization's consolidated bill. AWS bills five instances as Reserved Instances and the remaining four instances as regular instances.</p><p><img src=\"https://media.tutorialsdojo.com/sap_reserved_instances_basic.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_reserved_instances_basic.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Bob receives the cost-benefit from Susan's Reserved Instances only if he launches his instances in the same Availability Zone where Susan purchased her Reserved Instances. For example, if Susan specifies us-west-2a when she purchases her Reserved Instances, Bob must specify us-west-2a when he launches his instances to get the cost-benefit on the organization's consolidated bill. However, the actual locations of Availability Zones are independent from one account to another. For example, the us-west-2a Availability Zone for Bob's account might be in a different location than the location for Susan's account.</p><p>Therefore, the correct answer is: <strong>Currently, only the DEV account benefits from the Reserved Pricing.</strong></p><p>The option that says: <strong>Currently, only the UAT account benefits from the Reserved Pricing</strong> is incorrect. The UAT account can only benefit from the Reserved Instances of the PROD account if they were launched in the same Availability Zone where the PROD account purchased the Reserved Instances.</p><p>The option that says: <strong>Since both DEV and UAT accounts are running an EC2 instance type which exactly matches the Reserved Instance type, then the Reserved instance pricing will be applied to all EC2 instances in those two member accounts</strong> is incorrect. The UAT account can only benefit from the Reserved Instances of the PROD account if they were launched in the same Availability Zone where the PROD account purchased the Reserved Instances.</p><p>The option that says: <strong>None. Considering that the PROD account is the one that purchased the Reserved Instance and it does not have any running EC2 instance, there is currently no other member account that benefits from the Reserved Instance pricing</strong> is incorrect. The DEV account benefits from the Reserved Instances pricing because the instances are launched on the same AZ where the PROD account purchased the Reserved Instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html</a></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidatedbilling-other.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidatedbilling-other.html</a></p><p><br></p><p><strong>Check out this AWS Billing and Cost Management Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy\">https://tutorialsdojo.com/aws-billing-and-cost-management/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html",
      "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidatedbilling-other.html",
      "https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy"
    ]
  },
  {
    "id": 60,
    "question": "<p>An Internet-of-Things (IoT) company is building a portal that stores data coming from its 20,000 gas sensors. The gas sensors, which have unique IDs, are used to detect a gas leak or other emissions inside the oil facility. Every 15 minutes, the sensors will send a data point throughout the day containing its ID, current gas level data, as well as the timestamp. Each data point contains critical information coming from the gas sensors. The company would like to query the information coming from a particular gas sensor for the past week and would like to delete all data that are older than eight weeks. The application is using a NoSQL database which is why they are using the Amazon DynamoDB service.</p><p>How would you implement this in the most cost-effective way?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use one table every week, with a primary key which is the concatenated value of the sensor ID and the timestamp.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use one table every week, with a composite primary key which is the sensor ID as the partition key and the timestamp as the sort key.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use one table with a primary key which is the sensor ID. Use the timestamp as the hash key.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use one table with a primary key which is the concatenated value of the sensor ID and the timestamp.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>The gas sensors are generating a large amount of data every week. As mentioned in the question, there are 20,000 gas sensors that send data every 15 minutes. That translates to 1,920,000 records in a single day and with that large amount of data, there would be complexities in querying the DynamoDB database. In these scenarios, you can create new DynamoDB every week and then add a partition key and a sort key in the table.</p><p>When you create a table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key.</p><p><strong>Amazon DynamoDB</strong> supports two different kinds of primary keys:</p><p><strong>Partition key</strong> – A simple primary key, composed of one attribute known as the partition key. DynamoDB uses the partition key's value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored.</p><p><strong>Partition key and sort key</strong> – Referred to as a composite primary key, this type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key. DynamoDB uses the partition key value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored.</p><p>Take note that the partition key of an item is also known as its hash attribute. The term hash attribute derives from the use of an internal hash function in DynamoDB that evenly distributes data items across partitions, based on their partition key values.</p><p>Therefore, the correct answer is: <strong>Use one table every week, with a composite primary key which is the sensor ID as the partition key and the timestamp as the sort key.</strong></p><p>The option that says: <strong>Use one table with a primary key which is the sensor ID. Use the timestamp as the hash key</strong> is incorrect. If you only use one table, the table would be very enormous and the searching will take a very long time.</p><p>The option that says: <strong>Use one table every week, with a primary key which is the concatenated value of the sensor ID and the timestamp</strong> is incorrect. You need a fixed value for the primary key so you shouldn't concatenate the timestamp to the sensor ID.</p><p>The option that says: <strong>Use one table with a primary key which is the concatenated value of the sensor ID and the timestamp is</strong> incorrect. If you only use one table, the table would be very enormous and the search will take a very long time.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html",
      "https://tutorialsdojo.com/amazon-dynamodb/?src=udemy"
    ]
  },
  {
    "id": 61,
    "question": "<p>A company has released a new mobile game and its backend servers are hosted on the company’s on-premises data center. The game logic is exposed using REST APIs that have multiple functions depending on the user state. Access to the backend services is controlled with an API key, while any test traffic is distinguished by a different key. A central file server stores player session data. User traffic is variable throughout the day but the on-premises servers cannot handle traffic during peak hours. The game also has latency issues caused by the slow fetching of player session data. The management tasked the solutions architect to migrate this infrastructure to AWS in order to improve scalability and reduce the latency for data access while keeping the backend API model unchanged.</p><p>Which of the following is the recommended solution to meet the company requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a fleet of Amazon EC2 instances to host the backend services. Expose the REST APIs by placing the instances behind a Network Load Balancer (NLB). Use Amazon Aurora Serverless to store the player session data.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Lambda functions to run the backend game logic. Expose the REST APIs by using Amazon API Gateway. Use Amazon DynamoDB with auto-scaling to store the player session data.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Lambda functions to run the backend game logic. Expose the REST APIs by using AWS AppSync. Use Amazon Aurora Serverless to store the player session data.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Lambda functions to run the backend game logic. Expose the REST APIs by placing the Lambda functions behind an Application Load Balancer (ALB). Use Amazon DynamoDB with auto-scaling to store the player session data.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Lambda</strong> is a compute service that lets you run code without provisioning or managing servers. Lambda runs your code only when needed and scales automatically, from a few requests per day to thousands per second. You pay only for the compute time that you consume—there is no charge when your code is not running. With Lambda, you can run code for virtually any type of application or backend service, all with zero administration.</p><p>You can create a web API with an HTTP endpoint for your Lambda function by using <strong>Amazon API Gateway</strong>. API Gateway provides tools for creating and documenting web APIs that route HTTP requests to Lambda functions. You can secure access to your API with authentication and authorization controls. Your APIs can serve traffic over the Internet or can be accessible only within your VPC. Amazon API Gateway invokes your function synchronously with an event that contains a JSON representation of the HTTP request. For custom integration, the event is the body of the request. For a proxy integration, the event has a defined structure.</p><p>API Gateway allows you to throttle traffic and authorize API calls to ensure that backend operations withstand traffic spikes and backend systems are not unnecessarily called.</p><p><img src=\"https://media.tutorialsdojo.com/sap_lambda_api_function.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_lambda_api_function.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can use <strong>Amazon DynamoDB</strong> for storing player session data. Gaming companies use Amazon DynamoDB in all parts of game platforms, including game state, player data, session history, and leaderboards. The main benefits that these companies get from DynamoDB are its ability to scale reliably to millions of concurrent users and requests while ensuring consistently low latency—measured in single-digit milliseconds. In addition, as a fully managed service, DynamoDB has no operational overhead. Game developers can focus on developing their games instead of managing databases. Also, as game makers are looking to expand from a single AWS Region to multiple Regions, they can rely on DynamoDB global tables for multi-region, active-active data replication.</p><p>Therefore, the correct answer is: <strong>Use AWS Lambda functions to run the backend game logic. Expose the REST APIs by using Amazon API Gateway. Use Amazon DynamoDB with auto-scaling to store the player session data. </strong>API Gateway tightly integrates with AWS Lambda which enables you to quickly build REST compliant serverless applications. DynamoDB is well-suited for scalability to handle fast access to player session data.</p><p>The option that says: <strong>Create a fleet of Amazon EC2 instances to host the backend services. Expose the REST APIs by placing the instances behind a Network Load Balancer (NLB). Use Amazon Aurora Serverless to store the player session data</strong> is incorrect. Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora. It is recommended to use Aurora Serverless for lightly-used applications, with peaks of 30 minutes to several hours a few times each day or several times per year, such as human resources, budgeting, or operational reporting application.</p><p>The option that says: <strong>Use AWS Lambda functions to run the backend game logic. Expose the REST APIs by placing the Lambda functions behind an Application Load Balancer (ALB). Use Amazon DynamoDB with auto-scaling to store the player session data</strong> is incorrect. Although you can use an ALB with Lambda as the backend service, it is still recommended to use API Gateway to expose your APIs. You have a lot more options and flexibility on API Gateway including the ability to use API keys and add custom headers to each request.</p><p>The option that says: <strong>Use AWS Lambda functions to run the backend game logic. Expose the REST APIs by using AWS AppSync. Use Amazon Aurora Serverless to store the player session data </strong>is incorrect. AWS AppSync is recommended for applications that are written for GraphQL APIs, not REST APIs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/database/amazon-dynamodb-gaming-use-cases-and-design-patterns/\">https://aws.amazon.com/blogs/database/amazon-dynamodb-gaming-use-cases-and-design-patterns/</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/welcome.html\">https://docs.aws.amazon.com/lambda/latest/dg/welcome.html</a></p><p><br></p><p><strong>Check out these Amazon API Gateway and Amazon DynamoDB Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-api-gateway/?src=udemy\">https://tutorialsdojo.com/amazon-api-gateway/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/database/amazon-dynamodb-gaming-use-cases-and-design-patterns/",
      "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html",
      "https://tutorialsdojo.com/amazon-api-gateway/?src=udemy",
      "https://tutorialsdojo.com/amazon-dynamodb/?src=udemy"
    ]
  },
  {
    "id": 62,
    "question": "<p>A stock trading company is running its application on the AWS cloud. The mission-critical database is hosted on an Amazon RDS for MySQL instance deployed in a Multi-AZ configuration. An AWS Backup rule is in place to take automated snapshots hourly. The operations team recently performed an RDS database failover test and found that it caused an outage of approximately 40 seconds.</p><p>The management has asked the solutions architect to implement a solution that will reduce the outage to less than 20 seconds. Most connections should stay alive during failovers except for the ones that are in the middle of a transaction or SQL statement. New database connections should still be accepted, and the incoming write requests should be queued until the failover completes.</p><p>Which of the following options should the solutions architect implement to meet the company's requirements? (Choose THREE.)</p>",
    "corrects": [
      2,
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Ensure that Multi-AZ is enabled on Amazon RDS for MySQL and create one or more read replicas to ensure quick failover.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Migrate the Amazon RDS for MySQL cluster to an Amazon Aurora for MySQL cluster.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up an Amazon RDS Proxy in front of the database layer to automatically route traffic to healthy RDS instances.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Ensure that Multi-AZ is enabled on Amazon Aurora and create one or more Aurora Replicas.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Enable the Amazon RDS Optimized Writes feature and create an Amazon ElastiCache for Memcached cluster in front of the database layer to cache any frequently requested queries.</p>",
        "correct": false
      },
      {
        "id": 6,
        "answer": "<p>Enable the Amazon RDS Optimized Reads feature and launch an Amazon ElastiCache for Redis cluster in front of the database layer to temporarily serve database queries in case of RDS failure.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon RDS Proxy</strong> is a fully managed, highly available database proxy for Amazon Relational Database Service (RDS) that makes applications more scalable, more resilient to database failures, and more. Amazon RDS Proxy sits between your application and your relational database to efficiently manage connections to the database and improve the scalability of the application. Amazon RDS Proxy can be enabled for most applications with no code changes.</p><p>Your Amazon RDS Proxy instance maintains a pool of established connections to your RDS database instances, reducing the stress on database compute and memory resources that typically occurs when new connections are established. RDS Proxy also shares infrequently used database connections so that fewer connections access the RDS database.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_proxy_failover.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_rds_proxy_failover.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>With RDS Proxy, you can build applications that can transparently tolerate database failures without needing to write complex failure-handling code. The proxy automatically routes traffic to a new database instance while preserving application connections. It also bypasses Domain Name System (DNS) caches to reduce failover times by up to 66% for Aurora Multi-AZ databases</p><p>Connecting through a proxy makes your application more resilient to database failovers. When the original DB instance becomes unavailable, RDS Proxy connects to the standby database without dropping idle application connections. Doing so helps to speed up and simplify the failover process. The result is faster failover that's less disruptive to your application than a typical reboot or database problem.</p><p>Without RDS Proxy, a failover involves a brief outage. During the outage, you can't perform write operations on that database. Any existing database connections are disrupted, and your application must reopen them. The database becomes available for new connections and write operations when a read-only DB instance is promoted in place of one that's unavailable.</p><p><img src=\"https://media.tutorialsdojo.com/amazon-rds-proxy-DBS-C01.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/amazon-rds-proxy-DBS-C01.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>For applications that maintain their own connection pool, going through RDS Proxy means that most connections stay alive during failovers or other disruptions. Only connections that are in the middle of a transaction or SQL statement are canceled. RDS Proxy immediately accepts new connections. When the database writer is unavailable, RDS Proxy queues up incoming requests.</p><p>For applications that don't maintain their own connection pools, RDS Proxy offers faster connection rates and more open connections. It offloads the expensive overhead of frequent reconnects from the database. It does so by reusing database connections maintained in the RDS Proxy connection pool. This approach is particularly important for TLS connections, where setup costs are significant.</p><p><strong>Amazon Aurora</strong> is a modern relational database service offering performance and high availability at scale, fully open-source MySQL and PostgreSQL-compatible editions. Unlike other databases, after a database crash, Amazon Aurora does not need to replay the redo log from the last database checkpoint (typically five minutes) and confirm that all changes have been applied before making the database available for operations. Amazon Aurora moves the buffer cache out of the database process and makes it available immediately at restart time. This prevents you from having to throttle access until the cache is repopulated to avoid brownouts. This reduces database restart times to less than 60 seconds in most cases.</p><p>With properly configured Multi-AZ and Aurora replicas, Amazon Aurora can quickly recover from failures. Amazon Aurora does not need to replay the redo log from the last database checkpoint (typically five minutes) and confirm that all changes have been applied before making the database available for operations.</p><p>Thus, enabling RDS Proxy can further decrease the downtime of a failover to around 20 seconds (66% reduction from 60 seconds).</p><p>Therefore, the correct answers are:</p><p><strong>-Set up an Amazon RDS Proxy in front of the database layer to automatically route traffic to healthy RDS instances.</strong></p><p><strong>-Migrate the Amazon RDS for MySQL cluster to an Amazon Aurora for MySQL cluster.</strong></p><p><strong>-Ensure that Multi-AZ is enabled on Amazon Aurora and create one or more Aurora Replicas.</strong></p><p>The option that says: <strong>Enable the Amazon RDS Optimized Writes feature and create an Amazon ElastiCache for Memcached cluster in front of the database layer to cache any frequently requested queries </strong>is incorrect. Memcached is used for database caching only and not as a database proxy. Although it can respond to frequently queried data, it can't queue any write request to the database while the failover process is in place. The Amazon RDS Optimized Writes feature is not useful in this scenario as it only provides you with up to 2x improvement in write transaction throughput on RDS for MySQL by writing only once while protecting you from data loss.</p><p>The option that says: <strong>Enable the Amazon RDS Optimized Reads feature and launch an Amazon ElastiCache for Redis cluster in front of the database layer to temporarily serve database queries in case of RDS failure </strong>is incorrect. Although Redis can store data to some extent similar to a database, it is still no substitute for a real database storage system. Moreover, the Amazon RDS Optimized Reads feature is commonly used to achieve faster query processing by placing temporary tables generated by MySQL on NVMe-based SSD block storage that is physically connected to the host server. This feature cannot make most DB connections stay alive during failovers.</p><p>The option that says: <strong>Ensure that Multi-AZ is enabled on Amazon RDS for MySQL and create one or more read replicas to ensure quick failover</strong> is incorrect. This may be possible since the database is currently on RDS with Multi-AZ. However, RDS automatic database failover can take up to 60 seconds for one standby instance and up to <a href=\"https://aws.amazon.com/rds/features/multi-az\">35 seconds</a> for two readable standby instances. A better option is to use RDS Proxy to fulfill the aforementioned requirements.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html</a></p><p><a href=\"https://aws.amazon.com/rds/aurora/faqs/\">https://aws.amazon.com/rds/aurora/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html#Concepts.AuroraHighAvailability.Proxy\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html#Concepts.AuroraHighAvailability.Proxy</a></p><p><br></p><p><strong>Check out these Amazon Aurora and Amazon RDS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora/?src=udemy\">https://tutorialsdojo.com/amazon-aurora/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p><p><br></p><p><strong>Check out this Amazon Aurora vs Amazon RDS comparison:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora-vs-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-aurora-vs-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/rds/features/multi-az",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html",
      "https://aws.amazon.com/rds/aurora/faqs/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html#Concepts.AuroraHighAvailability.Proxy",
      "https://tutorialsdojo.com/amazon-aurora/?src=udemy",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy",
      "https://tutorialsdojo.com/amazon-aurora-vs-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 63,
    "question": "<p>A data analytics company has recently adopted a hybrid cloud infrastructure with AWS. They are in the business of collecting and processing vast amounts of data. Each data set generates up to several thousands of files which can range from 10 MB to 1 GB in size. The archived data is rarely restored and in case there is a request to retrieve it, the company has a maximum of 24 hours to send the files. The data sets can be searched using its file ID, set name, authors, tags, and other criteria. </p><p>Which of the following options provides the most cost-effective architecture to meet the above requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>1. For each completed data set, compress and concatenate all of the files into a single Glacier archive. </p><p>2. Store the associated archive ID for the compressed files along with other search metadata in a DynamoDB table. </p><p>3. For retrieving the data, query the DynamoDB table for files that match the search criteria and then restore the files from the retrieved archive ID.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>1. Store the files of the completed data sets into a single S3 bucket.</p><p>2. Store the S3 object key for the compressed files along with other search metadata in a DynamoDB table.</p><p>3. For retrieving the data, query the DynamoDB table for files that match the search criteria and then restore the files from the S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>1. Store individual files in Glacier using the filename as the archive name.</p><p>2. For retrieving the data, query the Glacier vault for files matching the search criteria.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>1. Store individual compressed files to an S3 bucket. Also store the search metadata and the S3 object key of the files in a separate S3 bucket.</p><p>2. Create a lifecycle rule to move the data from an S3 Standard class to Glacier after a certain a month.</p><p>3. For retrieving the data, query the S3 bucket for files matching the search criteria and then retrieve the file from the other S3 bucket. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>You can further lower the cost of storing data by compressing it to a zip or tar file. In addition, searching for archives in Glacier takes a long time, which is why it is advisable to store the search criteria and archive ID in a database for faster search. You can alternatively use Glacier Select to perform filtering operations using simple Structured Query Language (SQL).</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_s3_glacier.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_s3_glacier.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In this scenario, this option provides a more cost-effective option for the given architecture:</p><p><strong>1. For each completed data set, compress and concatenate all of the files into a single Glacier archive.</strong></p><p><strong>2. Store the associated archive ID for the compressed files along with other search metadata in a DynamoDB table.</strong></p><p><strong>3. For retrieving the data, query the DynamoDB table for files that match the search criteria and then restore the files from the retrieved archive ID.</strong></p><p><br></p><p>The following option is incorrect because storing the data in an S3 Standard class is costly. It is more cost-effective to use Amazon Glacier instead.</p><p><strong>1. Store the files of the completed data sets into a single S3 bucket. </strong><br><strong> 2. Store the S3 object key for the compressed files along with other search metadata in a DynamoDB table. </strong><br><strong> 3. For retrieving the data, query the DynamoDB table for files that match the search criteria and then restore the files from the S3 bucket.</strong></p><p><br></p><p>The following option is incorrect because initially storing the archive to an S3 Standard class for a month still entails additional cost. Since the company allows a maximum of 24 hours to retrieve the files, you can directly store the archives in Glacier instead:</p><p><strong>1. Store individual compressed files to an S3 bucket. Also store the search metadata and the S3 object key of the files in a separate S3 bucket. </strong><br><strong> 2. Create a lifecycle rule to move the data from an S3 Standard class to Glacier after a certain a month. </strong><br><strong> 3. For retrieving the data, query the S3 bucket for files matching the search criteria and then retrieve the file from the other S3 bucket.</strong></p><p><br></p><p>The following option is incorrect because Glacier doesn't have a built-in search function to help you retrieve the data. You have to store the archive ID in a database, such as DynamoDB, to help you effectively search the required data:</p><p><strong>1. Store individual files in Glacier using the filename as the archive name. </strong><br><strong>2. For retrieving the data, query the Glacier vault for files matching the search criteria.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Glacier Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-glacier/?src=udemy\">https://tutorialsdojo.com/amazon-glacier/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html",
      "https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html",
      "https://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html",
      "https://tutorialsdojo.com/amazon-glacier/?src=udemy",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 64,
    "question": "<p>A company recently switched to using Amazon CloudFront for its content delivery network. The development team already made the preparations necessary to optimize the application performance for global users. The company’s content management system (CMS) serves both dynamic and static content. The dynamic content is served from a fleet of Amazon EC2 instances behind an application load balancer (ALB) while the static assets are served from an Amazon S3 bucket. The ALB is configured as the default origin of the CloudFront distribution. An Origin Access Control (OAC) was created and applied to the S3 bucket policy to allow access only from the CloudFront distribution. Upon testing the CMS webpage, the static assets return an error 404 message.</p><p>Which of the following solutions must be implemented to solve this error? (Select TWO.)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Update the CloudFront distribution and create a new behavior that will forward to the origin of the static assets based on path pattern.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Update the application load balancer listener to check for HEADER condition if the request is from CloudFront and forward it to the Amazon S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Edit the CloudFront distribution and create another origin for serving the static assets.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Replace the CloudFront distribution with AWS Global Accelerator. Configure the AWS Global Accelerator with multiple endpoint groups that target endpoints on all AWS Regions. Use the accelerator’s static IP address to create a record in Amazon Route 53 for the apex domain.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Update the application load balancer listener and create a new path-based rule for the static assets so that it will forward requests to the Amazon S3 bucket.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon CloudFront</strong> is a web service that speeds up the distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay) so that content is delivered with the best possible performance.</p><p>You create a CloudFront distribution to tell CloudFront where you want the content to be delivered from and the details about how to track and manage content delivery. Then CloudFront uses computers—edge servers—that are close to your viewers to deliver that content quickly when someone wants to see it or use it.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_s3_http_steps.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_s3_http_steps.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In general, if you’re using an Amazon S3 bucket as the origin for a CloudFront distribution, you can either allow everyone to have access to the files there or you can restrict access. To restrict access to content that you serve from Amazon S3 buckets, follow these steps:</p><ol><li><p>Create a special CloudFront user called an <strong>origin access control (OAC)</strong> and associate it with your distribution.</p></li><li><p>Configure your S3 bucket permissions so that CloudFront can use the OAC to access the files in your bucket and serve them to your users. Make sure that users can’t use a direct URL to the S3 bucket to access a file there.</p></li></ol><p>After you take these steps, users can only access your files through CloudFront, not directly from the S3 bucket.</p><p>You can configure a <strong>single CloudFront web distribution</strong> to serve different types of requests from multiple origins. For example, if you are building a website that serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from a load balancer, you can serve both types of content from a CloudFront web distribution.</p><p>Follow these steps to configure a CloudFront web distribution to serve static content from an S3 bucket and dynamic content from a load balancer:</p><ol><li><p>Open your web distribution from the CloudFront console.</p></li><li><p>Choose the <strong>Origins</strong> tab.</p></li><li><p>Create <strong>one origin</strong> for your S3 bucket and <strong>another origin</strong> for your load balancer. <br><strong>Note:</strong> If you're using a custom origin server or an S3 website endpoint, you must enter the origin's domain name into the <strong>Origin Domain Name</strong> field.</p></li><li><p>From your distribution, choose the <strong>Behaviors</strong> tab.</p></li><li><p>Create a behavior that specifies a path pattern to route all static content requests to the S3 bucket. For example, you can set the \"images/*.jpg\" path pattern to route all requests for \".jpg\" files in the images directory to the S3 bucket.</p></li><li><p>Edit the <strong>Default (*) </strong>path pattern behavior and set its <strong>Origin</strong> as your load balancer.</p></li></ol><p>Therefore, the correct answers are:</p><p><strong>- Edit the CloudFront distribution and create another origin for serving the static assets.</strong></p><p><strong>- Update the CloudFront distribution and create a new behavior that will forward to the origin of the static assets based on path pattern.</strong></p><p>The option that says: <strong>Update the application load balancer listener and create a new path-based rule for the static assets so that it will forward requests to the Amazon S3 bucket</strong> is incorrect. The OAC is used to allow only CloudFront to access the static assets on the S3 bucket. Using the load balancer to forward the requests will result in the access denied error.</p><p>The option that says: <strong>Replace the CloudFront distribution with AWS Global Accelerator. Configure the AWS Global Accelerator with multiple endpoint groups that target endpoints on all AWS Regions. Use the accelerator’s static IP address to create a record in Amazon Route 53 for the apex domain </strong>is incorrect. AWS Global Accelerator is a network layer service that improves the availability and performance of your applications used by a wide global audience. This service is not a substitute for a CDN service like Amazon CloudFront.</p><p>The option that says: <strong>Update the application load balancer listener to check for HEADER condition if the request is from CloudFront and forward it to the Amazon S3 bucket</strong> is incorrect. Although an ALB can inspect the HTTP Header requests, using the load balancer to forward the requests will result in the access denied error because of the OAC on the S3 bucket policy.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html\">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html",
      "https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy"
    ]
  },
  {
    "id": 65,
    "question": "<p>A computer hardware manufacturer has a supply chain application that is written in NodeJS. The application is deployed on an Amazon EC2 Reserved instance which has been provisioned with an IAM Role that provides access to data files stored in an S3 bucket.</p><p>In this architecture, which of the following IAM policies control access to the data files in S3? (Select TWO.)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>An IAM trust policy that allows the Amazon S3 service to assume an EC2 instance role.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A bucket policy that allows the EC2 role to list objects in the S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>An IAM trust policy that allows the NodeJS supply chain application running on the EC2 instance to access the data files stored in the S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>An IAM trust policy that allows the Amazon EC2 service to assume an EC2 instance role.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>An IAM permissions policy that allows the EC2 role to access S3 objects.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>An <strong>IAM role</strong> is an IAM identity that you can create in your account that has specific permissions. An IAM role has some similarities to an IAM user. Roles and users are both AWS identities with permissions policies that determine what the identity can and cannot do in AWS.</p><p><strong>Applications that run on an Amazon EC2 instance</strong> must include AWS credentials in the AWS API requests. You can and should use an<strong> IAM role</strong> to manage temporary credentials for applications that run on an Amazon EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as sign-in credentials or access keys) to an Amazon EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an Amazon EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ec2_iam_role_s3_bucket.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ec2_iam_role_s3_bucket.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>IAM trust policies</strong> for IAM roles specify which entities, like users, roles, or services, can assume a role and request temporary credentials, whether across multiple accounts or within a single one. They're crucial for authorizing access to resources. Even within a single account, trust policies are necessary to define eligible principals, such as users, federated users, or AWS services. For instance, in an EC2 instance scenario, the trust policy ensures only EC2 instances can assume the role, enhancing security and access control.</p><p><strong>IAM permissions policies</strong>, on the other hand, define the specific actions and resources that are allowed or denied for IAM entities (such as roles or users). These policies control what actions can be performed on which AWS resources. For instance, an IAM permissions policy attached to an EC2 role might grant permissions to read from or write to specific S3 buckets.</p><p>Hence, the correct answers are:</p><p><strong>- An IAM trust policy that allows the Amazon EC2 service to assume an EC2 instance role.</strong></p><p><strong>- An IAM permissions policy that allows the EC2 role to access S3 objects.</strong></p><p>The option that says: <strong>An IAM trust policy that allows the NodeJS supply chain application running on the EC2 instance to access the data files stored in the S3 bucket</strong> is incorrect. IAM trust policies define which principals can assume a role; they do not directly grant permissions to access resources like S3 buckets. Thus, a trust policy wouldn't be used to allow application access to S3 data.</p><p>The option that says: <strong>A bucket policy that allows the EC2 role to list objects in the S3 bucket</strong> is incorrect because the question states the EC2 role needs access (i.e. download/read) the data files in S3. Therefore, a policy that only grants the role the ability to list objects does not fully meet the requirements stated in the question.</p><p>The option that says: <strong>An IAM trust policy that allows the Amazon S3 service to assume an EC2 instance role</strong> is incorrect. To enable an EC2 instance assume a role, Amazon EC2 must be specified as the principal. S3, as a service, does not assume roles to perform operations on behalf of EC2 instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#term_trust-policy\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#term_trust-policy</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#term_trust-policy",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy"
    ]
  },
  {
    "id": 66,
    "question": "<p>A company has several applications written in TypeScript and Python hosted on the AWS cloud. The company uses an automated deployment solution for its applications using AWS CloudFormation templates and AWS CodePipeline. The company recently acquired a new business unit that uses Python scripts to deploy applications on AWS. The developers from the new business are having difficulty migrating their deployments to AWS CloudFormation because they need to learn a new domain-specific language and their old Python scripts require programming loops, which are not supported in CloudFormation.</p><p>Which of the following is the recommended solution to address the developers’ concerns and help them update their deployment procedures?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Ask the developers to continue using Python scripts for deploying new resources. Once the resources are created, import them into a new CloudFormation stack.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Write TypeScript or Python code that will define AWS resources. Convert these codes to AWS CloudFormation templates by using AWS Cloud Development Kit (AWS CDK). Create CloudFormation stacks using AWS CDK. Create an AWS CodeBuild job that includes AWS CDK and add this stage to AWS CodePipeline.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a standard deployment process for the company and the new business unit by leveraging a third-party resource provisioning engine on AWS CodeBuild. Add a stage on AWS CodePipeline to integrate AWS CodeBuild on the application deployment.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Write new CloudFormation templates for the deployments of the new business unit. Extract parts of the Python scripts to be added as EC2 user data. Deploy the CloudFormation templates using the AWS Cloud Development Kit (AWS CDK). Add a stage on AWS CodePipeline to integrate AWS CDK using the templates for the application deployment.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS Cloud Development Kit (AWS CDK)</strong> is a software development framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation.</p><p><strong>AWS CloudFormation</strong> enables you to:</p><p>- Create and provision AWS infrastructure deployments predictably and repeatedly.</p><p>- Leverage AWS products such as Amazon EC2, Amazon Elastic Block Store, Amazon SNS, Elastic Load Balancing, and Auto Scaling.</p><p>- Build highly reliable, highly scalable, cost-effective applications in the cloud without worrying about creating and configuring the underlying AWS infrastructure.</p><p>- Use a template file to create and delete a collection of resources together as a single unit (a stack).</p><p>Use the AWS CDK to define your cloud resources in a familiar programming language. The AWS CDK supports TypeScript, JavaScript, Python, Java, and C#/.Net. Developers can use one of the supported programming languages to define reusable cloud components known as Constructs.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_cdk_appstacks.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_cdk_appstacks.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The <strong>AWS Cloud Development Kit (AWS CDK)</strong> lets you define your cloud infrastructure as code in one of five supported programming languages. It is intended for moderately to highly experienced AWS users. An AWS CDK app is an application written in TypeScript, JavaScript, Python, Java, or C# that uses the AWS CDK to define AWS infrastructure. An app defines one or more stacks. Stacks (equivalent to AWS CloudFormation stacks) contain constructs, each of which defines one or more concrete AWS resources, such as Amazon S3 buckets, Lambda functions, Amazon DynamoDB tables, and so on.</p><p>Constructs (as well as stacks and apps) are represented as types in your programming language of choice. You instantiate constructs within a stack to declare them to AWS and connect them to each other using well-defined interfaces.</p><p>The AWS CDK lets you easily define applications in the AWS Cloud using your programming language of choice. To deploy your applications, you can use <strong>AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline</strong>. Together, they allow you to build what's called a deployment pipeline for your application.</p><p>Therefore, the correct answer is: <strong>Write TypeScript or Python code that will define AWS resources. Convert these codes to AWS CloudFormation templates by using AWS Cloud Development Kit (AWS CDK). Create CloudFormation stacks using AWS CDK. Create an AWS CodeBuild job that includes AWS CDK and add this stage to AWS CodePipeline.</strong> With this solution, the developers no longer need to learn the AWS CloudFormation specific language as they can continue writing TypeScript or Python scripts. The AWS CDK stacks can be converted to AWS CloudFormation templates which can be integrated into the company deployment process.</p><p>The option that says: <strong>Write new CloudFormation templates for the deployments of the new business unit. Extract parts of the Python scripts to be added as EC2 user data. Deploy the CloudFormation templates using the AWS Cloud Development Kit (AWS CDK). Add a stage on AWS CodePipeline to integrate AWS CDK using the templates for the application deployment</strong> is incorrect. This is possible but you don't have to write new CloudFormation templates. AWS CDK can convert the TypeScript and Python code to create AWS CDK stacks and convert them to CloudFormation templates.</p><p>The option that says: <strong>Create a standard deployment process for the company and the new business unit by leveraging a third-party resource provisioning engine on AWS CodeBuild. Add a stage on AWS CodePipeline to integrate AWS CodeBuild on the application deployment</strong> is incorrect. You don't have to rely on third-party resources to standardize the deployment process. AWS CDK already allows you to write CloudFormation templates in TypeScript or Python.</p><p>The option that says: <strong>Ask the developers to continue using Python scripts for deploying new resources. Once the resources are created, import them into a new CloudFormation stack</strong> is incorrect. This approach requires a lot of work to make sure resources fit the CloudFormation template and could introduce errors and mismatches between manually made resources and their CloudFormation versions. You typically import existing resources into a CloudFormation stack to bring already-deployed AWS resources under CloudFormation's management. Deliberately creating resources outside of CloudFormation only to import them later is inefficient and counterintuitive.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cdk/latest/guide/codepipeline_example.html\">https://docs.aws.amazon.com/cdk/latest/guide/codepipeline_example.html</a></p><p><a href=\"https://docs.aws.amazon.com/cdk/latest/guide/home.html\">https://docs.aws.amazon.com/cdk/latest/guide/home.html</a></p><p><a href=\"https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html\">https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html</a></p><p><br></p><p><strong>Check out these AWS CloudFormation and AWS CodePipeline Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/cdk/latest/guide/codepipeline_example.html",
      "https://docs.aws.amazon.com/cdk/latest/guide/home.html",
      "https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html",
      "https://tutorialsdojo.com/aws-cloudformation/?src=udemy",
      "https://tutorialsdojo.com/aws-codepipeline/?src=udemy"
    ]
  },
  {
    "id": 67,
    "question": "<p>A multinational consumer goods company is currently using a VMWare vCenter Server to manage their virtual machines, multiple ESXi hosts, and all dependent components from a single centralized location. To save costs and to avail the benefits of cloud computing, the company decided to move its virtual machines to AWS. The Solutions Architect is required to generate new AMIs of the existing virtual machines which can then be launched as an EC2 instance in the company VPC.</p><p>Which combination of steps should the Solutions Architect do to properly execute the cloud migration? (Select TWO.)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AWS Application Migration Service to migrate your on-premises workloads to the AWS cloud.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create an AWS CloudFormation template that mirrors the on-premises virtualized environment. Deploy the stack to the AWS cloud.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Establish a Direct Connect connection between your data center and your VPC. Use AWS Service Catalog to centrally manage all your IT services and to quickly migrate virtual machines to your virtual private cloud.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Serverless Application Model (SAM) to migrate the virtual machines (VMs) to AWS and automatically launch an Amazon ECS Cluster to host the VMs.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Install the AWS Replication Agent in your on-premises virtualization environment.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p><strong>AWS Application Migration Service (MGN)</strong> is a highly automated lift-and-shift (rehost) solution that simplifies, expedites, and reduces the cost of migrating applications to AWS. It enables companies to lift and shift a large number of physical, virtual, or cloud servers without compatibility issues, performance disruption, or long cutover windows.</p><p>MGN replicates source servers into your AWS account. When you’re ready, it automatically converts and launches your servers on AWS so you can quickly benefit from the cost savings, productivity, resilience, and agility of the Cloud.</p><p>Once your applications are running on AWS, you can leverage AWS services and capabilities to quickly and easily re-platform or refactor those applications – which makes lift-and-shift a fast route to modernization.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_mgn_overview.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_mgn_overview.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answers are:</p><p><strong>- Use the AWS Application Migration Service to migrate your on-premises workloads to the AWS cloud.</strong></p><p><strong>- Install the AWS Replication Agent in your on-premises virtualization environment.</strong></p><p>The option that says: <strong>Use Serverless Application Model (SAM) to migrate the virtual machine (VMs) to AWS and</strong> <strong>automatically launch an Amazon ECS Cluster to host the VMs</strong> is incorrect because the Serverless Application Model (SAM) service is primarily used to build serverless applications on AWS, and not to migrate virtual machines from your on-premises data center.</p><p>The option that says:<strong> Create an AWS CloudFormation template that mirrors the on-premises virtualized environment. Deploy the stack to the AWS cloud </strong>is incorrect. This is possible; however, this will create new AMIs that are from AWS, not from the existing VMs. CloudFormation is ideal for creating and deploying new environments in the AWS Cloud.</p><p>The option that says: <strong>Establish a Direct Connect connection between your data center and your VPC. Use AWS Service Catalog to centrally manage all your IT services and to quickly migrate virtual machines to your virtual private cloud</strong> is incorrect. The AWS Service Catalog is primarily used to allow organizations to create and manage catalogs of IT services that are approved for use on AWS. It enables you to centrally manage commonly deployed IT services and helps you achieve consistent governance and meet your compliance requirements while enabling users to quickly deploy only the approved IT services they need. However, this service is not suitable for migrating your virtual machines.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/application-migration-service/\">https://aws.amazon.com/application-migration-service/</a></p><p><a href=\"https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html\">https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html</a></p><p><a href=\"https://docs.aws.amazon.com/mgn/latest/ug/quick-start-guide-gs.html\">https://docs.aws.amazon.com/mgn/latest/ug/quick-start-guide-gs.html</a></p><p><br></p><p><strong>Check out this AWS Application Migration Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-application-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-application-migration-service/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/application-migration-service/",
      "https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html",
      "https://docs.aws.amazon.com/mgn/latest/ug/quick-start-guide-gs.html",
      "https://tutorialsdojo.com/aws-application-migration-service/?src=udemy"
    ]
  },
  {
    "id": 68,
    "question": "<p>A technology company is developing an educational mobile app for students, with an exam feature that also allows them to submit their answers. The developers used React Native so the app can be deployed on both iOS and Android devices. They used AWS Lambda and Amazon API Gateway for the backend services and a DynamoDB table as the database service. After a month, the released app has been downloaded over 3 million times. However, there are a lot of users who complain about the slow processing of the app especially when they are submitting their answers in the multiple-choice exams. The diagrams and images on the exam also take a lot of time to load, which is not a good user experience.</p><p>Which of the following options provides the most cost-effective and scalable architecture for the application?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Increase the write capacity in DynamoDB to 10,000 WCU. Use a web distribution in CloudFront with associated CloudFront Functions to host the diagrams, images and other static assets of the mobile app in real-time</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Instead of DynamoDB, use RDS Multi-AZ configuration with Read Replicas. Use a web distribution in CloudFront and Amazon S3 to host the diagrams, images, and other static assets of the mobile app.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Enable Auto Scaling in DynamoDB with a Target Utilization of 100% and a maximum provisioned capacity of 1000 units. Use an S3 bucket to host the diagrams, images, and other static assets of the mobile app.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Launch an SQS queue and develop a custom service which integrates with SQS to buffer the incoming requests. Use a web distribution in CloudFront and Amazon S3 to host the diagrams, images, and other static assets of the mobile app.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In this scenario, the mobile app is both write and read-intensive. You can use SQS to buffer and scale the backend service to accommodate the large incoming traffic. CloudFront is the best choice to distribute the static assets of the mobile app to load it faster.</p><p><img src=\"https://media.tutorialsdojo.com/cloudfront_cdn_diagram.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/cloudfront_cdn_diagram.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>SQS can help with the slowness experienced when submitting answers. You can modify your application to use SQS. For example, when the users submit their answers, instead of waiting for the complete submission and processing of all the answers you can save the submitted answers on the SQS Queue and then quickly return the user to a new page stating that the answers are successfully submitted. This improves the user experience as the user will not wait on a \"loading submit\" button until all the answers are processed completely. When the answers are buffered in the queue, the backend instances can now process the answers in the queue. You can then have another webpage that will show the results separately. This is a simple cost-effective design that improves user experience without increasing the capacity of your systems.</p><p>With CloudFront, the images on the S3 bucket will be cached on Edge locations close to users. This further improves the load times of the web page. Images are quickly accessed and loaded because they are physically closer (using CloudFront cache) to users around the world.</p><p>Therefor the correct answer is: <strong>Launch an SQS queue and develop a custom service which integrates with SQS to buffer the incoming requests. Use a web distribution in CloudFront and Amazon S3 to host the diagrams, images, and other static assets of the mobile app.</strong></p><p>The option that says: <strong>Enable Auto Scaling in DynamoDB with a Target Utilization of 100% and a maximum provisioned capacity of 1000 units. Use an S3 bucket to host the diagrams, images, and other static assets of the mobile app</strong> is incorrect because you can only set the auto scaling target utilization values between 20 and 90 percent for your read and write capacity, not 100%.</p><p>The option that says:<strong> Increase the write capacity in DynamoDB to 10,000 WCU. Use a web distribution in CloudFront with associated CloudFront Functions to host the diagrams, images and other static assets of the mobile app in real-time</strong> is incorrect. Although using a CloudFront Function can reduce the latency of the processing, there are certain limitations on what it can do since this is just a JavaScript-based based feature that is commonly used to manipulate the requests and responses that flow through CloudFront, perform basic authentication and authorization, generate HTTP responses at the edge, and more. Increasing the write capacity (WCU) for the DynamoDB table may work but using a 10,000 WCU entails a significantly high cost.</p><p>The options that says: <strong>Instead of DynamoDB, use RDS Multi-AZ configuration with Read Replicas. Use a web distribution in CloudFront and Amazon S3 to host the diagrams, images, and other static assets of the mobile app</strong> is incorrect because this will require changing the whole application to use an SQL database and moving from DynamoDB to RDS Multi-AZ is not cost-effective.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-auto-scaling-for-amazon-dynamodb/\">https://aws.amazon.com/blogs/aws/new-auto-scaling-for-amazon-dynamodb/</a></p><p><br></p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/blogs/aws/new-auto-scaling-for-amazon-dynamodb/",
      "https://tutorialsdojo.com/amazon-sqs/?src=udemy"
    ]
  },
  {
    "id": 69,
    "question": "<p>A company runs its travel and tours website on AWS. The application only supports HTTP at the moment. To improve their SEO ranking and provide more security for their customers, they decided to enable SSL on their website. The company would also like to ensure the separation of roles between the Development team and the Security team in handling the sensitive SSL certificate. The Development team can log in to EC2 Instances but they should not have access to the SSL certificate, which only the Security team has exclusive control of. Currently, they are using an Application Load Balancer which provides loads of incoming traffic to an Auto Scaling group of On-Demand EC2 instances.</p><p>Which of the following options should the solutions architect implement to satisfy the above requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a new private S3 bucket and then upload the SSL certificate owned by the Security team. Configure the EC2 instance to have exclusive access to the certificate and block any access from the Development team.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Retrieve a read-only copy of the SSL certificate upon the boot of the EC2 instance from a CloudHSM, which is exclusively managed by the Security team.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Store the SSL certificate in IAM and authorize access only to the Security team using an IAM policy. Configure the Application Load Balancer to use the SSL certificate instead of the EC2 instances.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>In the web server, set the file owner of the SSL certificate to the Security team and set the file permissions to <code>700</code> which will deny all access to the Development team.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In the ALB, you can create a listener that uses encrypted connections (also known as SSL offload). This feature enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions. To use an HTTPS listener, you must deploy an SSL/TLS server certificate on your load balancer. The load balancer uses this certificate to terminate the connection and then decrypt requests from clients before sending them to the targets.</p><p>Although you can terminate the SSL in the EC2 instance, this setup is not recommended in this scenario. It is best to store the SSL certificate in IAM or in AWS Certificate Manager (ACM) where you can control which teams, either the Security or the Development team, can have access.</p><p><img src=\"https://media.tutorialsdojo.com/sap_alb_ssl_cert.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_alb_ssl_cert.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore the correct answer is: <strong>Store the SSL certificate in IAM and authorize access only to the Security team using an IAM policy. Configure the Application Load Balancer to use the SSL certificate instead of the EC2 instances.</strong></p><p>The option that says: <strong>In the web server, set the file owner of the SSL certificate to the Security team and set the file permissions to </strong><code><strong>700</strong></code><strong> which will deny all access to the Development team</strong> is incorrect. You don't have to store the SSL Cert on the EC2 instances. The SSL certificate should be at the Load Balancer since the EC2 instances are behind it and the application only supports HTTP.</p><p>The option that says: <strong>Retrieve a read-only copy of the SSL certificate upon the boot of the EC2 instance from a CloudHSM, which is exclusively managed by the Security team</strong> is incorrect. The clients access the website through the Load Balancer and not the EC2 instances directly. Using CloudHSM to store your SSL certificate will just increase your costs without satisfying the requirement.</p><p>The option that says: <strong>Create a new private S3 bucket and then upload the SSL certificate owned by the Security team. Configure the EC2 instance to have exclusive access to the certificate and block any access from the Development team</strong> is incorrect. The Developers can log in to the EC2, therefore, they will still be able to access the SSL certificate in the EC2 instances after they were downloaded from Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elb-ssl-tls-certificate-https/\">https://aws.amazon.com/premiumsupport/knowledge-center/elb-ssl-tls-certificate-https/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html#https-listener-certificates\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html#https-listener-certificates</a></p><p><br></p><p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/elb-ssl-tls-certificate-https/",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html#https-listener-certificates",
      "https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy"
    ]
  },
  {
    "id": 70,
    "question": "<p>A company hosts a serverless application on AWS using Amazon API Gateway and AWS Lambda with Amazon DynamoDB as the backend database. The application has a feature that allows users to create posts and reply to comments based on different topics. The API model currently uses the following methods:</p><p>- <code>GET /posts/[postid]</code> – used to get details about the post</p><p>- <code>GET /users/[userid]</code> – used to get details about a user</p><p>- <code>GET /comments/[commentid]</code> – used to get details of a comment</p><p>The application does not use API keys for request authorization. To increase user engagement on the web app, the company wants to reduce comment latency by making the comments appear in real-time.</p><p>Which of the following solution should be implemented to meet the requirements and improve user experience?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage AWS AppSync by building GraphQL APIs and using Websockets to deliver comments in real-time.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Lower the API response time of the Lambda functions by increasing the concurrency limit. This allows functions to run in parallel to deliver the comments in real-time.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Update the application code to call the <code>GET /comments/[commentid]</code> API every 3 seconds to show comments in real-time without sacrificing performance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a distribution on Amazon CloudFront and use edge-optimized APIs. Cache API responses in CloudFront to improve comment latency.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS AppSync</strong> is a fully managed service that makes it easy to develop GraphQL APIs by handling the heavy lifting of securely connecting to data sources like Amazon DynamoDB, Lambda, and more. Adding caches to improve performance, subscriptions to support real-time updates, and client-side data stores that keep offline clients in sync are just as easy. Once deployed, AWS AppSync automatically scales your GraphQL API execution engine up and down to meet API request volumes.</p><p>With managed GraphQL subscriptions, AWS AppSync can push real-time data updates over Websockets to millions of clients. For mobile and web applications, AppSync also provides local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online.</p><p>AppSync supports real-time chat applications. You can build conversational mobile or web applications that support multiple private chat rooms, offer access to conversation history, and queue outbound messages, even when a device is offline.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_appsync_overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_appsync_overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>AppSync can also be used for real-time collaboration. You can broadcast data from the backend to all connected clients (one-to-many) or between clients (many-to-many), such as in a second screen scenario where you broadcast the same data to all clients, who can then reply.</p><p>Therefore, the correct answer is: <strong>Leverage AWS AppSync by building GraphQL APIs and using Websockets to deliver comments in real-time.</strong> AWS AppSync can push real-time data updates over Websockets. This can automatically scale to millions of client requests.</p><p>The option that says: <strong>Create a distribution on Amazon CloudFront and use edge-optimized APIs. Cache API responses in CloudFront to improve comment latency</strong> is incorrect. Caching API responses can improve the loading of comments, however, this may not show the comments in real-time if the new comments are not yet on the cache.</p><p>The option that says: <strong>Update the application code to call the </strong><code><strong>GET /comments/[commentid]</strong></code><strong> API every 3 seconds to show comments in real-time without sacrificing performance</strong> is incorrect. This is possible but will heavily put a burden on your Lambda function executions. Even if there are no new comments, the API will still be called. This will add unnecessary cost to function executions.</p><p>The option that says: <strong>Lower the API response time of the Lambda functions by increasing the concurrency limit. This allows functions to run in parallel to deliver the comments in real-time</strong> is incorrect. Increasing the concurrency limit will not help in showing the comments in real-time. If you don't reach the Lambda concurrency limit regularly, there is no apparent advantage for this option.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html\">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html</a></p><p><a href=\"https://docs.aws.amazon.com/appsync/latest/devguide/real-time-websocket-client.html\">https://docs.aws.amazon.com/appsync/latest/devguide/real-time-websocket-client.html</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2020/04/aws-appsync-enables-support-for-generic-websockets-clients-graphql-real-time-subscriptions/\">https://aws.amazon.com/about-aws/whats-new/2020/04/aws-appsync-enables-support-for-generic-websockets-clients-graphql-real-time-subscriptions/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html",
      "https://docs.aws.amazon.com/appsync/latest/devguide/real-time-websocket-client.html",
      "https://aws.amazon.com/about-aws/whats-new/2020/04/aws-appsync-enables-support-for-generic-websockets-clients-graphql-real-time-subscriptions/",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 71,
    "question": "<p>A company wants to release a weather forecasting app for mobile users. The application servers generate a weather forecast every 15 minutes, and each forecast update overwrites the older forecast data. Each weather forecast outputs approximately 1 billion unique data points, where each point is about 20 bytes in size. This results in about 20GB of data for each forecast. Approximately 1,500 global users access the forecast data concurrently every second, and this traffic can spike up to 10 times more during weather events. The company wants users to have a good experience when using the weather forecast application so it requires that each user query must be processed in less than two seconds.</p><p>Which of the following solutions will meet the required application request rate and response time?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon OpenSearch cluster to store the weather forecast data points. Write AWS Lambda functions to query the ES cluster. Create an Amazon CloudFront distribution and point the origin to an Amazon API Gateway endpoint that invokes the Lambda functions. Configure a cache-control timeout of 15 minutes in the API caching section of the API Gateway stage.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon OpenSearch cluster to store the weather forecast data points. Write AWS Lambda functions to query the ES cluster. Create an Amazon CloudFront distribution and point the origin to an Amazon API Gateway endpoint that invokes the Lambda functions. Write an Amazon Lambda@Edge function to cache the data points on edge locations for a 15-minute duration.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an Amazon EFS volume to store the weather forecast data points. Mount this EFS volume on a fleet of Auto Scaling Amazon EC2 instances behind an Elastic Load Balancer. Create an Amazon CloudFront distribution and point the origin to the ELB. Configure a 15-minute cache-control timeout for the CloudFront distribution.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon S3 bucket to store the weather forecast data points as individual objects. Create a fleet of Auto Scaling Amazon EC2 instances behind an Elastic Load Balancer to query the objects on the S3 bucket. Create an Amazon CloudFront distribution and point the origin to the ELB. Configure a 15-minute cache-control timeout for the CloudFront distribution.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Elastic File System (Amazon EFS)</strong> provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.</p><p>Amazon EFS can provide very low and consistent operational latency as well as a throughput scale of 10+GB per second.</p><p>Amazon EFS file systems are distributed across an unconstrained number of storage servers. This distributed data storage design enables file systems to grow elastically to petabyte scale and enables massively parallel access from Amazon EC2 instances to your data. The Amazon EFS-distributed design avoids the bottlenecks and constraints inherent to traditional file servers.</p><p>This distributed data storage design means that multithreaded applications and applications that concurrently access data from multiple Amazon EC2 instances can drive substantial levels of aggregate throughput and IOPS. Big data and analytics workloads, media processing workflows, content management, and web serving are examples of these applications. In addition, Amazon EFS data is distributed across multiple Availability Zones, providing a high level of durability and availability.</p><p><img src=\"https://media.tutorialsdojo.com/sap_efs_how_it_works.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_efs_how_it_works.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Amazon CloudFront</strong> is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront can provide a reliable, low latency, and high throughput network connectivity for global users.</p><p>To deliver content to end-users with lower latency, Amazon CloudFront peers with thousands of Tier 1/2/3 telecom carriers globally is well connected with all major access networks for optimal performance and has hundreds of terabits of deployed capacity.</p><p>Therefore, the correct answer is: <strong>Use an Amazon EFS volume to store the weather forecast data points. Mount this EFS volume on a fleet of Auto Scaling Amazon EC2 instances behind an Elastic Load Balancer. Create an Amazon CloudFront distribution and point the origin to the ELB. Configure a 15-minute cache-control timeout for the CloudFront distribution.</strong></p><p>The option that says: <strong>Create an Amazon OpenSearch cluster to store the weather forecast data points. Write AWS Lambda functions to query the ES cluster. Create an Amazon CloudFront distribution and point the origin to an Amazon API Gateway endpoint that invokes the Lambda functions. Configure a cache-control timeout of 15 minutes in the API caching section of the API Gateway stage</strong> is incorrect. This is a possible implementation, but the Lambda functions won’t be able to quickly scale and serve requests during peak traffic. By default, the burst concurrency for Lambda functions is between 500-3000 requests per second (depending on region).</p><p>The option that says: <strong>Create an Amazon OpenSearch cluster to store the weather forecast data points. Write AWS Lambda functions to query the ES cluster. Create an Amazon CloudFront distribution and point the origin to an Amazon API Gateway endpoint that invokes the Lambda functions. Write an Amazon Lambda@Edge function to cache the data points on edge locations for a 15-minute duration</strong> is incorrect. While using Amazon OpenSearch and Lambda functions to query the data is a valid approach, the use of Lambda@Edge for caching the data points at the edge locations is not the most appropriate choice. Lambda@Edge is primarily used to customize the behavior of CloudFront distributions, not to serve as the core caching mechanism for large datasets.</p><p>The option that says: <strong>Create an Amazon S3 bucket to store the weather forecast data points as individual objects. Create a fleet of Auto Scaling Amazon EC2 instances behind an Elastic Load Balancer to query the objects on the S3 bucket. Create an Amazon CloudFront distribution and point the origin to the ELB. Configure a 15-minute cache-control timeout for the CloudFront distribution</strong> is incorrect. This solution is not recommended. Access from the EC2 instances to the S3 bucket via HTTP calls will increase the total response time of the application. Access to EFS is faster compared to calling objects on S3 buckets. Although AWS S3 supports strong read-after-write consistency, billions of objects will be overwritten on the S3 bucket every 15 minutes which could take longer to write than using Amazon EFS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a></p><p><a href=\"https://aws.amazon.com/cloudfront/features/\">https://aws.amazon.com/cloudfront/features/</a></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p><p><br></p><p><strong>Check out the Amazon EFS and comparison Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-efs/?src=udemy\">https://tutorialsdojo.com/amazon-efs/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/?src=udemy\">https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/cloudfront/features/",
      "https://docs.aws.amazon.com/efs/latest/ug/performance.html",
      "https://tutorialsdojo.com/amazon-efs/?src=udemy",
      "https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/?src=udemy"
    ]
  },
  {
    "id": 72,
    "question": "<p>An adventure company runs a PostgreSQL database that is used to store events from its monitoring application on its on-premises data center. The database is unable to scale enough to handle frequent write events that need to be ingested into the database. The management has tasked the solutions architect to create a hybrid solution that will utilize the existing company VPN connection to AWS. Additional requirements are as follows:</p><ul><li><p>Leverage AWS-managed services to minimize operation overhead</p></li><li><p>Create a buffer that automatically scales to accommodate the events that need to be ingested</p></li><li><p>A visualization tool to observe near real-time events and supports creating dashboards.</p></li><li><p>Has support for dynamic schemas and semi-structured JSON data.</p></li></ul><p>Which of the following options should the solutions architect implement to meet the company requirements? (Select TWO.)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon OpenSearch Service domain to reliably ingest the events. Leverage the OpenSearch Dashboards tool to create near-real-time dashboards and visualizations.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Levering Amazon Neptune DB auto-scaling feature to reliably ingest the events. Use Neptune DB as the DB source for Amazon QuickSight to create near-real-time dashboards and visualizations.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Provision an Amazon Aurora PostgreSQL DB cluster to reliably ingest the events. Use this DB as the source for Amazon QuickSight to create near-real-time dashboards and visualizations.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Ingest the events using Amazon Data Firehose. Write a Lambda function to process and transform the buffered events.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Use Amazon Kinesis Data Stream to reliably ingest the events. Write a Lambda function to process and transform the buffered events.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Data Firehose</strong> is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, <strong>Amazon OpenSearch Service</strong>, Amazon OpenSearch Serverless, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers. With Data Firehose, you don't need to write applications or manage resources. You configure your data producers to send data to Data Firehose, and it automatically delivers the data to the destination that you specified. You can also configure Data Firehose to transform your data before delivering it.</p><p><strong>Amazon OpenSearch Service</strong> is a managed service that makes it easy to deploy, operate, and scale OpenSearch clusters in the AWS Cloud. Amazon OpenSearch Service supports OpenSearch and legacy Elasticsearch OSS (up to 7.10, the final open-source version of the software). When you create a cluster, you have the option of which search engine to use.</p><p>OpenSearch is a fully open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. <strong>Amazon OpenSearch Service</strong> provisions all the resources for your cluster and launches it. It also automatically detects and replaces failed OpenSearch Service nodes, reducing the overhead associated with self-managed infrastructures.</p><p><img src=\"https://media.tutorialsdojo.com/sap_kinesis_elasticsearch.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_kinesis_elasticsearch.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>OpenSearch Dashboards</strong> is an open-source visualization tool designed to work with OpenSearch. Amazon OpenSearch Service provides an installation of OpenSearch Dashboards with every OpenSearch Service domain. You can find a link to Dashboards on your domain dashboard on the OpenSearch Service console.</p><p>Hence, the correct answers are:</p><p><strong>- Ingest the events using Amazon Data Firehose. Write a Lambda function to process and transform the buffered events.</strong></p><p><strong>- Create an Amazon OpenSearch Service domain to reliably ingest the events. Leverage the OpenSearch Dashboards tool to create near-real-time dashboards and visualizations.</strong></p><p>The option that says: <strong>Levering Amazon Neptune DB auto-scaling feature to reliably ingest the events. Use Neptune DB as the DB source for Amazon QuickSight to create near-real-time dashboards and visualizations</strong> is incorrect. Neptune DB is designed for graph application and loading CSV formatted data. Amazon QuickSight can directly use Neptune DB as a source.</p><p>The option that says: <strong>Provision an Amazon Aurora PostgreSQL DB cluster to reliably ingest the events. Use this DB as the source for Amazon QuickSight to create near-real-time dashboards and visualizations</strong> is incorrect. Amazon Aurora PostgreSQL may not be suited for ingesting semi-structured JSON or data with dynamic schemas. You will need to use a data transformer before ingesting to PostgreSQL database.</p><p>The option that says: <strong>Use Amazon Kinesis Data Stream to reliably ingest the events. Write a Lambda function to process and transform the buffered events</strong> is incorrect. This may be possible, but since you are going to use Kinesis as a buffer, you don't need the longer-term, durable storage offered by Kinesis Data Stream. Amazon Data Firehose has a feature to buffer data and supports data transformation in near-real-time.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html\">https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html</a></p><p><a href=\"https://aws.amazon.com/blogs/big-data/ingest-streaming-data-into-amazon-elasticsearch-service-within-the-privacy-of-your-vpc-with-amazon-kinesis-data-firehose/\">https://aws.amazon.com/blogs/big-data/ingest-streaming-data-into-amazon-elasticsearch-service-within-the-privacy-of-your-vpc-with-amazon-kinesis-data-firehose/</a></p><p><a href=\"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/dashboards.html\">https://docs.aws.amazon.com/opensearch-service/latest/developerguide/dashboards.html</a></p><p><br></p><p><strong>Check out these Amazon Kinesis and Amazon OpenSearch Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-opensearch-service/?src=udemy\">https://tutorialsdojo.com/amazon-opensearch-service/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html",
      "https://aws.amazon.com/blogs/big-data/ingest-streaming-data-into-amazon-elasticsearch-service-within-the-privacy-of-your-vpc-with-amazon-kinesis-data-firehose/",
      "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/dashboards.html",
      "https://tutorialsdojo.com/amazon-kinesis/?src=udemy",
      "https://tutorialsdojo.com/amazon-opensearch-service/?src=udemy"
    ]
  },
  {
    "id": 73,
    "question": "<p>A company stores confidential financial documents as well as sensitive corporate information in an Amazon S3 bucket. There is a new security policy that prohibits any public S3 objects in the company's S3 bucket. In the event that a public object was identified, the IT Compliance team must be notified immediately and the object's permissions must be remediated automatically. The notification must be sent as soon as a public object was created in the bucket. </p><p>What is the MOST suitable solution that should be implemented by the Solutions Architect to comply with this data policy?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Automatically track S3 actions using the Trusted Advisor API and AWS Cloud Development Kit (CDK). Set up an Amazon EventBridge rule with an Amazon SNS Topic to notify the IT Compliance team when the Trusted Advisor detected a <code>PutObject</code> API call with public-read permission. Launch another CloudWatch Events rule that invokes an AWS Lambda function to turn the newly uploaded public object to private.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up a Systems Manager (SSM) Automation document that changes any public object in the bucket to private. Integrate Amazon EventBridge with AWS Lambda to create a scheduled process that checks the S3 bucket every hour. Configure the Lambda function to invoke the SSM Automation document when a public object is identified and to notify the IT Compliance team via email using Amazon SNS.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable object-level logging in the S3 bucket to automatically track S3 actions using CloudTrail. Set up an Amazon EventBridge rule with an SNS Topic to notify the IT Compliance team when a <code>PutObject</code> API call with public-read permission is detected in the CloudTrail logs. Launch another CloudWatch Events rule that invokes an AWS Lambda function to turn the newly uploaded public object to private.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Integrate Amazon Lex with Amazon GuardDuty to detect public objects in the S3 bucket and to automatically update the permission of a public object to private. Associate an SNS Topic to Amazon Lex to notify the IT Compliance team via email if a public object was identified.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon S3</strong> is integrated with <strong>AWS CloudTrail</strong>, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon S3. CloudTrail captures a subset of API calls for Amazon S3 as events, including calls from the Amazon S3 console and from code calls to the Amazon S3 APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon S3. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in the Event history. Using the information collected by CloudTrail, you can determine the request that was made to Amazon S3, the IP address from which the request was made, who made the request, when it was made, and additional details.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_s3_lambda_sns.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_s3_lambda_sns.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can also get CloudTrail logs for object-level Amazon S3 actions. To do this, specify the Amazon S3 object for your trail. When an object-level action occurs in your account, CloudTrail evaluates your trail settings. If the event matches the object that you specified in a trail, the event is logged.</p><p><strong>Amazon EventBridge</strong> is a serverless event bus service that you can use to connect your applications with data from a variety of sources. EventBridge delivers a stream of real-time data from your applications, software as a service (SaaS) applications, and AWS services to targets such as AWS Lambda functions, HTTP invocation endpoints using API destinations, or event buses in other AWS accounts. A rule matches incoming events and sends them to targets for processing. A single rule can send an event to multiple targets, which then run in parallel. Rules are based either on an event pattern or a schedule. An event pattern defines the event structure and the fields that a rule matches. Rules that are based on a schedule perform an action at regular intervals.</p><p>Hence, the correct answer is: <strong>Enable object-level logging in the S3 bucket to automatically track S3 actions using CloudTrail. Set up an Amazon EventBridge rule with an SNS Topic to notify the IT Compliance team when a </strong><code><strong>PutObject</strong></code><strong> API call with public-read permission is detected in the CloudTrail logs. Launch another CloudWatch Events rule that invokes an AWS Lambda function to turn the newly uploaded public object to private.</strong></p><p>The option that says: <strong>Set up a Systems Manager (SSM) Automation document that changes any public object in the bucket to private. Integrate Amazon EventBridge with AWS Lambda to create a scheduled process that checks the S3 bucket every hour. Configure the Lambda function to invoke the SSM Automation document when a public object was identified and to notify the IT Compliance team via email using Amazon SNS</strong> is incorrect because the requirement says that the notification must be sent as soon as a public object was created in the bucket. Running the process to check for public objects ever hour will cause delays in detecting a public object in the S3 bucket.</p><p>The option that says: <strong>Automatically track S3 actions using the Trusted Advisor API and AWS Cloud Development Kit (CDK). Set up an Amazon EventBridge rule with an Amazon SNS Topic to notify the IT Compliance team when the Trusted Advisor detected a </strong><code><strong>PutObject</strong></code><strong> API call with public-read permission. Launch another CloudWatch Events rule that invokes an AWS Lambda function to turn the newly uploaded public object to private</strong> is incorrect. Using the Trusted Advisor as a web service to track all S3 actions is not enough as it only provides high-level data about your S3 bucket, excluding the object-level permissions. A better solution is to enable the object-level logging in the S3 bucket.</p><p>The option that says: <strong>Integrate Amazon Lex with Amazon GuardDuty to detect public objects in the S3 bucket and to automatically update the permission of a public object to private. Associate an SNS Topic to Amazon Lex to notify the IT Compliance team via email if a public object was identified</strong> is incorrect because Amazon Lex is simply a service for building conversational interfaces into any application using voice and text. In addition, Amazon GuardDuty is just a threat detection service that analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. These two services are not suitable in detecting public objects in your S3 bucket.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/log-s3-data-events.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/log-s3-data-events.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html</a></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rules.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rules.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/log-s3-data-events.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html",
      "https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html",
      "https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rules.html",
      "https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy"
    ]
  },
  {
    "id": 74,
    "question": "<p>A FinTech startup has recently consolidated its multiple AWS accounts using AWS Organizations. It currently has two teams in its organization, a security team and a development team. The former is responsible for protecting their cloud infrastructure and making sure that all of their resources are compliant, while the latter is responsible for developing new applications that are deployed to EC2 instances. The security team is required to set up a system that will check if all of the running EC2 instances are using an approved AMI. However, the solution should not stop the development team from deploying an EC2 instance running on a non-approved AMI. The disruption is only allowed once the deployment has been completed<strong>.</strong> In addition, they have to set up a notification system that sends the compliance state of the resources to determine whether they are compliant.</p><p>Which of the following options is the most suitable solution that the security team should implement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the Amazon Inspector service to automatically check all of the AMIs that are being used by your EC2 instances. Set up an SNS topic that will send a notification to both the security and development teams if there is a non-compliant EC2 instance running in their VPCs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use an AWS Config Managed Rule and specify a list of approved AMI IDs. This rule will check whether running EC2 instances are using specified AMIs. Configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic which will send a notification for non-compliant instances.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up a Trusted Advisor check that will verify whether the running EC2 instances in your VPCs are using approved AMIs. Create a CloudWatch alarm and integrate it with the Trusted Advisor metrics that will check all of the AMIs being used by your EC2 instances and that will send a notification if there is a running instance which uses an unapproved AMI.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create and assign an SCP and an IAM policy that restricts the AWS accounts and the development team from launching an EC2 instance using an unapproved AMI. Create a CloudWatch alarm that will automatically notify the security team if there are non-compliant EC2 instances running in their VPCs.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>When you run your applications on AWS, you usually use AWS resources, which you must create and manage collectively. As the demand for your application keeps growing, so does your need to keep track of your AWS resources. AWS Config is designed to help you oversee your application resources.</p><p><img src=\"https://media.tutorialsdojo.com/sap_config_how_it_works.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_config_how_it_works.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>AWS Config</strong> provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time. With AWS Config, you can do the following:</p><p>- Evaluate your AWS resource configurations for desired settings.</p><p>- Get a snapshot of the current configurations of the supported resources that are associated with your AWS account.</p><p>- Retrieve configurations of one or more resources that exist in your account.</p><p>- Retrieve historical configurations of one or more resources.</p><p>- Receive a notification whenever a resource is created, modified, or deleted.</p><p>- View relationships between resources. For example, you might want to find all resources that use a particular security group.</p><p>AWS Config provides <em>AWS managed rules</em>, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. In this scenario, you can use the <code><strong>approved-amis-by-id</strong></code> AWS managed rule which checks whether running instances are using specified AMIs.</p><p>Therefore, the correct answer is: <strong>Use an AWS Config Managed Rule and specify a list of approved AMI IDs. This rule will check whether running EC2 instances are using specified AMIs. Configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic which will send a notification for non-compliant instances.</strong></p><p>The option that says: <strong>Create and assign an SCP and an IAM policy that restricts the AWS accounts and the development team from launching an EC2 instance using an unapproved AMI. Create a CloudWatch alarm that will automatically notify the security team if there are non-compliant EC2 instances running in their VPCs</strong> is incorrect. Setting up an SCP and IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs. The scenario clearly says that the solution should not have this kind of restriction.</p><p>The option that says: <strong>Use the Amazon Inspector service to automatically check all of the AMIs that are being used by your EC2 instances. Set up an SNS topic that will send a notification to both the security and development teams if there is a non-compliant EC2 instance running in their VPCs</strong> is incorrect. The Amazon Inspector service is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It does not have the capability to detect EC2 instances that are using unapproved AMIs, unlike AWS Config.</p><p>The option that says: <strong>Set up a Trusted Advisor check that will verify whether the running EC2 instances in your VPCs are using approved AMIs. Create a CloudWatch alarm and integrate it with the Trusted Advisor metrics that will check all of the AMIs being used by your EC2 instances and that will send a notification if there is a running instance which uses an unapproved AMI</strong> is incorrect. AWS Trusted Advisor is primarily used to check if your cloud infrastructure is in compliance with the best practices and recommendations across five categories: cost optimization, security, fault tolerance, performance, and service limits. Their security checks for EC2 do not cover the checking of individual AMIs that are being used by your EC2 instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html",
      "https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html",
      "https://tutorialsdojo.com/aws-config/?src=udemy"
    ]
  },
  {
    "id": 75,
    "question": "<p><br>A company is hosting its application and MySQL database in its on-premises data center. The database increases at about 10GB per day and is approximately 25TB in total size. The company wants to migrate the database workload to the AWS cloud. A 50Mbps VPN connection is currently in place to connect the corporate network to AWS. The company plans to complete the migration to AWS within 3 weeks with the LEAST downtime possible.</p><p>Which of the following solutions should be implemented to meet the company requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an AWS Database Migration Service (DMS) instance and an Amazon Aurora MySQL DB instance. Define the on-premises database server details and the Amazon Aurora MySQL instance details on the AWS DMS instance. Use the VPN connection to start the replication from the on-premises database server to AWS. Wait until the replication is complete then update the database DNS entry to point to the Aurora DB instance. Stop the database replication.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Request for an AWS Snowball device. Create a database export of the on-premises database server and load it to the Snowball device. Once the data is imported to AWS, provision an Amazon Aurora MySQL DB instance and load the data. Using the VPN connection, configure replication from the on-premises database server to the Amazon Aurora DB instance. Wait until the replication is complete then update the database DNS entry to point to the Aurora DB instance. Stop the database replication.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a snapshot of the on-premises database server and use VM Import/Export service to import the snapshot to AWS. Provision a new Amazon EC2 instance from the imported snapshot. Configure replication from the on-premises database server to the EC2 instance through the VPN connection. Wait until the replication is complete then update the database DNS entry to point to the EC2 instance. Stop the database replication.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Temporarily stop the on-premises application to stop any database I/O operation. Request for an AWS Snowball device. Create a database export of the on-premises database server and load it to the Snowball device. Once the data is imported to AWS, provision an Amazon Aurora MySQL DB instance and load the data. Update the database DNS entry to point to the Aurora DB instance. Start the application again to resume normal operations.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p><strong>AWS Snowball</strong>, a part of the AWS Snow Family, is an edge computing, data migration, and edge storage device that comes in two options. <strong>Snowball Edge Storage Optimized</strong> devices provide both block storage and Amazon S3-compatible object storage, and 40 vCPUs. They are well suited for local storage and large scale-data transfer up to 80TB. <strong>Snowball Edge Compute Optimized</strong> devices provide 52 vCPUs, block and object storage, and an optional GPU for use cases like advanced machine learning and full-motion video analysis in disconnected environments.</p><p>Snowball moves terabytes of data in about a week. You can use it to move things like databases, backups, archives, healthcare records, analytics datasets, IoT sensor data, and media content, especially when network conditions prevent realistic timelines for transferring large amounts of data both into and out of AWS.</p><p>Each import job uses a single Snowball appliance. After you create a job in the AWS Snow Family Management Console or the job management API, AWS ships a Snowball to you. When it arrives in a few days, you connect the Snowball Edge device to your network and transfer the data that you want to be imported into Amazon S3 onto the device. When you’re done transferring data, ship the Snowball back to AWS, and AWS will import your data into Amazon S3.</p><p>Amazon Aurora MySQL integrates with other AWS services so that you can extend your Aurora MySQL DB cluster to use additional capabilities in the AWS Cloud. Your Aurora MySQL DB cluster can use AWS services to load data from text or XML files stored in an Amazon Simple Storage Service (Amazon S3) bucket into your DB cluster using the LOAD DATA FROM S3 or LOAD XML FROM S3 command.</p><p><img src=\"https://media.tutorialsdojo.com/sap_snowball_edge_transfer.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_snowball_edge_transfer.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Request for an AWS Snowball device. Create a database export of the on-premises database server and load it to the Snowball device. Once the data is imported to AWS, provision an Amazon Aurora MySQL DB instance and load the data. Using the VPN connection, configure replication from the on-premises database server to the Amazon Aurora DB instance. Wait until the replication is complete then update the database DNS entry to point to the Aurora DB instance. Stop the database replication.</strong> You can use the Snowball device to import TBs of data to AWS. Then you can load the data to the Amazon Aurora DB instance and replicate the missing data from the on-premises database server. You can cut over to the Aurora database after replication.</p><p>The option that says: <strong>Create a snapshot of the on-premises database server and use VM Import/Export service to import the snapshot to AWS. Provision a new Amazon EC2 instance from the imported snapshot. Configure replication from the on-premises database server to the EC2 instance through the VPN connection. Wait until the replication is complete then update the database DNS entry to point to the EC2 instance. Stop the database replication</strong> is incorrect. You cannot import a 25 TB snapshot using VM Import/Export. This option does not specify if it uses the company network to export the snapshot to AWS. If so, the 50Mbps won't be enough to export the entire database within the 3-week window.</p><p>The option that says: <strong>Create an AWS Database Migration Service (DMS) instance and an Amazon Aurora MySQL DB instance. Define the on-premises database server details and the Amazon Aurora MySQL instance details on the AWS DMS instance. Use the VPN connection to start the replication from the on-premises database server to AWS. Wait until the replication is complete then update the database DNS entry to point to the Aurora DB instance. Stop the database replication </strong>is incorrect. Replicating an entire 25 TB database via the 50Mbps VPN connection will take several weeks for the replication to catch up. Not to mention that there will be additional 10 GB of data to be replicated for each passing day.</p><p>The option that says: <strong>Temporarily stop the on-premises application to stop any database I/O operation. Request for an AWS Snowball device. Create a database export of the on-premises database server and load it to the Snowball device. Once the data is imported to AWS, provision an Amazon Aurora MySQL DB instance and load the data. Update the database DNS entry to point to the Aurora DB instance. Start the application again to resume normal operations</strong> is incorrect. This is possible but the downtime is too much. From the data export, loading to the Snowball device, and importing to the Amazon Aurora DB instance, the downtime will take at least a few days.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/snowball/latest/developer-guide/how-it-works.html\">https://docs.aws.amazon.com/snowball/latest/developer-guide/how-it-works.html</a></p><p><a href=\"https://aws.amazon.com/snowball/features/\">https://aws.amazon.com/snowball/features/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/replicate-amazon-rds-mysql-on-premises/\">https://aws.amazon.com/premiumsupport/knowledge-center/replicate-amazon-rds-mysql-on-premises/</a></p><p><br></p><p><strong>Check out these AWS Snowball Edge and Amazon Aurora Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-snowball-edge/?src=udemy\">https://tutorialsdojo.com/aws-snowball-edge/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora/?src=udemy\">https://tutorialsdojo.com/amazon-aurora/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/snowball/latest/developer-guide/how-it-works.html",
      "https://aws.amazon.com/snowball/features/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/replicate-amazon-rds-mysql-on-premises/",
      "https://tutorialsdojo.com/aws-snowball-edge/?src=udemy",
      "https://tutorialsdojo.com/amazon-aurora/?src=udemy"
    ]
  }
]