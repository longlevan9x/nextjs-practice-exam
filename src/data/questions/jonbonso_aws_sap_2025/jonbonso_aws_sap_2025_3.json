[
  {
    "id": 1,
    "question": "<p>A company runs a suite of web applications in AWS. The application is hosted in an Auto Scaling group of On-Demand Amazon EC2 instances behind an Application Load Balancer that handles traffic from multiple web domains. The solutions architect is responsible for securing the system by allowing multiple domains to serve SSL traffic without the need to re-authenticate and re-provision a new certificate whenever a new domain name is added. This change of architecture from HTTP to HTTPS will help improve the SEO and Google search ranking of the web application.</p><p>Which of the following options are valid solutions to meet the above requirements? (Select TWO.)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a new CloudFront web distribution and configure it to serve HTTPS requests using dedicated IP addresses in order to associate your alternate domain names with a dedicated IP address in each CloudFront edge location.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Add a Subject Alternative Name (SAN) for each additional domain to your certificate.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use a wildcard certificate to handle multiple sub-domains and different domains.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Upload all SSL certificates of the domains in the ALB using the console and bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client using Server Name Indication (SNI).</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Use a Gateway Load Balancer instead of an Application Load Balancer. Upload all SSL certificates of the domains and use Server Name Indication (SNI).</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>SNI Custom SSL</strong> relies on the SNI extension of the Transport Layer Security protocol, which allows multiple domains to serve SSL traffic over the same IP address by including the hostname to which the viewers are trying to connect.</p><p>You can host multiple TLS-secured applications, each with its own TLS certificate, behind a single load balancer. In order to use SNI, all you need to do is bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client. These features are provided at no additional charge.</p><p><img src=\"https://media.tutorialsdojo.com/sap_alb_sni_custom_ssl.gif\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_alb_sni_custom_ssl.gif\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can use your own SSL certificates with <strong>Amazon CloudFront</strong> at no additional charge with Server Name Indication (SNI) Custom SSL. Most modern browsers support SNI and provide an efficient way to deliver content over HTTPS using your own domain and SSL certificate. Amazon CloudFront delivers your content from each edge location and offers the same security as the Dedicated IP Custom SSL feature.</p><p>The option that says: <strong>Upload all SSL certificates of the domains in the ALB using the console and bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client using Server Name Indication (SNI)</strong> is correct. You can upload all SSL certificates of the domains in the ALB using the console and bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client using Server Name Indication (SNI).</p><p>The option that says: <strong>Create a new CloudFront web distribution and configure it to serve HTTPS requests using dedicated IP addresses in order to associate your alternate domain names with a dedicated IP address in each CloudFront edge location</strong> is correct. You can configure Amazon CloudFront to require viewers to interact with your content over an HTTPS connection using the HTTP to HTTPS Redirect feature. If you configure CloudFront to serve HTTPS requests using SNI, CloudFront associates your alternate domain name with an IP address for each edge location. The IP address to your domain name is determined during the SSL/TLS handshake negotiation and isn’t dedicated to your distribution.<strong><br></strong></p><p>The option that says: <strong>Use a wildcard certificate to handle multiple sub-domains and different domains</strong> is incorrect. A wildcard certificate can only handle multiple sub-domains but not different domain names.</p><p>The option that says: <strong>Add a Subject Alternative Name (SAN) for each additional domain to your certificate</strong> is incorrect. Although using Subject Alternative Name (SAN) is correct, you will still have to reauthenticate and reprovision your certificate every time you add a new domain. One of the requirements in the scenario is that you should not have to reauthenticate and reprovision your certificate; hence, this solution is incorrect.</p><p>The option that says: <strong>Use a Gateway Load Balancer instead of an Application Load Balancer. Upload all SSL certificates of the domains and use Server Name Indication (SNI)</strong> is incorrect because a Gateway Load Balancer does not support SNI.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/\">https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-https-dedicated-ip-or-sni.html#cnames-https-dedicated-ip\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-https-dedicated-ip-or-sni.html#cnames-https-dedicated-ip</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>SNI Custom SSL vs Dedicated IP Custom SSL:</strong></p><p><a href=\"https://tutorialsdojo.com/sni-custom-ssl-vs-dedicated-ip-custom-ssl/?src=udemy\">https://tutorialsdojo.com/sni-custom-ssl-vs-dedicated-ip-custom-ssl/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-https-dedicated-ip-or-sni.html#cnames-https-dedicated-ip",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy",
      "https://tutorialsdojo.com/sni-custom-ssl-vs-dedicated-ip-custom-ssl/?src=udemy",
      "https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy"
    ]
  },
  {
    "id": 2,
    "question": "<p>A company has several financial applications hosted in AWS that uses Amazon S3 buckets to store static data. The Solutions Architect recently discovered that some employees store highly classified data into S3 buckets without proper approval. To mitigate any security risks, the Architect needs to determine all possible S3 objects that contain personally identifiable information (PII) and determine whether the data has been accessed. Due to the sheer volume of data, the Architect must implement an automated solution to accomplish this important task.</p><p>Which of the following should the solutions architect implement for this scenario?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon GuardDuty to detect personally identifiable information (PII) on the Amazon S3 buckets. Determine if the objects with PII have been recently accessed by tracking the <code>GET</code> API calls in AWS CloudTrail that are used to download these objects.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Install the Amazon Inspector agent on the Amazon S3 buckets. Use AWS CloudTrail to determine if the objects with personally identifiable information (PII) have been recently accessed by tracking the <code>GET</code> API calls that are used to fetch these objects.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Detect personally identifiable information (PII) on the specific S3 buckets using Amazon Athena. Set up Amazon CloudWatch to determine if the objects with PII have been recently accessed by tracking the <code>GET</code> API calls that are used to download these objects.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable Amazon Macie on the S3 buckets to automatically classify the data and detect any objects with personally identifiable information (PII). Determine if the objects with PII have been recently accessed by tracking the <code>GET</code> API calls in AWS CloudTrail.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon Macie</strong> is an ML-powered security service that helps you prevent data loss by automatically discovering, classifying, and protecting sensitive data stored in Amazon S3. Amazon Macie uses machine learning to recognize sensitive data such as personally identifiable information (PII) or intellectual property, assigns a business value, and provides visibility into where this data is stored and how it is being used in your organization.</p><p>Amazon Macie continuously monitors data access activity for anomalies and delivers alerts when it detects the risk of unauthorized access or inadvertent data leaks. Amazon Macie has the ability to detect global access permissions inadvertently being set on sensitive data, detect uploading of API keys inside source code, and verify sensitive customer data is being stored and accessed in a manner that meets their compliance standards.</p><p><img src=\"https://media.tutorialsdojo.com/sap_macie_dashboard.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_macie_dashboard.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Hence, the correct answer is: <strong>Enable Amazon Macie on the S3 buckets to automatically classify the data and detect any objects with personally identifiable information (PII). Determine if the objects with PII have been recently accessed by tracking the </strong><code><strong>GET</strong></code><strong> API calls in AWS CloudTrail.</strong></p><p>The option that says: <strong>Detect personally identifiable information (PII) on the specific S3 buckets using Amazon Athena. Set up Amazon CloudWatch to determine if the objects with PII have been recently accessed by tracking the </strong><code><strong>GET</strong></code><strong> API calls that are used to download these objects</strong> is incorrect because Amazon Athena is not capable of detecting personally identifiable information (PII) in an Amazon S3 bucket. You have to use Amazon Macie instead. In addition, you have to use AWS CloudTrail to track the <code>GET</code> API calls that are used to fetch the objects with PII.</p><p>The option that says: <strong>Use Amazon GuardDuty to detect personally identifiable information (PII) on the Amazon S3 buckets. Determine if the objects with PII have been recently accessed by tracking the </strong><code><strong>GET</strong></code><strong> API calls in AWS CloudTrail that are used to download these objects </strong>is incorrect because GuardDuty is just a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. You have to use Amazon Macie instead.</p><p>The option that says: <strong>Install the Amazon Inspector agent on the Amazon S3 buckets. Use AWS CloudTrail to determine if the objects with personally identifiable information (PII) have been recently accessed by tracking the </strong><code><strong>GET</strong></code><strong> API calls that are used to fetch these objects<em> </em></strong>is incorrect because you can only install the Amazon Inspector agent to EC2 instances and not to S3 buckets. Amazon Inspector is basically an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. You have to use Amazon Macie instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html\">https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html</a></p><p><a href=\"https://aws.amazon.com/macie/faq/\">https://aws.amazon.com/macie/faq/</a></p><p><a href=\"https://docs.aws.amazon.com/macie/index.html\">https://docs.aws.amazon.com/macie/index.html</a></p><p><br></p><p><strong>Check out this Amazon Macie Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-macie/?src=udemy\">https://tutorialsdojo.com/amazon-macie/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html",
      "https://aws.amazon.com/macie/faq/",
      "https://docs.aws.amazon.com/macie/index.html",
      "https://tutorialsdojo.com/amazon-macie/?src=udemy"
    ]
  },
  {
    "id": 3,
    "question": "<p>A company is developing an application that will allow biologists from around the world to submit plant genomic information and share it with other biologists. The application will expect several submissions every minute and will push about 8KB of genomic data every second to the data platform. This data needs to be processed and analyzed to provide meaningful information back to the biologists. The following are the requirements for the data platform:</p><p>-The inbound genomic data must be processed near-real-time and provide analytics.</p><p>-The received data must be stored in a flexible, parallel, and durable manner.</p><p>-After processing the data, the resulting output must be delivered to a data warehouse.</p><p>Which of the following options should the Solutions Architect implement to meet the company's requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon Data Firehose stream to deliver the inbound data to an Amazon S3 bucket. Use a Kinesis client to analyze the stored data. For data warehousing, register the S3 bucket on AWS Lake Formation as a data lake. Use Amazon Quicksight to query the data lake.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a stream in Amazon Kinesis Data Streams to collect the inbound data. Use a Kinesis client to analyze the genomic data. After processing, use Amazon EMR to save the results to an Amazon Redshift cluster.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Leverage Amazon API Gateway to accept the inbound data and send it to an Amazon SQS queue. Write an AWS Lambda function that will process the messages on the SQS queue. After processing, use Amazon EMR to save the results to an Amazon Redshift cluster.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store all inbound data files directly to an Amazon S3 bucket. Use Amazon Kinesis with Amazon SQS to analyze the data stored in the S3 bucket. After processing, send the results to an Amazon Redshift cluster.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Kinesis Data Streams (KDS)</strong> is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources, such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p><p>You can make your streaming data available to multiple real-time analytics applications, to Amazon S3, or to AWS Lambda within 70 milliseconds of the data being collected. KDS is highly durable as it performs synchronous replication of your streaming data across three Availability Zones in an AWS Region and stores that data for up to 365 days to provide multiple layers of protection from data loss.</p><p>You can have your Kinesis Applications run real-time analytics on high-frequency event data, such as sensor data collected by Kinesis Data Streams, which enables you to gain insights from your data at a frequency of minutes instead of hours or days.</p><p><img src=\"https://media.tutorialsdojo.com/sap_kinesis_data_streams_overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_kinesis_data_streams_overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>One of the methods of developing custom consumer applications that can process data from KDS data streams is to use the <strong>Kinesis Client Library (KCL)</strong>. KCL helps you consume and process data from a Kinesis data stream by taking care of many of the complex tasks associated with distributed computing. These include load balancing across multiple consumer application instances, responding to consumer application instance failures, checkpointing processed records, and reacting to resharding.</p><p><strong>Amazon Redshift</strong> is a fully managed, petabyte-scale data warehouse service in the cloud. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. With Redshift, you can query and combine exabytes of structured and semi-structured data across your data warehouse, operational database, and data lake using standard SQL.</p><p>Therefore, the correct answer is: <strong>Create a stream in Amazon Kinesis Data Streams to collect the inbound data. Use a Kinesis client to analyze the genomic data. After processing, use Amazon EMR to save the results to an Amazon Redshift cluster.</strong> Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. Your data can be made available to real-time analytics applications and then saved to Amazon Redshift for data warehousing.</p><p>The option that says: <strong>Create an Amazon Data Firehose stream to deliver the inbound data to an Amazon S3 bucket. Use a Kinesis client to analyze the stored data. For data warehousing, register the S3 bucket on AWS Lake Formation as a data lake. Use Amazon Quicksight to query the data lake </strong>is incorrect. You can't use Amazon Quicksight to query data lakes from AWS Lake Formation. AWS recommends Amazon Reshift as a data warehouse solution.</p><p>The option that says: <strong>Store all inbound data files directly to an Amazon S3 bucket. Use Amazon Kinesis with Amazon SQS to analyze the data stored in the S3 bucket. After processing, send the results to an Amazon Redshift cluster</strong> is incorrect. Storing files to S3 first and then sending a message to an SQS queue takes some time. This solution may not meet the near-real-time requirement. You will also have to write your own Lambda function which increases operational overhead.</p><p>The option that says: <strong>Leverage Amazon API Gateway to accept the inbound data and send it to an Amazon SQS queue. Write an AWS Lambda function that will process the messages on the SQS queue. After processing, use Amazon EMR to save the results to an Amazon Redshift cluster</strong> is incorrect. It is possible to integrate API Gateway with Amazon SQS. However, the messages will not be processed in an orderly manner required for near-real-time analysis.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html\">https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html</a></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/welcome.html\">https://docs.aws.amazon.com/redshift/latest/dg/welcome.html</a></p><p><br></p><p><strong>Check out these Amazon Kinesis and Amazon Redshift Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-redshift/?src=udemy\">https://tutorialsdojo.com/amazon-redshift/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/welcome.html",
      "https://tutorialsdojo.com/amazon-redshift/?src=udemy",
      "https://tutorialsdojo.com/amazon-kinesis/?src=udemy"
    ]
  },
  {
    "id": 4,
    "question": "<p>A travel booking company runs its main web application on the AWS cloud. Its trip planner website provides timetables, travel alerts, and other public transportation information for trains, buses, ferries, and trams. The front-end tier is composed of an ALB in front of an Auto Scaling group of Amazon EC2 instances deployed across 3 Availability Zones and a Multi-AZ RDS for its database tier. When there are sporting events and popular concerts to be held in a city, the usage of the trip planner application spikes which causes the application servers to reach utilization of over 90%. The solutions architect must ensure that the website can quickly recover in the event that one of its Availability Zones failed during its peak usage.</p><p>Which of the following is the most cost-effective architectural design that should be implemented for this website to maintain high availability?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "To have the most cost-effective architecture, replace all of the Reserved and On-Demand EC2 instances with Spot instances across all Availability Zones. Configure an Auto Scaling group in one of the AZs for scalability.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Increase the capacity and scaling thresholds of the Auto Scaling group to allow the application servers to scale up across all Availability Zones, which will lower the aggregate utilization of the EC2 instances. Use Reserved Instances to handle the steady-state load and a combination of On-Demand and Spot Instances to process the peak load. When the peak usage is over, scale down the number of the On-Demand and Spot instances.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Deploy six Reserved and Spot EC2 Instances in each of the 3 Availability Zones. In this way, the remaining two Availability Zones can handle the load left behind by the Availability Zone that went down.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy one On-Demand EC2 instance and two Spot EC2 Instances in each of the 3 Availability Zones. In case that one Availability Zone fails, the remaining two Availability Zones can handle the peak load.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>Remember that Spot Instances are the most cost-effective type of instances that you can choose. However, this is not suitable for applications with steady state usage.</p><p>Amazon EC2 Spot instances allow you to request spare Amazon EC2 computing capacity for up to 90% off the On-Demand price. Spot instances are recommended for:</p><p>- Applications that have flexible start and end times</p><p>- Applications that are only feasible at very low compute prices</p><p>- Users with urgent computing needs for large amounts of additional capacity</p><p><img src=\"https://media.tutorialsdojo.com/sap_spot_instances_overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_spot_instances_overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>On-Demand instances are recommended for:</p><p>- Users that prefer the low cost and flexibility of Amazon EC2 without any up-front payment or long-term commitment</p><p>- Applications with short-term, spiky, or unpredictable workloads that cannot be interrupted</p><p>- Applications being developed or tested on Amazon EC2 for the first time</p><p>Reserved Instances are recommended for:</p><p>- Applications with <strong>steady state</strong> usage</p><p>- Applications that may require reserved capacity</p><p>- Customers that can commit to using EC2 over a 1 or 3 year term to reduce their total computing costs</p><p>The option that says: <strong>Increase the capacity and scaling thresholds of the Auto Scaling group to allow the application servers to scale up across all Availability Zones, which will lower the aggregate utilization of the EC2 instances. Use Reserved Instances to handle the steady-state load and a combination of On-Demand and Spot Instances to process the peak load. When the peak usage is over, scale down the number of the On-Demand and Spot instances</strong> is correct because by using Auto Scaling, you allow the application servers to scale up across all Availability Zones and handle the additional load. The combination of On-Demand and Spot Instances to process the peak load is a cost-effective solution because when the peak usage is over, you can scale down the number of your instances to save costs.</p><p>The option that says: <strong>Deploy six Reserved and Spot EC2 Instances in each of the 3 Availability Zones. In this way, the remaining two Availability Zones can handle the load left behind by the Availability Zone that went down</strong> is incorrect because having 6 Reserved Instances are still costly even if you add a Spot instance on each Availability Zone. You should also use an Auto Scaling group in order to properly scale your compute resources in accordance with your incoming traffic.</p><p>The option that says: <strong>Deploy one On-Demand EC2 instance and two Spot EC2 Instances in each of the 3 Availability Zones. In case that one Availability Zone fails, the remaining two Availability Zones can handle the peak load</strong> is incorrect because although this is a cost-effective solution, it doesn't use Auto Scaling and hence, the availability of the website is at risk. If the On-Demand instance fails then the Availability Zone is only left with 2 Spot instances, which would not be able to handle the steady state usage.</p><p>The option that says: <strong>To have the most cost-effective architecture, replace all of the Reserved and On-Demand EC2 instances with Spot instances across all Availability Zones. Configure an Auto Scaling group in one of the AZs for scalability</strong> is incorrect because although this is the most cost-effective solution, it is also the most unstable one considering that a Spot instance can be interrupted and hence, the availability of the website is compromised.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ec2/pricing\">https://aws.amazon.com/ec2/pricing</a></p><p><a href=\"https://aws.amazon.com/ec2/spot/\">https://aws.amazon.com/ec2/spot/</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/ec2/pricing",
      "https://aws.amazon.com/ec2/spot/",
      "https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy"
    ]
  },
  {
    "id": 5,
    "question": "<p>A visual effects studio has over 40-TB worth of video files stored in the company's on-premises tape library. The tape drives are managed by a Media Asset Management (MAM) solution. The video files contain a variety of footage which includes faces, objects, sceneries, cars, and many others. The company wants to automatically build a metadata library for the video files based on these objects. This will then be used as a catalog for the search feature of the MAM solution. The company already has a catalog of people’s photos and names that appeared on the video footage. The company wants to migrate all the video files of the MAM solution to AWS so a Direct Connect connection was provisioned from the on-premises data center to AWS to facilitate this.</p><p>Which of the following is the MOST suitable implementation that will meet the company's requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a stream in Amazon Kinesis Video Streams that will ingest the videos from the MAM system and store the videos to an Amazon S3 bucket. Configure the MAM solution to stream the videos into Kinesis Video Streams. Use Amazon Rekognition to build a collection based on the videos by using the catalog of people’s faces and names. Set up a stream consumer that will retrieve the generated metadata and then push it to the MAM solution search catalog.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Provision an AWS Storage Gateway – file gateway appliance on the on-premises data center. Configure the MAM solution to extract the video files from the current tape archives and move them to the file gateway share which is then synced to Amazon S3. Use Amazon Rekognition to build a collection based on the videos by using the catalog of people’s faces and names. Create an AWS Lambda function that will invoke Rekognition to pull the video files from the S3 bucket, retrieve the generated metadata and then push it to the MAM solution search catalog.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Securely upload the files to an Amazon S3 bucket using AWS Transfer for SFTP. Create an Amazon EC2 instance that will run GluonCV libraries to generate metadata information from the video files in the S3 bucket. Store the catalog of people’s faces and names in the Amazon EBS volume to be used by GluonCV. After processing the videos, push the generated metadata to the MAM solution search catalog.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the MAM solution to extract the video files from the current tape archives and move them to an Amazon S3 bucket using AWS DataSync. Use an Amazon SageMaker Jupyter notebook instance to build a collection based on the videos by using the catalog of people’s faces and names. Create an AWS Lambda function that will invoke Amazon SageMaker to pull the video files from the S3 bucket, retrieve the generated metadata, and then push it to the MAM solution search catalog.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Storage Gateway</strong> connects an on-premises software appliance with cloud-based storage to provide seamless integration with data security features between your on-premises IT environment and the AWS storage infrastructure. You can use the service to store data in the AWS Cloud for scalable and cost-effective storage that helps maintain data security.</p><p>AWS Storage Gateway offers file-based, volume-based, and tape-based storage solutions. A file gateway supports a file interface into Amazon Simple Storage Service (Amazon S3) and combines a service and a virtual software appliance. By using this combination, you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB). A file gateway simplifies file storage in Amazon S3, integrates with existing applications through industry-standard file system protocols, and provides a cost-effective alternative to on-premises storage. It also provides low-latency access to data through transparent local caching.</p><p><img src=\"https://media.tutorialsdojo.com/sap_file_gateway_how_it_works.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_file_gateway_how_it_works.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Amazon Rekognition</strong> makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use. With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos, as well as detect any inappropriate content.</p><p>Amazon Rekognition also provides highly accurate facial analysis and facial search capabilities that you can use to detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.</p><p>Therefore, the correct answer is: <strong>Provision an AWS Storage Gateway – file gateway appliance on the on-premises data center. Configure the MAM solution to extract the video files from the current tape archives and move them to the file gateway share which is then synced to Amazon S3. Use Amazon Rekognition to build a collection based on the videos by using the catalog of people’s faces and names. Create an AWS Lambda function that will invoke Rekognition to pull the video files from the S3 bucket, retrieve the generated metadata and then push it to the MAM solution search catalog.</strong></p><p>The option that says:<strong> Configure the MAM solution to extract the video files from the current tape archives and move them to an Amazon S3 bucket using AWS DataSync. Use an Amazon SageMaker Jupyter notebook instance to build a collection based on the videos by using the catalog of people’s faces and names. Create an AWS Lambda function that will invoke Amazon SageMaker to pull the video files from the S3 bucket, retrieve the generated metadata and then push it to the MAM solution search catalog</strong> is incorrect. An Amazon SageMaker notebook instance is simply a machine learning (ML) compute instance running the Jupyter Notebook App. It is not capable of doing image detection or recognition. Jupyter notebooks are primarily used to prepare your data and write code to train models. A better solution for this is to use Amazon Rekognition.</p><p>The option that says: <strong>Create a stream in Amazon Kinesis Video Streams that will ingest the videos from the MAM system and store the videos to an Amazon S3 bucket. Configure the MAM solution to stream the videos into Kinesis Video Streams. Use Amazon Rekognition to build a collection based on the videos by using the catalog of people’s faces and names. Set up a stream consumer that will retrieve the generated metadata and then push it to the MAM solution search catalog</strong> is incorrect. This is not cost-effective and will require more changes to the existing MAM solution. In addition, it is not stated in the scenario that the MAM solution supports video streaming directly to Kinesis Video Stream. Most of the time, you need to set up a custom Kinesis Video Streams Producer client in order to send data to a Kinesis Video Stream.</p><p>The option that says:<strong> Securely upload the files to an Amazon S3 bucket using AWS Transfer for SFTP. Create an Amazon EC2 instance that will run GluonCV libraries to generate metadata information from the video files in the S3 bucket. Store the catalog of people’s faces and names in the Amazon EBS volume to be used by GluonCV. After processing the videos, push the generated metadata to the MAM solution search catalog </strong>is incorrect. Uploading files to Amazon S3 is secured by default because it is using HTTPS. Unless there is a requirement for using the SFTP protocol, AWS Transfer for SFTP is not recommended for this scenario. This is not cost-effective and will require more management overhead in maintaining and configuring the GluonCV libraries on the EC2 instance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html</a></p><p><a href=\"https://aws.amazon.com/rekognition/\">https://aws.amazon.com/rekognition/</a></p><p><a href=\"https://aws.amazon.com/rekognition/media-analysis/\">https://aws.amazon.com/rekognition/media-analysis/</a></p><p><br></p><p><strong>Check out these Amazon Rekognition and Storage Gateway Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-rekognition/?src=udemy\">https://tutorialsdojo.com/amazon-rekognition/</a></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html",
      "https://aws.amazon.com/rekognition/",
      "https://aws.amazon.com/rekognition/media-analysis/",
      "https://tutorialsdojo.com/amazon-rekognition/?src=udemy",
      "https://tutorialsdojo.com/aws-storage-gateway/?src=udemy"
    ]
  },
  {
    "id": 6,
    "question": "<p>An Amazon partner company plans to host its application on a fleet of Amazon EC2 instances in an Auto Scaling group on a public subnet inside a VPC. A single security group is associated with all the EC2 instances. On a private subnet in the same region, an Amazon Aurora MySQL DB Cluster is created to be accessed by the application. A different security group is associated with the DB Cluster. The solutions architect has been tasked to provide access from the application to the DB Cluster.</p><p>Which of the following options is the recommended implementation to meet the application requirements while providing the least-privilege permissions? (Select TWO.)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>On the Amazon EC2 instances’ security group, create an outbound rule with the destination as the DB cluster’s security group using the default Aurora port 3306.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>On the Amazon EC2 instances’ security group, create an inbound rule with the source as the DB cluster’s security group using the default Aurora port 3306.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>To restrict communication in a different subnet, update the inbound Network Access Control List (NACL) of the private subnet to allow access from the public subnet CIDR with default Aurora port 3306.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>On the Amazon Aurora cluster’s security group, create an inbound rule with the source as the Amazon EC2 instances’ security group using the default Aurora port 3306.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>On the Amazon Aurora cluster’s security group, create an outbound rule with the destination as the Amazon EC2 instances’ security group using the default Aurora port 3306.</p>",
        "correct": false
      },
      {
        "id": 6,
        "answer": "<p>To restrict communication in a different subnet, update the outbound Network Access Control List (NACL) of the public subnet to allow access to the private subnet CIDR with the default Aurora port 3306.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>A <strong>security group</strong> controls the traffic that is allowed to reach and leave the resources that it is associated with. For example, after you associate a security group with an EC2 instance, it controls the inbound and outbound traffic for the instance.</p><p>When you create a VPC, it comes with a default security group. You can create additional security groups for each VPC. You can associate a security group only with resources in the VPC for which it is created. For each security group, you add rules that control the traffic based on protocols and port numbers. There are separate sets of rules for inbound traffic and outbound traffic.</p><p>The rules of a security group control the inbound traffic that's allowed to reach the resources that are associated with the security group. The rules also control the outbound traffic that's allowed to leave them.</p><p>You can add or remove rules for a security group (also referred to as authorizing or revoking inbound or outbound access). A rule applies either to inbound traffic (ingress) or outbound traffic (egress). You can grant access to a specific source or destination.</p><p>Security groups are <strong>stateful</strong>. For example, if you send a request from an instance, the response traffic for that request is allowed to reach the instance regardless of the inbound security group rules. Responses to allowed inbound traffic are allowed to leave the instance, regardless of the outbound rules.</p><p><img src=\"https://media.tutorialsdojo.com/sap_vpc_sec_group_ec2_rds.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_vpc_sec_group_ec2_rds.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The option that says: <strong>On the Amazon EC2 instances’ security group, create an outbound rule with the destination as the DB cluster’s security group using the default Aurora port 3306</strong> is correct. The application on the EC2 instance will initiate the connection to the DB cluster, thus, an outbound rule is needed on the instances security group with the destination set as the DB cluster security group.</p><p>The option that says: <strong>On the Amazon Aurora cluster’s security group, create an inbound rule with the source as the Amazon EC2 instances’ security group using the default Aurora port 3306</strong> is correct. The DB cluster will be receiving requests from the EC2 instances, thus, an inbound rule is needed, and the source is the EC2 instances security group.</p><p>The option that says: <strong>On the Amazon EC2 instances’ security group, create an inbound rule with the source as the DB cluster’s security group using the default Aurora port 3306</strong> is incorrect. The application on the EC2 instance will initiate the connection to the DB cluster, thus, an outbound rule is needed on the instances security group.</p><p>The option that says: <strong>On the Amazon Aurora cluster’s security group, create an outbound rule with the destination as the Amazon EC2 instances’ security group using the default Aurora port 3306</strong> is incorrect. You don't need to add an outbound rule on the DB security because the DB clusters do not initiate connections. Any request received with the inbound rule will be automatically allowed to reply to the source.</p><p>The option that says: <strong>To restrict communication in a different subnet, update the outbound Network Access Control List (NACL) of the public subnet to allow access to the private subnet CIDR with the default Aurora port 3306</strong> is incorrect. This may be possible, however, you should not use CIDR as this will open the communication from other EC2 instances on the subnet as well. It is recommended to use security group IDs for this scenario.</p><p>The option that says: <strong>To restrict communication in a different subnet, update the inbound Network Access Control List (NACL) of the private subnet to allow access from the public subnet CIDR with default Aurora port 3306</strong> is incorrect. This may be possible, however, you should not use CIDR as this will open the communication from other EC2 instances on the subnet as well. It is recommended to use security group IDs for this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html\">https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Tutorials.WebServerDB.CreateVPC.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Tutorials.WebServerDB.CreateVPC.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><br></p><p><strong>Check out this Security Group vs. NACL comparison:</strong></p><p><a href=\"https://tutorialsdojo.com/security-group-vs-nacl/?src=udemy\">https://tutorialsdojo.com/security-group-vs-nacl/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Tutorials.WebServerDB.CreateVPC.html",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy",
      "https://tutorialsdojo.com/security-group-vs-nacl/?src=udemy"
    ]
  },
  {
    "id": 7,
    "question": "<p>A financial services company uses hardware security modules (HSMs) to generate encryption master keys. Since the company application logs include personally identifiable information, encryption is required as part of regulatory compliance. The application logs are going to be stored on a central Amazon S3 bucket and should be encrypted at rest. The security team wants to use the company HSMs to generate the key material for encryption on the S3 bucket.</p><p>Which of the following options should the solutions architect implement to meet the company’s requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Request to provision an AWS Direct Connect connection from the on-premises data center to AWS VPC. Ensure that the network addresses do not overlap. Apply an Amazon S3 bucket policy on the central logging bucket to allow only encrypted object uploads. Configure the application to generate a unique KMS key for each logging event by querying the on-premises HSMs through the Direct Connection network.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a new AWS CloudHSM cluster and set it as the key material source in AWS Key Management Service (KMS) when you generate a new KMS key. Set a 1-year duration for the KMS key automatic key rotation. Apply an Amazon S3 bucket policy on the central logging bucket to require AWS KMS as the encryption source and deny unencrypted object uploads.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Using AWS CLI, create a new KMS key with no key material and use EXTERNAL as the origin of the key. Generate a key from the on-premises HSMs and import it as KMS key using the public key and import token from AWS. Apply an Amazon S3 bucket policy on the central logging bucket to require AWS KMS as the encryption source and deny unencrypted object uploads.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Using AWS CLI, create a new KMS key with AWS-provided key material and use AWS_KMS as the origin of the key. Overwrite this KMS key with a generated key from the on-premises HSMs by using the public key and import token provided by AWS. Set a 1-year duration for the KMS key automatic key rotation. Apply an Amazon S3 bucket policy on the central logging bucket to require AWS KMS as the encryption source and deny unencrypted object uploads.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>You can protect data at rest in Amazon S3 by using different modes of server-side encryption:</p><p><strong>- SSE-S3: </strong>All Amazon S3 buckets have encryption configured by default. The default option for server-side encryption is with Amazon S3 managed keys (SSE-S3). Each object is encrypted with a unique key. As an additional safeguard, SSE-S3 encrypts the key itself with a root key that it regularly rotates. SSE-S3 uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p><p><strong>- SSE-C: </strong>With server-side encryption with customer-provided keys (SSE-C), you manage the encryption keys, and Amazon S3 manages the encryption as it writes to disks and the decryption when you access your objects</p><p><strong>- SSE-KMS:</strong> Server-side encryption with AWS KMS keys (SSE-KMS) is provided through an integration of the AWS KMS service with Amazon S3. With AWS KMS, you have more control over your keys.</p><p>- <strong>DSSE-KMS: </strong>Dual-layer server-side encryption with AWS KMS keys (DSSE-KMS) is similar to SSE-KMS, but DSSE-KMS applies two individual layers of object-level encryption instead of one layer. Because both layers of encryption are applied to an object on the server side, you can use a wide range of AWS services and tools to analyze data in S3 while using an encryption method that can satisfy your compliance requirements.</p><p><strong>Server-side encryption</strong> is the encryption of data at its destination by the application or service that receives it. <strong>AWS Key Management Service (AWS KMS)</strong> is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. Amazon S3 uses server-side encryption with AWS KMS (SSE-KMS) to encrypt your S3 object data. Also, when SSE-KMS is requested for the object, the S3 checksum (as part of the object's metadata) is stored in encrypted form.</p><p>If you use KMS keys, you use AWS KMS via the AWS Management Console or AWS KMS APIs to centrally create KMS keys, define the policies that control how KMS keys can be used, and audit their usage to prove that they are being used correctly. You can use these KMS keys to protect your data in Amazon S3 buckets. When you use SSE-KMS encryption with an S3 bucket, the AWS KMS keys must be in the same Region as the bucket. If you want to use a customer managed key for SSE-KMS, create a symmetric encryption customer managed key before you configure SSE-KMS. Then, when you configure SSE-KMS for your bucket, specify the existing customer managed key.</p><p>Creating a customer managed key gives you more flexibility and control. For example, you can create, rotate, and disable customer managed keys. You can also define access controls and audit the customer managed key that you use to protect your data.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-aws-kms-key-importing-external-12-July-2024.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/public/td-aws-kms-key-importing-external-12-July-2024.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Using existing on-premises Hardware Security Modules (HSMs) to generate the encryption key materials is essential for regulatory compliance in the financial industry, where strict controls are mandated for handling sensitive data like personally identifiable information (PII) present in the application logs. By generating the key material from their trusted on-premises HSMs and importing it into AWS Key Management Service (KMS), the company can ensure that the encryption keys used to protect their data originate from a secure and compliant source. Furthermore, the capabilities of AWS KMS for key management, provides benefits such as key rotation, auditing, and integration with other AWS services. This allows the company to take advantage of the robust key management features offered by AWS while still maintaining control over the key material generation process. Lastly, by applying an S3 bucket policy that enforces encryption using the KMS key with the imported key material from the on-premises HSMs, the company can ensure that all objects uploaded to the central logging bucket are encrypted, protecting the PII data and meeting regulatory compliance requirements.</p><p>Therefore, the correct answer is: <strong>Using AWS CLI, create a new KMS key with no key material and use EXTERNAL as the origin of the key. Generate a key from the on-premises HSMs and import it as KMS key using the public key and import token from AWS. Apply an Amazon S3 bucket policy on the central logging bucket to require AWS KMS as the encryption source and deny unencrypted object uploads.</strong></p><p>The option that says:<strong> Create a new AWS CloudHSM cluster and set it as the key material source in AWS Key Management Service (KMS) when you generate a new KMS key. Set a 1-year duration for the KMS key automatic key rotation. Apply an Amazon S3 bucket policy on the central logging bucket to require AWS KMS as the encryption source and deny unencrypted object uploads</strong> is incorrect because it uses an AWS CloudHSM cluster instead of the company's existing on-premises HSMs to generate the key material.</p><p>The option that says: <strong>Request to provision an AWS Direct Connect connection from the on-premises data center to AWS VPC. Ensure that the network addresses do not overlap. Apply an Amazon S3 bucket policy on the central logging bucket to allow only encrypted object uploads. Configure the application to generate a unique KMS key for each logging event by querying the on-premises HSMs through the Direct Connection network</strong> is incorrect because it involves setting up an AWS Direct Connect connection from the on-premises data center to AWS VPC, which may not be necessary if the on-premises HSMs can securely communicate with AWS over the internet. It also requires generating a unique KMS key for each logging event by querying the on-premises HSMs through the Direct Connect network. This approach is not scalable or efficient, as it would result in a large number of KMS keys being created and managed. Lastly, it does not explicitly enforce encryption on the central Amazon S3 bucket using the KMS key with the imported key material from the on-premises HSMs.</p><p>The option that says: <strong>Using AWS CLI, create a new KMS key with AWS-provided key material and use AWS_KMS as the origin of the key. Overwrite this KMS key with a generated key from the on-premises HSMs by using the public key and import token provided by AWS. Set a 1-year duration for the KMS key automatic key rotation. Apply an Amazon S3 bucket policy on the central logging bucket to require AWS KMS as the encryption source and deny unencrypted object uploads</strong> is incorrect because it creates a KMS key with AWS-provided key material (origin set to AWS_KMS) and then overwrites this key material with the key generated from the on-premises HSMs. This approach is not recommended, as overwriting an existing key material can lead to security risks and potential data loss if not handled correctly. It is better to create a new KMS key with no key material (origin set to EXTERNAL) and import the key material from the on-premises HSMs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p><p><br></p><p><strong>Check out these AWS KMS and Amazon S3 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-key-management-service-aws-kms/?src=udemy\">https://tutorialsdojo.com/aws-key-management-service-aws-kms/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html",
      "https://tutorialsdojo.com/aws-key-management-service-aws-kms/?src=udemy",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 8,
    "question": "<p>A national library is planning to store around 50 TB of data containing all their books, articles, and other written materials in AWS. One of the requirements is to have a search feature to enable the users to look for their collection on their dynamic website.</p><p>As a Cloud Engineer, what is the most suitable solution that you should implement in AWS to satisfy the needed functionality?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Utilize CloudFormation as the deployment service to deploy the needed AWS resources such as an S3 bucket for storage, OpenSearch to provide the needed search functionality, and an EC2 instance to host their website.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS CodePipeline as the deployment service to deploy Amazon Kinesis as the storage service and an EC2 instance to serve their website.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Elastic Beanstalk as the deployment service. Deploy the needed AWS resources such as the Multi-AZ RDS for storage and an EC2 instance to host their website.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use CodeDeploy as the deployment service to deploy two S3 buckets in which the first one serves as the storage service and the second one for hosting their dynamic website. Use the native search functionality of S3 to satisfy the search feature requirement.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon S3</strong> offers a highly durable, scalable, and secure destination for backing up and archiving your critical data. You can use S3’s versioning capability to provide even further protection for your stored data. Whether you’re storing pharmaceutical or financial data, or multimedia files such as photos and videos, Amazon S3 can be used as your data lake for big data analytics.</p><p><strong>Amazon OpenSearch Service</strong> is a fully managed service that enables interactive search, real-time analytics, and logging on large datasets. It is ideal for providing search functionality for applications like a dynamic website that needs to index and query vast amounts of data, such as books and articles in this case. OpenSearch supports full-text search, faceting, and filtering, making it a perfect match for the library’s requirement to search their collection.</p><p><strong>Amazon Elastic Compute Cloud (Amazon EC2)</strong> provides scalable computing capacity in the Amazon Web Services (AWS) Cloud. Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic.</p><p><img src=\"https://media.tutorialsdojo.com/public/Amazon-OpenSearch-Service-sample-30sept2024.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/public/Amazon-OpenSearch-Service-sample-30sept2024.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Amazon S3 is great for storage, but it lacks an effective search feature. Hence, it is better to use a service like Amazon OpenSearch Service to fulfill the requirements in the scenario.</p><p>Therefore, the correct answer is: <strong>Utilize CloudFormation as the deployment service to deploy the needed AWS resources such as an S3 bucket for storage, OpenSearch to provide the needed search functionality, and an EC2 instance to host their website.</strong></p><p>The option that says: <strong>Use Elastic Beanstalk as the deployment service. Deploy the needed AWS resources such as the Multi-AZ RDS for storage and an EC2 instance to host their website</strong> is incorrect. You can simply use RDS as your database for the application; however, this will be very expensive. It is recommended to use Amazon OpenSearch service instead to have a cost-effective search solution for the website application.</p><p>The option that says: <strong>Use AWS CodePipeline as the deployment service to deploy Amazon Kinesis as the storage service and an EC2 instance to serve their website</strong> is incorrect. Although AWS CodePipeline is useful for automating release pipelines, it's not designed to handle the complexities of deploying and managing applications. Additionally, though Amazon Kinesis is typically great for real-time data streaming, it's not an ideal long-term storage service.</p><p>The option that says: <strong>Use CodeDeploy as the deployment service to deploy two S3 buckets in which the first one serves as the storage service and the second one for hosting their dynamic website. Use the native search functionality of S3 to satisfy the search feature requirement</strong> is incorrect. You cannot host the dynamic application on Amazon S3, and there is no built-in search feature on Amazon S3 that will satisfy the application requirements.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html\">https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html</a></p><p><a href=\"https://aws.amazon.com/what-is-cloud-object-storage/\">https://aws.amazon.com/what-is-cloud-object-storage/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html</a></p><p><br></p><p><strong>Check out this Amazon OpenSearch Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-opensearch-service/?src=udemy\">https://tutorialsdojo.com/amazon-opensearch-service/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html",
      "https://aws.amazon.com/what-is-cloud-object-storage/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html",
      "https://tutorialsdojo.com/amazon-opensearch-service/?src=udemy"
    ]
  },
  {
    "id": 9,
    "question": "<p>A company has a web service portal on which users can perform read and write operations to its semi-structured data. The company wants to refactor the current application and leverage AWS-managed services to have more scalability and higher availability. To ensure optimal user experience, the service is expected to respond to short but significant system load spikes. The service must be highly available and fault-tolerant in the event of a regional AWS failure.</p><p>Which of the following options is the suitable solution to meet the company's requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a primary Amazon S3 bucket to store the semi-structured data. Enable S3 Cross-Region Replication to sync objects to the backup region. Create an Amazon API Gateway and AWS Lambda-based web service on each region, and set them as the origin for the two Amazon CloudFront distributions. Add the company’s domain as an alternate name on the CloudFront distributions. Create Amazon Route 53 Alias records pointed to each distribution using a failover routing policy.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon Aurora global database to store the semi-structured data in two Regions. Configure Auto Scaling replicas on both regions. Run the web service on an Auto Scaling group of Amazon EC2 instances on both regions using the user data script to download the application code. Place each Auto Scaling group behind their own Application Load Balancer (ALB). Create a single Amazon Route 53 Alias record pointed to each ALB in using a multi-value answer routing policy with health checks enabled.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use DocumentDB to store the semi-structured data. Create an edge-optimized Amazon API Gateway and AWS Lambda-based web service, and set it as the origin of a global Amazon CloudFront distribution. Add the company’s domain as an alternate name on the CloudFront distribution. Create an Amazon Route 53 Alias record pointed to the CloudFront distribution.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon DynamoDB global table to store the semi-structured data on two Regions. Use on-demand capacity mode to allow DynamoDB scaling. Run the web service on an Auto Scaling Amazon ECS Fargate cluster on each region. Place each Fargate cluster behind their own Application Load Balancer (ALB). Create Amazon Route 53 Alias records pointed to each ALB in using latency routing policy with health checks enabled.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon DynamoDB</strong> is a fast, fully-managed NoSQL database service that makes it simple and cost effective to store and retrieve any amount of data, and serve any level of request traffic. DynamoDB helps offload the administrative burden of operating and scaling a highly-available distributed database cluster. This storage alternative meets the latency and throughput requirements of highly demanding applications by providing single-digit millisecond latency and predictable performance with seamless throughput and storage scalability.</p><p>DynamoDB stores structured data in tables, indexed by primary key, and allows low-latency read and write access to items ranging from 1 byte up to 400 KB. DynamoDB supports three data types (number, string, and binary), in both scalar and multi-valued sets. It supports document stores such as JSON, XML, or HTML in these data types. Tables do not have a fixed schema, so each data item can have a different number of attributes. The primary key can either be a single-attribute hash key or a composite hash-range key.</p><p><strong>Global tables</strong> build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_global_table.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_global_table.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>DynamoDB global tables are ideal for massively scaled applications with globally dispersed users. In such an environment, users expect very fast application performance. Global tables provide automatic multi-active replication to AWS Regions worldwide. They enable you to deliver low-latency data access to your users no matter where they are located.</p><p><strong>AWS Fargate</strong> is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design. Fargate allocates the right amount of compute, eliminating the need to choose instances and scale cluster capacity. Fargate runs each task or pod in its own kernel providing the tasks and pods their own isolated compute environment.</p><p>Therefore, the correct answer is: <strong>Create an Amazon DynamoDB global table to store the semi-structured data on two Regions. Use on-demand capacity mode to allow DynamoDB scaling. Run the web service on an Auto Scaling Amazon ECS Fargate cluster on each region. Place each Fargate cluster behind their own Application Load Balancer (ALB). Create Amazon Route 53 Alias records pointed to each ALB in using latency routing policy with health checks enabled. </strong>DynamoDB global table offers a multi-active database on multiple regions. AWS Fargate clusters scale up or down significantly faster than EC2 instances because they are only containers and can be provisioned quickly. Latency routing policy with health checks enabled ensures that users are sent to the fastest healthy region based on their location.</p><p>The option that says: <strong>Use DocumentDB to store the semi-structured data. Create an edge-optimized Amazon API Gateway and AWS Lambda-based web service, and set it as the origin of a global Amazon CloudFront distribution. Add the company’s domain as an alternate name on the CloudFront distribution. Create an Amazon Route 53 Alias record pointed to the CloudFront distribution</strong> is incorrect. DocumentDb does not offer a native cross-region replication or multi-region operation. You can only automate snapshots to another region which can take some time to restore in the event of regional failure.</p><p>The option that says: <strong>Create a primary Amazon S3 bucket to store the semi-structured data. Enable S3 Cross-Region Replication to sync objects to the backup region. Create an Amazon API Gateway and AWS Lambda-based web service on each region, and set them as the origin for the two Amazon CloudFront distributions. Add the company’s domain as an alternate name on the CloudFront distributions. Create Amazon Route 53 Alias records pointed to each distribution using a failover routing policy</strong> is incorrect. Although you can store structured and unstructured data, there is a delay of up to 15 minutes in Cross-Region Replication. In the event of a failover, some data might not have been replicated yet on the secondary region.</p><p>The option that says: <strong>Create an Amazon Aurora global database to store the semi-structured data in two Regions. Configure Auto Scaling replicas on both regions. Run the web service on an Auto Scaling group of Amazon EC2 instances on both regions using the user data script to download the application code. Place each Auto Scaling group behind their own Application Load Balancer (ALB). Create a single Amazon Route 53 Alias record pointed to each ALB in using a multi-value answer routing policy with health checks enabled</strong> is incorrect. Scaling EC2 instances can take a few minutes because the EBS volumes need to be provisioned and the OS needs to load along with the user script. This scaling time might not be quick enough for the expected short but significant spikes on the system load.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/big-data-analytics-options/amazon-dynamodb.html\">https://docs.aws.amazon.com/whitepapers/latest/big-data-analytics-options/amazon-dynamodb.html</a></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html</a></p><p><a href=\"https://aws.amazon.com/fargate/\">https://aws.amazon.com/fargate/</a></p><p><br></p><p><strong>Check out these Amazon DynamoDB and AWS Fargate Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><a href=\"https://tutorialsdojo.com/aws-fargate/?src=udemy\">https://tutorialsdojo.com/aws-fargate/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/big-data-analytics-options/amazon-dynamodb.html",
      "https://aws.amazon.com/dynamodb/global-tables/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html",
      "https://aws.amazon.com/fargate/",
      "https://tutorialsdojo.com/amazon-dynamodb/?src=udemy",
      "https://tutorialsdojo.com/aws-fargate/?src=udemy"
    ]
  },
  {
    "id": 10,
    "question": "<p>A leading insurance firm has several new members in its development team. The solutions architect was instructed to provision access to certain IAM users who perform application development tasks in the VPC. The access should allow the users to create and configure various AWS resources such as deploying Windows EC2 servers. In addition, the users should be able to see the permissions in AWS Organizations to view information about the user's organization, including the master account email and organization limitations.</p><p>Which of the following should the solutions architect implement to follow the standard security advice of granting the least privilege?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Attach the <code>AdministratorAccess</code> AWS managed policy to the IAM users.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a new IAM role and attach the <code>AdministratorAccess</code> AWS managed policy to it. Assign the IAM Role to the IAM users.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a new IAM role and attach the <code>SystemAdministrator</code> AWS managed policy to it. Assign the IAM Role to the IAM users.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Attach the <code>PowerUserAccess</code> AWS managed policy to the IAM users.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p><strong>AWS managed policies</strong> for job functions are designed to closely align to common job functions in the IT industry. You can use these policies to easily grant the permissions needed to carry out the tasks expected of someone in a specific job function. These policies consolidate permissions for many services into a single policy that's easier to work with than having permissions scattered across many policies.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_iam_managed_policies.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_iam_managed_policies.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>There are a lot of available AWS Managed Policies that you can directly attach to your IAM Users, such as Administrator, Billing, Database Administrator, Data Scientist, Developer Power User, Network Administrator, Security Auditor, System Administrator and many others.</p><p>For Administrators, you can use the AWS managed policy name: <strong>AdministratorAccess</strong> if you want to provision full access to a specific IAM User. This will enable the user to delegate permissions to every service and resource in AWS as this policy grants all actions for all AWS services and for all resources in the account.</p><p>For Developer Power Users, you can use the AWS managed policy name: <strong>PowerUserAccess</strong> if you have users who perform application development tasks. This policy will enable them to create and configure resources and services that support AWS aware application development. The first statement of this policy uses the <em>NotAction</em> element to allow all actions for all AWS services and for all resources except AWS Identity and Access Management and AWS Organizations. The second statement grants IAM permissions to create a service-linked role. This is required by some services that must access resources in another service, such as an Amazon S3 bucket. It also grants Organizations permissions to view information about the user's organization, including the master account email and organization limitations.</p><p>Therefore, the correct answer is: <strong>Attach the </strong><code><strong>PowerUserAccess</strong></code><strong> AWS managed policy to the IAM users.</strong></p><p>The options that say: <strong>Attach the AdministratorAccess AWS managed policy to the IAM users </strong>and <strong>Create a new IAM role and attach the </strong><code><strong>AdministratorAccess</strong></code><strong> AWS managed policy to it. Assign the IAM Role to the IAM users</strong> are incorrect. Although an AdministratorAccess policy can meet the requirement, it is more suitable to attach a PowerUserAccess to the IAM users since this policy can provide the required access. Take note that you have to follow the standard security best practice of granting the least privilege. In addition, a managed policy can be directly attached to your IAM Users, which is one of the reasons why the latter option is incorrect.</p><p>The option that says: <strong>Create a new IAM role and attach the </strong><code><strong>SystemAdministrator</strong></code><strong> AWS managed policy to it. Assign the IAM Role to the IAM users </strong>is incorrect because the SystemAdministrator managed policy does not have AWS Organizations permissions to view information about the user's organization such as the master account email or the organization limitations. In this scenario, you have to use PowerUserAccess instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html?shortFooter=true#access_policies_job-functions_create-policies\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html?shortFooter=true#access_policies_job-functions_create-policies</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html?shortFooter=true#access_policies_job-functions_create-policies",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy"
    ]
  },
  {
    "id": 11,
    "question": "<p>A company plans to migrate its on-premises workload to the AWS cloud. The solutions architect has been tasked to perform a Total Cost of Ownership (TCO) analysis and prepare a cost-optimized migration plan for the systems hosted in your on-premises network to AWS. It is required to collect detail about configuration, usage, and behavioral data from the on-premises servers to help better understand the current workloads before doing the migration.</p><p>Which of the following option is the recommended solution that should be implemented to meet the company requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AWS SAM service to move your data to AWS which will also help you perform the TCO analysis.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the AWS Application Discovery Service to gather data about your on-premises data center and perform the TCO analysis.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use the AWS Application Migration Service (MGN) to migrate VM servers to AWS and collect the data required to complete your TCO analysis.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the AWS Migration Hub service to collect data from each server in your on-premises data center and perform the TCO analysis.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p><strong>AWS Application Discovery Service</strong> helps enterprise customers plan migration projects by gathering information about their on-premises data centers.</p><p>Planning data center migrations can involve thousands of workloads that are often deeply interdependent. Server utilization data and dependency mapping are important early first steps in the migration process. AWS Application Discovery Service collects and presents configuration, usage, and behavior data from your servers to help you better understand your workloads.</p><p><img src=\"https://media.tutorialsdojo.com/sap_application_dicovery.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_application_dicovery.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The collected data is retained in encrypted format in an AWS Application Discovery Service data store. You can export this data as a CSV file and use it to estimate the Total Cost of Ownership (TCO) of running on AWS and to plan your migration to AWS. In addition, this data is also available in AWS Migration Hub, where you can migrate the discovered servers and track their progress as they get migrated to AWS.</p><p>Therefore, the correct answer is: <strong>Use the AWS Application Discovery Service to gather data about your on-premises data center and perform the TCO analysis.</strong></p><p>The option that says: <strong>Use the AWS SAM service to move your data to AWS which will also help you perform the TCO analysis</strong> is incorrect. The AWS Serverless Application Model (AWS SAM) service is just an open-source framework that you can use to build serverless applications on AWS. It is not a suitable migration service to be used for your on-premises systems.</p><p>The option that says: <strong>Use the AWS Migration Hub service to collect data from each server in your on-premises data center and performing the TCO analysis</strong> is incorrect. AWS Migration Hub simply provides a single location to track the progress of application migrations across multiple AWS and partner solutions. Using Migration Hub just allows you to choose the AWS and partner migration tools that best fits your needs while providing visibility into the status of migrations across your portfolio of applications. Although AWS Application Discovery Service can be integrated with AWS Migration Hub, this service alone is not enough to meet the requirement in this scenario.</p><p>The option that says: <strong>Use the AWS Application Migration Service (MGN) to migrate VM servers to AWS and collect the data required to complete your TCO analysis</strong> is incorrect. Although AWS Application Migration Service is a migration tool, it is used to replicate or mirror your on-premises VMs to the AWS cloud. It does not provide a helpful dashboard on applications running on each VM, unlike AWS Application Discovery Service.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/application-discovery/\">https://aws.amazon.com/application-discovery/</a></p><p><a href=\"https://aws.amazon.com/server-migration-service/?nc=sn&amp;loc=1\">https://aws.amazon.com/server-migration-service/?nc=sn&amp;loc=1</a></p><p><a href=\"https://aws.amazon.com/cloud-migration/\">https://aws.amazon.com/cloud-migration/</a></p><p><br></p><p><strong>Check out this AWS Application Migration (MGN) Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-application-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-application-migration-service/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/application-discovery/",
      "https://aws.amazon.com/server-migration-service/?nc=sn&amp;loc=1",
      "https://aws.amazon.com/cloud-migration/",
      "https://tutorialsdojo.com/aws-application-migration-service/?src=udemy"
    ]
  },
  {
    "id": 12,
    "question": "<p>A global finance company has multiple data centers around the globe. Due to the ever-growing data that your company is storing, the solutions architect was instructed to set up a durable, cost-effective solution to archive sensitive data from the existing on-premises tape-based backup infrastructure to AWS Cloud.</p><p>Which of the following options is the recommended implementation to achieve the company requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a Tape Gateway to back up your data in Amazon S3 with point-in-time backups as tapes which will be stored in the Virtual Tape Shelf.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up a Tape Gateway to back up your data in Amazon S3 and archive it in Amazon Glacier using your existing tape-based processes.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up a File Gateway to back up your data in Amazon S3 and archive in Amazon Glacier using your existing tape-based processes.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a Stored Volume Gateway to back up your data in Amazon S3 with point-in-time backups as EBS snapshots.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p><strong>AWS Storage Gateway</strong> connects an on-premises software appliance with cloud-based storage to provide seamless integration with data security features between your on-premises IT environment and the AWS storage infrastructure. You can use the service to store data in the Amazon Web Services Cloud for scalable and cost-effective storage that helps maintain data security.</p><p>AWS Storage Gateway offers file-based file gateways (Amazon S3 File and Amazon FSx File), volume-based (Cached and Stored), and tape-based storage solutions.</p><p><strong>Tape Gateway</strong> offers a durable, cost-effective solution to archive your data in the AWS Cloud. With its virtual tape library (VTL) interface, you use your existing tape-based backup infrastructure to store data on virtual tape cartridges that you create on your tape gateway. Each tape gateway is preconfigured with a media changer and tape drives. These are available to your existing client backup applications as iSCSI devices. You add tape cartridges as you need to archive your data.</p><p><img src=\"https://media.tutorialsdojo.com/sap_storage_tape_gateway.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_storage_tape_gateway.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Set up a Tape Gateway to back up your data in Amazon S3 and archive it in Amazon Glacier using your existing tape-based processes.</strong></p><p>The option that says: <strong>Set up a Stored Volume Gateway to back up your data in Amazon S3 with point-in-time backups as EBS snapshots</strong> is incorrect. Stored Volume Gateway is not recommended for Tape archives, you should use Tape Gateway instead.</p><p>The option that says: <strong>Set up a Tape Gateway to back up your data in Amazon S3 with point-in-time backups as tapes which will be stored in the Virtual Tape Shelf</strong> is incorrect. The archival should be on Amazon Glacier for cost-effectiveness.</p><p>The option that says: <strong>Set up a File Gateway to back up your data in Amazon S3 and archive in Amazon Glacier using your existing tape-based processes</strong> is incorrect. A File Gateway is not recommended for Tape archives, you should use Tape Gateway instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#storage-gateway-vtl-concepts\">https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#storage-gateway-vtl-concepts</a></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/create-tape-gateway.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/create-tape-gateway.html</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#storage-gateway-vtl-concepts",
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html",
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/create-tape-gateway.html",
      "https://tutorialsdojo.com/aws-storage-gateway/?src=udemy"
    ]
  },
  {
    "id": 13,
    "question": "<p>A company is hosting its three-tier web application on the us-east-1 region of AWS. The web and application tiers are stateless and both are running on their own fleet of On-Demand Amazon EC2 instances, each with its respective Auto Scaling group. The database tier is running on an Amazon Aurora database with about 40 TB of data. As part of the business continuity strategy of the company, the Solutions Architect must design a disaster recovery plan in case the primary region fails. The application requires an RTO of 30 minutes and the data tier requires an RPO of 5 minutes.</p><p>Which of the following options should the Solution Architect implement to achieve the company requirements in a cost-effective manner? (Select TWO.)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Schedule a daily snapshot of the Amazon EC2 instances for the web and application tier. Copy the snapshot to the backup region. Restore the backups in case of a disaster in the primary region.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure an automated snapshot of the Amazon Aurora database every 5 minutes. Quickly restore the database on the backup region in case of a disaster in the primary region.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up a cross-Region read replica of the Amazon Aurora database to the backup region. Promote this read replica as the master database in case of a disaster in the primary region.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>For a quick recovery time, set up a hot-standby of web and application tier on the backup region. Redirect the traffic to the backup region in case of a disaster in the primary region.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use AWS Backup to create a backup job that will copy the EC2 EBS volumes and RDS data to an Amazon S3 bucket in another region. Restore the backups in case of a disaster in the primary region.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon EC2 EBS volumes</strong> are the primary persistent storage option for Amazon EC2. You can use this block storage for structured data, such as databases, or unstructured data, such as files in a file system on a volume. With Amazon EBS, you can create point-in-time snapshots of volumes, which we store for you in Amazon S3. After you create a snapshot and it has finished copying to Amazon S3, you can copy it from one AWS Region to another, or within the same Region.</p><p>Snapshots are useful if you want to back up your data and logs across different geographical locations at regular intervals. In case of disaster, you can restore your applications using point-in-time backups stored in the secondary Region. This minimizes data loss and recovery time.</p><p>You can create <strong>cross-region read replicas for Amazon Aurora</strong>. This allows you to serve read traffic from your users in different geographic regions and increases your application’s responsiveness. This feature also provides you with improved disaster recovery capabilities in case of regional disruptions. You can seamlessly migrate your database from one region to another by creating a cross-region read replica and promoting it to be the new primary database.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_cross_region_replica.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_rds_cross_region_replica.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can create an Amazon Aurora MySQL DB cluster as a read replica in a different AWS Region than the source DB cluster. You can promote an Aurora MySQL read replica to a standalone DB cluster. When you promote an Aurora MySQL read replica, its DB instances are rebooted before they become available. Typically, you promote an Aurora MySQL read replica to a standalone DB cluster as a data recovery scheme if the source DB cluster fails.</p><p>When restoring Amazon Aurora snapshots, or point-in-time restore, the restoration may take several minutes to hours. Long restore times are caused by long-running transactions in the source database at the time the backup was taken.</p><p>The option that says: <strong>Schedule a daily snapshot of the Amazon EC2 instances for the web and application tier. Copy the snapshot to the backup region. Restore the backups in case of a disaster in the primary region</strong> is correct. The web and application tiers are stateless, meaning they don’t have any important data stored on them. Therefore, copying the daily snapshot of the EC2 instance to the backup region will suffice. The RTO of 30 minutes is ample time to spawn new EC2 instances on the backup region.</p><p>The option that says: <strong>Set up a cross-Region read replica of the Amazon Aurora database to the backup region. Promote this read-replica as the master database in case of a disaster in the primary region</strong> is correct. Given that the RPO for the data tier is 5 minutes, it is better to create a cross-Region read-replica on the backup region. The primary DB instance will asynchronously replicate the data to the Read Replica. So in the event that the primary DB failed, the Read Replica contains the updated data. You can also quickly promote this as the master DB instance in case of a disaster in the primary region. You don’t have to wait for a long database snapshot restore time too, which might exceed the 30-minute RTO requirement.</p><p>The option that says: <strong>For a quick recovery time, set up a hot-standby of web and application tier on the backup region. Redirect the traffic to the backup region in case of a disaster in the primary region</strong> is incorrect. Although this is possible, this is not the most cost-effective solution as it entails a significant number of resources that are continuously running. With an RTO of 30 minutes, you can quickly restore backups of the EC2 snapshots of the web and application tier instead of running a hot-standby environment. Take note that in Disaster Recovery, a \"hot-standby\" means that the application runs in the DR region. Because it's always running, you will incur a significant amount of cost.</p><p>The option that says: <strong>Configure an automated snapshot of the Amazon Aurora database every 5 minutes. Quickly restore the database on the backup region in case of a disaster in the primary region</strong> is incorrect. Restoring 40 TB of data may not be possible if you have an RTO requirement of 30 minutes. Depending on how busy the database was during the time the snapshot was taken, the restoration process may take longer than 30 minutes to complete. Moreover, automated backups only occur once every day during the defined backup window. You can't configure it to run every 5 minutes.</p><p>The option that says: <strong>Use AWS Backup to create a backup job that will copy the EC2 EBS volumes and RDS data to an Amazon S3 bucket in another region. Restore the backups in case of a disaster in the primary region </strong>is incorrect. This may be possible, but the restoration time from the RDS backup may take more time than the required 30 minutes of RTO. The highest backup frequency in AWS Backup is every 12 hours only and not every 5-minutes. Thus, it can only provide a maximum RPO of 12 hours. A better solution is to use a Read Replica with a replication latency of only about a few minutes, providing a higher RPO.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/aurora-mysql-slow-snapshot-restore/\">https://aws.amazon.com/premiumsupport/knowledge-center/aurora-mysql-slow-snapshot-restore/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2016/06/amazon-aurora-now-supports-cross-region-replication/\">https://aws.amazon.com/about-aws/whats-new/2016/06/amazon-aurora-now-supports-cross-region-replication/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-copy-snapshot.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-copy-snapshot.html</a></p><p><br></p><p><strong>Check out these Amazon EBS and Amazon RDS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-ebs/?src=udemy\">https://tutorialsdojo.com/amazon-ebs/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/aurora-mysql-slow-snapshot-restore/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html",
      "https://aws.amazon.com/about-aws/whats-new/2016/06/amazon-aurora-now-supports-cross-region-replication/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-copy-snapshot.html",
      "https://tutorialsdojo.com/amazon-ebs/?src=udemy",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 14,
    "question": "<p>A tech company in the USA has sold millions of sensors that collect temperature information from different locations in a household. These sensors send data to the IoT application developed by the company which is hosted on the AWS cloud using the domain iot.tutorialsdojotest.com. The domain is registered using Amazon Route 53. The sensors use the MQTT protocol to connect to a custom MQTT broker which is hosted on a large Amazon EC2 instance. After processing the received IoT data, the application then sends the data to an Amazon DynamoDB table for storage.</p><p>In the past month, the MQTT broker crashed a few times because it was overloaded by the large amount of data being received. This outage caused sensor data to be lost. The management wants to improve the reliability of the IoT workflow to prevent this from happening again.</p><p>Which of the following options is the recommended solution to meet the company's requirements while being cost-effective?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Auto Scaling group of Amazon EC2 instances for the message broker. Put the Auto Scaling group behind a Network Load Balancer (NLB) since it supports TCP connection for the MQTT protocol. Update the Route 53 DNS zone record to point to the NLB with an Alias type record. Use Amazon EFS as shared storage for the Auto Scaling group to improve the durability of received data.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Leverage AWS IoT Device Management to update the firmware of the IoT devices. Push a firmware update with a random timer on each device to prevent them from sending data all at once. Set up another Amazon EC2 instance for the MQTT broker in another US region. Create a grouping of devices from IoT Device Management to send the other groups' traffic to this new instance.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS IoT Core with MQTT to create a new Data-ATS endpoint. Update the Route 53 DNS zone record to point to the new endpoint and allow the IoT devices to send data using the MQTT protocol. Create an AWS IoT rule to directly insert the data into the Amazon DynamoDB table.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create a new Data-ATS endpoint in AWS IoT Greengrass. Point the Route 53 DNS zone record to this new endpoint to allow IoT devices on the edge to send data using the MQTT protocol. Create an AWS IoT rule that will invoke a Lambda function to store the received data in the Amazon DynamoDB Table.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS IoT Core</strong> is a managed cloud service that enables connected devices to securely interact with cloud applications and other devices. AWS IoT Core can support many devices and messages and process and route those messages to AWS IoT endpoints and other devices. With AWS IoT Core, your applications can interact with all your devices even when disconnected. AWS IoT Core services connect IoT devices to AWS IoT services and other AWS services. AWS IoT Core includes the device gateway and the message broker, which connect and process messages between your IoT devices and the cloud.<br>AWS IoT lets you select the most appropriate and up-to-date technologies for your solution. To help you manage and support your IoT devices in the field, AWS IoT Core supports these protocols:</p><p>-MQTT (Message Queuing and Telemetry Transport)</p><p>-MQTT over WSS (WebSockets Secure)</p><p>-HTTPS (Hypertext Transfer Protocol - Secure)</p><p>-LoRaWAN (Long Range Wide Area Network)</p><p>The <strong>AWS IoT Core</strong> message broker supports devices and clients that use MQTT and MQTT over WSS protocols to publish and subscribe to messages. It also supports devices and clients that use the HTTPS protocol to publish messages.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_iot_core_overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_iot_core_overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The <strong>AWS IoT Core - data plane endpoints</strong> are specific to each AWS account and AWS Region. The Data-ATS endpoint sends and receives data to and from the message broker, Device Shadow, and Rules Engine components of AWS IoT.</p><p>You configure <strong>AWS IoT rules</strong> to route data from your connected things. A rule includes one or more actions that AWS IoT performs when enacting the rule. For example, you can insert data into a DynamoDB table, write data to an Amazon S3 bucket, publish to an Amazon SNS topic, or invoke a Lambda function.</p><p>Therefore, the correct answer is: <strong>Use AWS IoT Core with MQTT to create a new Data-ATS endpoint. Update the Route 53 DNS zone record to point to the new endpoint and allow the IoT devices to send data using the MQTT protocol. Create an AWS IoT rule to directly insert the data into the Amazon DynamoDB table. </strong>AWS IoT Core supports several protocols, including MQTT without the need to provision or manage servers. It can receive the data from IoT devices, and with an IoT rule, you can create an action to insert data into an Amazon DynamoDB table.</p><p>The option that says: <strong>Create a new Data-ATS endpoint in AWS IoT Greengrass. Point the Route 53 DNS zone record to this new endpoint to allow IoT devices on the edge to send data using the MQTT protocol. Create an AWS IoT rule that will invoke a Lambda function to store the received data in the Amazon DynamoDB Table</strong> is incorrect. AWS IoT Greengrass needs a client software that brings intelligence to edge devices. This setup will not improve the data broker's performance on AWS.</p><p>The option that says: <strong>Create an Auto Scaling group of Amazon EC2 instances for the message broker. Put the Auto Scaling group behind a Network Load Balancer (NLB) since it supports TCP connection for the MQTT protocol. Update the Route 53 DNS zone record to point to the NLB with an Alias type record. Use Amazon EFS as shared storage for the Auto Scaling group to improve the durability of received data</strong> is incorrect. This option may be possible; however, it may cost more for using Amazon EFS, the NLB, and multiple EC2 instances. It also increases the operational overhead for managing the instances.</p><p>The option that says: <strong>Leverage AWS IoT Device Management to update the firmware of the IoT devices. Push a firmware update with a random timer on each device to prevent them from sending data all at once. Set up another Amazon EC2 instance for the MQTT broker in another US region. Create a grouping of devices from IoT Device Management to send the other groups' traffic to this new instance</strong> is incorrect. This option may be possible; however, creating another instance in another US region will cost more. It does not mention how to handle the Route 53 record. Additionally, this solution is not easily scalable if more devices will be added in the future.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/general/latest/gr/iot-core.html\">https://docs.aws.amazon.com/general/latest/gr/iot-core.html</a></p><p><a href=\"https://docs.aws.amazon.com/iot/latest/developerguide/iot-create-rule.html\">https://docs.aws.amazon.com/iot/latest/developerguide/iot-create-rule.html</a></p><p><a href=\"https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html\">https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/general/latest/gr/iot-core.html",
      "https://docs.aws.amazon.com/iot/latest/developerguide/iot-create-rule.html",
      "https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html"
    ]
  },
  {
    "id": 15,
    "question": "<p>A photo-sharing website uses a CloudFront distribution with a default name (dtut0r1al5doj0.cloudfront.net) to distribute its static contents. It uses an ELB in front of an Auto Scaling group of Spot EC2 instances deployed across two Availability Zones. The website has a poor search ranking in Google as it doesn't use a secure HTTPS/SSL on its site.</p><p>Which of the following are valid options in order to require HTTPS for communication between the viewers and CloudFront? (Select TWO.)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the ELB to use its default SSL/TLS certificate.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure CloudFront to use its default SSL/TLS certificate by changing the <code>Viewer Protocol Policy</code> setting for one or more cache behaviors to require HTTPS communication.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use a self-signed certificate in the ELB.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a self-signed SSL/TLS certificate in the ELB which is stored in a private S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Set the <code>Viewer Protocol Policy&lt;</code> to use <code>Redirect HTTP to HTTPS</code> or <code>HTTPS Only</code>.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>If you're using the domain name that <strong>CloudFront</strong> assigned to your distribution, such as dtut0ria1sd0jo.cloudfront.net, you can change the Viewer Protocol Policy setting for one or more cache behaviors to require HTTPS communication by setting it as either <code>Redirect HTTP to HTTPS</code> or <code>HTTPS Only</code>. In that configuration, CloudFront provides its default SSL/TLS certificate.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>If your origin is an Elastic Load Balancing load balancer, you can use a certificate provided by AWS Certificate Manager (ACM). You can also use a certificate that is signed by a trusted third-party certificate authority and imported into ACM. Note that you can't use a self-signed certificate for HTTPS communication between CloudFront and your origin.</p><p>Therefore, the following options are correct:</p><p><strong>- Set the </strong><code><strong>Viewer Protocol Policy</strong></code><strong> to use </strong><code><strong>Redirect HTTP to HTTPS</strong></code><strong> or </strong><code><strong>HTTPS Only</strong></code><strong>.</strong></p><p><strong>- Configure CloudFront to use its default SSL/TLS certificate by changing the </strong><code><strong>Viewer Protocol Policy</strong></code><strong> setting for one or more cache behaviors to require HTTPS communication.</strong></p><p>The option that says: <strong>Use a self-assigned SSL/TLS certificate in the ELB which is stored in a private S3 bucket</strong> is incorrect because you don't need to add an SSL certificate if you only require HTTPS for communication between the viewers and CloudFront. You should only do this if you require HTTPS between your origin and CloudFront. In addition, you can't use a self-signed certificate in this scenario even though it is stored in a private S3 bucket. You need to use either a certificate from ACM or a third-party certificate.</p><p>The option that says: <strong>Use a self-signed certificate in the ELB</strong> is incorrect. As explained in the previous paragraph, adding an SSL certificate in the ELB is not required. Additionally, using a self-signed certificate for public websites is not recommended.</p><p>The option that says: <strong>Configure the ELB to use its default SSL/TLS certificate </strong>is incorrect. There is no default SSL certificate in ELB, unlike what we have in CloudFront (*.cloudfront.net). As previously explained, adding an SSL certificate in the ELB is not required.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html#using-https-cloudfront-to-origin-certificate\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html#using-https-cloudfront-to-origin-certificate</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html#using-https-cloudfront-to-origin-certificate",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy"
    ]
  },
  {
    "id": 16,
    "question": "<p>A company is running a financial modeling application on the AWS cloud. The application tier runs on an Auto Scaling group of Amazon EC2 instances. A separate EC2 cluster with a fixed number of instances is hosting the 200 TB of financial data in a shared file system. The application reads and processes the data on the shared filesystem to generate an overall financial report, which takes about 72 hours to complete. This whole process only needs to run at the end of each month, but the storage tier instances are running continuously to retain all the data in the shared file system.</p><p>As the storage tier takes up a large percentage of operational costs, the management wants to reduce the cost of the storage tier while maintaining the high-performance access needed by the application during its 72-hour run.</p><p>Which of the following options should the solutions architect implement that will have the largest overall cost reduction?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>For the data tier, create an Amazon EFS filesystem and move the objects of the existing shared file system to it. Since the data will only be used once a month, use EFS Standard–Infrequent Access (IA) class to save costs. Mount this filesystem as shared storage for the application tier EC2 instance for the duration of the job.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>For the data tier, create an Amazon S3 bucket and move the objects of the existing shared file system to it. Use S3 Glacier Instant Retrieval Storage class to save costs. Use lazy-loading on an Amazon FSx for Lustre filesystem to import the contents of the S3 bucket. Use this filesystem as shared storage for the application tier EC2 instances for the duration of the job and delete it once the job is completed.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>For the data tier, create a large EBS volume with Multi-Attach enabled. Move the objects of the existing shared file system to it. This will save cost since only 1 EBS volume needs to be mounted on all application tier EC2 instances for the duration of the job. The EBS volume will be retained once the job is completed.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>For the data tier, create an Amazon S3 bucket and move the objects of the existing shared file system to it. Use S3 Intelligent-Tiering Storage class to save costs. Use lazy-loading on an Amazon FSx for Lustre filesystem to import the contents of the S3 bucket. Use this filesystem as shared storage for the application tier EC2 instances for the duration of the job and delete it once the job is completed.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon FSx for Lustre</strong> is a large-scale, distributed parallel file system powering the workloads of most of the largest supercomputers. It is popular among AWS customers for high-performance computing workloads, such as meteorology, life science, and engineering simulations. It is also used in media and entertainment, as well as the financial services industry. If your workloads require fast, POSIX-compliant file system access to your S3 buckets, then you can use FSx for Lustre to link your S3 buckets to a file system and keep data synchronized between the file system and S3 in both directions.</p><p>Amazon FSx for Lustre offers a choice of solid-state drive (SSD) and hard disk drive (HDD) storage types that are optimized for different data processing requirements.</p><p><strong>FSx for Lustre integrates with Amazon S3</strong>, making it easier for you to process cloud datasets using the Lustre high-performance file system. When linked to an Amazon S3 bucket, an FSx for Lustre file system transparently presents S3 objects as files. Amazon FSx imports listings of all existing files in your S3 bucket at file system creation. Amazon FSx can also import listings of files added to the data repository after the file system is created. You can set the import preferences to match your workflow needs. The file system also makes it possible for you to write file system data back to S3. Data repository tasks simplify the transfer of data and metadata between your FSx for Lustre file system and its durable data repository on Amazon S3.</p><p><img src=\"https://media.tutorialsdojo.com/sap_fsx_s3_source.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_fsx_s3_source.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>For the data tier, create an Amazon S3 bucket and move the objects of the existing shared file system to it. Use S3 Intelligent-Tiering Storage class to save costs. Use lazy-loading on an Amazon FSx for Lustre filesystem to import the contents of the S3 bucket. Use this filesystem as shared storage for the application tier EC2 instances for the duration of the job and delete it once the job is completed.</strong> The FSx for Lustre filesystem can temporarily load the data from S3 and share it among the application tier instances. This is cheaper as we can use S3 for long-term storage and FSx for Lustre for the temporary shared file system.</p><p>The option that says: <strong>For the data tier, create an Amazon EFS filesystem and move the objects of the existing shared file system to it. Since the data will only be used once a month, use EFS Standard–Infrequent Access (IA) class to save costs. Mount this filesystem as shared storage for the application tier EC2 instance for the duration of the job</strong> is incorrect. Amazon EFS Filesystem costs significantly more than standard Amazon S3 storage costs.</p><p>The option that says: <strong>For the data tier, create an Amazon S3 bucket and move the objects of the existing shared file system to it. Use S3 Glacier Instant Retrieval Storage class to save costs. Use lazy-loading on an Amazon FSx for Lustre filesystem to import the contents of the S3 bucket. Use this filesystem as shared storage for the application tier EC2 instances for the duration of the job and delete it once the job is completed</strong> is incorrect. S3 Glacier is recommended for long-lived archive data accessed once a quarter. Also, S3 Glacier has additional charges for retrieving data from cold storage.</p><p>The option that says: <strong>For the data tier, create a large EBS volume with Multi-Attach enabled. Move the objects of the existing shared file system to it. This will save cost since only 1 EBS volume needs to be mounted on all application tier EC2 instances for the duration of the job. The EBS volume will be retained once the job is completed</strong> is incorrect. A multi-attached EBS volume can only be attached in 16 EC2 instances simultaneously. Also, it is only supported for Provisioned IOPS SSD volumes which cost significantly more.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/storage/persistent-storage-for-high-performance-workloads-using-amazon-fsx-for-lustre/\">https://aws.amazon.com/blogs/storage/persistent-storage-for-high-performance-workloads-using-amazon-fsx-for-lustre/</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/enhanced-amazon-s3-integration-for-amazon-fsx-for-lustre/\">https://aws.amazon.com/blogs/aws/enhanced-amazon-s3-integration-for-amazon-fsx-for-lustre/</a></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html\">https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html</a></p><p><br></p><p><strong>Check out these Amazon S3 and Amazon FSx for Lustre Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-efs-vs-amazon-fsx-for-windows-vs-amazon-fsx-for-lustre/?src=udemy\">https://tutorialsdojo.com/amazon-efs-vs-amazon-fsx-for-windows-vs-amazon-fsx-for-lustre/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/storage/persistent-storage-for-high-performance-workloads-using-amazon-fsx-for-lustre/",
      "https://aws.amazon.com/blogs/aws/enhanced-amazon-s3-integration-for-amazon-fsx-for-lustre/",
      "https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy",
      "https://tutorialsdojo.com/amazon-efs-vs-amazon-fsx-for-windows-vs-amazon-fsx-for-lustre/?src=udemy"
    ]
  },
  {
    "id": 17,
    "question": "<p>An analytics company provides big data services to various clients worldwide. For performance-testing activities, a Big Data Analytics application is using an Elastic MapReduce cluster which will only be run once. The cluster is designed to ingest 20 TB of data with a total of 30 EC2 instances and is expected to run for about 48 hours.</p><p>Which of the following options is the most cost-effective architecture to implement for this scenario without sacrificing data integrity?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "For both the master and core nodes, use Reserved EC2 instances. For the task nodes, use Spot EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "For both the master and core nodes, use On-Demand EC2 instances. For the task nodes, use Spot EC2 instances.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use On-Demand instances for the core nodes. Use Reserved EC2 instances for the master node and Spot EC2 instances for the task nodes.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use a combination of On-Demand instance and Spot Instance types for both the master and core nodes. Use On-Demand EC2 instances for the task nodes.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>When you set up a cluster, you choose a purchasing option for EC2 instances. You can choose On-Demand Instances, Spot Instances, or both. With On-Demand Instances, you pay for compute capacity by the hour. Optionally, you can have these On-Demand Instances use Reserved Instance or Dedicated Instance purchasing options.</p><p>With Reserved Instances, you make a one-time payment for an instance to reserve capacity. Dedicated Instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. After you purchase a Reserved Instance, if all of the following conditions are true, Amazon EMR uses the Reserved Instance when a cluster launches:</p><p>- An On-Demand Instance is specified in the cluster configuration that matches the Reserved Instance specification.</p><p>- The cluster is launched within the scope of the instance reservation (the Availability Zone or Region).</p><p>- The Reserved Instance capacity is still available.</p><p><strong>Spot Instances</strong> in <strong>Amazon EMR</strong> provide an option for you to purchase Amazon EC2 instance capacity at a reduced cost as compared to On-Demand purchasing. The disadvantage of using Spot Instances is that instances may terminate unpredictably as prices fluctuate.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_emr_kerberos.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_emr_kerberos.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>With the uniform instance group configuration, you can have up to a total of 48 task instance groups. The ability to add instance groups in this way allows you to mix EC2 instance types and pricing options, such as On-Demand Instances and Spot Instances. This gives you the flexibility to respond to workload requirements in a cost-effective way.</p><p>With the instance fleet configuration, the ability to mix instance types and purchasing options are built-in, so there is only one task instance fleet.</p><p>Because Spot Instances are often used to run task nodes, Amazon EMR has default functionality for scheduling YARN jobs so that running jobs do not fail when task nodes running on Spot Instances are terminated. Amazon EMR does this by allowing application master processes to run only on core nodes. The application master process controls running jobs and needs to stay alive for the life of the job.</p><p>On-Demand instances cost more than Spot instances, which is why it is better to use Spot Instances for the task nodes to save costs. However, using Spot Instances for the master and core nodes is not recommended since a Spot Instance can potentially become unavailable during the 48-hour process.</p><p>Hence, this option is the correct answer: <strong>For both the master and core nodes, use On-Demand EC2 instances. For the task nodes, use Spot EC2 instances.</strong></p><p>Remember that in this scenario, the processing will only take 48 hours. That is why it is not suitable to use Reserved Instances since the minimum reservation for this instance purchasing option is 1 year.</p><p>Therefore, the following options are incorrect:</p><p><strong>- Use On-Demand instances for the core nodes. Use Reserved EC2 instances for the master node and Spot EC2 instances for the task nodes.</strong></p><p><strong>- For both the master and core nodes, use Reserved EC2 instances. For the task nodes, use Spot EC2 instances.</strong></p><p>The option that says: <strong>Use a combination of On-Demand instance and Spot Instance types for both the master and core nodes. Use On-Demand EC2 instances for the task nodes is</strong> incorrect because using On-Demand EC2 instances for the task nodes is not cost-efficient. A better setup is to use Spot Instances for the task nodes.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11\">https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11</a></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html</a></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html</a></p><p><br></p><p><strong>Check out this Amazon EMR Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-emr/?src=udemy\">https://tutorialsdojo.com/amazon-emr/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11",
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html",
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html",
      "https://tutorialsdojo.com/amazon-emr/?src=udemy"
    ]
  },
  {
    "id": 18,
    "question": "<p>The European Organization for Nuclear Research, also known as CERN, is a research organization that operates the largest particle accelerator in the world and generates terabytes of experimental data every day. A group of data scientists is planning to use an Elastic MapReduce cluster for their data analysis, which will only be run once. The cluster is designed to ingest 300 TB of data with a total of 200 EC2 instances and is expected to run for about 8 hours. The resulting data set must be stored temporarily until it is permanently stored in their AWS Redshift database.</p><p>Which of the following options is the best and most cost-effective solution to satisfy the above requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Reserved EC2 instances for both the master and core nodes and use Spot EC2 instances for the task nodes.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use On-Demand EC2 instances for both the master and core nodes and use Spot EC2 instances for the task nodes.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Reserved EC2 instances for the master node; On-Demand instances for the core nodes; and use Spot EC2 instances for the task nodes.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use a combination of On-Demand instance and Spot instance types for both the master and core nodes. Use Spot EC2 instances for the task nodes.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In this scenario, the scientists are doing a one-time processing of their data in 8 hours. Hence, a Reserved instance is not a suitable type to use as this project is not for the long term.</p><p><strong>Amazon EMR</strong> provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data using EC2 instances. When using Amazon EMR, you don’t need to worry about installing, upgrading, and maintaining Spark software (or any other tool from the Hadoop framework). You also don’t need to worry about installing and maintaining underlying hardware or operating systems. Instead, you can focus on your business applications and use Amazon EMR to remove the undifferentiated heavy lifting.</p><p>The central component of Amazon EMR is the <strong><em>cluster</em></strong>. A cluster is a collection of Amazon Elastic Compute Cloud (Amazon EC2) instances. Each instance in the cluster is called a <strong><em>node</em></strong>. Each node has a role within the cluster, referred to as the <strong><em>node</em></strong><em> type</em>. Amazon EMR also installs different software components on each node type, giving each node a role in a distributed application like Apache Hadoop.</p><p>The node types in Amazon EMR are as follows:</p><p><strong>Master node</strong>: A node that manages the cluster by running software components to coordinate the distribution of data and tasks among other nodes for processing. The master node tracks the status of tasks and monitors the health of the cluster. Every cluster has a master node, and it's possible to create a single-node cluster with only the master node.</p><p><strong>Core node</strong>: A node with software components that run tasks and store data in the Hadoop Distributed File System (HDFS) on your cluster. Multi-node clusters have at least one core node.</p><p><strong>Task node</strong>: A node with software components that only runs tasks and does not store data in HDFS. Task nodes are optional.</p><p>For optimizing costs and performance in choosing the instance types:</p><p><strong>Master node:</strong> Unless your cluster is very short-lived and the runs are cost-driven, avoid running your Master node on a Spot Instance. We suggest this because a Spot interruption on the Master node terminates the entire cluster. Alternatively to On-Demand, you can set up the Master node on a Spot Block. You do so by setting the defined duration of the node and failing over to On-Demand if the Spot Block capacity is unavailable.</p><p><strong>Core nodes:</strong> Avoid using Spot Instances for Core nodes if the jobs on the cluster use HDFS. That prevents a situation where Spot interruptions cause data loss for data that was written to the HDFS volumes on the instances.</p><p><strong>Task nodes:</strong> Use Spot Instances for your task nodes by selecting up to five instance types that match your hardware requirement. Amazon EMR fulfills the most suitable capacity by price and capacity availability.</p><p><img src=\"https://media.tutorialsdojo.com/sap_emr_node_types.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_emr_node_types.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Use On-Demand EC2 instances for both the master and core nodes and use Spot EC2 instances for the task nodes.</strong></p><p>The option that says: <strong>Use a combination of On-Demand instance and Spot instance types for both the master and core nodes. Use Spot EC2 instances for the task nodes</strong> is incorrect. It is not recommended to use a Spot instance for the Master node. Also, if dealing with a large amount of data shared on the cluster, you don't want to use spot instances for the Core nodes as it may cause data loss or interruption on your processing.</p><p>The option that says: <strong>Use Reserved EC2 instances for both the master and core nodes and use Spot EC2 instances for the task nodes </strong>is incorrect. The scenario is not a long-term project so a Reserved instance is not a suitable type to use in this case.<strong><br></strong></p><p>The option that says: <strong>Use Reserved EC2 instances for the master node; On-Demand instances for the core nodes; and use Spot EC2 instances for the task nodes</strong> is incorrect. The scenario is not a long-term project so a Reserved instance is not a suitable type to use in this case.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html</a></p><p><a href=\"https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-spark-applications-using-amazon-ec2-spot-instances-with-amazon-emr/\">https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-spark-applications-using-amazon-ec2-spot-instances-with-amazon-emr/</a></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy</a></p><p><br></p><p><strong>Check out this Amazon EMR Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-emr/?src=udemy\">https://tutorialsdojo.com/amazon-emr/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html",
      "https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-spark-applications-using-amazon-ec2-spot-instances-with-amazon-emr/",
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy",
      "https://tutorialsdojo.com/amazon-emr/?src=udemy"
    ]
  },
  {
    "id": 19,
    "question": "<p>A company hosts its application on several Amazon EC2 instances inside a VPC. A known security vulnerability was discovered in the outdated Operating System of the company's EC2 fleet. The solutions architect is responsible for mitigating the vulnerability as soon as possible to safeguard your systems from various cybersecurity attacks. In addition, it is also required to record all of the changes to patches and association compliance statuses.</p><p>Which of the following options is the recommended way to meet the above requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Systems Manager Patch Manager to deploy the OS security patches on the EC2 instances. Use AWS Config to manage, detect and record the security compliance of the EC2 instances.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a new AMI that automatically installs the OS security patches every week on the provided maintenance window. Roll out the new AMI to the Amazon EC2 instances fleet.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up Amazon Control Tower to deploy OS security updates to the Amazon EC2 instances. Create an Amazon Managed Grafana dashboard to visualize the security compliance of the EC2 instance fleet.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Systems Manager State Manager to ensure that OS security patches are installed on the Amazon EC2 instances. Use Amazon OpenSearch service to record, monitor, and visualize the patch statuses of the entire EC2 instance fleet.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Since you are also required to record all of the changes to patch and association compliance statuses, you can use <strong>AWS Config</strong> to meet this requirement. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.</p><p>Therefore, the correct answer is: <strong>Use AWS Systems Manager Patch Manager to deploy the OS security patches on the EC2 instances. Use AWS Config to manage, detect and record the security compliance of the EC2 instances.</strong></p><p>The option that says: <strong>Create a new AMI that automatically installs the OS security patches every week on the provided maintenance window. Roll out the new AMI to the Amazon EC2 instances fleet </strong>is incorrect. This approach is possible but is not recommended because you won't have control over each particular patch that needs to be installed. And it will be cumbersome to have a rollback operation in case a certain update broke the normal operations of your servers.</p><p>The option that says: <strong>Set up Amazon Control Tower to deploy OS security updates to the Amazon EC2 instances. Create an Amazon Managed Grafana dashboard to visualize the security compliance of the EC2 instance fleet </strong>is incorrect. Amazon Control Tower is used to automate the setup of your multi-account AWS environment, not install security patches.</p><p>The option that says: <strong>Use AWS Systems Manager State Manager to ensure that OS security patches are installed on the Amazon EC2 instances. Use Amazon OpenSearch service to record, monitor, and visualize the patch statuses of the entire EC2 instance fleet </strong>is incorrect. AWS Systems Manager State Manager is designed to be a configuration management service that automates the process of keeping your managed nodes and other AWS resources in a state that you define. Although it may be possible to use it to install security patches, AWS SSM Patch Manager is the recommended service for installing OS updates to your EC2 instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p><p><br></p><p><strong>Check out this AWS Systems Manager and AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html",
      "https://aws.amazon.com/config/",
      "https://tutorialsdojo.com/aws-systems-manager/?src=udemy",
      "https://tutorialsdojo.com/aws-config/?src=udemy"
    ]
  },
  {
    "id": 20,
    "question": "<p>A company recently patched a vulnerability in its web application hosted on AWS. The solutions architect was tasked to improve the security of the company’s AWS resources as well as secure the web applications from common web vulnerabilities and cyber attacks. One example is a Distributed Denial of Service attack (DDoS) in which there is numerous incoming traffic coming from many different locations that simultaneously target the company web application and floods the network with bogus requests.</p><p>Which of the following options are recommended strategies for reducing DDoS attack surface and minimizing the blast radius in the cloud infrastructure? (Select TWO.)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Strictly implement Multi-Factor Authentication (MFA) in AWS. Use a combination of Amazon Fraud Detector, AWS Config, and Trusted Advisor to fortify your AWS resources.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure the Network Access Control Lists (ACLs) to only allow the required ports to your network. Identify and block common DDoS request patterns to effectively mitigate a DDoS attack by using AWS WAF.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Always add a security group that only allows certain ports and authorized servers and protects your origin servers by putting it behind a CloudFront distribution. Enable AWS Shield Advanced which provides enhanced DDoS attack detection and monitoring for application-layer traffic to your AWS resources.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Add Elastic Load Balancing and Auto Scaling to your EC2 instances to improve availability and scalability. Use extra large EC2 instances to accommodate a surge of incoming traffic caused by a DDoS attack and utilize AWS Systems Manager Session Manager to filter all client-side web sessions to your instances.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Allow versioning in your S3 bucket. Ensure that the OS of all of your EC2 instances are properly patched using Systems Manager Patch Manager.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>Another important consideration when architecting on AWS is to limit the opportunities that an attacker may have to target your application. For example, if you do not expect an end-user to interact directly with certain resources, you will want to make sure that those resources are not accessible from the Internet. Similarly, if you do not expect end-users or external applications to communicate with your application on certain ports or protocols, you will want to make sure that traffic is not accepted. This concept is known as attack surface reduction. Resources that are not exposed to the Internet are more difficult to attack, which limits the options an attacker might have to target the availability of your application.</p><p><img src=\"https://media.tutorialsdojo.com/sap_waf_shield_cloudfront.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_waf_shield_cloudfront.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>AWS Shield</strong> is a managed DDoS protection service that is available in two tiers: Standard and Advanced. AWS Shield Standard applies always-on detection and inline mitigation techniques, such as deterministic packet filtering and priority-based traffic shaping, to minimize application downtime and latency. AWS Shield Standard is included automatically and transparently to your Elastic Load Balancing load balancers, Amazon CloudFront distributions, and Amazon Route 53 resources at no additional cost. When you use these services that include AWS Shield Standard, you receive comprehensive availability protection against all known infrastructure layer attacks. Customers who have the technical expertise to manage their own monitoring and mitigation of application layer attacks can use AWS Shield together with AWS WAF rules to create a comprehensive DDoS attack mitigation strategy.</p><p>The following options are both correct as they are best practices for reducing the DDoS attack surface, which then limits the extent to which your application is exposed:</p><p><strong>1. Always add a security group that only allows certain ports and authorized servers and protect your origin servers by putting it behind a CloudFront distribution. Enable AWS Shield Advanced which provides enhanced DDoS attack detection and monitoring for application-layer traffic to your AWS resources.</strong></p><p><strong>2. Configure the Network Access Control Lists (ACLs) to only allow the required ports to your network. Identify and block common DDoS request patterns to effectively mitigate a DDoS attack by using AWS WAF.</strong></p><p>Using AWS Shield together with AWS WAF rules provide a comprehensive DDoS attack mitigation strategy.</p><p>The option that says:<strong><em> </em>Add Elastic Load Balancing and Auto Scaling to your EC2 instances to improve availability and scalability. Use extra large EC2 instances to accommodate a surge of incoming traffic caused by a DDoS attack and utilize AWS Systems Manager Session Manager to filter all client-side web sessions to your instances</strong> is incorrect. Although it improves the scalability of your network in case of an ongoing DDoS attack, it simply absorbs the heavy application layer traffic and doesn't minimize the attack surface in your cloud architecture. In addition, AWS Systems Manager Session Manager is primarily used to provide secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys, but not to filter client-side web sessions.</p><p>The options that say: <strong>Allow versioning in your S3 bucket. Ensure that the OS of all of your EC2 instances are properly patched using Systems Manager Patch Manager</strong> and <strong>Strictly implement Multi-Factor Authentication (MFA) in AWS. Use a combination of Amazon Fraud Detector, AWS Config, and Trusted Advisor to fortify your AWS resources </strong>are incorrect because MFA, as well as the Versioning feature in S3, don't minimize the DDoS attack surface area. Although it is recommended that all of your instances are properly patched using the Systems Manager Patch Manager, it is still not enough to protect your cloud infrastructure against DDoS attacks. In addition, Amazon Fraud Detector is used in the detection of potentially fraudulent activities online. This service will not help you minimize the blast radius of a security attack.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/\">https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf\">https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a></p><p><a href=\"https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf\">https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/",
      "https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf",
      "https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 21,
    "question": "<p>A company recently developed a web application that processes customer behavioral data and stores the results in a DynamoDB table. The application is expected to receive a high usage load. To ensure that data is not lost when DynamoDB write requests are throttled, the solutions architect must reduce the load taken by the table.</p><p>Which of the following is the MOST cost-effective strategy for reducing the load on the DynamoDB table?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Provision more DynamoDB tables to absorb the load.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Provision higher write-capacity units (WCUs) to your DynamoDB table.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an SQS queue to decouple messages from the application and the database.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Replicate the DynamoDB table to another AWS region using global tables.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>Queuing is a commonly used solution for separating computation components in a distributed processing system. It is a form of the asynchronous communication system used in serverless and microservices architectures. Messages wait in a queue for processing, and leave the queue when received by a single consumer.</p><p><strong>Amazon Simple Queue Service (Amazon SQS)</strong> is a scalable message queuing system that stores messages as they travel between various components of your application architecture. Amazon SQS enables web service applications to quickly and reliably queue messages that are generated by one component and consumed by another component. A queue is a temporary repository for messages that are awaiting processing.</p><p>In the below diagram, the goal is to smooth out the traffic from the application, so that the load process into DynamoDB is much more consistent. The key service used to achieve this is Amazon SQS, which holds all the items until a loader process stores the data in DynamoDB. The architecture looks like this:</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_dynamodb.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_sqs_dynamodb.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Use an SQS queue to decouple messages from the application and the database.</strong> Data can be lost if the application fails to store it in DynamoDB due to throttling. Amazon SQS can reduce the load by temporarily holding the data until the DynamoDB throttling subsides. It is scalable as well as cost-efficient.</p><p>The option that says: <strong>Replicate the DynamoDB table to another AWS region using global tables</strong> is incorrect. The global table is a DynamoDB feature that is primarily used for applications requiring multi-region fault tolerance. It won't reduce the load on the existing DynamoDB table.</p><p>The option that says: <strong>Provision higher write-capacity units (WCUs) to your DynamoDB table</strong> is incorrect because increasing the write capacity is an expensive option.</p><p>The option that says: <strong>Provision more DynamoDB tables to absorb the load</strong> is incorrect. While it is possible to create another table for the application to use, it is an anti-pattern and will require a lot of development overhead to keep the data from the tables in sync, not to mention when joining queries. Remember that JOIN operations are not possible in DynamoDB so you need to implement this at the application level. Furthermore, this is not a cost-efficient solution.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/creating-a-scalable-serverless-import-process-for-amazon-dynamodb/\">https://aws.amazon.com/blogs/compute/creating-a-scalable-serverless-import-process-for-amazon-dynamodb/</a></p><p><a href=\"https://aws.amazon.com/blogs/database/implementing-priority-queueing-with-amazon-dynamodb/\">https://aws.amazon.com/blogs/database/implementing-priority-queueing-with-amazon-dynamodb/</a></p><p><br></p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/blogs/compute/creating-a-scalable-serverless-import-process-for-amazon-dynamodb/",
      "https://aws.amazon.com/blogs/database/implementing-priority-queueing-with-amazon-dynamodb/",
      "https://tutorialsdojo.com/amazon-sqs/?src=udemy"
    ]
  },
  {
    "id": 22,
    "question": "<p>A company has a large Microsoft Windows Server running on a public subnet. There are EC2 instances hosted on a private subnet that allows Remote Desktop Protocol (RDP) connections to the Windows Server via port 3389. These instances enable the Microsoft Administrators to connect to the public servers and troubleshoot any server failures.</p><p>The server must always have the latest operating system upgrades to improve security and it must be accessible at any given point in time. The administrators are tasked to refactor the existing solution and manage the server patching activities effectively, even outside the regular maintenance window.</p><p>Which of the following provides the LEAST amount of administrative overhead in managing the server?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch a hardened machine image from the AWS Marketplace and host the server using AWS CloudShell. Set up the AWS Systems Manager Patch Manager to automatically apply system updates. Use Amazon AppStream 2.0 to act as a bastion host.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Launch the Windows Server on Amazon EC2 instances. Use AWS Systems Manager Patch Manager to manage the patching process for the server. Configure it to automatically apply patches as they become available, ensuring that the server is always up-to-date with the latest operating system upgrades.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Launch an AWS AppSync environment with a single EC2 instance that runs the Windows Server. Set up the environment with a custom AMI to utilize a hardened machine image that can be downloaded from AWS Marketplace. Configure the AWS Systems Manager Patch Manager to automatically apply the OS updates.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Launch the server in Amazon Lightsail with the recommended Amazon AMI. Set up a combination of Amazon EventBridge and AWS Lambda scheduled event to call the <code>Upgrade Operating System</code> API in Amazon Lightsail to apply system updates.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>Amazon EC2 is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers.</p><p>AWS Systems Manager Patch Manager can automate the process of patching managed instances, including both security-related updates and other types of updates. It uses the appropriate built-in mechanism for an operating system type to install updates on a managed node.</p><p>Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as optional lists of approved and rejected patches. This ensures that the server is always up-to-date with the latest operating system upgrades.</p><p><img src=\"https://media.tutorialsdojo.com/public/patch-manager-011024.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/public/patch-manager-011024.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can also leverage patch groups to organize instances for patching, such as different environments/tagged instances like development, test, and production.</p><p>Furthermore, you can schedule patching to run as a Maintenance Windows task, ensuring that patching activities are managed effectively even outside the regular maintenance window.</p><p>This solution would provide the least amount of administrative overhead as it automates the patching process, reducing the need for manual intervention.</p><p>Hence, the correct answer is: <strong>Launch the Windows Server on Amazon EC2 instances. Use AWS Systems Manager Patch Manager to manage the patching process for the server. Configure it to automatically apply patches as they become available, ensuring that the server is always up-to-date with the latest operating system upgrades.</strong></p><p>The option that says: <strong>Launch an AWS AppSync environment with a single EC2 instance that runs the Windows Server. Set up the environment with a custom AMI to utilize a hardened machine image that can be downloaded from AWS Marketplace. Configure the AWS Systems Manager Patch Manager to automatically apply the OS updates</strong> is incorrect because the AWS AppSync service is simply a fully managed service for developing GraphQL APIs, and not for creating an environment with a single Window Server instance.</p><p>The option that says: <strong>Launch a hardened machine image from the AWS Marketplace and host the server using AWS CloudShell. Set up the AWS Systems Manager Patch Manager to automatically apply system updates. Use Amazon AppStream 2.0 to act as a bastion host</strong> is incorrect. AWS CloudShell is just a browser-based command-line tool intended for interacting with AWS resources through the CLI. It’s not suitable for hosting a production Windows Server. Additionally, using Amazon AppStream 2.0 as a bastion host adds unnecessary complexity and overhead to the solution. This approach has more administrative overhead compared to using AWS Systems Manager directly on an EC2 instance.</p><p>The option that says: <strong>Launch the server in Amazon Lightsail with the recommended Amazon AMI. Set up a combination of Amazon EventBridge and AWS Lambda scheduled event to call the </strong><code><strong>Upgrade Operating System</strong></code><strong> API in Amazon Lightsail to apply system updates</strong> is incorrect because although Amazon Lightsail is a simplified offering, setting up EventBridge and Lambda to manage operating system updates introduces additional manual configuration and complexity. AWS Systems Manager Patch Manager on EC2 offers a more streamlined and automated solution for patch management, making it a better fit for this requirement.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/EC2_GetStarted.html\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/EC2_GetStarted.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/\">https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/EC2_GetStarted.html",
      "https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/",
      "https://tutorialsdojo.com/aws-systems-manager/?src=udemy"
    ]
  },
  {
    "id": 23,
    "question": "<p>A company has a gaming store platform hosted in its on-premises data center for a whole variety of digital games. The application just experienced downtime last week due to a large burst in web traffic caused by a year-end sale on almost all of the games. Due to the success of the previous promotion, the CEO has planned to do the same in a few weeks, which will drive similar unpredictable bursts in web traffic. The solutions architects are looking to find ways to quickly improve the infrastructure's ability to handle unexpected increases in traffic. The web application is currently made up of a 2-tier web tier which consists of a load balancer and several web app servers, as well as a database tier that hosts an Oracle database.</p><p>Which of the following infrastructure changes should the team implement to avoid any further incidences of downtime considering that the new announcement will be done in a few weeks?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Migrate your environment to AWS by using AWS VM Import to quickly convert your web server into an AMI. Then set up an Auto Scaling group that uses the imported AMI. Also, create an RDS read replica and migrate the Oracle database to an RDS instance through replication.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up an Amazon S3 bucket for website hosting. Migrate your DNS to Route 53 using zone import, and use DNS failover to failover to the hosted website in S3.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an AMI that can be used to launch new EC2 web servers. Then create an Auto Scaling group which will use the AMI to scale the web tier. Finally, place an Application Load Balancer to distribute traffic between your on-premises servers and servers running in AWS.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a CloudFront distribution to cache objects from a custom origin to offload traffic from your on-premises environment. Customize your object cache behavior, and choose a time-to-live that will determine how long objects will reside in the cache.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon CloudFront</strong> is a global content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to your viewers with low latency and high transfer speeds. CloudFront is integrated with AWS – including physical locations that are directly connected to the AWS global infrastructure, as well as software that works seamlessly with services including AWS Shield for DDoS mitigation, Amazon S3, Elastic Load Balancing or Amazon EC2 as origins for your applications, and Lambda@Edge to run custom code close to your viewers. In this scenario, the major points of consideration are: your application may get unpredictable bursts of traffic, you need to improve the current infrastructure in the shortest period possible, and your web servers that are on-premises.</p><p>CloudFront caches content at edge locations for a period of time that you specify. If a visitor requests content that has been cached for longer than the expiration date, CloudFront checks the origin server to see if a newer version of the content is available. If a newer version is available, CloudFront copies the new version to the edge location. Changes that you make to the original content are replicated to edge locations as visitors request the content.</p><p>Since the time period at hand is short, instead of migrating the app to AWS, you need to consider different ways where the performance would improve without doing much modification to the existing infrastructure.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_cache.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_cache.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Set up a CloudFront distribution to cache objects from a custom origin to offload traffic from your on-premises environment. Customize your object cache behavior, and choose a time-to-live that will determine how long objects will reside in the cache.</strong> CloudFront is a highly scalable, highly available content delivery service, which can perform excellently even in case of sudden unpredictable burst of traffic. Plus, the only change you need to make is the on-premises load balancer as the custom origin of the CloudFront distribution.</p><p>The option that says: <strong>Create an AMI that can be used to launch new EC2 web servers. Then create an Auto Scaling group which will use the AMI to scale the web tier. Finally, place an Application Load Balancer to distribute traffic between your on-premises servers and servers running in AWS</strong> is incorrect. ELB cannot do load balancing to your on-premises instances if it is not connected to your VPC either through a DirectConnect connection or a VPN.</p><p>The option that says: <strong>Migrate your environment to AWS by using AWS VM Import to quickly convert your web server into an AMI. Then set up an Auto Scaling group that uses the imported AMI. Also, create an RDS read replica and migrate the Oracle database to an RDS instance through replication</strong> is incorrect. You are supposed to improve the current situation at the shortest time possible. Migrating to AWS would be more time consuming than simply setting up the CloudFront distribution.</p><p>The option that says: <strong>Set up an Amazon S3 bucket for website hosting. Migrate your DNS to Route 53 using zone import, and use DNS failover to failover to the hosted website in S3</strong> is incorrect. You cannot host dynamic websites on S3 bucket. Also, this option provides insufficient infrastructure set up options.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/cloudfront\">https://aws.amazon.com/cloudfront</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/improve-your-website-performance-with-amazon-cloudfront/\">https://aws.amazon.com/blogs/networking-and-content-delivery/improve-your-website-performance-with-amazon-cloudfront/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-cloudfront-walkthrough.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-cloudfront-walkthrough.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/cloudfront",
      "https://aws.amazon.com/blogs/networking-and-content-delivery/improve-your-website-performance-with-amazon-cloudfront/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-cloudfront-walkthrough.html",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy"
    ]
  },
  {
    "id": 24,
    "question": "<p>A company has scheduled to launch a promotional sale on its e-commerce platform. The company’s web application is hosted on a fleet of Amazon EC2 instances in an Auto Scaling group. The database tier is hosted on an Amazon RDS for PostgreSQL DB instance. This is a large event so the management expects a sudden spike and unpredictable user traffic for the duration of the event. New users are also expected to register and participate in the event so there will be a lot of database writes during the event. The Solutions Architect has been tasked to create a solution that will ensure all submissions are committed to the database without changing the underlying data model.</p><p>Which of the following options is the recommended solution for this scenario?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon ElastiCache for Memcached cluster between the application and database tier. The cache will temporarily store the user submissions until the database is able to commit those entries.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Decouple the application and database tier by creating an Amazon SQS queue between them. Create an AWS Lambda function that picks up the messages on the SQS queue and writes them into the database.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>To minimize any changes on the application or the current infrastructure, manually scale the current DB instance to a significantly larger instance size before the event. Choose a larger instance size depending on the anticipated user traffic, and scale down after the event is completed.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Instead of using Amazon RDS, migrate the database to an Amazon DynamoDB table instead. Utilize the built-in automatic scaling in DynamoDB to scale the database based on user traffic.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Simple Queue Service (SQS)</strong> is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p><p>Use Amazon SQS to transmit any volume of data, at any level of throughput, without losing messages or requiring other services to be available. SQS lets you decouple application components so that they run and fail independently, increasing the overall fault tolerance of the system. Multiple copies of every message are stored redundantly across multiple availability zones so that they are available whenever needed.</p><p>Amazon SQS leverages the AWS cloud to dynamically scale based on demand. SQS scales elastically with your application so you don’t have to worry about capacity planning and pre-provisioning. There is no limit to the number of messages per queue, and standard queues provide nearly unlimited throughput.</p><p>You can use Amazon Simple Queue Service (SQS) to trigger AWS Lambda functions. Lambda functions can act as message consumers. Message consumers are processes that make the <strong>ReceiveMessage</strong> API call on SQS. Messages from queues can be processed either in batch or as a single message at a time. Each approach has its advantages and disadvantages.</p><p><strong>- Batch processing</strong>: This is where each message can be processed independently, and an error on a single message is not required to disrupt the entire batch. Batch processing provides the most throughput for processing, and also provides the most optimizations for resources involved in reading messages.</p><p><strong>- Single message processing</strong>: Single message processing is commonly used in scenarios where each message may trigger multiple processes within the consumer. In case of errors, the retry is confined to the single message.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_lambda_consumption.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_sqs_lambda_consumption.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Decouple the application and database tier by creating an Amazon SQS queue between them. Create an AWS Lambda function that picks up the messages on the SQS queue and writes them into the database.</strong> This is an excellent scenario for which Amazon SQS is designed for. The SQS queue can scale reliably to hold user submissions to ensure they will be written to the database. The SQS queue is highly durable which ensures that you will not lose any submissions.</p><p>The option that says: <strong>To minimize any changes on the application or the current infrastructure, manually scale the current DB instance to a significantly larger instance size before the event. Choose a larger instance size depending on the anticipated user traffic, and scale down after the event is completed</strong> is incorrect. There is no mention in the question if the database is Multi-AZ configuration. If Multi-AZ is not enabled, this will result in a brief downtime as the database is being scaled up or down. Additionally, since the user traffic is unpredictable, there is no guarantee the larger instance can handle the user traffic.</p><p>The option that says: <strong>Instead of using Amazon RDS, migrate the database to an Amazon DynamoDB table instead. Utilize the built-in automatic scaling in DynamoDB to scale the database based on user traffic</strong> is incorrect. This is not recommended for the scenario as changing the database requires major changes in the application and database layers.</p><p>The option that says: <strong>Create an Amazon ElastiCache for Memcached cluster between the application and database tier. The cache will temporarily store the user submissions until the database is able to commit those entries</strong> is incorrect. Amazon ElastiCache is designed for caching frequent requests to the database and not for holding data that is waiting to be written to the database.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><a href=\"https://aws.amazon.com/blogs/architecture/application-integration-using-queues-and-messages/\">https://aws.amazon.com/blogs/architecture/application-integration-using-queues-and-messages/</a></p><p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p><p><br></p><p><strong>Check out the Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html",
      "https://aws.amazon.com/blogs/architecture/application-integration-using-queues-and-messages/",
      "https://aws.amazon.com/sqs/faqs/",
      "https://tutorialsdojo.com/amazon-sqs/?src=udemy"
    ]
  },
  {
    "id": 25,
    "question": "<p>A company is running a serverless backend API service on AWS. It has several AWS Lambda functions written in Python and an Amazon API Gateway that is configured to invoke the functions. The company wants to secure the API endpoint by ensuring that only authorized IAM users or roles can access the Amazon API Gateway endpoint. The Solutions Architect was also tasked to provide the ability to inspect each request end-to-end to check the latency of the request and to generate service maps.</p><p>Which of the following implementation will fulfill the above company requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Generate a new client certificate on Amazon API Gateway. Distribute this certificate to all AWS users or roles that require access to the API endpoint. Ensure that each user will pass the client certification for every request made to the API endpoint. Trace and analyze each user request on API Gateway using Amazon CloudWatch Logs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Write a separate AWS Lambda function that will act as a custom authorizer. For every call to the API gateway, require the client to pass the access key and secret key. Use the Lambda function to validate the key/secret pair against a valid IAM user. Trace and analyze each user request on API Gateway by using AWS X-Ray.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure authorization to use <code>AWS_IAM</code> for the API Gateway method. Create the IAM users or roles that have the <code>execute-api:Invoke</code> permission to the ARN of the API resource. Enable request signing with AWS Signature for every call to the API endpoint. Trace and analyze each user request on API Gateway by using AWS X-Ray.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Ensure that the API Gateway resource is secured by only returning the company’s domain in Access-Control-Allow-Origin headers and enabling Cross-origin resource sharing (CORS). Create the IAM users or roles that have the <code>execute-api:Invoke</code> permission to the ARN of the API resource. Trace and analyze each user request on API Gateway using Amazon CloudWatch Logs.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>API Gateway</strong> supports multiple mechanisms for controlling and managing access to your API.</p><p>You can use the following mechanisms for authentication and authorization:</p><p><strong>Resource policies</strong> let you create resource-based policies to allow or deny access to your APIs and methods from specified source IP addresses or VPC endpoints.</p><p><strong>Standard AWS IAM roles and policies</strong> offer flexible and robust access controls that can be applied to an entire API or individual methods. IAM roles and policies can be used for controlling who can create and manage your APIs, as well as who can invoke them.</p><p><strong>IAM tags</strong> can be used together with IAM policies to control access.</p><p><strong>Endpoint policies for interface VPC endpoints</strong> allow you to attach IAM resource policies to interface VPC endpoints to improve the security of your private APIs.</p><p><strong>Lambda authorizers</strong> are Lambda functions that control access to REST API methods using bearer token authentication—as well as information described by headers, paths, query strings, stage variables, or context variables request parameters. Lambda authorizers are used to control who can invoke REST API methods.</p><p><strong>Amazon Cognito user pools</strong> let you create customizable authentication and authorization solutions for your REST APIs. Amazon Cognito user pools are used to control who can invoke REST API methods.</p><p><img src=\"https://media.tutorialsdojo.com/API_GATEWAY_AWS_IAM.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/API_GATEWAY_AWS_IAM.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can enable IAM authentication for an API method in the API Gateway console. Then, you can use IAM policies and resource policies to designate permissions for your API's users. Here are the general steps to implement this:</p><p>In the API Gateway console, enable IAM authentication for your API method by going to <strong>Settings</strong> &gt; <strong>Authorization</strong>, and choosing <strong>AWS_IAM</strong>.</p><p>Deploy the API to ensure the changes are applied.</p><p>Grant API authorization to a group of IAM users by creating an IAM policy document with the required permissions.</p><p>For the IAM policy, ensure that it allows the <code>execute-api:Invoke</code> action and the resource is set to the ARN of the API gateway method.</p><p>Attach this policy to the required IAM users or IAM roles.</p><p><strong>AWS X-Ray</strong> helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. AWS X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.</p><p>Therefore, the correct answer is: <strong>Configure authorization to use </strong><code><strong>AWS_IAM</strong></code><strong> for the API Gateway method. Create the IAM users or roles that have the </strong><code><strong>execute-api:Invoke</strong></code><strong> permission to the ARN of the API resource. Enable request signing with AWS Signature for every call to the API endpoint. Trace and analyze each user request on API Gateway by using AWS X-Ray.</strong></p><p>The option that says: <strong>Ensure that the API Gateway resource is secured by only returning the company’s domain in Access-Control-Allow-Origin headers and enabling Cross-origin resource sharing (CORS). Create the IAM users or roles that have the </strong><code><strong>execute-api:Invoke</strong></code><strong> permission to the ARN of the API resource. Trace and analyze each user request on API Gateway using Amazon CloudWatch Logs</strong> is incorrect because CORS does not provide any IAM authentication capability. Although API Gateway can send execution logs to CloudWatch Logs, you can only view the logs with it for monitoring purposes. It does not provide end-to-end tracing or inspection for the request.</p><p>The option that says: <strong>Write a separate AWS Lambda function that will act as a custom authorizer. For every call to the API gateway, require the client to pass the access key and secret key. Use the Lambda function to validate the key/secret pair against a valid IAM user. Trace and analyze each user request on API Gateway by using AWS X-Ray </strong>is incorrect. Creating a custom Lambda authorizer will require more work compared to just using the AWS_IAM authorizer. Sending AWS access and secret key as part of each HTTPS call is not recommended from a security standpoint since these are sensitive security credentials.</p><p>The option that says: <strong>Generate a new client certificate on Amazon API Gateway. Distribute this certificate to all AWS users or roles that require access to the API endpoint. Ensure that each user will pass the client certification for every request made to the API endpoint. Trace and analyze each user request on API Gateway using Amazon CloudWatch Logs </strong>is incorrect. Using client certificates for authentication does not provide any IAM authentication capability and although API Gateway can send execution logs to CloudWatch Logs, the logs do not provide end-to-end tracing or inspection for request.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-iam-policy-examples-for-api-execution.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-iam-policy-examples-for-api-execution.html</a></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/</a></p><p><br></p><p><strong>Check out the AWS X-Ray and Amazon API Gateway Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-api-gateway/?src=udemy\">https://tutorialsdojo.com/amazon-api-gateway/</a></p><p><a href=\"https://tutorialsdojo.com/aws-x-ray/?src=udemy\">https://tutorialsdojo.com/aws-x-ray/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-iam-policy-examples-for-api-execution.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/",
      "https://tutorialsdojo.com/amazon-api-gateway/?src=udemy",
      "https://tutorialsdojo.com/aws-x-ray/?src=udemy"
    ]
  },
  {
    "id": 26,
    "question": "<p>A company plans to release a public beta of its new video game. The release package is approximately 5GB in size. Based on previous releases and community feedback, millions of users from around the world are expected to download the new game. Currently, the company has a Linux-based website that lists the files which are hosted on its on-premises data center. Public Internet users are able to download the game via the website. However, the company wants a new solution that is cost-effective and will allow faster download performance for its users regardless of their location.</p><p>Which of the following options is the recommended solution to meet the company’s requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon S3 bucket with website hosting enabled and upload the game package on it. Create an Amazon CloudFront distribution with the S3 bucket as the origin. Create an Amazon Route 53 entry pointing to the CloudFront distribution. Publish the Route 53 entry as the download URL to allow users to download the game package.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Host the service on an Auto Scaling group of Amazon EC2 instances. Save the game files on the mounted Amazon EFS volume on each instance. Place the Auto Scaling group behind a Network Load Balancer. Create an Amazon Route 53 entry pointing to the NLB. Publish the Route 53 entry as the download URL to allow users to download the game package.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon S3 bucket with website hosting enabled and upload the game package on it. To improve cost-effectiveness, enable the “Requestor Pays” option for the S3 bucket. Create an Amazon Route 53 entry pointing to the S3 bucket. Publish the Route 53 entry as the download URL to allow users to download the game package.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Host the service on an Auto Scaling group of Amazon EC2 instances. Save the game files on the mounted Amazon EBS volumes on each instance. Place the Auto Scaling group behind an Application Load Balancer. Create an Amazon Route 53 entry pointing to the ALB. Publish the Route 53 entry as the download URL to allow users to download the game package.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>You can use <strong>Amazon S3</strong> to host a static website. Hosting a static website on Amazon S3 delivers a highly performant and scalable website at a fraction of the cost of a traditional web server. To host a static website on Amazon S3, configure an Amazon S3 bucket for website hosting and upload your website content. Using the AWS Management Console, you can configure your Amazon S3 bucket as a static website without writing any code. Depending on your website requirements, you can also use some optional configurations, including redirects, web traffic logging, and custom error documents.</p><p>When you configure a bucket as a static website, you must enable static website hosting, configure an index document, and set permissions.</p><p>You can use <strong>Amazon CloudFront</strong> to improve the performance of your Amazon S3 website. CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (known as edge locations). When a visitor requests a file from your website, CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away.</p><p>CloudFront caches content at edge locations for a period of time that you specify. If a visitor requests content that has been cached for longer than the expiration date, CloudFront checks the origin server to see if a newer version of the content is available. If a newer version is available, CloudFront copies the new version to the edge location. Changes that you make to the original content are replicated to edge locations as visitors request the content.</p><p><img src=\"https://media.tutorialsdojo.com/s3-transfer-acceleration.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/s3-transfer-acceleration.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>To serve a static website hosted on Amazon S3, you can deploy a CloudFront distribution using one of these configurations:</p><p>- Using a REST API endpoint as the origin, with access restricted by an origin access control (OAC)</p><p>- Using a website endpoint as the origin, with anonymous (public) access allowed. It allows public users to download files.</p><p>- Using a website endpoint as the origin, with access restricted by a Referer header</p><p>- Using AWS CloudFormation to deploy a REST API endpoint as the origin, with access restricted by an OAC and a custom domain pointing to CloudFront</p><p>Hence, the correct answer is: <strong>Create an Amazon S3 bucket with website hosting enabled and upload the game package on it. Create an Amazon CloudFront distribution with the S3 bucket as the origin. Create an Amazon Route 53 entry pointing to the CloudFront distribution. Publish the Route 53 entry as the download URL to allow users to download the game package. </strong>Storing the game package on an Amazon S3 bucket is very cost-effective. Using CloudFront will ensure that users will have a consistently high download performance regardless of their location.</p><p>The option that says: <strong>Host the service on an Auto Scaling group of Amazon EC2 instances. Save the game files on the mounted Amazon EBS volumes on each instance. Place the Auto Scaling group behind an Application Load Balancer. Create an Amazon Route 53 entry pointing to the ALB. Publish the Route 53 entry as the download URL to allow users to download the game package</strong> is incorrect. This is possible but very expensive as you have to attach EBS volumes to each instance. Additionally, this is limited to only one region. Other users from around the world may experience a slower download speed.</p><p>The option that says: <strong>Host the service on an Auto Scaling group of Amazon EC2 instances. Save the game files on the mounted Amazon EFS volume on each instance. Place the Auto Scaling group behind a Network Load Balancer. Create an Amazon Route 53 entry pointing to the NLB. Publish the Route 53 entry as the download URL to allow users to download the game package</strong> is incorrect. This is not recommended because using EFS is typically more expensive than using an S3 bucket. A Network Load Balancer is also very expensive.</p><p>The option that says: <strong>Create an Amazon S3 bucket with website hosting enabled and upload the game package on it. To improve cost-effectiveness, enable the “Requestor Pays” option for the S3 bucket. Create an Amazon Route 53 entry pointing to the S3 bucket. Publish the Route 53 entry as the download URL to allow users to download the game package</strong> is incorrect. This is not possible because the users will need their own AWS accounts in order to download an object from a bucket that has \"Requestor Pays\" enabled.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-custom-domain-walkthrough.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-custom-domain-walkthrough.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-cloudfront-walkthrough.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-cloudfront-walkthrough.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><br></p><p><strong>Check out these Amazon S3 and Amazon CloudFront Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-custom-domain-walkthrough.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-cloudfront-walkthrough.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 27,
    "question": "<p>A company runs a live flight tracking service hosted on the AWS cloud. The application gets updated every 10 minutes with the latest flight information from every airline. The tracking website has a global audience and uses an Auto Scaling group behind an Elastic Load Balancer and an Amazon RDS database. A simple web interface is hosted as static content on an Amazon S3 bucket. The Auto Scaling group is set to trigger a scale-up event at 90% CPU utilization. The average load time of the web page is<strong> </strong>around 7 seconds but the management wants to bring it down to less than 3 seconds.</p><p>Which combination of options will make the page load time faster in the MOST cost-effective way? (Select TWO.)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a second installation in another region, and utilize Amazon Route 53's latency-based routing feature to direct requests to the appropriate region.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Have CloudFront enable caching of re-usable content from your website.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Replace your existing Auto Scaling group with the AWS Systems Manager State Manager which provides a more effective way to manage and scale your EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Add a caching layer using Amazon ElastiCache Service to be used for storing sessions and frequent DB queries.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Scale more frequently by setting the scale up trigger of the Auto Scaling group to 30%.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>You can use a content delivery network (CDN) like <strong>Amazon CloudFront</strong> to improve the performance of your website by securely delivering data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. To improve performance, you can simply configure your website’s traffic to be delivered over CloudFront’s globally distributed edge network by setting up a CloudFront distribution. In addition, CloudFront offers a variety of optimization options.</p><p><strong>Amazon ElastiCache</strong> allows you to seamlessly set up, run, and scale popular open-source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Both Redis and MemCached are in-memory, open-source data stores. Memcached, a high-performance distributed memory cache service, is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases.</p><p>In this scenario, you can improve the page load times of your application by using a combination of Amazon ElastiCache, CloudFront, or alternatively, an upgraded RDS instance to increase the read capacity.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_elasticache.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_rds_elasticache.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The option that says: <strong>Add a caching layer using Amazon ElastiCache Service to be used for storing sessions and frequent DB queries</strong> is correct. This uses ElastiCache for storing sessions as well as frequent DB queries hence, reducing the load on the database. This should help increase the read performance.</p><p>The option that says: <strong>Having CloudFront enable caching of re-usable content from your website</strong> is correct. This uses CloudFront which is a network of globally distributed edge-locations that caches the content and improves the user experience.</p><p>The option that says: <strong>Scale more frequently by setting the scale up trigger of the Auto Scaling group to 30%</strong> is incorrect. This will increase the number of web server instances but will not reduce the load on the database and hence, will not improve the read performance.</p><p>The option that says: <strong>Create a second installation in another region, and utilize Amazon Route 53's latency-based routing feature to direct requests to the appropriate region</strong> is incorrect. This will not improve read performance. In fact, this setup would add to the cost.</p><p>The option that says: <strong>Replace your existing Auto Scaling group with the AWS Systems Manager State Manager which provides a more effective way to manage and scale your EC2 instances</strong> is incorrect. The AWS Systems Manager State Manager is a secure and scalable service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. It is not a suitable solution to improve the load times of your web application. This is primarily used to control the configuration detail of your instances in your VPC as well as your servers located in your on-premises data centers, such as server configurations, anti-virus definitions, firewall settings, and many others.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><a href=\"https://aws.amazon.com/cloudfront/dynamic-content/\">https://aws.amazon.com/cloudfront/dynamic-content/</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/improve-your-website-performance-with-amazon-cloudfront/\">https://aws.amazon.com/blogs/networking-and-content-delivery/improve-your-website-performance-with-amazon-cloudfront/</a></p><p><br></p><p><strong>Check out these Amazon CloudFront, Amazon ElastiCache, and Amazon RDS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/amazon-elasticache/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/elasticache/",
      "https://aws.amazon.com/cloudfront/dynamic-content/",
      "https://aws.amazon.com/blogs/networking-and-content-delivery/improve-your-website-performance-with-amazon-cloudfront/",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy",
      "https://tutorialsdojo.com/amazon-elasticache/?src=udemy",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 28,
    "question": "<p>A leading financial company runs its application in an Amazon ECS Cluster. The application processes a large stream of intraday data and stores the generated result in a DynamoDB table. To comply with the financial regulatory policy, the solutions architect was tasked to design a system that detects new entries in the DynamoDB table and then automatically run tests to verify the results using a Lambda function.</p><p>Which of the following options can satisfy the company’s requirement with minimal configuration changes?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a DynamoDB stream to detect the new entries and automatically trigger the Lambda function.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Detect the new entries in the DynamoDB table using Systems Manager Automation then automatically invoke the Lambda function for processing.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Migrate the table to Amazon DocumentDB to take advantage of its integration with Amazon EventBridge which can invoke a Lambda function for specific database events.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Run an AWS Lambda function using Amazon SNS as a trigger each time the ECS Cluster successfully processes financial data.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon DynamoDB</strong> is integrated with <strong>AWS Lambda</strong> so that you can create <em>triggers</em>—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.</p><p>If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_lambda_triggers.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_lambda_triggers.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can create a Lambda function which can perform a specific action that you specify, such as sending a notification or initiating a workflow. For instance, you can set up a Lambda function to simply copy each stream record to persistent storage, such as EFS or S3, to create a permanent audit trail of write activity in your table.</p><p>Suppose you have a mobile gaming app that writes to a <code>TutorialsDojoCourses</code> table. Whenever the <code>TopCourse</code> attribute of the <code>TutorialsDojoScores</code> table is updated, a corresponding stream record is written to the table's stream. This event could then trigger a Lambda function that posts a congratulatory message on a social media network. (The function would simply ignore any stream records that are not updates to <code>TutorialsDojoCourses</code> or that do not modify the <code>TopCourse</code> attribute.)</p><p>Therefore, the correct answer is:<strong> Set up a DynamoDB stream to detect the new entries and automatically trigger the Lambda function. </strong>In this way, the requirement can be met with a minimal configuration change. DynamoDB streams can be used as an event source to automatically trigger Lambda functions whenever there is a new entry.</p><p>The option that says:<strong> Run an AWS Lambda function using Amazon SNS as a trigger each time the ECS Cluster successfully processes financial data</strong> is incorrect. You don't need to create an SNS topic just to invoke Lambda functions. You can simply enable DynamoDB streams to meet the requirement with less configuration.</p><p>The option that says:<strong> Migrate the table to Amazon DocumentDB to take advantage of its integration with Amazon EventBridge which can invoke a Lambda function for specific database events </strong>is incorrect. The question requires minimal configuration changes so migrating the database to another service is not recommended as this will require a effort and changes on the application.</p><p>The option that says: <strong>Detect the new entries in the DynamoDB table using Systems Manager Automation then automatically invoking the Lambda function for processing</strong> is incorrect. The Systems Manager Automation service is primarily used to simplify common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. It does not have the capability to detect new entries in a DynamoDB table.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB cheat sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html",
      "https://tutorialsdojo.com/amazon-dynamodb/?src=udemy"
    ]
  },
  {
    "id": 29,
    "question": "<p>A company has a CRM application that uses a MySQL database hosted in Amazon RDS, and a central data warehouse that runs on Amazon Redshift. There is a batch analytics process that runs every day and reads data from RDS. During the execution of the batch analytics, the RDS utilization spikes up, which results in the CRM application becoming unresponsive.<strong> </strong>The top management dashboard must also be updated with new data right after the batch analytics processing completes. However, the dashboard is on another system running on-premises and cannot be modified directly. The only way to update the dashboard is to send an email with the new data to the dashboard system via SMTP, which will then be parsed and processed to update the dashboard with the latest data.</p><p>How would the solutions architect optimize this scenario to solve performance issues and automate the process as much as possible?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add read replicas for the RDS database to speed up batch analytics and use Amazon SNS to notify the on-premises system to update the dashboard.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Consider using Amazon Redshift instead of Amazon RDS as the database for the CRM application. Use Amazon SQS to notify the on-premises system to update the dashboard.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Consider using Amazon Redshift as the main OLTP transactional database instead of RDS for the batch analytics and use Redshift Spectrum to run SQL queries directly against Exabytes of structured or unstructured data in S3 without the need for unnecessary data movement. Utilize Amazon SNS to notify the on-premises system to update the dashboard.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Add read replicas for the RDS database to speed up batch analytics and use Amazon SQS to notify the on-premises system to update the dashboard.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In this scenario, the use of <strong>Amazon RDS Read Replicas</strong> is the best option. It provides enhanced performance and durability for database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, Oracle, and PostgreSQL as well as Amazon Aurora.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_read_replica_promote.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_rds_read_replica_promote.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Adding read replicas for the RDS database to speed up batch analytics and using Amazon SNS to notify the on-premises system to update the dashboard.</strong> It uses Read Replicas which improves the read performance, and it uses SNS which automates the process of notifying the on-premises system to update the dashboard.</p><p>The option that says: <strong>Consider using Amazon Redshift as the main OLTP transactional database instead of RDS for the batch analytics and use Redshift Spectrum to run SQL queries directly against Exabytes of structured or unstructured data in S3 without the need for unnecessary data movement. Utilize Amazon SNS to notify the on-premises system to update the dashboard </strong>is incorrect because Redshift is primarily used for OLAP scenarios whereas RDS is used for OLTP scenarios. Hence, replacing RDS with Redshift is not a valid solution. Although using Redshift Spectrum to run SQL queries is valid, it is still incorrect to replace RDS with Redshift as your main OLTP database.</p><p>The option that says: <strong>Consider using Amazon Redshift instead of Amazon RDS as the database for the CRM application. Use Amazon SQS to notify the on-premises system to update the dashboard</strong> is incorrect because Redshift is used for OLAP scenarios whereas RDS is used for OLTP scenarios. Hence, replacing RDS with Redshift is not a solution.</p><p>The option that says: <strong>Adding read replicas for the RDS database to speed up batch analytics and using Amazon SQS to notify the on-premises system to update the dashboard</strong> is incorrect because SQS is not a service to be used for sending the notification.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/details/read-replicas/\">https://aws.amazon.com/rds/details/read-replicas/</a></p><p><a href=\"https://aws.amazon.com/big-data/datalakes-and-analytics/\">https://aws.amazon.com/big-data/datalakes-and-analytics/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/rds/details/read-replicas/",
      "https://aws.amazon.com/big-data/datalakes-and-analytics/",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 30,
    "question": "<p>A cryptocurrency trading platform uses a Lambda function which has recently been integrated with DynamoDB Streams as its event source. Whenever there is a new deployment, the incoming traffic to the function must be shifted in two increments using CodeDeploy. Ten percent of the incoming traffic should be shifted to the new version and then the remaining 90 percent should be deployed five minutes later. It is also required to trace the event source that invoked the Lambda function including the downstream calls that the function made.</p><p>Which of the following options should the solutions architect implement to satisfy this requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure a <code>Linear</code> deployment configuration for your Lambda function and use AWS Config to trace the event source and downstream calls.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure an <code>All-at-once</code> deployment configuration for your Lambda function and use AWS Config to trace the event source and downstream calls.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure a <code>Canary</code> deployment configuration for your Lambda function. Enable active tracing to integrate AWS X-Ray to your AWS Lambda function.&nbsp; </p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure a <code>Rolling with additional batch</code> deployment configuration for your Lambda function and use X-Ray to trace the event source and downstream calls.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy.</p><p>When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application.</p><p><img src=\"https://media.tutorialsdojo.com/sap_codedeploy_lambda.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_codedeploy_lambda.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>In a Canary deployment configuration, the traffic is shifted in <strong>two</strong> increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p><p>Therefore, the correct answer is: <strong>Configure a </strong><code><strong>Canary</strong></code><strong> deployment configuration for your Lambda function. Enable active tracing to integrate AWS X-Ray to your AWS Lambda function.</strong></p><p>The option that says: <strong>Configure an </strong><code><strong>All-at-once</strong></code><strong> deployment configuration for your Lambda function and using AWS Config to trace the event source and downstream calls</strong> is incorrect. If you use this deployment configuration, the traffic is shifted from the original Lambda function to the updated Lambda function version all at once. In addition, you can't use AWS Config to trace the event source and downstream calls. You have to use X-Ray instead and this can be done by simply enabling active tracing.</p><p>The option that says: <strong>Configure a </strong><code><strong>Linear</strong></code><strong> deployment configuration for your Lambda function and using AWS Config to trace the event source and downstream calls</strong> is incorrect. A deployment configuration will cause the traffic to be shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment.</p><p>The option that says: <strong>Configure a </strong><code><strong>Rolling with additional batch</strong></code><strong> deployment configuration for your Lambda function and using X-Ray to trace the event source and downstream calls</strong> is incorrect. Although X-Ray can be used to trace the event source and downstream calls,Rolling with additional batch is only applicable in Elastic Beanstalk and not for Lambda.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html",
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html",
      "https://tutorialsdojo.com/aws-codedeploy/?src=udemy"
    ]
  },
  {
    "id": 31,
    "question": "<p>The www.tutorialsdojonews.com website is using the WordPress platform that runs on a fleet of Amazon EC2 instances behind an application load balancer to deliver news around the globe. There are a lot of customers complaining about the slow loading time of the website. The solutions architect has created a CloudFront distribution and set the ALB as the origin to improve the read performance. After several days, the IT Security team reported that the setup is not secure and it should enable end-to-end HTTPS connections from the user's browser to the origin via CloudFront.</p><p>Which of the following options should the solutions architect implement to satisfy the above requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use third-party CA certificate on both the origin and CloudFront.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure the CloudFront distribution to redirect HTTP to HTTPS protocol. Generate a new SSL certificate on AWS Certificate Manager and use it as the CloudFront distribution and origin certificate.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use a self-signed certificate in both the origin and CloudFront.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure CloudFront to use its default certificate. Configure the CloudFront distribution to redirect HTTP to HTTPS protocol. For the origin, generate a new SSL certificate on AWS Certificate Manager.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>The certificate issuer you must use depends on whether you want to require HTTPS between viewers and CloudFront or between CloudFront and your origin:</p><p><strong>HTTPS between viewers and CloudFront</strong></p><p>- You can use a certificate that was issued by a trusted certificate authority (CA) such as Comodo, DigiCert, Symantec or other third-party providers.</p><p>- You can use a certificate provided by AWS Certificate Manager (ACM)</p><p><strong>HTTPS between CloudFront and a custom origin</strong></p><p>- If the origin is not an ELB load balancer, such as Amazon EC2, the certificate must be issued by a trusted CA such as Comodo, DigiCert, Symantec or other third-party providers.</p><p>- If your origin is an ELB load balancer, you can also use a certificate provided by ACM.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>If you're using your own domain name, such as tutorialsdojo.com, you need to change several CloudFront settings. You also need to use an SSL/TLS certificate provided by AWS Certificate Manager (ACM), or import a certificate from a third-party certificate authority into ACM or the IAM certificate store.</p><p>Therefore, the correct answer is: <strong>Configure the CloudFront distribution to redirect HTTP to HTTPS protocol. Generate a new SSL certificate on AWS Certificate Manager and use it as the CloudFront distribution and origin certificate.</strong> You can use ACM to generate a valid SSL certificate for your ALB and CloudFront distribution.</p><p>The option that says: <strong>Use a third-party CA certificate on both the origin and CloudFront</strong> is incorrect. You can use a third-party CA certificate on both the custom origin and CloudFront, however, it is cost-effective to just use an ACM-generated SSL as it also supports automatic renewal.</p><p>The options that says:<strong> Use a self-signed certificate in both the origin and CloudFront</strong> is incorrect. The website is hosted in Amazon EC2 and the certificate must be issued by a trusted CA such as Comodo, DigiCert, Symantec, or other third-party providers.</p><p>The option that says: <strong>Configure CloudFront to use its default certificate. Configure the CloudFront distribution to redirect HTTP to HTTPS protocol. For the origin, generate a new SSL certificate on AWS Certificate Manager </strong>is incorrect. You cannot use the default certificate in CloudFront since the website is using a custom domain (www.tutorialsdojonews.com). You can generate a valid SSL certificate for your domain on AWS Certificate Manager.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy"
    ]
  },
  {
    "id": 32,
    "question": "<p>A company recently adopted a modern design for its legacy application. The new application is now suitable for native cloud deployments so the CI/CD pipelines need to be updated as well. The following deployment requirements are needed to support the new application:</p><p>- The pipeline should support deployments of new versions several times every hour.</p><p>- The pipeline should be able to quickly rollback to the previous application version if any problems are encountered on the new version.</p><p>Which of the following options is the recommended solution to meet the company requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Reconfigure the pipeline to create a Staging environment on AWS Elastic Beanstalk. Deploy the newer version on the Staging environment. Swap the Staging and Production environment URLs to shift traffic to the newer version.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Package the newer application version on AMIs including the needed configurations. Update the Launch Template of the Auto Scaling group and trigger a scale-out to use this new AMI. Ensure that the configured termination policy is to delete the old instances using the previous AMI.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Lightsail to handle the deployment of new Amazon EC2 instances and the needed load balancers. Add an Amazon EC2 user data script to download the latest application artifact from an Amazon S3 bucket. Use a weighted routing policy on Amazon Route 53 to slowly shift traffic to the newer version.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Package the newer application version on AMIs including the needed configurations. Reconfigure the CI/CD pipeline to deploy this AMI by replacing the current Amazon EC2 instances.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS Elastic Beanstalk</strong> provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, Immutable, and Traffic splitting) and options that let you configure the batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's a scalable environment, it uses rolling deployments.</p><p>You can use the AWS Elastic Beanstalk console to upload an updated source bundle and deploy it to your Elastic Beanstalk environment or redeploy a previously uploaded version. The following list provides summary information about the different deployment policies and adds related considerations.</p><p><strong>All at once</strong> – The quickest deployment method. Suitable if you can accept a short loss of service and if quick deployments are important to you. With this method, Elastic Beanstalk deploys the new application version to each instance.</p><p><strong>Rolling</strong> – Avoids downtime and minimizes reduced availability at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service. With this method, your application is deployed to your environment one batch of instances at a time.</p><p><strong>Rolling with additional batch</strong> – Avoids any reduced availability at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment.</p><p><strong>Immutable</strong> – A slower deployment method that ensures your new application version is always deployed to new instances instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails.</p><p><strong>Traffic splitting</strong> – A canary testing deployment method. Suitable if you want to test the health of your new application version using a portion of incoming traffic while keeping the rest of the traffic served by the old application version.</p><p>For deployments that depend on resource configuration changes or a new version that can't run alongside the old version, you can launch a new environment with the new version and perform a CNAME swap for a <strong>blue/green deployment</strong>.</p><p>Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment and then <strong>swap CNAMEs of the two environments</strong> to redirect traffic to the new version instantly. With this method, you can have two independent environments and you can quickly switch between the version by swapping the URLs.</p><p><img src=\"https://media.tutorialsdojo.com/sap_elastic_beanstalk_swap_env_urls.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_elastic_beanstalk_swap_env_urls.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Reconfigure the pipeline to create a Staging environment on AWS Elastic Beanstalk. Deploy the newer version on the Staging environment. Swap the Staging and Production environment URLs to shift traffic to the newer version.</strong> This is a blue/green deployment on Elastic Beanstalk. You can deploy a newer version without affecting the current version and quickly roll back by just swapping the URLs again.</p><p>The option that says: <strong>Package the newer application version on AMIs including the needed configurations. Reconfigure the CI/CD pipeline to deploy this AMI by replacing the current Amazon EC2 instances</strong> is incorrect. This is a possible deployment procedure, however, the rollback procedure will take too long because it will need to replace the current EC2 instances again.</p><p>The option that says: <strong>Use Amazon Lightsail to handle the deployment of new Amazon EC2 instances and the needed load balancers. Add an Amazon EC2 user data script to download the latest application artifact from an Amazon S3 bucket. Use a weighted routing policy on Amazon Route 53 to slowly shift traffic to the newer version</strong> is incorrect. This is possible, however, is designed to deploy simple web applications in Dev/Test environments, not for production, mission-critical workloads. The rollback for this procedure will take longer too, as there is a need to re-create newer instances again to run the EC2 user data script.</p><p>The options that says: <strong>Package the newer application version on AMIs including the needed configurations. Update the Launch Template of the Auto Scaling group and trigger a scale-out to use this new AMI. Ensure that the configured termination policy is to delete the old instances using the previous AMI</strong> is incorrect. This is also a possible deployment, however, it is not recommended for this scenario. With this solution, you will have to re-deploy the older AMI in case of a rollback which takes more time compared to just swapping the environment URLs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p><p><br></p><p><strong>Check out the AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html",
      "https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy"
    ]
  },
  {
    "id": 33,
    "question": "<p>A company has multiple database servers hosted on extra-large Reserved Amazon EC2 instances which are all deployed to a private subnet. A single NAT instance is in place to allow the servers to fetch data from the Internet. The solutions architect noticed that whenever there is a new database patch update, the processing takes a lot of time which results in request time-outs. As a workaround, the developers just manually re-run the database patch update on the servers that failed to complete the process the first time.</p><p>What could be the possible root cause of the issue and what steps should the solutions architect implement to solve it?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "The database servers are not in a Placement Group, which means that the inter-instance communications are not optimal. This is causing the timeout issue. Place all the database servers on either a Spread or a Cluster type Placement group to fix the problem.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "The timeout behavior of a NAT instance is that, when there is a connection time out, it sends a FIN packet to resources behind the NAT instance to close the connection. It does not attempt to continue the connection which is why some database updates are failing. For better performance, use a NAT Gateway instead.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "There is no Internet Gateway (IGW) attached to the VPC. Simply add an IGW and the issue will be resolved.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "There is no Virtual Private Gateway attached to the VPC that links up to the Customer Gateway of the database provider. Simply add the missing gateway and the issue will be resolved",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>A <strong>NAT gateway</strong> is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances.</p><p>The NAT gateway replaces the source IPv4 address of the instances with the private IP address of the NAT gateway. When sending response traffic to the instances, the NAT device translates the addresses back to the original source IPv4 addresses.</p><p>When you create a NAT gateway, you specify one of the following connectivity types:</p><p><strong>Public</strong> – (Default) Instances in private subnets can connect to the internet through a public NAT gateway, but cannot receive unsolicited inbound connections from the internet. You create a public NAT gateway in a public subnet and must associate an elastic IP address with the NAT gateway at creation. You route traffic from the NAT gateway to the internet gateway for the VPC. Alternatively, you can use a public NAT gateway to connect to other VPCs or your on-premises network. In this case, you route traffic from the NAT gateway through a transit gateway or a virtual private gateway.</p><p><strong>Private</strong> – Instances in private subnets can connect to other VPCs or your on-premises network through a private NAT gateway. You can route traffic from the NAT gateway through a transit gateway or a virtual private gateway. You cannot associate an elastic IP address with a private NAT gateway. You can attach an internet gateway to a VPC with a private NAT gateway, but if you route traffic from the private NAT gateway to the internet gateway, the internet gateway drops the traffic.</p><p>Take note of the following difference between a NAT Instance and a NAT Gateway when handling a timeout:</p><p><strong>NAT Instance</strong> - When there is a connection time out, a NAT instance sends a FIN packet to resources behind the NAT instance to close the connection.</p><p><strong>NAT Gateway</strong> - When there is a connection time out, a NAT gateway returns an RST packet to any resources behind the NAT gateway that attempt to continue the connection (it does not send a FIN packet).</p><p><img src=\"https://media.tutorialsdojo.com/sap_nat_subnet.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_nat_subnet.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>The timeout behavior of a NAT instance is that, when there is a connection time out, it sends a FIN packet to resources behind the NAT instance to close the connection. It does not attempt to continue the connection which is why some database updates are failing. For better performance, use a NAT Gateway instead.</strong></p><p>The option that says: <strong>There is no Internet Gateway (IGW) attached to the VPC. Simply add an IGW and the issue will be resolved</strong> is incorrect. If there is no Internet Gateway attached to the VPC, any communication to the outside internet will fail, which is not the case here.</p><p>The option that says: <strong>There is no Virtual Private Gateway attached to the VPC that links up to the Customer Gateway of the database provider. Simply add the missing gateway and the issue will be resolved</strong> is incorrect. A Virtual Private Gateway is used for VPN connections. This will not resolve the issue for this scenario.</p><p>The option that says: <strong>The database servers are not in a Placement Group, which means that the inter-instance communications are not optimal. This is causing the timeout issue. Place all the database servers on either a Spread or a Cluster type Placement group to fix the problem</strong> is incorrect. Placement groups are just allocation of servers to be as close together as possible. This does not improve the performance of the NAT instance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 34,
    "question": "<p>A company offers a service that allows users to upload media files through a web portal. The web servers accept the media files and are directly uploaded on the on-premises Network Attached Storage (NAS). For each uploaded media file, a corresponding message is sent to the message queue. A processing server picks up each message and processes each media file which can take up to 30 minutes to process. The company noticed that the number of media files waiting in the processing queue is significantly higher during business hours, but the processing server quickly catches up after business hours. To save costs, the company hired a Solutions Architect to improve the media processing by migrating the workload to AWS Cloud.</p><p>Which of the following options is the most cost-effective solution?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Reconfigure the existing web servers to publish messages to a queue in Amazon MQ. Create an Auto Scaling group of Amazon EC2 instances that will pull requests from the queue and process the media files. Configure the Auto Scaling group to scale based on the length of the SQS queue. Send the processed media files into an Amazon EFS mount point and shut down the EC2 instances after processing is complete.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Reconfigure the existing web servers to publish messages to a standard queue on Amazon SQS. Create an Auto Scaling group of Amazon EC2 instances that will pull requests from the queue and process the media files. Configure the Auto Scaling group to scale based on the length of the SQS queue. Send the processed media files into an Amazon S3 bucket.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Reconfigure the existing web servers to publish messages to a standard queue on Amazon SQS. Create an AWS Lambda function that will pull requests from the SQS queue and process the media files. Invoke the Lambda function every time a new message is sent to the queue. Send the processed media files into an Amazon S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Reconfigure the existing web servers to publish messages to a queue in Amazon MQ. Create an AWS Lambda function that will pull requests from the SQS queue and process the media files. Invoke the Lambda function every time a new message is sent to the queue. Send the processed media files into an Amazon EFS volume.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p><strong>Amazon Simple Queue Service (Amazon SQS)</strong> offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. Amazon SQS offers \"standard\" as the default queue type. Standard queues support at-least-once message delivery.</p><p>You can use standard message queues in many scenarios, as long as your application can process messages that arrive more than once and out of order, for example:</p><p>- Decouple live user requests from intensive background work – Let users upload media while resizing or encoding it.</p><p>- Allocate tasks to multiple worker nodes – Process a high number of credit card validation requests.</p><p>- Batch messages for future processing – Schedule multiple entries to be added to a database.</p><p>You can scale-in/scale-out your Amazon EC2 Auto Scaling group in response to changes in system load in an Amazon Simple Queue Service (Amazon SQS) queue. For example, suppose that you have a web app that lets users upload images and use them online. In this scenario, each image requires resizing and encoding before it can be published. The app runs on EC2 instances in an Auto Scaling group, and it's configured to handle your typical upload rates. Unhealthy instances are terminated and replaced to maintain current instance levels at all times.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_cloudwatch_metric.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_sqs_cloudwatch_metric.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The app places the raw bitmap data of the images in an SQS queue for processing. It processes the images and then publishes the processed images where they can be viewed by users. The architecture for this scenario works well if the number of image uploads doesn't vary over time. But if the number of uploads changes over time, you might consider using dynamic scaling to scale the capacity of your Auto Scaling group. If you use a target tracking scaling policy based on a custom Amazon SQS queue metric, dynamic scaling can adjust to the demand curve of your application more effectively.</p><p><strong>Amazon SQS</strong> is a queue service that is highly scalable, simple to use, and doesn't require you to set up message brokers. AWS recommends this service for new applications that can benefit from nearly unlimited scalability and simple APIs.</p><p><strong>Amazon MQ</strong> is a managed message broker service that provides compatibility with many popular message brokers. AWS recommends Amazon MQ for migrating applications from existing message brokers that rely on compatibility with APIs such as JMS or protocols such as AMQP, MQTT, OpenWire, and STOMP.</p><p>Therefore, the correct answer is: <strong>Reconfigure the existing web servers to publish messages to a standard queue on Amazon SQS. Create an Auto Scaling group of Amazon EC2 instances that will pull requests from the queue and process the media files. Configure the Auto Scaling group to scale based on the length of the SQS queue. Send the processed media files into an Amazon S3 bucket.</strong></p><p>The option that says: <strong>Reconfigure the existing web servers to publish messages to a standard queue on Amazon SQS. Create an AWS Lambda function that will pull requests from the SQS queue and process the media files. Invoke the Lambda function every time a new message is sent to the queue. Send the processed media files into an Amazon S3 bucket</strong> is incorrect. Although this answer is the most cost-effective, AWS Lambda only allows functions to run up to 15 minutes. Remember that the media files can take up to 30 minutes to process.</p><p>The option that says: <strong>Reconfigure the existing web servers to publish messages to a queue in Amazon MQ. Create an Auto Scaling group of Amazon EC2 instances that will pull requests from the queue and process the media files. Configure the Auto Scaling group to scale based on the length of the SQS queue. Send the processed media files into an Amazon EFS mount point and shut down the EC2 instances after processing is complete</strong> is incorrect. It is recommended to use Amazon SQS for this because it is not stated in the scenario to have compatibility with JMS or other protocols like MQTT AMQP, etc. Also, storing media files on Amazon EFS is more expensive than using Amazon S3.</p><p>The option that says: <strong>Reconfigure the existing web servers to publish messages to a queue in Amazon MQ. Create an AWS Lambda function that will pull requests from the SQS queue and process the media files. Invoke the Lambda function every time a new message is sent to the queue. Send the processed media files into an Amazon EFS volume</strong> is incorrect. Lambda functions cannot run for more than 15 minutes, while in the scenario, the processing time is 30 minutes. Moreover, storing media files on Amazon EFS is more expensive than using Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p><p><br></p><p><strong>Check out these Amazon SQS and Amazon MQ Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-mq/?src=udemy\">https://tutorialsdojo.com/amazon-mq/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html",
      "https://tutorialsdojo.com/amazon-sqs/?src=udemy",
      "https://tutorialsdojo.com/amazon-mq/?src=udemy"
    ]
  },
  {
    "id": 35,
    "question": "<p>A company has a multi-tier web application hosted in AWS. It leverages Amazon CloudFront to reliably scale and quickly serve requests from users around the world. After several months in operation, the company received user complaints of slow response time from the web application. The monitoring team reported that the CloudFront cache hit ratio metric is steadily dropping for the past months. This metric indicates that there are inconsistent query strings on user requests and queries that contain upper-case or mixed-case letters. These requests cause CloudFront to send unnecessary origin queries.</p><p>Which of the following actions will increase the cache hit ratio of the CloudFront distribution?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Reconfigure the CloudFront distribution to remove the caching behavior based on query string parameters. This will cache the requests regardless of the order or case of the query parameters.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Reconfigure the CloudFront distribution to ensure that the “case insensitive” option is enabled for processing query string parameters.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Write a Lamda@Edge function that will normalize the query parameters by sorting them in alphabetical order and converting them into lower case. Deploy this function with the CloudFront distribution and set “viewer request” as the trigger to invoke the function.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Launch a reverse proxy inside the application VPC to intercept the requests going to the origin instances. Process the query parameters to sort them by name and convert them to lowercase letters before forwarding them to the instances.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>For each query string parameter that your web application forwards to <strong>CloudFront</strong>, CloudFront forwards requests to your origin for every parameter value and caches a separate version of the object for every parameter value. This is true even if your origin always returns the same object regardless of the parameter value. For multiple parameters, the number of requests and the number of objects multiply. For example, if requests for an object include two parameters that each have three different values, CloudFront caches six versions of that object. AWS recommends that you configure CloudFront to cache based only on the query string parameters for which your origin returns different versions, and that you carefully consider the merits of caching based on each parameter.</p><p>If you configure CloudFront to cache based on query string parameters, you can improve caching if you do the following:</p><p>- Configure CloudFront to forward only the query string parameters for which your origin will return unique objects.</p><p>- Use the same case (uppercase or lowercase) for all instances of the same parameter. For example, if one request contains <code><strong>parameter1=A</strong></code> and another contains <code><strong>parameter1=a</strong></code>, CloudFront forwards separate requests to your origin when a request contains <code><strong>parameter1=A</strong></code> and when a request contains <code><strong>parameter1=a</strong></code>. CloudFront then separately caches the corresponding objects returned by your origin separately even if the objects are identical. If you use just <strong>A</strong> or <strong>a</strong>, CloudFront forwards fewer requests to your origin.</p><p>- List parameters in the same order. As with differences in case, if one request for an object contains the query string <code><strong>parameter1=a&amp;parameter2=b</strong></code> and another request for the same object contains <code><strong>parameter2=b&amp;parameter1=a</strong></code>, CloudFront forwards both requests to your origin and separately caches the corresponding objects even if they're identical. If you always use the same order for parameters, CloudFront forwards fewer requests to your origin.</p><p><strong>Lambda@Edge</strong> is an extension of AWS Lambda, a compute service that lets you execute functions that customize the content that CloudFront delivers. You can author Node.js or Python functions in one Region, US-East-1 (N. Virginia), and then execute them in AWS locations globally that are closer to the viewer, without provisioning or managing servers. Lambda@Edge scales automatically, from a few requests per day to thousands per second. Processing requests at AWS locations closer to the viewer instead of on origin servers significantly reduces latency and improves the user experience.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_cache.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_cache.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>With Lambda@Edge, you can improve your cache hit ratio by making the following changes to query strings before CloudFront forwards requests to your origin:</p><p>- Alphabetize key-value pairs by the name of the parameter.</p><p>- Change the case of key-value pairs to lowercase.</p><p>Therefore, the correct answer is: <strong>Write a Lamda@Edge function that will normalize the query parameters by sorting them in alphabetical order and converting them into lower case. Deploy this function with the CloudFront distribution and set “viewer request” as the trigger to invoke the function. </strong>With the \"viewer request\" set as the trigger, the Lambda@Edge function will normalize the query string before CloudFront processes it. CloudFront will then see the matching cache item for the normalized request, thus increasing the cache hit ratio.</p><p>The option that says: <strong>Reconfigure the CloudFront distribution to remove the caching behavior based on query string parameters. This will cache the requests regardless of the order or case of the query parameters</strong> is incorrect. This will ignore any query parameters and will cache all requests which will cause CloudFront to return incorrect cached items to users.</p><p>The option that says: <strong>Launch a reverse proxy inside the application VPC to intercept the requests going to the origin instances. Process the query parameters to sort them by name and convert them to lowercase letters before forwarding them to the instances</strong> is incorrect. This will not increase the cache hit ratio because CloudFront already forwarded the request to the origin as the proxy processes it which defeats the purpose of having a CloudFront cache.</p><p>The option that says: <strong>Reconfigure the CloudFront distribution to ensure that the “case insensitive” option is enabled for processing query string parameters</strong> is incorrect. CloudFront is case-sensitive when caching objects. There is no \"case-insensitive\" option in CloudFront. You will have to normalize your query parameters if you want all requests to be in lower-case.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-normalize-query-string-parameters\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-normalize-query-string-parameters</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html</a></p><p><a href=\"https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html\">https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html</a></p><p><a href=\"https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html#cache-hit-ratio-query-string-parameters\">https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html#cache-hit-ratio-query-string-parameters</a></p><p><br></p><p><strong>Check out these Amazon CloudFront and AWS Lambda Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><a href=\"https://tutorialsdojo.com/aws-lambda/?src=udemy\">https://tutorialsdojo.com/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-normalize-query-string-parameters",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html",
      "https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html",
      "https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html#cache-hit-ratio-query-string-parameters",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy",
      "https://tutorialsdojo.com/aws-lambda/?src=udemy"
    ]
  },
  {
    "id": 36,
    "question": "<p>A company uses an AWS CloudFormation template to deploy its three-tier web application on the AWS Cloud. The CloudFormation template contains a custom AMI value used by the Auto Scaling group of Amazon EC2 instances. Every new version of the application corresponds to a new AMI that needs to be deployed. The company doesn’t want any downtime during the deployment process. The Solutions Architect has been tasked to implement a solution that will streamline its AMI deployment process by doing the following steps:</p><p> - Update the CloudFormation template to refer to the new AMI.</p><p> - Launch new EC2 instances from the new AMI by using the calling the <code>UpdateStack</code> API to replace the&nbsp;old EC2 instances.</p><p>Which of the following actions should the Solutions Architect take to achieve the above requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Copy the updated template and deploy it to a new CloudFormation stack. After its successful deployment, update the Amazon Route 53 records to point to the new stack and delete the old stack.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Update the CloudFormation template <code>AWS::AutoScaling::LaunchConfiguration</code> resource section and specify a <code>DeletionPolicy</code> attribute with <code>MinSuccessfulInstancesPercent</code> of 50.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a new CloudFormation change set to view the changes in the new version of the template. Verify that the correct AMI is listed on the change before executing the change set.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Update the CloudFormation template <code>AWS::AutoScaling::AutoScalingGroup</code> resource section and specify an <code>UpdatePolicy</code> attribute with an <code>AutoScalingRollingUpdate</code>.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS CloudFormation</strong> gives you an easy way to model a collection of related AWS and third-party resources, provision them quickly and consistently, and manage them throughout their lifecycles, by treating infrastructure as code. You can use a template to create, update, and delete an entire stack as a single unit, as often as you need to, instead of managing resources individually.</p><p>The <code><strong>AWS::AutoScaling::AutoScalingGroup</strong></code> resource defines an Amazon EC2 Auto Scaling group, which is a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>When you update the launch template for an Auto Scaling group, this update action does not deploy any change across the running Amazon EC2 instances in the Auto Scaling group. All new instances will get the updated configuration, but existing instances continue to run with the configuration that they were originally launched with. This works the same way as any other Auto Scaling group.</p><p>You can add an <code><strong>UpdatePolicy</strong></code> attribute to your stack to perform rolling updates (or replace the group) when a change has been made to the group. Alternatively, you can force a rolling update on your instances at any time after updating the stack by starting an instance refresh.</p><p>The <code>UpdatePolicy</code> on the <code>AutoScalingGroup</code> will automatically execute the rolling deployment of the new AMI instances when the Cloudformation template is updated.</p><p>To specify how AWS CloudFormation handles rolling updates for an Auto Scaling group, use the <code><strong>AutoScalingRollingUpdate</strong></code> policy. Rolling updates enable you to specify whether AWS CloudFormation updates instances that are in an Auto Scaling group in batches or all at once. For example, suppose you have updated the <code>MaxBatchSize</code> in your stack template's <code>UpdatePolicy</code> from 1 to 10. This allows you to perform updates without causing downtime to your currently running application.</p><p>Therefore, the correct answer is: <strong>Update the CloudFormation template </strong><code><strong>AWS::AutoScaling::AutoScalingGroup</strong></code><strong> resource section and specify an </strong><code><strong>UpdatePolicy</strong></code><strong> attribute with an </strong><code><strong>AutoScalingRollingUpdatepolicy</strong></code><strong>.</strong></p><p>The option that says: <strong>Create a new CloudFormation change set to view the changes in the new version of the template. Verify that the correct AMI is listed on the change before executing the change set</strong> is incorrect. Although this is possible, the existing EC2 instances won't be replaced immediately this way. The change set will only update the Launch Template with the new AMI and will only be applied to the newly spawned instances, excluding the existing ones.</p><p>The option that says: <strong>Update the CloudFormation template </strong><code><strong>AWS::AutoScaling::LaunchConfiguration</strong></code><strong> resource section and specify a </strong><code><strong>DeletionPolicy</strong></code><strong> attribute with </strong><code><strong>MinSuccessfulInstancesPercentof</strong></code><strong> 50</strong> is incorrect. This option should have used a <code>CreationPolicy</code> attribute instead of a <code>DeletionPolicy</code> attribute because there is no <code>MinSuccessfulInstancesPercentof</code> element in the <code>DeletionPolicy</code> attribute.</p><p>The option that says: <strong>Copy the updated template and deploy it to a new CloudFormation stack. After its successful deployment, update the Amazon Route 53 records to point to the new stack and delete the old stack</strong> is incorrect. Although this is possible, it involves an extra step that requires updating the Route 53 entry for every deployment. This does not satisfy the deployment requirement of just calling the UpdateStack API to replace the instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_UpdateStack.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_UpdateStack.html</a></p><p><br></p><p><strong>Check out the Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_UpdateStack.html",
      "https://tutorialsdojo.com/amazon-cloudfront/?src=udemy"
    ]
  },
  {
    "id": 37,
    "question": "<p>A media company runs its new content management system (CMS) on a Windows-based Amazon EC2 instance. This is a test setup with a single instance. After a few weeks of testing, the application will be deployed on a production environment. For high availability, the application will be hosted on at least three Amazon EC2 instances across multiple Availability Zones. The current test EC2 instance has a 1 TB Amazon Elastic Block Store (EBS) volume as its root device. This is where all the static content is stored.</p><p>The solutions architect must ensure that all instances will have the same data at all times, for the application to work properly. The filesystem must also support Windows ACLs to control access to file contents. Additionally, all instances must be joined to the company’s Active Directory domain. The solution should have the least amount of management overhead.</p><p>Which of the following options should the Solutions Architect implement to meet the company's requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy a new Windows AMI for an Auto Scaling group with a minimum size of three instances and spans across three Availability Zones (AZs). Use an Amazon EBS volume with Multi-Attach enabled to allow multiple Amazon EC2 instances to share the volume. Write a user data script to install the CMS application and join the instances to the AD domain.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy a new Windows AMI for an Auto Scaling group with a minimum size of three instances and spans across three Availability Zones (AZs). Create an Amazon FSx for Windows File Server file system that will be used for shared storage. Write a user data script to install the CMS application, mount the FSx for Windows File Server file system and join the instances to the AD domain.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon Machine Image (AMI) of the test Amazon EC2 instance. Use the AMI for an Auto Scaling group with a minimum size of three instances that spans three Availability Zones (AZs). Create an Amazon FSx for Lustre filesystem that will be used for shared storage. Write a user data script to join the instances on the AD domain and mount the EFS share upon boot-up.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon Machine Image (AMI) of the test Amazon EC2 instance. Use the AMI for an Auto Scaling group with a minimum size of three instances that spans three Availability Zones (AZs). Create an Amazon Elastic Filesystem (Amazon EFS) volume. Write a user data script to join the instances on the AD domain and mount the EFS share upon boot-up.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon FSx for Windows File Server</strong> provides fully managed Microsoft Windows file servers, backed by a fully native Windows file system. FSx for Windows File Server has the features, performance, and compatibility to easily lift and shift enterprise applications to the AWS Cloud.</p><p>Amazon FSx supports a broad set of enterprise Windows workloads with fully managed file storage built on Microsoft Windows Server. Amazon FSx has native support for Windows file system features and for the industry-standard <strong>Server Message Block (SMB)</strong> protocol to access file storage over a network.</p><p>As a fully managed service, FSx for Windows File Server eliminates the administrative overhead of setting up and provisioning file servers and storage volumes. Additionally, Amazon FSx keeps Windows software up to date, detects and addresses hardware failures, and performs backups.</p><p><img src=\"https://media.tutorialsdojo.com/sap_fsx_windows_direct_connect.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_fsx_windows_direct_connect.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>AWS recommends using a staging environment with the same configuration as your production environment. For example, use the same <strong>Active Directory (AD)</strong> and networking configurations, file system size and configuration, and Windows features, such as data deduplication and shadow copies. Running test workloads in a staging environment that simulates your desired production traffic helps the process run smoothly.</p><p>Therefore, the correct answer is: <strong>Deploy a new Windows AMI for an Auto Scaling group with a minimum size of three instances and spans across three Availability Zones (AZs). Create an Amazon FSx for Windows File Server file system that will be used for shared storage. Write a user data script to install the CMS application, mount the FSx for Windows File Server file system and join the instances to the AD domain.</strong> Amazon FSx for Windows File Server is a scalable file storage that is accessible over SMB protocol. Since it is built on Windows Server, it natively supports administrative features such as user quotas, end-user file restore, and Microsoft Active Directory integration.</p><p>The option that says: <strong>Create an Amazon Machine Image (AMI) of the test Amazon EC2 instance. Use the AMI for an Auto Scaling group with a minimum size of three instances that spans three Availability Zones (AZs). Create an Amazon Elastic Filesystem (Amazon EFS) volume. Write a user data script to join the instances on the AD domain and mount the EFS share upon boot-up</strong> is incorrect. Amazon EFS uses the NFS protocol which is primarily used by Linux AMIs. This filesystem does not support Windows ACLs.</p><p>The option that says: <strong>Create an Amazon Machine Image (AMI) of the test Amazon EC2 instance. Use the AMI for an Auto Scaling group with a minimum size of three instances that spans three Availability Zones (AZs). Create an Amazon FSx for Lustre filesystem that will be used for shared storage. Write a user data script to join the instances on the AD domain and mount the EFS share upon boot-up</strong> is incorrect. Amazon FSx for Lustre is POSIX-compliant file system that runs on Lustre. It can only be used by Linux-based instances.</p><p>The option that says: <strong>Deploy a new Windows AMI for an Auto Scaling group with a minimum size of three instances and spans across three Availability Zones (AZs). Use an Amazon EBS volume with Multi-Attach enabled to allow multiple Amazon EC2 instances to share the volume. Write a user data script to install the CMS application and join the instances to the AD domain</strong> is incorrect. This is not an ideal solution because Multi-Attach EBS volumes can only be attached on instances within the same Availability Zone.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html</a></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/getting-started.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/getting-started.html</a></p><p><br></p><p><strong>Check out this Amazon FSx Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-fsx/?src=udemy\">https://tutorialsdojo.com/amazon-fsx/</a></p><p><br></p><p><strong>Check out this Amazon EFS, Amazon FSx for Windows and Amazon FSx for Lustre Comparison:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-efs-vs-amazon-fsx-for-windows-vs-amazon-fsx-for-lustre/?src=udemy\">https://tutorialsdojo.com/amazon-efs-vs-amazon-fsx-for-windows-vs-amazon-fsx-for-lustre/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html",
      "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html",
      "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/getting-started.html",
      "https://tutorialsdojo.com/amazon-fsx/?src=udemy",
      "https://tutorialsdojo.com/amazon-efs-vs-amazon-fsx-for-windows-vs-amazon-fsx-for-lustre/?src=udemy"
    ]
  },
  {
    "id": 38,
    "question": "<p>A financial company is building a new online document portal system that allows its employees and developers to upload yearly and bi-annual corporate earnings report files to a private S3 bucket in which other confidential corporate files will also be stored. You are working as a Solutions Architect and you were instructed to create the private S3 bucket as well as the IAM users for the application developers to start their work. You assigned the required policies in IAM to the developers that allows them read and write access to the S3 bucket. After a few weeks, they have completed the new online portal and hosted it on a fleet of Spot EC2 instances. One of the application developers created a pre-signed URL that points to the correct S3 bucket and after a few testing, he has successfully uploaded the files from his laptop using the generated URL. He then made the necessary code change to the online portal to generate the pre-signed URL to upload the files in S3. However, after a few days, the development team complained that they cannot upload the files anymore using the online portal.&nbsp; &nbsp;Which of the following options are valid reasons for this behavior? (Select TWO.) </p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "The ACL of the S3 bucket blocks the online portal and prevents the developers from uploading any files.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The required AWS credentials in the <code>~/.aws/credentials</code> configuration file located on the EC2 instances of the online portal were misconfigured</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "The application developers do not have access to either read or upload objects to the S3 bucket.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "There was a recent change in the S3 bucket that allows object versioning which invalidates all presigned URLs.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>The expiration date of the pre-signed URL is incorrectly set to expire too quickly and thus, may have already expired when they used it.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In this scenario, the main issue is that the online portal cannot upload files to the S3 bucket but the application developers can successfully upload files on their laptops. Take note that in this scenario, the online portal is deployed to a group of EC2 instances and it was not mentioned that you attached an IAM Role to these instances nor added security credentials in the <code>~/.aws/credentials</code> configuration file.</p><p>With all of these data in mind, we can deduce that the online portal is generating pre-signed URLs that are set to have an overly tight expiration date which causes the issue. In addition, there might be no security credentials added in the EC2 instances that host the online portal considering that it is not mentioned in the scenario. Remember that this is required to properly generate the pre-signed URLs.</p><p><img src=\"https://media.tutorialsdojo.com/sap_linux_aws_cli_credentials.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_linux_aws_cli_credentials.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answers are:</p><p><strong>- The expiration date of the pre-signed URL is incorrectly set to expire too quickly and thus, may have already expired when they used it</strong>.</p><p><strong>- The required AWS credentials in the </strong><code><strong>~/.aws/credentials</strong></code><strong> configuration file located on the EC2 instances of the online portal were misconfigured</strong></p><p>The option that says:<strong> There was a recent change in the S3 bucket that allows object versioning which invalidates all presigned URLs </strong>is incorrect. Enabling object versioning in S3 will not hinder uploads that are done via a pre-signed URL.</p><p>The options that says: <strong>The application developers do not have access to either read or upload objects to the S3 bucket</strong> is incorrect. You have already provided the IAM role.</p><p>The option that says: <strong>The ACL of the S3 bucket blocks the online portal and prevents the developers from uploading any files</strong> is incorrect based on the fact that one developer has managed to successfully upload one file in the S3 bucket. The S3 ACL of the bucket is not an issue here.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html</a></p><p><a href=\"https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/s3-example-presigned-urls.html\">https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/s3-example-presigned-urls.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html",
      "https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/s3-example-presigned-urls.html",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 39,
    "question": "<p>A company is running its main web service in a fleet of Amazon EC2 instances in the us-east-1 AWS Region. The EC2 instances are launched by an Auto Scaling group behind an Application Load Balancer (ALB). The EC2 instances are spread across multiple Availability Zones. The MySQL database is hosted on an Amazon EC2 instance in a private subnet. To improve the resiliency of the web service in case of a disaster, the Solutions Architect must design a data recovery strategy in another region using the available AWS services to lessen the operational overhead. The target RPO is less than a minute and the target RTO is less than 5 minutes. The Solutions Architect has started to provision the ALB and the Auto Scaling group on the us-west-2 region.</p><p>Which of the following steps should be implemented next to achieve the above requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Migrate the database from the Amazon EC2 instance to an Amazon Aurora global database. Set the us-east-1 region as the primary database and the us-west-2 region as the secondary database. Configure Amazon Route 53 DNS entry with health checks and failover routing policy to the us-west-2 region.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Migrate the database from the Amazon EC2 instance to an Amazon RDS for MySQL instance. Enable Multi-AZ deployment for this database. Configure Amazon Route 53 DNS entry with failover routing policy to the us-west-2 region.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Migrate the database from the Amazon EC2 instance to an Amazon RDS for MySQL instance. Set the us-east-1 region database as the master and configure a cross-Region read replica to the us-west-2 region. Configure Amazon Route 53 DNS entry with health checks and failover routing policy to the us-west-2 region.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a snapshot of the current Amazon EC2 database instance and restore the snapshot to the us-west-2 region. Configure the new EC2 instance as MySQL standby database of the us-east-1 instance. Configure Amazon Route 53 DNS entry with failover routing policy to the us-west-2 region.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Aurora Global Database</strong> is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.</p><p>Critical workloads with a global footprint, such as financial, travel, or gaming applications, have strict availability requirements and may need to tolerate a region-wide outage. Traditionally this required difficult tradeoffs between performance, availability, cost, and data integrity. Global Database uses storage-based replication with typical latency of less than 1 second, using dedicated infrastructure that leaves your database fully available to serve application workloads. In the unlikely event of a regional degradation or outage, one of the secondary regions can be promoted to read and write capabilities in less than 1 minute.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_aurora_global_database.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_rds_aurora_global_database.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Aurora Global Database lets you easily scale database reads across the world and place your applications close to your users. Your applications enjoy quick data access regardless of the number and location of secondary regions, with typical cross-region replication latencies below 1 second. If your primary region suffers a performance degradation or outage, you can promote one of the secondary regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute even in the event of a complete regional outage. This provides your application with an effective Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan.</p><p><strong>Amazon Route 53 health checks</strong> monitor the health and performance of your web applications, web servers, and other resources. If you have multiple resources that perform the same function, you can configure DNS failover so that Route 53 will route your traffic from an unhealthy resource to a healthy resource. Each health check that you create can monitor one of the following:</p><p>- The health of a specified resource, such as a web server</p><p>- The status of other health checks</p><p>- The status of an Amazon CloudWatch alarm</p><p>Therefore, the correct answer is: <strong>Migrate the database from the Amazon EC2 instance to an Amazon Aurora global database. Set the us-east-1 region as the primary database and the us-west-2 region as the secondary database. Configure Amazon Route 53 DNS entry with health checks and failover routing policy to the us-west-2 region.</strong></p><p>The option that says: <strong>Migrate the database from the Amazon EC2 instance to an Amazon RDS for MySQL instance. Set the us-east-1 region database as the master and configure a cross-Region read replica to the us-west-2 region. Configure Amazon Route 53 DNS entry with health checks and failover routing policy to the us-west-2 region</strong> is incorrect. Although this is possible, there is no automatic way to promote the read replica on the backup region as the master database. You need to manually configure this, and when you do, the RDS instance will reboot. In this case, you might exceed the RPO of 1 minute and RTO of 5 minutes.</p><p>The option that says: <strong>Migrate the database from the Amazon EC2 instance to an Amazon RDS for MySQL instance. Enable Multi-AZ deployment for this database. Configure Amazon Route 53 DNS entry with failover routing policy to the us-west-2 region</strong> is incorrect. Multi-AZ deployment will protect you from outages on single AZ’s only. It will not protect your database from regional outages.</p><p>The option that says: <strong>Create a snapshot of the current Amazon EC2 database instance and restore the snapshot to the us-west-2 region. Configure the new EC2 instance as MySQL standby database of the us-east-1 instance. Configure Amazon Route 53 DNS entry with failover routing policy to the us-west-2 region</strong> is incorrect. Although this is a possible solution, the requirement is to use the available AWS services for lower operational overhead. This requires extra management effort to set up, configure and manage the database on the EC2 instance, instead of using a managed Amazon RDS database. Moreover, it won't be able to satisfy the requirement of providing a 1-minute RPO and 5-minute RTO.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><br></p><p><strong>Check out these Amazon Aurora Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora/?src=udemy\">https://tutorialsdojo.com/amazon-aurora/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora-vs-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-aurora-vs-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/rds/aurora/global-database/",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
      "https://tutorialsdojo.com/amazon-aurora/?src=udemy",
      "https://tutorialsdojo.com/amazon-aurora-vs-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 40,
    "question": "<p>A company runs an application in a fleet of Amazon EC2 instances in the us-east-2 region. A database server is hosted on the on-premises data center which complies with the BASE (Basically Available, Soft state, Eventual consistency) model rather than the ACID (Atomicity, Consistency, Isolation, Durability) consistency model. The on-premises network has a 10 GB AWS Direct Connect connection to the Amazon VPC in us-east-2. The application relies on this database for normal operations. Whenever there are lots of database write requests, the application behavior becomes erratic.</p><p>Which of the following options should the solutions architect implement to improve the performance of the application in a cost-effective way?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon RDS multi-AZ instance that will synchronize with the on-premises database server using Amazon EventBridge. Redirect the write operations of the application to the Amazon RDS endpoint via the Amazon Elastic Transcoder service.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon SQS queue and develop a consumer process to flush the queue to the on-premises database server. Update the application to enable writing to the SQS queue.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a Hadoop cluster using Amazon Elastic Map Reduce (EMR) and use the S3DistCp tool to synchronize data between the on-premises database and the Hadoop cluster.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Update the application to write to an Amazon DynamoDB table. Feed the table to an Amazon EMR cluster and create a map function that will update the on-premises database for every table update.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Simple Queue Service (Amazon SQS)</strong> offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. Since the application relies on an eventual consistency model, there should be no problem on adding an SQS queue in front of the database.</p><p>Decoupling message queuing from the database improves database availability and enables greater message queue scalability. It also provides a more cost-effective use of the database, and mitigates backpressure created when database performance is constrained by message management.</p><p>You can use standard message queues in many scenarios, as long as your application can process messages that arrive more than once and out of order, for example:</p><p>- Decouple live user requests from intensive background work: Let users upload media while resizing or encoding it.</p><p>- Allocate tasks to multiple worker nodes: Process a high number of credit card validation requests.</p><p>- Batch messages for future processing: Schedule multiple entries to be added to a database.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_database.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_sqs_database.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Create an Amazon SQS queue and develop a consumer process to flush the queue to the on-premises database server. Update the application to enable writing to the SQS queue.</strong> Since the application follows the eventual consistency model, an SQS can be used to temporarily hold the write requests, while a worker process flushes the queue to the on-premises database.</p><p>The option that says: <strong>Create a Hadoop cluster using Amazon Elastic Map Reduce (EMR) and use the S3DistCp tool to synchronize data between the on-premises database and the Hadoop cluster </strong>is incorrect. S3DistCp tool is used to copy large amounts of data from Amazon S3 into HDFS. It is not suitable for synchronizing data to an on-premises database.</p><p>The option that says:<strong> Create an Amazon RDS multi-AZ instance that will synchronize with the on-premises database server using Amazon EventBridge. Redirect the write operations of the application to the Amazon RDS endpoint via the Amazon Elastic Transcoder service</strong> is incorrect. The application is using the BASE consistency model so an SQL-based database, such as an Amazon RDS, may not be compatible with the data to be written by the application. Moreover, you cannot directly synchronize your on-premises database server using Amazon EventBridge. Amazon Elastic Transcoder is simply a media transcoding service in AWS; thus, it can't be used to send write operations to your Amazon RDS endpoint.</p><p>The option that says: <strong>Update the application to write to an Amazon DynamoDB table. Feed the table to an Amazon EMR cluster and create a map function that will update the on-premises database for every table update</strong> is incorrect. This may be possible but creating a DynamoDB table with a high WCU and an EMR cluster significantly increases operational costs compared to using an SQS queue.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/architecture/modernized-database-queuing-using-amazon-sqs-and-aws-services/\">https://aws.amazon.com/blogs/architecture/modernized-database-queuing-using-amazon-sqs-and-aws-services/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p><p><br></p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/architecture/modernized-database-queuing-using-amazon-sqs-and-aws-services/",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html",
      "https://aws.amazon.com/sqs/features/",
      "https://tutorialsdojo.com/amazon-sqs/?src=udemy"
    ]
  },
  {
    "id": 41,
    "question": "<p>A startup currently runs a web application on an extra-large Amazon EC2 instance. The application allows users to upload and download various pdf files from a private Amazon S3 bucket using a pre-signed URL. The web application checks if the file being requested actually exists in the S3 bucket before generating the URL.</p><p>In this scenario, how should the solutions architect configure the web application to access the Amazon S3 bucket securely?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>1. Create an IAM user with the appropriate permissions allowing access and listing of all of the objects of the S3 bucket. Associate the EC2 instance with the IAM user. </p><p>2. Program your web application to retrieve the user credentials from the EC2 instance metadata. </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>1. Create an IAM role with a policy that allows listing of the objects in the S3 bucket. Launch the EC2 instance with the IAM role.</p><p>2. Program your web application to retrieve the temporary security credentials from the EC2 instance user data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>1. Store your access keys inside the EC2 instance. </p><p>2. Program your web application to retrieve the AWS credentials from the instance to interact with the objects in the S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>1. Create an IAM role with a policy that allows listing and uploading of the objects in the S3 bucket. Launch the EC2 instance with the IAM role. </p><p>2. Program your web application to retrieve the temporary security credentials from the EC2 instance metadata.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>Applications that run on an EC2 instance must include AWS credentials in their AWS API requests. You could store AWS credentials directly within the EC2 instance and allow applications in that instance to use those credentials. But you would then have to manage the credentials and ensure that they securely pass the credentials to each instance and update each EC2 instance when it's time to rotate the credentials. That's a lot of additional work. Instead, you can and should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as a user name and password or access keys) to an EC2 instance.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ec2_iam_role_s3_bucket.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ec2_iam_role_s3_bucket.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests.</p><p>When the application runs, it obtains temporary security credentials from Amazon EC2 instance metadata. These are temporary security credentials that represent the role and are valid for a limited period of time to access various AWS resources. You can fetch the temporary security credentials from the instance by requesting it from this endpoint:</p><p><code>curl http://169.254.169.254/latest/meta-data/iam/security-credentials/s3access</code></p><p>In this particular scenario, you have to use a combination of IAM Role and EC2 instance metadata to provide the web application the required access it needs to access the S3 bucket. Hence, the correct answer is the following option:</p><p><strong>1. Create an IAM role with a policy that allows listing and uploading of the objects in the S3 bucket. Launch the EC2 instance with the IAM role.</strong></p><p><strong>2. Program your web application to retrieve the temporary security credentials from the EC2 instance metadata.</strong></p><p><br></p><p>The following option is incorrect because it is a security vulnerability to store the AWS credentials in your EC2 instances:</p><p><strong>1. Store your access keys inside the EC2 instance.</strong></p><p><strong>2. Program your web application to retrieve the AWS credentials from the instance to interact with the objects in the S3 bucket.</strong></p><p>Anyone who has access to that EC2 instance can find your AWS credentials and hence, this is not a recommended option.</p><p><br></p><p>The following option is incorrect because you should have used an IAM Role instead of an IAM User:</p><p><strong>1. Create an IAM user with the appropriate permissions allowing access and listing of all of the objects of the S3 bucket. Associate the EC2 instance with the IAM user.</strong></p><p><strong>2. Program your web application to retrieve the user credentials from the EC2 instance metadata.</strong></p><p><br></p><p>The following option is incorrect because you should retrieve the IAM role's credentials from the EC2 instance metadata and not from the user data:</p><p><strong>1. Create an IAM role with a policy that allows listing of the objects in the S3 bucket. Launch the EC2 instance with the IAM role.</strong></p><p><strong>2. Program your web application to retrieve the temporary security credentials from the EC2 instance user data.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#instance-metadata-security-credentials\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#instance-metadata-security-credentials</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#instance-metadata-security-credentials",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy"
    ]
  },
  {
    "id": 42,
    "question": "<p>A leading aerospace engineering company is experiencing high growth and demand on their highly available and fault-tolerant cloud services platform that is hosted in AWS. The technical lead of your team has asked you to virtually extend two existing on-premises data centers into AWS cloud to support an online flight-tracking service that is used by a lot of airline companies. The online service heavily depends on existing, on-premises resources located in multiple data centers and static content that is served from an S3 bucket. To meet the requirement, you launched a dual-tunnel VPN connection between your CGW and VGW.</p><p>In this scenario, which component of your cloud architecture represents a potential single point of failure, which you should consider changing to make the solution more highly available?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a second Virtual Gateway in a different AZ and a Customer Gateway in a different data center. Create another dual-tunnel connection to ensure high-availability and fault-tolerance.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up a NAT Gateway in a different data center and set up another dual-tunnel VPN connection.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create another Virtual Gateway in a different AZ and create another dual-tunnel VPN connection.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create another Customer Gateway in a different data center and set up another dual-tunnel VPN connection.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In this question, you will easily get confused if you do not know the basics of VPC and other AWS fundamentals. You can eliminate the obviously wrong answers and then just choose between the remaining options.</p><p>Remember that only one virtual private gateway (VGW) can be attached to a VPC at a time.</p><p>A Site-to-Site VPN connection offers two VPN tunnels between a virtual private gateway or a transit gateway on the AWS side, and a customer gateway (which represents a VPN device) on the remote (on-premises) side. A <strong>virtual private gateway</strong> is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection. You create a virtual private gateway and attach it to the VPC from which you want to create the Site-to-Site VPN connection.</p><p><img src=\"https://media.tutorialsdojo.com/sap_virtual_private_gateway.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_virtual_private_gateway.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The correct answer is: <strong>Create another Customer Gateway in a different data center and setting up another dual-tunnel VPN connection.</strong> This will ensure high availability for your online flight-tracking service.</p><p>The option that says:<strong> Create another Virtual Gateway in a different AZ and create another dual-tunnel VPN connection</strong> is incorrect. There can only be one VGW attached to a VPC at a given time.</p><p>The option that says: <strong>Create a second Virtual Gateway in a different AZ and a Customer Gateway in a different data center. Create another dual-tunnel connection to ensure high-availability and fault-tolerance</strong> is incorrect. There can only be one VGW attached to a VPC at a given time.</p><p>The option that says: <strong>Setting up a NAT Gateway in a different data center and setting up another dual-tunnel VPN connection</strong> is incorrect. You don't need to use a NAT gateway in this situation. NAT is basically used to enable EC2 instances launched in the private subnet to access the Internet while blocking incoming public requests to the VPC.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Appendix_Limits.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Appendix_Limits.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/NetworkAdminGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonVPC/latest/NetworkAdminGuide/Introduction.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Appendix_Limits.html",
      "https://docs.aws.amazon.com/AmazonVPC/latest/NetworkAdminGuide/Introduction.html",
      "https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy"
    ]
  },
  {
    "id": 43,
    "question": "<p>A cryptocurrency startup owns multiple AWS accounts which are all linked under AWS Organizations. Due to the financial nature of the business, the DevOps lead has been instructed by the CTO to prepare for IT auditing activities to meet industry compliance requirements.</p><p>Which of the following provides the most durable and secure logging solution that can be used to track changes made to all of the company’s AWS resources globally?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "1. Launch a new CloudTrail trail using the AWS console with an existing S3 bucket to store the logs and with the \"Apply trail to all regions\" checkbox enabled.\n2. Enable MFA Delete on the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "1. Launch a new CloudTrail with one new S3 bucket to store the logs. \n2. Configure SNS to send log file delivery notifications to your management system. \n3. Enable MFA Delete and Log Encryption on the S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>1. Launch a new CloudTrail trail using the AWS console with one new S3 bucket to store the logs and with the \"Enable for all accounts in my organization\" checkbox enabled.</p><p>2. Enable MFA Delete and Log Encryption on the S3 bucket.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>1. Launch three new CloudTrail trails using three new S3 buckets to store the logs for the AWS Management console, for AWS SDKs, and for the AWS CLI.</p><p>2. Enable MFA Delete and Log Encryption on the S3 bucket.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS CloudTrail</strong> is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail. Events include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_enable_organization.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_enable_organization.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>CloudTrail is enabled on your AWS account when you create it. When activity occurs in your AWS account, that activity is recorded in a CloudTrail event. You can easily view recent events in the CloudTrail console by going to Event history. You can also enable the tracking of multi-region and global events. By default, the log files delivered by CloudTrail to your bucket are encrypted by Amazon server-side encryption with Amazon S3-managed encryption keys (SSE-S3). To provide a security layer that is directly manageable, you can instead use server-side encryption with AWS KMS keys (SSE-KMS) for your CloudTrail log files.</p><p>If you have created an organization in <strong>AWS Organizations</strong>, you can create a trail that will log all events for all AWS accounts in that organization. This is sometimes referred to as an <strong>organization trail</strong>. You can also choose to edit an existing trail in the management account and apply it to an organization, making it an organization trail. Organization trails log events for the management account, and all member accounts in the organization.</p><p><strong>Organization trails</strong> are similar to regular trails in many ways. You can create multiple trails for your organization and choose whether to create an organization trail in all regions or a single region, and what kinds of events you want logged in your organization trail, just as in any other trail.</p><p>To create an organization trail, ensure that the \"Enable for all accounts in my organization\" option is checked when you create a new CloudTrail trail.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_organization.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_organization.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>To protect your logs, you can encrypt the S3 bucket and add MFA Delete to protect your trail logs from accidental deletions. In this scenario, the following option is the best answer as it provides all of the things mentioned above:</p><p><strong>1. Launch a new CloudTrail trail using the AWS console with one new S3 bucket to store the logs and with the \"Enable for all accounts in my organization\" checkbox enabled.</strong></p><p><strong>2. Enable MFA Delete and Log Encryption on the S3 bucket.</strong></p><p>The following option is incorrect because although CloudTrail encrypts the data by default using SSE-S3, it is still more secure if you enabled log encryption and use SSE-KMS. Take note that the scenario asked for the most durable and secure logging solution:</p><p><strong>1. Launch a new CloudTrail trail using the AWS console with an existing S3 bucket to store the logs and with the \"Apply trail to all regions\" checkbox</strong></p><p><strong>2. Enable MFA Delete on the S3 bucket.</strong></p><p>The following option is incorrect because the multi-region option is not enabled which is needed to fetch all CloudTrail trail from all AWS regions:</p><p><strong>1. Launch a new CloudTrail with one new S3 bucket to store the logs.</strong></p><p><strong>2. Configure SNS to send log file delivery notifications to your management system.</strong></p><p><strong>3. Enable MFA Delete and Log Encryption on the S3 bucket.</strong></p><p>The following option is incorrect because this option creates too many S3 buckets that are unnecessary, whereas all of the events can be easily logged in just a single S3 bucket:</p><p><strong>1. Launch three new CloudTrail trails using three new S3 buckets to store the logs for the AWS Management console, for AWS SDKs, and for the AWS CLI.</strong></p><p><strong>2. Enable MFA Delete and Log Encryption on the S3 bucket.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-encryption-cli.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-encryption-cli.html</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudtrail/?src=udemy\">https://tutorialsdojo.com/aws-cloudtrail/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-encryption-cli.html",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html",
      "https://tutorialsdojo.com/aws-cloudtrail/?src=udemy"
    ]
  },
  {
    "id": 44,
    "question": "<p>A small telecommunications company has recently adopted a hybrid cloud architecture with AWS. They are storing static files of their on-premises web application on a 5 TB gateway-stored volume in AWS Storage Gateway, which is attached to the application server via an iSCSI interface. As part of their disaster recovery plan, they should be able to run the web application on AWS in case their on-premises network encountered any technical issues.</p><p>Which of the following options is the MOST suitable solution that you should implement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Generate an EBS snapshot of the static content from the AWS Storage Gateway service. Afterward, restore it to an EBS volume that you can then attach to the EC2 instance where the application server is hosted.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "For the static content, create an EFS file system from the AWS Storage Gateway service and mount it to the EC2 instance where the application server is hosted.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Restore the static content from an AWS Storage Gateway to an S3 bucket and link it to the EC2 instance where the app server is running.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Restore the static content by&nbsp;attaching the AWS Storage Gateway to the EC2 instance that hosts the application server.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>By using stored volumes, you can store your primary data locally, while asynchronously backing up that data to AWS. <strong>Stored volumes</strong> provide your on-premises applications with low-latency access to their entire datasets. At the same time, they provide durable, offsite backups. You can create storage volumes and mount them as iSCSI devices from your on-premises application servers. Data written to your stored volumes are stored on your on-premises storage hardware. This data is asynchronously backed up to Amazon S3 as Amazon Elastic Block Store (Amazon EBS) snapshots.</p><p><img src=\"https://media.tutorialsdojo.com/sap_storage_gateway_stored.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_storage_gateway_stored.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can restore an Amazon EBS snapshot to an on-premises gateway storage volume if you need to recover a backup of your data. You can also use the snapshot as a starting point for a new Amazon EBS volume, which you can then attach to an Amazon EC2 instance.</p><p>Since this is using a <strong>Volume Storage Gateway</strong>, you have to generate an EBS snapshot and generate an EBS Volume to restore the data.</p><p>Therefore, the correct answer is: <strong>Generate an EBS snapshot of the static content from the AWS Storage Gateway service. Afterwards, restore it to an EBS volume that you can then attach to the EC2 instance where the application server is hosted.</strong></p><p>The option that says: <strong>Restore the static content from an AWS Storage Gateway to an S3 bucket and linking it on the EC2 instance where the app server is running</strong> is incorrect because linking the S3 bucket to the EC2 instance is not a suitable option to restore data from AWS Storage Gateway. You should generate a snapshot first and generate an EBS Volume that you can attach to the instance.</p><p>The option that says: <strong>Restore the static content by attaching the AWS Storage Gateway to the EC2 instance that hosts the application server</strong> is incorrect because you cannot directly attach the AWS Storage Gateway to a running EC2 instance which runs your application server. Although you can deploy and activate a volume or tape gateway on EC2, this instance has a different AMI than your application server which runs on a different EC2 instance. You have to generate an EBS Volume first, based on the generated snapshot from AWS Storage Gateway.</p><p>The option that says: <strong>Create an EFS file system from the AWS Storage Gateway service and mounting it to the EC2 instance where the application server is hosted for the static content</strong> is incorrect because using EFS in this scenario is not appropriate. You should use EBS Volumes to restore your data from Storage Gateway.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#storage-gateway-stored-volume-concepts\">https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#storage-gateway-stored-volume-concepts</a></p><p><a href=\"https://aws.amazon.com/storagegateway/faqs/\">https://aws.amazon.com/storagegateway/faqs/</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#storage-gateway-stored-volume-concepts",
      "https://aws.amazon.com/storagegateway/faqs/",
      "https://tutorialsdojo.com/aws-storage-gateway/?src=udemy",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 45,
    "question": "<p>A leading commercial bank has a hybrid network architecture and is extensively using AWS for its day-to-day operations. The bank uses an Amazon S3 bucket to store sensitive bank records. It has versioning enabled and does not have any encryption. The new solutions architect for the company was asked to implement Server-Side Encryption with Customer-Provided Encryption Keys (SSE-C) for the Amazon S3 bucket to ensure data inside it is secured both at rest and in transit.</p><p>Which of the following options should the solutions architect implement to achieve the company requirements? (Select TWO.)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Only use the S3 console to upload and update objects with SSE-C encryption.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use WSS (WebSocket Secure)",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>For Amazon S3 REST API calls, use the following HTTP Request Headers:\n\n<code>x-amz-server-side​-encryption​-customer-algorithm</code> \n<code>x-amz-server-side​-encryption​-customer-key</code>\n<code>x-amz-server-side​-encryption​-customer-key-MD5</code></p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>For presigned URLs, specify the algorithm using the <code>x-amz-server-side​-encryption​-customer-key-MD5</code> request header</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>For presigned URLs, specify the algorithm using the <code>x-amz-server-side​-encryption​-customer-algorithm</code> request header</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Server-side encryptio</strong>n is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your own encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption as it writes to disks, and decryption when you access your objects. Therefore, you don't need to maintain any code to perform data encryption and decryption. The only thing you do is manage the encryption keys you provide.</p><p>When you upload an object, Amazon S3 uses the encryption key you provide to apply AES-256 encryption to your data and removes the encryption key from memory. Amazon S3 will reject any requests made over HTTP when using SSE-C. For security reasons, it is recommended that you consider any key you send erroneously using HTTP to be compromised. You should discard the key and rotate as appropriate.</p><p>At the time of object creation—that is, when you are uploading a new object or making a copy of an existing object—you can specify if you want Amazon S3 to encrypt your data by adding the <code>x-amz-server-side-encryption</code> header to the request. Set the value of the header to the encryption algorithm <code>AES256</code> that Amazon S3 supports. Amazon S3 confirms that your object is stored using server-side encryption by returning the response header <code>x-amz-server-side-encryption</code>.</p><p><img src=\"https://media.tutorialsdojo.com/sap_s3_sse_encryption.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_s3_sse_encryption.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the following options are correct:</p><p><strong>- For Amazon S3 REST API calls, you have to include the following HTTP Request Headers:</strong></p><p><code><strong>x-amz-server-side-encryption-customer-algorithm</strong></code></p><p><code><strong>x-amz-server-side-encryption-customer-key</strong></code></p><p><code><strong>x-amz-server-side-encryption-customer-key-MD5</strong></code></p><p><strong>- For presigned URLs, you should specify the algorithm using the </strong><code><strong>x-amz-server-side-encryption-customer-algorithm</strong></code><strong> request header.</strong></p><p>The option that says: <strong>Using WSS (WebSocket Secure)</strong> is incorrect as you have to use HTTPS and not WSS.</p><p>The option that says: <strong>Specifying the algorithm using the </strong><code><strong>x-amz-server-side​-encryption​-customer-key-MD5</strong></code><strong> request header for presigned URLs </strong>is incorrect. You should use the <code>x-amz-server-side-encryption-customer-algorithm</code> request header instead.</p><p>The option that says:<strong> Only use the S3 console to upload and update objects with SSE-C encryption</strong> is incorrect because you should use S3 REST APIs instead of the S3 web console.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-the-rest-api-to-encrypt-s3-objects-by-using-aws-kms/\">https://aws.amazon.com/blogs/security/how-to-use-the-rest-api-to-encrypt-s3-objects-by-using-aws-kms/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/specifying-s3-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/specifying-s3-encryption.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html",
      "https://aws.amazon.com/blogs/security/how-to-use-the-rest-api-to-encrypt-s3-objects-by-using-aws-kms/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/specifying-s3-encryption.html",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 46,
    "question": "<p>A leading financial company owns multiple AWS accounts that are consolidated under one AWS Organization. To ensure that tags are always added when users create any resources across all accounts, the solutions architect should enforce the use of centralized resource provisioning tools and infrastructure-as-code templates that apply tags automatically upon resource creation across all account.</p><p>Which of the following options are the recommended actions to achieve the company requirements? (Select TWO.)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up AWS Config to add the corresponding tags to your resources right from the very moment that they are created.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up the CloudFormation Resource Tags property to apply tags to certain resource types upon creation.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up AWS Systems Manager Automation to automatically add tags to your provisioned resources.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up AWS Service Catalog to tag the provisioned resources with corresponding unique identifiers for portfolio, product, and users.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Set up AWS generated tags by activating it in the Billing and Cost Management console of the member account.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>AWS offers a variety of tools to help you implement proactive tag governance practices by ensuring that tags are consistently applied when resources are created.</p><p><strong>AWS CloudFormation</strong> provides a common language for provisioning all the infrastructure resources in your cloud environment. CloudFormation templates are simple text files that create AWS resources in an automated and secure manner. When you create AWS resources using AWS CloudFormation templates, you can use the CloudFormation Resource Tags property to apply tags to certain resource types upon creation.</p><p><strong>AWS Service Catalog</strong> allows organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application environments. AWS Service Catalog enables a self-service capability for users, allowing them to provision the services they need while also helping you to maintain consistent governance – including the application of required tags and tag values.</p><p><strong>AWS Identity and Access Management (IAM)</strong> enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow or deny their access to AWS resources. When you create IAM policies, you can specify resource-level permissions, which include specific permissions for creating and deleting tags. In addition, you can include condition keys, such as <code>aws:RequestTag</code> and <code>aws:TagKeys</code>, which will prevent resources from being created if specific tags or tag values are not present.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ec2_tags.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ec2_tags.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>With AWS Service Catalog, you can create products that represent the resources you want to provision, and you can define tagging policies for those products. When users provision resources through Service Catalog, the resources will be automatically tagged according to the defined tagging policies. This ensures that resources are consistently tagged with the required identifiers for portfolio, product, and users across all accounts.</p><p>AWS CloudFormation allows you to provision and manage AWS resources using infrastructure-as-code templates. The Resource Tags property in CloudFormation templates enables you to specify tags that should be applied to resources upon creation. This option aligns with the company's requirements for centralized resource provisioning and automatic tagging using infrastructure-as-code templates.</p><p>Therefore, the correct answers are:</p><p><strong>- Set up AWS Service Catalog to tag the provisioned resources with corresponding unique identifiers for portfolio, product, and users.</strong></p><p><strong>- Set up the CloudFormation Resource Tags property to apply tags to certain resource types upon creation.</strong></p><p>The option that says: <strong>Set up AWS config to add the corresponding tags to your resources right from the very moment that they are created</strong> is incorrect. AWS Config is a service that continuously monitors and records your AWS resource configurations, including resource metadata like tags. While AWS Config can help you identify resources that are not compliant with your tagging policies, it does not directly add tags to resources upon creation. Instead, AWS Config is typically used in combination with other services like AWS Lambda or AWS Systems Manager Automation to remediate non-compliant resources by adding missing tags. However, this approach is reactive, as it relies on first detecting resources without the required tags and then triggering a remediation action. Additionally, the requirement in this scenario is to proactively enforce tagging upon resource creation, which is why AWS Config alone is not the recommended solution.</p><p>The option that says: <strong>Set up AWS generated tags by activating it in the Billing and Cost Management console of the member account</strong> is incorrect. While enabling AWS generated tags can be beneficial for cost allocation and management purposes, it does not directly address the requirement of automatically adding tags upon resource creation across all accounts.</p><p>The option that says: <strong>Set up AWS Systems Manager Automation to automatically add tags to your provisioned resources</strong> is incorrect because AWS Systems Manager Automation is a service that allows you to automate common maintenance and deployment tasks across AWS resources. While Automation can be used to add tags to existing resources, it is not designed for automatically tagging resources upon creation. Automation is more suitable for managing and maintaining resources after they have been provisioned, not for the initial resource creation and tagging process. Additionally, it is not designed as a centralized resource provisioning tool or infrastructure-as-code solution, which are the recommended approaches for enforcing tagging upon resource creation in this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/tagging-best-practices.html\">https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/tagging-best-practices.html</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/03/aws-service-catalog-announces-autotags-for-automatic-tagging-of-provisioned-resources/\">https://aws.amazon.com/about-aws/whats-new/2018/03/aws-service-catalog-announces-autotags-for-automatic-tagging-of-provisioned-resources/</a></p><p><br></p><p><strong>Check out this AWS Service Catalog Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-service-catalog/?src=udemy\">https://tutorialsdojo.com/aws-service-catalog/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/tagging-best-practices.html",
      "https://aws.amazon.com/about-aws/whats-new/2018/03/aws-service-catalog-announces-autotags-for-automatic-tagging-of-provisioned-resources/",
      "https://tutorialsdojo.com/aws-service-catalog/?src=udemy"
    ]
  },
  {
    "id": 47,
    "question": "<p>A medical firm uses an image analysis application that extracts data from multiple images. The input stream analyzes a batch of images and for each file, it writes the result data to an output stream of files. The number of input files per day grows and peaks for a few hours in a day. The application is hosted on an Amazon EC2 instance with a large EBS volume that hosts the input data, but the results still take almost 20 hours per day to be processed.</p><p>Which of the following solutions can be implemented to reduce the processing time and improve the availability of the application?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Store I/O files in S3 instead and use SQS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the number of SNS notifications.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Store I/O files in an EBS Provisioned IOPS volume, and use SNS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the number of SNS notifications.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Store I/O files in an EBS Provisioned IOPS volume, and use SNS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the length of your SQS queue.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Store I/O files in S3 instead and use SQS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the length of your SQS queue.",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Simple Queue Service (SQS)</strong> is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications. Building applications from individual components that each perform a discrete function improves scalability and reliability, and is best practice design for modern applications. SQS makes it simple and cost-effective to decouple and coordinate the components of a cloud application. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be always available.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_s3_consumption.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_sqs_s3_consumption.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Store I/O files in S3 instead and use SQS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the length of your SQS queue.</strong> It provides high availability and can store the massive amount of data. Auto-scaling of EC2 instances reduces the overall processing time and SQS helps in distributing the commands/tasks to the group of EC2 instances.</p><p>The option that says: <strong>Store I/O files in an EBS Provisioned IOPS volume, and use SNS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the number of SNS notifications </strong>is incorrect because EBS is for storage and SNS is only for notification, not for scaling.</p><p>The option that says: <strong>Store I/O files in an EBS Provisioned IOPS volume, and use SNS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the length of your SQS queue </strong>is incorrect because EBS is only for block storage and not for scaling.</p><p>The option that says: <strong>Store I/O files in S3 instead and use SQS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the number of SNS notifications</strong> is incorrect because SNS is only for notification and not for scaling.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/auto-scaling-with-sqs/\">https://aws.amazon.com/blogs/aws/auto-scaling-with-sqs/</a></p><p><br></p><p><strong>Check out these AWS Auto Scaling and Amazon SQS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html",
      "https://aws.amazon.com/blogs/aws/auto-scaling-with-sqs/",
      "https://tutorialsdojo.com/aws-auto-scaling/?src=udemy",
      "https://tutorialsdojo.com/amazon-sqs/?src=udemy"
    ]
  },
  {
    "id": 48,
    "question": "<p>A leading telecommunications company is moving all of its mission-critical, multi-tier applications to AWS. At present, their architecture is composed of desktop client applications and several servers that are all located in their on-premises data center. The application-tier is using a MySQL database that is hosted on a single VM while both the presentation and business logic layers are distributed across multiple VMs. There has been a lot of reports that their users, who access the applications remotely, are experiencing increased connection latency and slow load times.</p><p>Which of the following is the MOST cost-effective solution to improve the uptime of the application with MINIMAL change and improve the overall user experience?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon ElastiCache to improve the overall user experience of your desktop applications. Directly migrate the MySQL database from your VM to a DynamoDB database. Host the application and presentation layers in AWS Fargate containers behind an Application Load Balancer.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon AppStream 2.0 to centrally manage your desktop applications and improve the overall user experience. Migrate the MySQL database from your VM to Amazon Aurora. Host the application and presentation layers in an Auto Scaling group on EC2 instances behind an Application Load Balancer.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Using Amazon WorkSpaces, set up and allocate a workspace for each user to improve the overall user experience. Migrate the MySQL database from your VM to a self-hosted MySQL database in a large EC2 instance. Host the application and presentation layers in Amazon ECS containers behind an Application Load Balancer.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a new CloudFront web distribution to improve the overall user experience of your desktop applications. Migrate the MySQL database from your VM to a Redshift cluster. Host the application and presentation layers in ECS containers behind a Network Load Balancer.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Amazon AppStream 2.0</strong> is a fully managed application streaming service. You centrally manage your desktop applications on AppStream 2.0 and securely deliver them to any computer. You can easily scale to any number of users across the globe without acquiring, provisioning, and operating hardware or infrastructure. AppStream 2.0 is built on AWS, so you benefit from a data center and network architecture designed for the most security-sensitive organizations. Each user has a fluid and responsive experience with your applications, including GPU-intensive 3D design and engineering ones, because your applications run on virtual machines (VMs) optimized for specific use cases and each streaming session automatically adjusts to network conditions.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_appstream_overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_appstream_overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Enterprises can use AppStream 2.0 to simplify application delivery and complete their migration to the cloud. Educational institutions can provide every student access to the applications they need for class on any computer. Software vendors can use AppStream 2.0 to deliver trials, demos, and training for their applications with no downloads or installations. They can also develop a full software-as-a-service (SaaS) solution without rewriting their application.</p><p>Therefore, the correct answer is the option that says: <strong>Use Amazon AppStream 2.0 to centrally manage your desktop applications and improve the overall user experience. Migrate the MySQL database from your VM to Amazon Aurora. Host the application and presentation layers in an Auto Scaling group on EC2 instances behind an Application Load Balancer.</strong></p><p>The option that says: <strong>Set up a new CloudFront web distribution to improve the overall user experience of your desktop applications. Migrate the MySQL database from your VM to a Redshift cluster. Host the application and presentation layers in ECS containers behind a Network Load Balancer</strong> is incorrect. It is not suitable to migrate your MySQL database to a Redshift cluster.</p><p>The option that says: <strong>Use Amazon ElastiCache to improve the overall user experience of your desktop applications. Directly migrate the MySQL database from your VM to a DynamoDB database. Host the application and presentation layers in AWS Fargate containers behind an Application Load Balancer</strong> is incorrect. You cannot directly migrate your MySQL database, which is relational in type, to DynamoDB which is a NoSQL database or non-relational. The use of Amazon ElastiCache alone will not significantly improve the overall user experience since this service is primarily used for caching.</p><p>The option that says: <strong>Using Amazon WorkSpaces, set up and allocate a workspace for each user to improve the overall user experience. Migrate the MySQL database from your VM to a self-hosted MySQL database in a large EC2 instance. Host the application and presentation layers in Amazon ECS containers behind an Application Load Balancer</strong> is incorrect. Amazon WorkSpaces is primarily used as a managed, secure Desktop-as-a-Service (DaaS) solution and it costs a lot to implement. Take note that in the scenario, it was mentioned that they are using desktop applications but there was no mention about the need for Windows or Linux desktops which is why the use of Amazon WorkSpaces is incorrect.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/appstream2/\">https://aws.amazon.com/appstream2/</a></p><p><a href=\"https://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html\">https://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/appstream2/",
      "https://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 49,
    "question": "<p>A company hosts an internal web portal on a fleet of Amazon EC2 instances that allows access to confidential files stored in an encrypted Amazon S3 bucket. Because the files contain sensitive information, the company does not want any files to traverse the public Internet. Bucket access should be restricted to only allow the web portal’s EC2 instances. To comply with the requirements, the Solutions Architect created an Amazon S3 VPC endpoint and associated it with the web portal’s VPC.</p><p>Which of the following actions should the Solutions Architect take to fully comply with the company requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a VPC endpoint policy that restricts access to the specific Amazon S3 bucket on the current region. Apply an Amazon S3 bucket policy that only allows access from the VPC private subnets. Update the VPC’s Network Access Control List (NACL) to deny other EC2 instances from accessing the gateway prefix list.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a VPC endpoint policy that restricts access to the specific Amazon S3 bucket. Create an IAM role that grants access to the S3 bucket and attach it to the application EC2 instances. Apply an Amazon S3 bucket policy that only allows access from the VPC endpoint and those using the IAM role.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a VPC endpoint policy that restricts access to the specific Amazon S3 bucket. Apply an Amazon S3 bucket policy that only allows access from the VPC endpoint. Update the VPC’s Network Access Control List (NACL) to deny other EC2 instances from accessing the gateway prefix list.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Apply an Amazon S3 bucket policy that includes the <code>aws:SourceIp</code> condition to deny all access except those coming from the application EC2 instances IP addresses. Update the route table for the VPC to ensure that the VPC endpoint is associated only with the application instances subnets.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>When you create an <strong>interface or gateway endpoint</strong>, you can attach an endpoint policy to it that controls access to the service to which you are connecting. Endpoint policies must be written in JSON format. Not all services support endpoint policies.</p><p>A <strong>VPC endpoint policy</strong> is an IAM resource policy that you attach to an endpoint when you create or modify the endpoint. If you do not attach a policy when you create an endpoint, we attach a default policy for you that allows full access to the service. If a service does not support endpoint policies, the endpoint allows full access to the service. An endpoint policy does not override or replace IAM user policies or service-specific policies (such as S3 bucket policies). It is a separate policy for controlling access from the endpoint to the specified service.</p><p>You cannot attach more than one policy to an endpoint. However, you can modify the policy at any time. If you do modify a policy, it can take a few minutes for the changes to take effect.</p><p>Your endpoint policy can be like any IAM policy; however, take note of the following:</p><p>- Your policy must contain a Principal element.</p><p>- The size of an endpoint policy cannot exceed 20,480 characters.</p><p>When you create or modify an endpoint, you specify the VPC route tables that are used to access the service via the endpoint. A route is automatically added to each of the route tables with a destination that specifies the AWS prefix list ID of the service (<code>pl-xxxxxxxx</code>), and a target with the endpoint ID (<code>vpce-xxxxxxxx</code>).</p><p>The following example bucket policy blocks traffic to the bucket unless the request is from specified VPC endpoints (<code>aws:sourceVpce</code>):</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"pln\">&nbsp; </span><span class=\"str\">\"Id\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"VPCe\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"pln\">&nbsp; </span><span class=\"str\">\"Version\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"2012-10-17\"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"pln\">&nbsp; </span><span class=\"str\">\"Statement\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L4\"><span class=\"pln\">&nbsp;&nbsp;&nbsp; </span><span class=\"pun\">{</span></li><li class=\"L5\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"VPCe\"</span><span class=\"pun\">,</span></li><li class=\"L6\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"s3:*\"</span><span class=\"pun\">,</span></li><li class=\"L7\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Deny\"</span><span class=\"pun\">,</span></li><li class=\"L8\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L9\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"str\">\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"</span><span class=\"pun\">,</span></li><li class=\"L0\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"str\">\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"</span></li><li class=\"L1\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"pun\">],</span></li><li class=\"L2\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"str\">\"Condition\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">{</span></li><li class=\"L3\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"str\">\"StringNotEquals\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">{</span></li><li class=\"L4\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"str\">\"aws:SourceVpce\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L5\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"str\">\"vpce-1111111\"</span><span class=\"pun\">,</span></li><li class=\"L6\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"str\">\"vpce-2222222\"</span></li><li class=\"L7\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"pun\">]</span></li><li class=\"L8\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"pun\">}</span></li><li class=\"L9\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"pun\">},</span></li><li class=\"L0\"><span class=\"pln\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class=\"str\">\"Principal\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"*\"</span></li><li class=\"L1\"><span class=\"pln\">&nbsp;&nbsp;&nbsp; </span><span class=\"pun\">}</span></li><li class=\"L2\"><span class=\"pln\">&nbsp; </span><span class=\"pun\">]</span></li><li class=\"L3\"><span class=\"pun\">}</span></li></ol></pre></div></div><p>- To use this policy with the <code>aws:sourceVpce</code> condition, you must attach a VPC endpoint for Amazon S3. The VPC endpoint must be attached to the route table of the EC2 instance's subnet, and in the same AWS Region as the bucket.</p><p>- To allow users to perform S3 actions on the bucket from the VPC endpoints or IP addresses, you must explicitly allow the user-level permissions. You can explicitly allow user-level permissions on either an AWS Identity and Access Management (IAM) policy or another statement in the bucket policy.</p><p>Therefore, the correct answer is: <strong>Create a VPC endpoint policy that restricts access to the specific Amazon S3 bucket. Create an IAM role that grants access to the S3 bucket and attach it to the application EC2 instances. Apply an Amazon S3 bucket policy that only allows access from the VPC endpoint and those using the IAM role.</strong> This ensures that traffic to the S3 bucket are all coming from the VPC endpoint and that the application EC2 instances are the only ones allowed to access it.</p><p>The option that says: <strong>Create a VPC endpoint policy that restricts access to the specific Amazon S3 bucket. Apply an Amazon S3 bucket policy that only allows access from the VPC endpoint. Update the VPC’s Network Access Control List (NACL) to deny other EC2 instances from accessing the gateway prefix list </strong>is incorrect. The gateway prefix list ID should be added to the route table in the VPC to allow access for the specific subnet, and not on the NACL.</p><p>The option that says: <strong>Apply an Amazon S3 bucket policy that includes the </strong><code><strong>aws:SourceIp</strong></code><strong> condition to deny all access except those coming from the application EC2 instances IP addresses. Update the route table for the VPC to ensure that the VPC endpoint is associated only with the application instances subnets</strong> is incorrect. The <code>aws:SourceIp</code> is used for specifying external IP addresses (from the public Internet or from within the VPC). You cannot use the <code>aws:SourceIp</code> condition in your bucket policies for Amazon S3 requests coming from a VPC endpoint. When you associate a VPC endpoint to your VPC, the route tables are automatically updated to include the AWS prefix list ID.</p><p>The option that says: <strong>Create a VPC endpoint policy that restricts access to the specific Amazon S3 bucket on the current region. Apply an Amazon S3 bucket policy that only allows access from the VPC private subnets. Update the VPC’s Network Access Control List (NACL) to deny other EC2 instances from accessing the gateway prefix list</strong> is incorrect. You cannot input subnet IDs as restrictions on the bucket policies. You should use VPC endpoint or source IPs instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-access.html#vpc-endpoint-policies\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-access.html#vpc-endpoint-policies</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/block-s3-traffic-vpc-ip/\">https://aws.amazon.com/premiumsupport/knowledge-center/block-s3-traffic-vpc-ip/</a></p><p><br></p><p><strong>Check out these Amazon VPC and Amazon S3 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-access.html#vpc-endpoint-policies",
      "https://aws.amazon.com/premiumsupport/knowledge-center/block-s3-traffic-vpc-ip/",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 50,
    "question": "<p>A startup is building a mobile app and a custom GraphQL API backend that lets people post photos and videos of road potholes, faulty street lights, bridge damages, and other issues in the public infrastructure with 100-character summaries. The data gathered by the system will be used by the department of public works to facilitate fast resolution. The developers used a javascript-based React Native mobile framework so that it would run on various mobile and tablet devices. The app will be connecting to a custom GraphQL API that will be responsible for storing the photos and videos in an Amazon S3 bucket and will also access a DynamoDB table to store the summaries. The developers have recently deployed the mobile app prototype but it was found that there is an availability issue with the custom GraphQL API. To proceed with the project, the team decided to remove the API and instead, re-model the mobile app so that it will directly connect to both DynamoDB and S3 as well as handle user authentication.</p><p>Which of the following options provides the most cost-effective and scalable architecture for this project?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>1. Set up a web identity federation using the AssumeRole API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.\n</p><p>2. Create an IAM user for that provider and set up permissions for the IAM user to allow access to S3 and DynamoDB. \n</p><p>3. The mobile app will use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>1. Set up a web identity federation using the AssumeRoleWithWebIdentity API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.\n</p><p>2. Create an IAM role for that provider and set up permissions for the IAM role to allow access to S3 and DynamoDB. \n</p><p>3. The mobile app will use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>1. Set up a web identity federation using the AssumeRoleWithWebIdentity API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.\n</p><p>2. Create an IAM role for that provider and set up permissions for the IAM role to allow access to S3 and DynamoDB. \n</p><p>3. The mobile app will use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>1. Set up a web identity federation using Cognito and social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.\n</p><p>2. Configure the IAM role in Cognito to allow access to S3 and DynamoDB. \n</p><p>3. The mobile app will use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>1. Set up a web identity federation using the AssumeRoleWithSAML API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.\n</p><p>2. Create an IAM role for that provider and set up permissions for the IAM role to allow access to S3 and DynamoDB. \n</p><p>3. The mobile app will use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>With <strong>web identity federatio</strong>n, you don't need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. They can receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. Using an IdP helps you keep your AWS account secure, because you don't have to embed and distribute long-term security credentials with your application.</p><p>In this scenario, you have a mobile app that needs to have access to the DynamoDB and S3 bucket. You can achieve this by using Web Identity Federation with AssumeRoleWithWebIdentity API which provides temporary security credentials and an IAM role.</p><p><img src=\"https://media.tutorialsdojo.com/sap_assumeRole.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_assumeRole.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Thus, the correct answer here is the following option:</p><p><strong>1. Set up a web identity federation using the AssumeRoleWithWebIdentity API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.</strong></p><p><strong>2. Create an IAM role for that provider and set up permissions for the IAM role to allow access to S3 and DynamoDB.</strong></p><p><strong>3. The mobile app will use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</strong></p><p><br></p><p>The following option is incorrect because you cannot use AssumeRole API and an IAM user in this scenario:</p><p><strong>1. Set up a web identity federation using the AssumeRole API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.</strong></p><p><strong>2. Create an IAM user for that provider and set up permissions for the IAM user to allow access to S3 and DynamoDB.</strong></p><p><strong>3. The mobile app will use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</strong></p><p><br></p><p>The following option is incorrect because you should have used AssumRoleWithWebIdentity instead of AssumeRoleWithSAML API:</p><p><strong>1. Set up a web identity federation using the AssumeRoleWithSAML API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.</strong></p><p><strong>2. Create an IAM role for that provider and set up permissions for the IAM role to allow access to S3 and DynamoDB.</strong></p><p><strong>3. The mobile app will use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</strong></p><p><br></p><p>The following option is incorrect because it is a security risk to store and use the AWS access and secret keys from the mobile app itself:</p><p><strong>1. Set up a web identity federation using the AssumeRoleWithWebIdentity API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.</strong></p><p><strong>2. Create an IAM role for that provider and set up permissions for the IAM role to allow access to S3 and DynamoDB.</strong></p><p><strong>3. The mobile app will use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</strong></p><p><br></p><p>The following option is incorrect. Even though the use of Cognito is valid, it is wrong to store and use the AWS access and secret keys from the mobile app itself. This is a security risk and you should use the temporary security credentials instead:</p><p><strong>1. Set up a web identity federation using Cognito and social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.</strong></p><p><strong>2. Configure the IAM role in Cognito to allow access to S3 and DynamoDB.</strong></p><p><strong>3. The mobile app will use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy"
    ]
  },
  {
    "id": 51,
    "question": "<p>An enterprise has several development and production AWS accounts managed under its AWS Organization. Consolidated billing is enabled in the organization but the management wants more visibility on the AWS Billing and Cost Management. With the sudden increase in the Amazon RDS and Amazon DynamoDB costs, the management required all CloudFormation templates to enforce a consistent tagging with cost center numbers and project ID numbers on all resources that will be provisioned. The management also wants these tags to be enforced in all existing and future DynamoDB and RDS instances.</p><p>Which of the following options is the recommended strategy to meet the company requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>On the Billing and Cost Management page, create new cost allocation tags for the cost center and project ID. Wait for at least 24 hours to allow AWS to propagate the tags and gather cost reports. Update existing federated roles to deny users from creating resources that do not have the cost center and project ID tags.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an AWS Config rule to check for any untagged resource and send a notification email to the finance team. Write a Lambda function that has a cross-account role to tag all RDS databases and DynamoDB resources on all accounts under the organization. Schedule this function to run every hour.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Tag all existing resources in bulk using the Tag Editor. On the Billing and Cost Management page, create new cost allocation tags for the cost center and project ID. Apply an SCP on the organizational unit that denies users from creating resources that do not have the cost center and project ID tags.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Tag all existing resources in bulk using the Tag Editor. On the Billing and Cost Management page, create new cost allocation tags for the cost center and project ID. Wait for at least 24 hours to allow AWS to propagate the tags and gather cost reports.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Tags</strong> are words or phrases that act as metadata that you can use to identify and organize your AWS resources. A resource can have up to 50 user-applied tags. It can also have read-only system tags. Each tag consists of a key and one optional value.</p><p>You can add tags to resources when you create the resource. You can use the resource's service console or API to add, change, or remove those tags one resource at a time. To add tags to—or edit or delete tags of—multiple resources at once, use <strong>Tag Editor</strong>. With Tag Editor, you search for the resources that you want to tag, and then manage tags for the resources in your search results.</p><p>For each resource, each tag key must be unique, and each tag key can have only one value. You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level. After you activate cost allocation tags, AWS uses the cost allocation tags to organize your resource costs on your cost allocation report, to make it easier for you to categorize and track your AWS costs. AWS provides two types of cost allocation tags, <strong>AWS generated tags</strong> and<strong> user-defined tags</strong>. AWS, or AWS Marketplace ISV defines, creates, and applies the AWS generated tags for you, and you define, create, and apply user-defined tags. You must activate both types of tags separately before they can appear in Cost Explorer or on a cost allocation report.</p><p><img src=\"https://media.tutorialsdojo.com/sap_billing_tags.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_billing_tags.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>User-defined tags</strong> are tags that you define, create, and apply to resources. After you have created and applied the user-defined tags, you can activate them by using the Billing and Cost Management console for cost allocation tracking. Cost Allocation Tags appear on the console after you've enabled Cost Explorer, Budgets, AWS Cost and Usage Reports, or legacy reports. After you activate the AWS services, they appear on your cost allocation report. You can then use the tags on your cost allocation report to track your AWS costs. Tags are not applied to resources that were created before the tags were created.</p><p><strong>Service control policies (SCPs)</strong> are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines. SCPs are available only in an organization that has all features enabled. SCPs aren't available if your organization has enabled only the consolidated billing features.</p><p>Therefore, the correct answer is: <strong>Tag all existing resources in bulk using the Tag Editor. On the Billing and Cost Management page, create new cost allocation tags for the cost center and project ID. Apply an SCP on the organizational unit that denies users from creating resources that do not have the cost center and project ID tags.</strong> Tag Editor allows bulk tagging to easily tag your AWS resources. The SCP rules will enforce the company tagging policy by preventing users from creating resources that do not have the appropriate tags.</p><p>The option that says: <strong>Tag all existing resources in bulk using the Tag Editor. On the Billing and Cost Management page, create new cost allocation tags for the cost center and project ID. Wait for at least 24 hours to allow AWS to propagate the tags and gather cost reports </strong>is incorrect. This solution is incomplete. It does not prevent users from creating untagged resources in the future.</p><p>The option that says: <strong>Create an AWS Config rule to check for any untagged resource and send a notification email to the finance team. Write a Lambda function that has a cross-account role to tag all RDS databases and DynamoDB resources on all accounts under the organization. Schedule this function to run every hour </strong>is incorrect. This solution does not prevent users from creating untagged resources in the future and the AWS Config rule does not automatically tag non-compliant resources.</p><p>The option that says: <strong>On the Billing and Cost Management page, create new cost allocation tags for the cost center and project ID. Wait for at least 24 hours to allow AWS to propagate the tags and gather cost reports. Update existing federated roles to deny users from creating resources that do not have the cost center and project ID tags </strong>is incorrect. This may be possible but it will be cumbersome to edit all IAM policies across all the company AWS accounts. It is better to use an SCP applied at the organization unit level to enforce the tagging policy.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples.html#example-require-tag-on-create\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples.html#example-require-tag-on-create</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf\">https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf</a></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html</a></p><p><a href=\"https://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html\">https://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html</a></p><p><br></p><p><strong>Check out this AWS Billing and Cost Management Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy\">https://tutorialsdojo.com/aws-billing-and-cost-management/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples.html#example-require-tag-on-create",
      "https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf",
      "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html",
      "https://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html",
      "https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy"
    ]
  },
  {
    "id": 52,
    "question": "<p>A company runs its legacy web application in its on-premises data center. The solutions architect has been tasked to move the legacy web application in a virtual machine running inside the data center to the Amazon VPC. However, this application requires a private and dedicated connection to a number of servers hosted on the on-premises network in order for it to work.</p><p>Which combination of options provides the most suitable way to configure the web application running inside the VPC to reach back and access its internal dependencies on the company’s on-premises network? (Select TWO.)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "An Internet Gateway to allow a VPN connection.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "An AWS Direct Connect link between the VPC and the network housing the internal services.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "A network device in your data center that supports Border Gateway Protocol (BGP) and BGP MD5 authentication.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up a Transit VPC between your on-premises data center and your VPC.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "An Elastic IP address on the VPC instance.",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p><strong>AWS Direct Connect</strong> links your internal network to an AWS Direct Connect location over a standard Ethernet fiber-optic cable. One end of the cable is connected to your router, the other to an AWS Direct Connect router.</p><p><img src=\"https://media.tutorialsdojo.com/sap_awsDirectConnect.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_awsDirectConnect.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>With this connection, you can create <em>virtual interfaces</em> directly to public AWS services (for example, to Amazon S3) or to Amazon VPC, bypassing Internet service providers in your network path. An AWS Direct Connect location provides access to AWS in the Region with which it is associated. You can use a single connection in a public Region or AWS GovCloud (US) to access public AWS services in all other public Regions.</p><p>The option that says: <strong>A network device in your data center that supports Border Gateway Protocol (BGP) and BGP MD5 authentication</strong> is correct as a network device that supports Border Gateway Protocol (BGP) and BGP MD5 authentication is needed to establish a Direct Connect link from your data center to your VPC.</p><p>The option that says: <strong>An AWS Direct Connect link between the VPC and the network housing the internal services</strong> is correct because Direct Connect sets up a dedicated connection between on-premises data center and Amazon VPC, and provides you with the ability to connect your on-premises servers with the instances in your VPC.</p><p>The option that says: <strong>An Internet Gateway to allow a VPN connection</strong> is incorrect as you normally create a VPN connection based on a customer gateway and a virtual private gateway (VPG) in AWS.</p><p>The option that says: <strong>An Elastic IP address on the VPC instance</strong> is incorrect because EIPs are not needed as the instances in the VPC can communicate with on-premises servers via their private IP address.</p><p>The option that says: <strong>Setting up a Transit VPC between your on-premises data center and your VPC</strong> is incorrect. Although a Transit VPC can establish a connection to your VPC and on-premises data center, this option is not suitable for this scenario. Remember that a Transit VPC just simplifies network management and minimizes the number of connections required to connect multiple VPCs and remote networks.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/getting_started.html#ConnectionRequest\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/getting_started.html#ConnectionRequest</a></p><p><br></p><p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html",
      "https://docs.aws.amazon.com/directconnect/latest/UserGuide/getting_started.html#ConnectionRequest",
      "https://tutorialsdojo.com/aws-direct-connect/?src=udemy"
    ]
  },
  {
    "id": 53,
    "question": "<p>A company wants to migrate its on-premises application to the AWS cloud. Due to limited manpower, the company wants to utilize fully managed AWS services as much as possible. This way, there will be less maintenance work after the migration. The application processes large files containing sensitive information so the company has the following requirements:</p><p> - Data encryption at rest and in transit are both required on all files that will be processed by the application.</p><p> - The storage solution must be highly durable and available.</p><p> - The company must be able to use its own encryption key and then periodically rotated for improved security.</p><p> - Amazon Redshift Spectrum will be used to analyze the migrated data.</p><p>Which of the following should the Solutions Architect implement to achieve these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the data files on an Amazon DynamoDB table. Leverage on the default SSL connection settings of the DynamoDB table. Use AWS KMS to encrypt the table and enable automatic key rotation.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon S3 bucket to store all data. Enable server-side encryption with AWS KMS (SSE-KMS). Apply a bucket policy that enforces HTTPS only connections to the S3 bucket.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Provision an AWS Storage Gateway – File Gateway device in the on-premises data center. Enable encryption on the file share with AWS KMS. The data will be transferred securely to an Amazon S3 bucket with SSE-S3 encryption.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the data files in an Amazon EC2 instance with an encrypted EBS volume. Use AWS KMS to encrypt the EBS volume and enable automatic key rotation.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Simple Storage Service (Amazon S3)</strong> is an object storage service that offers scalability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.</p><p>Amazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket. You can set default encryption on a bucket so that all new objects are encrypted when they are stored in the bucket. The objects are encrypted using server-side encryption with either Amazon S3-managed keys (SSE-S3) or KMS keys stored in AWS Key Management Service (AWS KMS).</p><p><strong>SSE-S3</strong> requires that Amazon S3 manage the data and the encryption keys.</p><p><strong>SSE-C</strong> requires that you manage the encryption key.</p><p><strong>SSE-KMS</strong> requires that AWS manage the data key but you manage the KMS key in AWS KMS.</p><p>When you configure your bucket to use default encryption with SSE-KMS, you can also enable an S3 Bucket Key to decrease request traffic from Amazon S3 to AWS Key Management Service (AWS KMS) and reduce the cost of encryption. When you configure your bucket to use an S3 Bucket Key for SSE-KMS on new objects, AWS KMS generates a bucket-level key that is used to create a unique data key for objects in the bucket. This bucket key is used for a time-limited period within Amazon S3, reducing the need for Amazon S3 to make requests to AWS KMS to complete encryption operations.</p><p><img src=\"https://media.tutorialsdojo.com/sap_redshift_on_premises.PNG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_redshift_on_premises.PNG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>By default, Amazon S3 allows both HTTP and HTTPS requests. To comply with the<code> s3-bucket-ssl-requests-only</code> rule (only accepting HTTPS connections), your bucket policy should explicitly deny access to HTTP requests. To determine HTTP or HTTPS requests in a bucket policy, use a condition that checks for the key <code>\"aws:SecureTransport\"</code>. When this key is set to <code>true</code>, this means that the request is sent through HTTPS. To be sure to comply with the <code>s3-bucket-ssl-requests-only</code> rule, create a bucket policy that explicitly denies access when the request meets the condition <code>\"aws:SecureTransport\": \"false\"</code>. This policy explicitly denies access to HTTP requests.</p><p>Therefore, the correct answer is: <strong>Create an Amazon S3 bucket to store all data. Enable server-side encryption with AWS KMS (SSE-KMS). Apply a bucket policy that enforces HTTPS only connections to the S3 bucket.</strong></p><p>The option that says: <strong>Provision an AWS Storage Gateway – File Gateway device in the on-premises data center. Enable encryption on the file share with AWS KMS. The data will be transferred securely to an Amazon S3 bucket with SSE-S3</strong> <strong>encryption</strong> is incorrect. Although this is possible, the encryption key on the S3 with SSE-S3 is managed by AWS and not the customer. This also entails more management overhead because you need to manage your file gateway. Less maintenance is needed if data is just sent directly to an encrypted S3 bucket.</p><p>The option that says:<strong> Store the data files on an Amazon DynamoDB table. Leverage on the default SSL connection settings of the DynamoDB table. Use AWS KMS to encrypt the table and enable automatic key rotation</strong> is incorrect. The application processes large documents that may not fit on an Amazon DynamoDB table in which each entry is limited to 400 KB only.</p><p>The option that says: <strong>Store the data files in an Amazon EC2 instance with an encrypted EBS volume. Use AWS KMS to encrypt the EBS volume and enable automatic key rotation </strong>is incorrect as this entails more management overhead as you need to provision your own EC2 instance. Amazon S3 is also more durable compared to a single Amazon EC2 instance with EBS volume.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/</a></p><p><br></p><p><strong>Check out these Amazon S3 and AWS KMS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/aws-kms-keys-policy-management-in-aws-kms/?src=udemy\">https://tutorialsdojo.com/aws-kms-keys-policy-management-in-aws-kms/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy",
      "https://tutorialsdojo.com/aws-kms-keys-policy-management-in-aws-kms/?src=udemy"
    ]
  },
  {
    "id": 54,
    "question": "<p>A company wants to improve the security of its cloud resources by ensuring that all running EC2 instances were launched from pre-approved AMIs only, which are set by the Security team. Their Development team has an agile CI/CD process which should not be stalled by the new automated solution that they’ll implement. Any new application release must be deployed first before the solution could analyze if it is using a pre-approved AMI or not.</p><p>Which of the following options enforces the required controls with the LEAST impact on the development process? (Select TWO.)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up IAM policies to restrict the ability of users to launch EC2 instances based on a specific set of pre-approved AMIs which were tagged by the Security team.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up a scheduled Lambda function to search through the list of running EC2 instances within your VPC and determine if any of these are based on unauthorized AMIs. Afterward, publish a new message to an SNS topic to inform the Security team that this occurred and then terminate the EC2 instance.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up the required policies, roles, and permissions to a centralized IT Operations team, which will manually process the security approval steps to ensure that EC2 instances are only launched from pre-approved AMIs.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up AWS Config rules to determine any launches of EC2 instances based on non-approved AMIs and then trigger an AWS Lambda function to automatically terminate the instance. Afterward, publish a message to an SNS topic to inform the Security team about the occurrence.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Set up Amazon Inspector to do regular scans using a custom assessment template to determine if the EC2 instance is based upon a pre-approved AMI. Terminate the instances and inform the Security team by email about the security breach.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>When you run your applications on AWS, you usually use AWS resources, which you must create and manage collectively. As the demand for your application keeps growing, so does your need to keep track of your AWS resources. AWS Config is designed to help you oversee your application resources.</p><p><img src=\"https://media.tutorialsdojo.com/sap_config_how_it_works.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_config_how_it_works.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>AWS Config provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time. With AWS Config, you can do the following:</p><p>- Evaluate your AWS resource configurations for desired settings.</p><p>- Get a snapshot of the current configurations of the supported resources that are associated with your AWS account.</p><p>- Retrieve configurations of one or more resources that exist in your account.</p><p>- Retrieve historical configurations of one or more resources.</p><p>- Receive a notification whenever a resource is created, modified, or deleted.</p><p>- View relationships between resources. For example, you might want to find all resources that use a particular security group.</p><p><strong>AWS Config</strong> provides <strong>AWS managed rules</strong>, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. In this scenario, you can use the <code><strong>approved-amis-by-id</strong></code> AWS manage rule which checks whether running instances are using specified AMIs. You can also use a Lambda function which is scheduled to run regularly to scan all of the running EC2 instances in your VPC and check if there is an instance that was launched using an unauthorized AMI. Hence, the following options are the correct answers:</p><p><strong>- Set up AWS Config rules to determine any launches of EC2 instances based on non-approved AMIs and then trigger an AWS Lambda function to automatically terminate the instance. Afterward, publish a message to an SNS topic to inform the Security team about the occurrence.</strong></p><p><strong>- Set up a scheduled Lambda function to search through the list of running EC2 instances within your VPC and determine if any of these are based on unauthorized AMIs. Afterward, publish a new message to an SNS topic to inform the Security team that this occurred and then terminate the EC2 instance.</strong></p><p>The option that says: <strong>Set up the required policies, roles, and permissions to a centralized IT Operations team, which will manually process the security approval steps to ensure that EC2 instances are only launched from pre-approved AMIs</strong> is incorrect because having manual information security approval will impact the development process. A better solution is to implement an automated process using AWS Config and a scheduled AWS Lambda function.</p><p>The option that says: <strong>Set up IAM policies to restrict the ability of users to launch EC2 instances based on a specific set of pre-approved AMIs which were tagged by the Security team</strong> is incorrect because setting up an IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs which could impact their CI/CD process. The scenario clearly says that the solution should not have any interruption in the company's development process.</p><p>The option that says:<strong> Set up Amazon Inspector to do regular scans using a custom assessment template to determine if the EC2 instance is based upon a pre-approved AMI. Terminate the instances and inform the Security team by email about the security breach</strong> is incorrect because the Amazon Inspector service is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It does not have the capability to detect EC2 instances that are using unapproved AMIs, unlike AWS Config.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html",
      "https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html",
      "https://tutorialsdojo.com/aws-config/?src=udemy"
    ]
  },
  {
    "id": 55,
    "question": "<p>A large software company has an on-premises LDAP server and has established an IPSec VPN connection between its on-premises network and its VPC in AWS. The company wants to enable employees to access AWS resources using the same corporate account used inside the company network.</p><p>Which of the following actions should the solutions architect implement to achieve the company’s requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Integrate the on-premises LDAP server with AWS IAM so the employees can log into IAM using their corporate LDAP credentials. Once authenticated, they can use the temporary credentials to access any AWS resource.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an identity broker that authenticates against the on-premises LDAP server and then calls AWS STS to get IAM federated user credentials. The employees can use these credentials to access AWS resources.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up a federation between the on-premises LDAP server and AWS IAM. When employees authenticate against the LDAP server, retrieve the name of an IAM role associated with the user. Use AWS STS to assume that IAM role and provide the employees with temporary credentials to access AWS resources.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an identity broker that authenticates against the on-premises LDAP server and then calls AWS STS to assume an IAM role, generating temporary AWS security credentials. The employees can use these credentials to access AWS resources.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>If your identity store is not compatible with SAML 2.0, then you can build a custom identity broker application to perform a similar function. The broker application authenticates users, requests temporary credentials for users from AWS, and then provides them to the user to access AWS resources.</p><p>To enable corporate employees to access the company's AWS resources, you can develop a custom identity broker application. The application verifies that employees are signed into the company's existing identity and authentication system (which might use LDAP, Active Directory, or another system). The identity broker application then obtains temporary security credentials for the employees. To get temporary security credentials, the identity broker application calls either the <code>AssumeRole</code> or <code>GetFederationToken</code> actions in STS to obtain temporary security credentials.</p><p><img src=\"https://media.tutorialsdojo.com/public/enterprise-authentication-with-identity-broker-application.diagram-021424.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/public/enterprise-authentication-with-identity-broker-application.diagram-021424.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Hence, the correct answer is: <strong>Create an identity broker that authenticates against the on-premises LDAP server and then calls AWS STS to assume an IAM role, generating temporary AWS security credentials. The employees can use these credentials to access AWS resources</strong> is correct because it involves creating an identity broker that authenticates against the on-premises LDAP server and then calls AWS STS to assume an IAM role, generating temporary AWS security credentials. This allows the employees to access AWS resources using their corporate account.</p><p>The option that says: <strong>Set up a federation between the on-premises LDAP server and AWS IAM. When employees authenticate against the LDAP server, retrieve the name of an IAM role associated with the user. Use AWS STS to assume that IAM role and provide the employees with temporary credentials to access AWS resources</strong> is incorrect because it primarily involves setting up a federation between the on-premises LDAP server and AWS IAM, which does not align with the company's requirement of enabling employees to access AWS resources using the same corporate account used inside the company network.</p><p>The option that says: <strong>Create an identity broker that authenticates against the on-premises LDAP server and then calls AWS STS to get IAM federated user credentials. The employees can use these credentials to access AWS resources</strong> is incorrect as the users need to be authenticated using LDAP first and not just via STS. In addition, the temporary credentials to log into AWS should be provided by STS and not the identity broker.</p><p>The option that says: <strong>Integrate the on-premises LDAP server with AWS IAM so the employees can log into IAM using their corporate LDAP credentials. Once authenticated, they can use the temporary credentials to access any AWS resource</strong> is incorrect because you cannot use the LDAP credentials to log into IAM. IAM supports identity federation, which allows you to manage access to your AWS resources centrally through your existing identity provider (IdP), but it does not directly integrate with LDAP for user authentication.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></p><p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_ldap.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_ldap.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html",
      "https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_ldap.html",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy"
    ]
  },
  {
    "id": 56,
    "question": "<p>A fashion company in France sells bags, clothes, and other luxury items in its online web store. The online store is currently hosted on the company’s on-premises data center. The company has recently decided to move all of its on-premises infrastructure to the AWS cloud. The main application is running on an NGINX web server and a database with an Oracle Real Application Clusters (RAC) One Node configuration.</p><p>Which of the following is the best way to migrate the application to AWS and set up an automated backup?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch an EC2 instance for both the NGINX server as well as for the database. Attach EBS volumes to the EC2 instance of the database and then use the Data Lifecycle Manager to automatically create scheduled snapshots against the EBS volumes.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Launch an On-Demand EC2 instance and run an NGINX server to host the application. Deploy an RDS instance with a Multi-AZ deployment configuration and enable automated backups on the RDS RAC cluster.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Launch an EC2 instance for both the NGINX server as well as for the database. Attach EBS Volumes on the EC2 instance of the database and then write a shell script that runs the manual snapshot of the volumes.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Launch an EC2 instance and run an NGINX server to host the application. Deploy an RDS instance and enable automated backups on the RDS RAC cluster.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Accelerate Workload Migration and Modernization",
    "explanation": "<p>Oracle RAC is not supported by RDS. That is why you need to deploy the database in an EC2 instance and then either create a shell script to automate the backup or use the Data Lifecycle Manager to automate the process.</p><p>An Oracle Real Application Clusters (RAC) One Node option provides virtualized servers on a single machine. This provides an 'always on' availability for single-instance databases for a fraction of a cost.</p><p><img src=\"https://media.tutorialsdojo.com/sap_data_lifecycle_manager.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_data_lifecycle_manager.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Amazon Data Lifecycle Manager (DLM)</strong> for EBS Snapshots provides a simple, automated way to back up data stored on Amazon EBS volumes. You can define backup and retention schedules for EBS snapshots by creating lifecycle policies based on tags. With this feature, you no longer have to rely on custom scripts to create and manage your backups.</p><p>Hence, the correct answer is: <strong>Launch an EC2 instance for both the NGINX server as well as for the database. Attach EBS volumes to the EC2 instance of the database and then use the Data Lifecycle Manager to automatically create scheduled snapshots against the EBS volumes.</strong></p><p>The option that says: <strong>Launch an EC2 instance for both the NGINX server as well as for the database. Attach EBS Volumes on the EC2 instance of the database and then write a shell script that runs the manual snapshot of the volumes</strong> is incorrect. Although this approach is valid, a more suitable option is to use the Data Lifecycle Manager (DLM) to automatically take the snapshot of the EC2 instance. The DLM can also reduce storage costs by deleting outdated backups.</p><p>The following options are incorrect as these two use Amazon RDS, which doesn't natively support Oracle RAC:</p><p><strong>- Launch an EC2 instance and run an NGINX server to host the application. Deploy an RDS instance and enable automated backups on the RDS RAC cluster.</strong></p><p><strong>- Launch an On-Demand EC2 instance and run an NGINX server to host the application. Deploy an RDS instance with a Multi-AZ deployment configuration and enable automated backups on the RDS RAC cluster.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/oracle/faqs/\">https://aws.amazon.com/rds/oracle/faqs/</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2015/11/self-managed-oracle-rac-on-ec2/\">https://aws.amazon.com/about-aws/whats-new/2015/11/self-managed-oracle-rac-on-ec2/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/rds/oracle/faqs/",
      "https://aws.amazon.com/about-aws/whats-new/2015/11/self-managed-oracle-rac-on-ec2/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html",
      "https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy"
    ]
  },
  {
    "id": 57,
    "question": "<p>A multinational manufacturing company has multiple AWS accounts in multiple AWS regions across North America, Europe, and Asia. The solutions architect has been tasked to set up AWS Organizations to centrally manage policies and have full administrative control across the multiple AWS accounts owned by the company.</p><p>Which of the following options is the recommended implementation to achieve this requirement with the LEAST effort?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Control Tower from the master account and enroll all the member AWS accounts of the company. AWS Control Tower will automatically provision the needed IAM permissions to have full administrative control across all member accounts.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up AWS Organizations by sending an invitation to the master account of your organization from each of the member accounts of the company. Create an <code>OrganizationAccountAccessRole</code> IAM role in the member account and grant permission to the master account to assume the role.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up AWS Organizations by sending an invitation to all member accounts of the company from the master account of your organization. Create an <code>OrganizationAccountAccessRole</code> IAM role in the member account and grant permission to the master account to assume the role.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up AWS Organizations by establishing cross-account access from the master account to all member AWS accounts of the company. The master account will automatically have full administrative control across all member accounts.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>After you create an <strong>Organization</strong> and verify that you own the email address associated with the master account, you can invite existing AWS accounts to join your organization. When you invite an account, AWS Organizations sends an invitation to the account owner, who decides whether to accept or decline the invitation. You can use the AWS Organizations console to initiate and manage invitations that you send to other accounts. You can send an invitation to another account only from the master account of your organization.</p><p>If you are the administrator of an AWS account, you also can accept or decline an invitation from an organization. If you accept, your account becomes a member of that organization. Your account can join only one organization, so if you receive multiple invitations to join, you can accept only one.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_organization_steps.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_organization_steps.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>When an invited account joins your organization, you <em>do not</em> automatically have full administrator control over the account, unlike created accounts. If you want the master account to have full administrative control over an invited member account, you must create the <code>OrganizationAccountAccessRole</code> IAM role in the member account and grant permission to the master account to assume the role.</p><p>Therefore, the correct answer is: <strong>Set up AWS Organizations by sending an invitation to all member accounts of the company from the master account of your organization. Create an </strong><code><strong>OrganizationAccountAccessRole</strong></code><strong> IAM role in the member account and grant permission to the master account to assume the role.</strong></p><p>The option that says: <strong>Set up AWS Organizations by establishing cross-account access from the master account to all member AWS accounts of the company. The master account will automatically have full administrative control across all member accounts</strong> is incorrect. Cross-account access is primarily used for scenarios where you need to grant your IAM users permission to switch to roles within your AWS account or to roles defined in other AWS accounts that you own.</p><p>The option that says: <strong>Set up AWS Organizations by sending an invitation to the master account of your organization from each of the member accounts of the company. Create an </strong><code><strong>OrganizationAccountAccessRole</strong></code><strong> IAM role in the member account and grant permission to the master account to assume the role</strong> is incorrect. It entails a lot of effort to send an individual invitation to the master account from each of the member accounts of the company. It's stated in the scenario that you should achieve this requirement with the LEAST effort, and you can do this by sending an invitation to all member accounts of the company from the master account of your organization.</p><p>The option that says:<strong> Use AWS Control Tower from the master account and enroll all the member AWS accounts of the company. AWS Control Tower will automatically provision the needed IAM permissions to have full administrative control across all member accounts </strong>is incorrect. AWS Control Tower can be typically used to set up and manage multiple AWS accounts. However, it will not automatically provision IAM permissions for all member accounts.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create.html",
      "https://tutorialsdojo.com/aws-organizations/?src=udemy"
    ]
  },
  {
    "id": 58,
    "question": "<p>A financial services company operates a multi-account AWS environment managed by AWS Control Tower. The security team must centralize the management of compliance and security findings across all accounts. The solution should implement preventive, detective, and responsive controls to align with AWS best practices for securing multi-account environments and ensuring compliance with organizational policies.</p><p>Which of the following options will meet the given requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable AWS CloudTrail in each account to log and analyze all API activity. Compile the security findings in a central report.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up Amazon GuardDuty in each account and export the findings to a central Amazon S3 bucket for aggregation and analysis.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a new member account in AWS Organizations. Enable AWS Security Hub and designate the account as the delegated administrator.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Config conformance packs with AWS Control Tower and deploy them using AWS CloudFormation StackSets to apply organizational compliance rules across all accounts.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>AWS Security Hub is a security service that provides a comprehensive view of an organization’s security posture. It collects and aggregates findings from multiple AWS services like Amazon GuardDuty, AWS Config, Amazon Inspector, and partner solutions. Security Hub enables centralization of findings, making it easier to monitor and manage security compliance across accounts.</p><p><img src=\"https://media.tutorialsdojo.com/public/aws-security-hub-delegated-administrator-06Jan2025.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/public/aws-security-hub-delegated-administrator-06Jan2025.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>AWS Organizations allows central management of multiple AWS accounts. It supports service delegation, enabling specific accounts to act as delegated administrators for various AWS services, including Security Hub. This helps streamline security operations by consolidating security management into a single account.</p><p>A delegated administrator account is a member account within an AWS Organization that has been assigned the authority to manage specific services on behalf of the management account. For AWS Security Hub, the delegated administrator can:</p><ol><li><p>Enable Security Hub across all member accounts.</p></li><li><p>Aggregate findings from all accounts into the delegated administrator account.</p></li><li><p>Provide a unified security view, reducing the need for individual account management and aligning with AWS best practices.</p></li></ol><p>Hence, the correct answer is: <strong>Create a new member account in AWS Organizations. Enable AWS Security Hub and designate the account as the delegated administrator.</strong></p><p>The option that says: <strong>Use AWS Config conformance packs with AWS Control Tower and deploy them using AWS CloudFormation StackSets to apply organizational compliance rules across all accounts </strong>is incorrect because while this option provides a good way to ensure compliance, it lacks the central visibility and integration of security findings that AWS Security Hub offers. AWS Config focuses only on ensuring compliance rather than comprehensive threat detection and response.</p><p>The option that says: <strong>Enable AWS CloudTrail in each account to log and analyze all API activity. Compile the security findings in a central report</strong> is incorrect. This option only addresses detective controls and does not centralize or implement preventive or responsive controls, which are required in the scenario. Also, compiling findings manually would not be efficient and does not align with AWS best practices for streamlined security management in a multi-account environment.</p><p>The option that says: <strong>Set up Amazon GuardDuty in each account and export the findings to a central Amazon S3 bucket for aggregation and analysis</strong> is incorrect because this approach would typically result in a significant amount of manual effort and administrative overhead. While Amazon GuardDuty can detect threats, manually exporting and aggregating findings is not efficient or scalable for a multi-account environment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html\">https://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html</a></p><p><a href=\"https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-accounts.html\">https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-accounts.html</a></p><p><a href=\"https://docs.aws.amazon.com/securityhub/latest/userguide/designate-orgs-admin-account.html\">https://docs.aws.amazon.com/securityhub/latest/userguide/designate-orgs-admin-account.html</a></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-security-controls/security-control-types.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-security-controls/security-control-types.html</a></p><p><br></p><p><strong>Check out these AWS Security Hub and AWS Organizations Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-security-hub/?src=udemy\">https://tutorialsdojo.com/aws-security-hub/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html",
      "https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-accounts.html",
      "https://docs.aws.amazon.com/securityhub/latest/userguide/designate-orgs-admin-account.html",
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-security-controls/security-control-types.html",
      "https://tutorialsdojo.com/aws-security-hub/?src=udemy"
    ]
  },
  {
    "id": 59,
    "question": "<p>A company is running its enterprise resource planning application in AWS that handles supply chain, order management, and delivery tracking. The architecture has a set of RESTful web services that enable third-party companies to search for data that will be consumed by their respective applications. The public web services consist of several AWS Lambda functions. DynamoDB is used for its database tier and is integrated with an Amazon OpenSearch domain, which stores the indexes and supports the search feature. A Solutions Architect has been instructed to ensure that in the event of a failed deployment, there should be no downtime, and a system should be in place to prevent subsequent deployments. The service must strictly maintain full capacity during API deployment without any reduced compute capacity to avoid degradation of the service.</p><p>Among the options below, which can the Architect use to meet the requirements in the MOST efficient way?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Do a blue/green deployment on all upcoming changes using AWS CodeDeploy. Using AWS CloudFormation, launch the Amazon DynamoDB tables, AWS Lambda functions, and Amazon OpenSearch domain in your AWS VPC. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Immutable</code>.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Do an in-place deployment on all upcoming changes using AWS CodeDeploy. Using AWS SAM, launch the Amazon DynamoDB tables, Lambda functions, and Amazon OpenSearch domain in your AWS VPC. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Rolling</code>.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Do a blue/green deployment on all upcoming changes using AWS CodeDeploy. Using AWS SAM, launch the DynamoDB tables, Lambda functions, and Amazon OpenSearch domain in your AWS VPC. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>All at Once</code>.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Do a blue/green deployment on all upcoming changes using Amazon Lightsail. Let Amazon Lightsail handle the provisioning of database instances, EC2 instances, and load balancers needed by the web application. Use CloudFormation to deploy the AWS Lambda functions, provision DynamoDB tables, and create an Amazon OpenSearch domain in your VPC.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS Elastic Beanstalk</strong> provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, and Immutable) and options that let you configure the batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's an automatically scaling environment (you didn't specify the <code>--single</code> option), it uses rolling deployments.</p><p>With rolling deployments, Elastic Beanstalk splits the environment's EC2 instances into batches and deploys the new version of the application to one batch at a time, leaving the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version.</p><p><img src=\"https://media.tutorialsdojo.com/sap_elastic_beanstalk_immutable_configuration.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_elastic_beanstalk_immutable_configuration.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.''</p><p>Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched.</p><p>Therefore, the correct answer is:<strong> Do a blue/green deployment on all upcoming changes using AWS CodeDeploy. Using AWS CloudFormation, launch the Amazon DynamoDB tables, AWS Lambda functions, and Amazon OpenSearch domain in your AWS VPC. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>Immutable</strong></code><strong>.<br></strong></p><p>The option that says: <strong>Do a blue/green deployment on all upcoming changes using AWS CodeDeploy. Using AWS SAM, launch the DynamoDB tables, Lambda functions, and Amazon OpenSearch domain in your AWS VPC. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>All at Once</strong></code> is incorrect. This policy deploys the new version to all instances simultaneously which means that the instances in your environment are out of service for a short time while the deployment occurs.</p><p>The option that says: <strong>Do an in-place deployment on all upcoming changes using AWS CodeDeploy. Using AWS SAM, launch the Amazon DynamoDB tables, Lambda functions, and Amazon OpenSearch domain in your AWS VPC. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>Rolling</strong></code> is incorrect. This policy will deploy the new version in batches where each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch.</p><p>The option that says:<strong> Do a blue/green deployment on all upcoming changes using Amazon Lightsail. Let Amazon Lightsail handle the provisioning of database instances, EC2 instances, and load balancers needed by the web application. Use CloudFormation to deploy the AWS Lambda functions, provision DynamoDB tables, and create an Amazon OpenSearch domain in your VPC </strong>is incorrect. Amazon Lightsail is a cost-effective solution for deploying simple web applications. AWS recommends it for DEV/Test environments, not for enterprise-ready, mission-critical workloads.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html",
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html",
      "https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy"
    ]
  },
  {
    "id": 60,
    "question": "<p>A company runs a finance-related application on a fleet of Amazon EC2 instances inside a private subnet of a VPC in AWS. To access the application, the instances are behind an internet-facing Application Load Balancer (ALB). As part of security compliance, the company is required to have a solution that allows it to inspect network payloads that are being sent to the application. Analyzing the network payloads will help in reverse-engineering sophisticated network attacks that the application may experience.</p><p>Which of the following options should the solutions architect implement to meet the company requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Traffic Mirroring on the elastic network interface of the EC2 instances. Send the mirrored traffic to a monitoring appliance for storage and inspection.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Go to the Amazon VPC console and create a VPC flow log. Set the destination of flow log data to an Amazon S3 bucket for analysis.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a new AWS web ACL with blank rules and a default “Allow” action. Associate the ALB to this web ACL. Enable logging on web ACL and send them to Amazon CloudWatch Logs for analysis.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Go to the Amazon EC2 console and enable “Access logs” for the ALB. Send the ALB access logs to Amazon AppFlow for payload inspection and to an Amazon S3 bucket for long-term storage.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Traffic Mirroring</strong> is an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of Amazon EC2 instances. You can then send the traffic to out-of-band security and monitoring appliances for:</p><p>-Content inspection</p><p>-Threat monitoring</p><p>-Troubleshooting</p><p>The security and monitoring appliances can be deployed as individual instances or as a fleet of instances behind a Network Load Balancer with a UDP listener. Traffic Mirroring supports filters and packet truncation so that you only extract the traffic of interest to monitor by using monitoring tools of your choice.</p><p><img src=\"https://media.tutorialsdojo.com/sap_traffic_mirroring.PNG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_traffic_mirroring.PNG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Traffic Mirroring copies inbound and outbound traffic from the network interfaces that are attached to your Amazon EC2 instances. You can send the mirrored traffic to the network interface of another EC2 instance or a Network Load Balancer that has a UDP listener. The traffic mirror source and the traffic mirror target (monitoring appliance) can be in the same VPC. Or they can be in different VPCs that are connected through intra-Region VPC peering or a transit gateway.</p><p>Therefore, the correct answer is: <strong>Configure Traffic Mirroring on the elastic network interface of the EC2 instances. Send the mirrored traffic to a monitoring appliance for storage and inspection.</strong> Traffic Mirroring can copy network traffic from an elastic network interface and send it to a monitoring appliance for inspection.</p><p>The option that says: <strong>Go to the Amazon VPC console and create a VPC flow log. Set the destination of flow log data to an Amazon S3 bucket for analysis</strong> is incorrect. This will log network data on all resources on the VPC, not just the EC2 cluster in question. Additionally, VPC flow log data only contain OSI Layer 4 (Transport) information. This does not include payload contents.</p><p>The option that says: <strong>Go to the Amazon EC2 console and enable “Access logs” for the ALB. Send the ALB access logs to Amazon AppFlow for payload inspection and to an Amazon S3 bucket for long-term storage </strong>is incorrect. Amazon AppFlow is used for transferring data between Software-as-a-Service (SaaS) applications, not for payload inspection. ALB access logs have similar content to HTTP/HTTPS logs, however, it only contains the request path and some headers on the logs. The payload itself is not recorded.</p><p>The option that says: <strong>Create a new AWS web ACL with blank rules and a default “Allow” action. Associate the ALB to this web ACL. Enable logging on web ACL and send them to Amazon CloudWatch Logs for analysis</strong> is incorrect. You can capture information about the web requests that are evaluated by AWS WAF and send them to AWS CloudWatch Logs. However, CloudWatch Logs is designed for searching and querying logs. You will only see the details of the request packet and not the payload itself. If you need a more sophisticated analysis and inspection of the request payload, it would be better to use a dedicated monitoring appliance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/mirroring/what-is-traffic-mirroring.html\">https://docs.aws.amazon.com/vpc/latest/mirroring/what-is-traffic-mirroring.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html\">https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/mirroring/tm-example-inbound-tcp.html\">https://docs.aws.amazon.com/vpc/latest/mirroring/tm-example-inbound-tcp.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/mirroring/what-is-traffic-mirroring.html",
      "https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html",
      "https://docs.aws.amazon.com/vpc/latest/mirroring/tm-example-inbound-tcp.html",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy"
    ]
  },
  {
    "id": 61,
    "question": "<p>A company runs its critical application in an Auto Scaling group of Amazon EC2 instances that uses ElastiCache with Append Only Files (AOF) enabled in multiple AWS regions. Recently, one of the regions experienced a power outage due to a storm which has affected the business revenue.</p><p>Assuming that only a short recovery downtime period is allowed, how should the solutions architect maintain site availability in case an event like this occurs again in the future?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable Domain Name System Security Extensions (DNSSEC) in your domain. Configure Route 53 to automatically failover the traffic to a secondary group of healthy resources on standby. Configure the 'Evaluate Target Health' attribute to No.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a dedicated Transit VPC to directly route multi-VPC traffic over a VPN connection across multiple regions.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Consolidate all of your VPCs across multiple regions into a single Private Hosted Zone using Route 53.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a DNS active-active failover using latency based routing policy that resolves to an ELB. Configure the 'Evaluate Target Health' attribute to Yes.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>DNS active-active failover allows access to your unhealthy instances to be redirected to active instances. Together with latency-based routing, customers accessing your web servers will be balanced throughout available healthy instances based on latency.</p><p><img src=\"https://media.tutorialsdojo.com/sap_route53_active_avtive.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_route53_active_avtive.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Hence, the correct answer is: <strong>Set up a DNS active-active failover using latency based routing policy that resolves to an ELB. Configure the 'Evaluate Target Health' attribute to Yes.</strong></p><p>The option that says: <strong>Console all of your VPCs across multiple regions into a single Private Hosted Zone using Route 53</strong> is incorrect because private hosted zones are primarily used to specify how you want to route traffic in a single VPC or a group of VPCs, instead of going through the public Internet. This does not provide an active-active failover.</p><p>The option that says: <strong>Enable Domain Name System Security Extensions (DNSSEC) in your domain. Configure Route 53 to automatically failover the traffic to a secondary group of healthy resources on standby. Configure the 'Evaluate Target Health' attribute to No</strong> is incorrect because DNSSEC is primarily used to protect your domain from DNS spoofing or man-in-the-middle attacks. If you set Evaluate Target Health to No, Route 53 continues to route traffic to the records that an alias record refers to even if health checks for those records are failing. Thus, this configuration will lead to unavailability issues on the application.</p><p>The option that says: <strong>Create a dedicated Transit VPC to directly route multi-VPC traffic over a VPN connection across multiple regions</strong> is incorrect because a Transit VPC is not suitable in providing a failover routing for your resources. This set up is more suitable for scenarios where you are designing a Multiple-VPC VPN connection sharing.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html",
      "https://tutorialsdojo.com/amazon-route-53/?src=udemy"
    ]
  },
  {
    "id": 62,
    "question": "<p>A business news portal is visited by thousands of readers each day to check on the latest hot topics in the world of business and technology. The news portal runs on a fleet of Spot Amazon EC2 instances behind an Application Load Balancer (ALB). Readers can also submit comments in every article. Currently, the system's database is running on an on-premises data center, and the CTO is concerned that the content delivery time is not meeting company objectives. The portal's page load time is of utmost importance for the company to maintain its daily visitors.</p><p>Which of the following options would allow the solutions architect to quickly and cost-effectively modify the current infrastructure to reduce latency for customers?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Migrate your database on-premises to Amazon Aurora using the AWS Database Migration Service (AWS DMS) and AWS Schema Conversion Tool (SCT). Create Aurora Replicas across Availability Zones and reconfigure the web servers to query from the Aurora database instead.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon CloudFront web distribution to speed up the delivery of data to their readers around the globe. Migrate the entire portal to an Amazon S3 bucket and then enable static web hosting. Set the S3 bucket as the origin of your CloudFront distribution.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add an in-memory datastore using Amazon ElastiCache for Redis to reduce the burden on the database. Enable Redis replication to scale database reads and to have highly available clusters.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Replace the on-premises database of the news portal with a fast, scalable full-text search engine using Amazon OpenSearch Service by setting up an ELK stack (Elasticsearch, Logstash, and Kibana). Use an Amazon CloudFront web distribution to speed up the delivery of data to your users across the globe.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>The success of the website and business is significantly affected by the speed at which you deliver content. Even the most optimized database query or remote API call is going to be noticeably slower than retrieving a flat key from an in-memory cache.</p><p>The primary purpose of an in-memory key-value store is to provide ultrafast (submillisecond latency) and inexpensive access to copies of data. Most data stores have areas of data that are frequently accessed but seldom updated. Additionally, querying a database is always slower and more expensive than locating a key in a key-value pair cache. By caching such query results, you pay the price of the query once and then are able to quickly retrieve the data multiple times without having to re-execute the query.</p><p><img src=\"https://media.tutorialsdojo.com/sap_elasticache_how_it_works.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_elasticache_how_it_works.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>With the cache inside the VPC along with the web servers and application servers, the application doesn’t have to constantly go from AWS to the local data center. This change makes the physical distance between servers irrelevant. The new architecture should greatly improve the customer experience.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_reduce_latency.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_reduce_latency.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Add an in-memory datastore using Amazon ElastiCache for Redis to reduce the burden on the database. Enable Redis replication to scale database reads and to have highly available clusters.</strong> The main issue in this problem involves latency to the backend database. This solution dramatically reduces data retrieval latency. It also scales request volume considerably, because Amazon ElastiCache can deliver extremely high request rates, measured at over 20 million per second.</p><p>The option that says:<strong> Migrate your database on-premises to Amazon Aurora using the AWS Database Migration Service (AWS DMS) and AWS Schema Conversion Tool (SCT). Create Aurora Replicas across Availability Zones and reconfigure the web servers to query from the Aurora database instead</strong> is incorrect. There is no guarantee that the database engine currently being used by the news portal is supported by Amazon Aurora. It is also important to take note that the question asks for a quick and cost-effective solution. Migrating the database may typically take a long amount of time, and the number of requests being sent each day can rack up costs.</p><p>The option that says: <strong>Replace the on-premises database of the news portal with a fast, scalable full-text search engine using Amazon OpenSearch Service by setting up an ELK stack (Elasticsearch, Logstash, and Kibana). Use an Amazon CloudFront web distribution to speed up the delivery of data to your users across the globe</strong> is incorrect. OpenSearch is primarily used for log analytics, full-text search, security intelligence, business analytics, and operational intelligence use cases but not as a full-fledged database. A search engine could possibly be beneficial to the news portal as an additional layer but not as its sole data source. This solution might provide some advantages when they are both integrated together.</p><p>The option that says: <strong>Create an Amazon CloudFront web distribution to speed up the delivery of data to their readers around the globe. Migrate the entire portal to an Amazon S3 bucket and then enable static web hosting. Set the S3 bucket as the origin of your CloudFront distribution</strong> is incorrect. It is stated in the scenario that the readers can also submit their comments in every article, which means that the news portal is a dynamic website and not static. Hence, using S3 is not suitable for this scenario which is why this option is incorrect, even though it mentions the use of CloudFront web distribution.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p><p><a href=\"https://aws.amazon.com/blogs/database/latency-reduction-of-hybrid-architectures-with-amazon-elasticache/\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis-RedisCluster.html</a></p><p><br></p><p><strong>Check out this Amazon ElastiCache Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/amazon-elasticache/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html",
      "https://aws.amazon.com/blogs/database/latency-reduction-of-hybrid-architectures-with-amazon-elasticache/",
      "https://tutorialsdojo.com/amazon-elasticache/?src=udemy"
    ]
  },
  {
    "id": 63,
    "question": "<p>A major telecommunications company is planning to set up a disaster recovery solution for its Amazon Redshift cluster which is being used by its online data analytics application. Database encryption is enabled on their clusters using AWS KMS and it is required that the recovery site should be at least 500 miles from their primary cloud location.</p><p>Which of the following is the most suitable solution to meet these requirements and to make its architecture highly available?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a new AWS CloudFormation stack that will deploy the cluster in another region and will regularly back up the data to an S3 bucket, configured with cross-region replication. In case of an outage in the primary region, just use the snapshot from the S3 bucket and then start the cluster.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Develop a scheduled job using AWS Lambda which will regularly take a snapshot of the Redshift cluster and copy it to another region.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>In your Redshift cluster, enable the cross-region snapshot copy feature to copy snapshots to another region.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a <code>snapshot copy grant</code> for a master key in the destination region and enable cross-region snapshots in your Redshift cluster to copy snapshots of the cluster to another region.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>Snapshots</strong> are point-in-time backups of a cluster. There are two types of snapshots: automated and manual. Amazon Redshift stores these snapshots internally in Amazon S3 by using an encrypted Secure Sockets Layer (SSL) connection. Amazon Redshift automatically takes incremental snapshots that track changes to the cluster since the previous automated snapshot.</p><p><img src=\"https://media.tutorialsdojo.com/sap_redshift_cross_region_snapshot.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_redshift_cross_region_snapshot.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Automated snapshots retain all of the data required to restore a cluster from a snapshot. You can take a manual snapshot any time. When you restore from a snapshot, Amazon Redshift creates a new cluster and makes the new cluster available before all of the data is loaded, so you can begin querying the new cluster immediately. The cluster streams data on demand from the snapshot in response to active queries, then loads the remaining data in the background.</p><p>When you launch an <strong>Amazon Redshift</strong> cluster, you can choose to encrypt it with a master key from the AWS Key Management Service (AWS KMS). AWS KMS keys are specific to a region. If you want to enable cross-region snapshot copy for an AWS KMS-encrypted cluster, you must configure a <em>snapshot copy grant</em> for a master key in the destination region so that Amazon Redshift can perform encryption operations in the destination region.</p><p>Therefore, the correct answer is: <strong>Set up a </strong><code><strong>snapshot copy grant</strong></code><strong> for a master key in the destination region and enable cross-region snapshots in your Redshift cluster to copy snapshots of the cluster to another region.</strong></p><p>The option that says: <strong>Create a new AWS CloudFormation stack that will deploy the cluster in another region and will regularly back up the data to an S3 bucket, configured with cross-region replication. In case of an outage in the primary region, just use the snapshot from the S3 bucket and then start the cluster</strong> is incorrect. Using a combination of CloudFormation and a separate S3 bucket entails a lot of configuration and set up compared with just enabling cross-region snapshot copy in your Redshift cluster.</p><p>The option that says: <strong>Develop a scheduled job using AWS Lambda which will regularly take a snapshot of the Redshift cluster and copy it to another region</strong> is incorrect. It is not recommended to use AWS Lambda to copy data on your Redshift cluster to another region. You simply have to enable cross-region snapshot copy in your Redshift cluster in order to meet the requirement.</p><p>The option that says: <strong>In your Redshift cluster, enable the cross-region snapshot copy feature to copy snapshots to another region</strong> is incorrect. Although it is right to use the cross-region snapshot copy feature, you still have to configure a snapshot copy grant for a master key in the destination region so that Amazon Redshift can perform encryption operations in the destination region.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#snapshot-crossregioncopy-configure\">https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#snapshot-crossregioncopy-configure</a></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html</a></p><p><br></p><p><strong>Check out this Amazon Redshift Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-redshift/?src=udemy\">https://tutorialsdojo.com/amazon-redshift/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#snapshot-crossregioncopy-configure",
      "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html",
      "https://tutorialsdojo.com/amazon-redshift/?src=udemy",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 64,
    "question": "<p>A data analytics company is running simulations on a high-performance computing (HPC) cluster in AWS. The compute node and storage are tightly coupled to achieve the best performance possible. The running simulations on the cluster produce thousands of large files stored on an Amazon EFS share that is shared across 200 Amazon EC2 instances. Several more simulation jobs need to be run on the cluster so the number of nodes has been increased to 1000 instances. However, the bigger cluster performed below the expectations of the company. The Solutions Architect was tasked to implement a solution that will achieve maximum performance from the HPC cluster.</p><p>Which of the following options are the recommended actions to achieve this? (Select THREE.)</p>",
    "corrects": [
      1,
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Improve the network performance of each node by using Amazon EC2 instances with an Elastic Fabric Adapter (EFA) network interface.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Improve the storage performance by using Amazon EBS Throughput Optimized volumes configured on RAID 0 mode. This allows for much higher IOPS compared to Amazon EFS.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Improve the storage performance by implementing Amazon FSx for Lustre instead of using Amazon EFS.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Improve the performance and scalability of the HPC cluster by spreading the Amazon EC2 instances into multiple Availability Zones. Improve the storage performance by using Amazon EBS volumes configured on RAID 0 mode. This allows for much higher IOPS compared to Amazon EFS.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Improve the performance by placing all the compute nodes as close to each other. Re-launch all the Amazon EC2 instances within a single Availability Zone in a cluster placement group.</p>",
        "correct": true
      },
      {
        "id": 6,
        "answer": "<p>Improve the network performance of each node by attaching multiple network interfaces to the Amazon EC2 instances. This ensures that the network bandwidth is not a bottleneck.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In this scenario, the shared Amazon EFS storage and the spreading of the compute nodes on multiple AZs cause sluggishness on both the storage and network performance of the HPC cluster. For a tightly-coupled HPC cluster, you want the Amazon EC2 instances to be launched on a single Availability Zone.</p><p><img src=\"https://media.tutorialsdojo.com/sap_az_placement_group.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_az_placement_group.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:</p><p><strong>Cluster</strong> – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is <strong>typical of HPC applications.</strong></p><p><strong>Partition</strong> – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.</p><p><strong>Spread</strong> – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.</p><p><strong>Elastic Fabric Adapter (EFA)</strong> is a network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at scale on AWS. Its custom-built operating system (OS) bypass hardware interface enhances the performance of inter-instance communications, which is critical to scaling these applications. EFA’s unique OS bypass networking mechanism provides a low-latency, low-jitter channel for inter-instance communications. This enables your tightly-coupled HPC or distributed machine learning applications to scale to thousands of cores, making your applications run faster.</p><p><img src=\"https://media.tutorialsdojo.com/sap_fsx_for_lustre.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_fsx_for_lustre.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>Amazon FSx for Lustre</strong> makes it easy and cost-effective to launch and run the popular, high-performance Lustre file system. You use Lustre for workloads where speed matters, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. Amazon FSx file systems provide up to multiple GB/s of throughput and hundreds of thousands of IOPS. The specific amount of throughput and IOPS that your workload can drive on your file system depends on the throughput capacity and storage capacity configuration of your file system, along with the nature of your workload, including the size of the active working set.</p><p>The open-source Lustre file system is designed for applications that require fast storage—where you want your storage to keep up with your computing capacity. Lustre was built to solve the problem of quickly and cheaply processing the world's ever-growing datasets. It's a widely used file system designed for the fastest computers in the world. It provides submillisecond latencies, up to hundreds of Gbps of throughput, and up to millions of IOPS.</p><p>Therefore, the correct answers are:</p><p><strong>- Improve the performance by placing all the compute nodes as close to each other. Re-launch all the Amazon EC2 instances within a single Availability Zone in a cluster placement group.</strong></p><p><strong>- Improve the network performance of each node by using Amazon EC2 instances with an Elastic Fabric Adapter (EFA) network interface.</strong></p><p><strong>- Improve the storage performance by implementing Amazon FSx for Lustre instead of using Amazon EFS.</strong></p><p>The option that says: <strong>Improve the network performance of each node by attaching multiple network interfaces to the Amazon EC2 instances. This ensures that the network bandwidth is not a bottleneck</strong> is incorrect. Although this may increase the network performance of the individual compute node, the Amazon EFS could still be a bottleneck because there are too many instances accessing the EFS shared storage simultaneously. A better solution is to use Amazon FSx for Lustre which supports multiple GB/s of throughput and hundreds of thousands of IOPS.</p><p>The option that says: <strong>Improve the performance and scalability of the HPC cluster by spreading the Amazon EC2 instances into multiple Availability Zones</strong> is incorrect. The HPC cluster relies on tight communication between the compute nodes and the storage solution. Spreading the EC2 instances into multiple Availability Zones will lower the performance of the cluster because the underlying physical servers will have to communicate with each other over long distances.</p><p>The option that says: <strong>Improve the storage performance by using Amazon EBS Throughput Optimized volumes configured on RAID 0 mode. This allows for much higher IOPS compared to Amazon EFS</strong> is incorrect. The cluster relies on shared storage across all the compute nodes. A Throughput Optimized EBS volume cannot be shared between EC2 instances. The Amazon EBS Multi-Attach feature is only applicable for Provisioned IOPS SSD volumes.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p><p><a href=\"https://aws.amazon.com/hpc/efa/\">https://aws.amazon.com/hpc/efa/</a></p><p><a href=\"https://aws.amazon.com/fsx/lustre/\">https://aws.amazon.com/fsx/lustre/</a></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html\">https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html</a></p><p><br></p><p><strong>Check out the Amazon FSx Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-fsx/?src=udemy\">https://tutorialsdojo.com/amazon-fsx/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "https://aws.amazon.com/hpc/efa/",
      "https://aws.amazon.com/fsx/lustre/",
      "https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html",
      "https://tutorialsdojo.com/amazon-fsx/?src=udemy"
    ]
  },
  {
    "id": 65,
    "question": "<p>A company is running hundreds of Linux-based Amazon EC2 instances launched with custom AMIs that are dedicated to specific products and services. As part of the security compliance requirements, vulnerability scanning must be done on all EC2 instances wherein each instance must be scanned and pass a Common Vulnerabilities and Exposures (CVE) assessment. Since the development team relies heavily on the custom AMIs for their deployments, the company wants to have an automated process to run the security assessment on any new AMIs and properly tag them before they can be used by the developers. To ensure continuous compliance, the security-approved AMIs must also be scanned every 30 days to check for new vulnerabilities and apply the necessary patches.</p><p>Which of the following steps should the Solutions Architect implement to achieve the security requirements? (Select TWO.)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Write a Lambda function that will create automatic approval rules. Create a parameter on AWS SSM Parameter Store to save the list of all security-approved AMI. Set up a managed rule on AWS Config to continuously scan all running EC2 instances. For any detected vulnerability, run the designated SSM Automation document.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Check AWS CloudTrail logs to determine the Amazon EC2 instance IDs that were launched from the AMIs that need scanning. Use AWS Config managed rule to run CVE assessment and remediation on the instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Develop a Lambda function that will create automatic approval rules. Create a parameter on AWS SSM Parameter Store to save the list of all security-approved AMI. Set up a 30-day interval cron rule on Amazon EventBridge to trigger an AWS SSM Automation document run on all EC2 instances.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Install the AWS Systems Manager (SSM) agent on all EC2 instances. With the agent running, run a detailed CVE assessment scan on the EC2 instances launched from the AMIs that need scanning.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Create an Assessment template on Amazon Inspector to target the EC2 instances. Run a detailed CVE assessment scan on all running Amazon EC2 instances launched from the AMIs that need scanning.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS Systems Manager Automation</strong> simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following:</p><p>- Build automations to configure and manage instances and AWS resources.</p><p>- Create custom runbooks or use pre-defined runbooks maintained by AWS.</p><p>- Receive notifications about Automation tasks and runbooks by using Amazon EventBridge.</p><p>- Monitor Automation progress and details by using the AWS Systems Manager console.</p><p>SSM Automation offers one-click automation for simplifying complex tasks such as creating golden Amazon Machines Images (AMIs) and recovering unreachable EC2 instances. For example, you can use Use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> runbooks to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_ssm_overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>With <strong>AWS EventBridge</strong>, you can create rules that self-trigger on an automated schedule in EventBridge using cron or rate expressions. Rate expressions are simpler to define but don't offer the fine-grained schedule control that cron expressions support. For example, with a cron expression, you can define a rule that triggers at a specified time on a certain day of each week or month. With this, you can schedule running AWS SSM Automation documents to remediate the vulnerable AMIs.</p><p>You can use <strong>Amazon Inspector</strong> to conduct a detailed scan for CVE in your fleet of EC2 instances. Amazon Inspector offers predefined software called an agent that you can optionally install in the operating system of the EC2 instances that you want to assess. Amazon Inspector also has rules packages that help verify whether the EC2 instances in your assessment targets are exposed to common vulnerabilities and exposures (CVEs). Attacks can exploit unpatched vulnerabilities to compromise the confidentiality, integrity, or availability of your service or data. The CVE system provides a reference method for publicly known information security vulnerabilities and exposures.</p><p>The option that says:<strong> Develop a Lambda function that will create automatic approval rules. Create a parameter on AWS SSM Parameter Store to save the list of all security-approved AMI. Set up a 30-day interval cron rule on Amazon EventBridge to trigger an AWS SSM Automation document run on all EC2 instances</strong> is correct because it satisfies the requirement for updating the security-approved AMI, along with scheduled patches every 30-days using SSM Automation document. AWS SSM Automation can automatically pack AMIs after patches are applied.</p><p>The option that says: <strong>Create an Assessment template on Amazon Inspector to target the EC2 instances. Run a detailed CVE assessment scan on all running Amazon EC2 instances launched from the AMIs that need scanning</strong> is correct because Amazon Inspector can run assessments on target EC2 instances to check if they are exposed to common vulnerabilities and exposures (CVEs).</p><p>The option that says: <strong>Install the AWS Systems Manager (SSM) agent on all EC2 instances. With the agent running, run a detailed CVE assessment scan on the EC2 instances launched from the AMIs that need scanning</strong> is incorrect because the SSM agent cannot run a detailed CVE assessment scan on EC2 instances. You have to use Amazon Inspector to satisfy the given requirement.</p><p>The option that says: <strong>Write a Lambda function that will create automatic approval rules. Create a parameter on AWS SSM Parameter Store to save the list of all security-approved AMI. Set up a managed rule on AWS Config to continuously scan all running EC2 instances. For any detected vulnerability, run the designated SSM Automation document</strong> is incorrect because AWS Config cannot automatically run checks on the operating system of your Amazon EC2 instances. The requirement is to run the assessment every 30-days only and not continuously.</p><p>The option that says: <strong>Check AWS CloudTrail logs to determine the Amazon EC2 instance IDs that were launched from the AMIs that need scanning. Use AWS Config managed rule to run CVE assessment and remediation on the instances</strong> is incorrect. Although it is possible to parse the EC2 instance IDs from CloudTrail and determine the vulnerable instances, you still cannot run the CVE assessment in AWS Config for your Amazon EC2 instances. Using Amazon Inspector is the most suitable service to use in running the CVE assessment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html\">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/scheduled-events.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/scheduled-events.html</a></p><p><br></p><p><strong>Check out the AWS Systems Manager and AWS Inspector Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-inspector/?src=udemy\">https://tutorialsdojo.com/amazon-inspector/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html",
      "https://docs.aws.amazon.com/eventbridge/latest/userguide/scheduled-events.html",
      "https://tutorialsdojo.com/aws-systems-manager/?src=udemy",
      "https://tutorialsdojo.com/amazon-inspector/?src=udemy"
    ]
  },
  {
    "id": 66,
    "question": "<p>A tech company is about to undergo a financial audit. It has been planned to use a third-party web application that needs to have certain AWS access to issue several API commands. It will discover Amazon EC2 resources running within the enterprise's account. The company has internal security policies that require any outside access to its environment to conform to the principles of least privilege. The solutions architect must ensure that the credentials used by the third-party vendor cannot be used by any other third party. The third-party vendor also has an AWS account where it runs its web application and it already provided a unique customer ID, including their AWS account number.</p><p>Which of the following options would allow the solutions architect to give permissions to the third-party vendor in compliance with the company requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Provide your own access key and secret key to the third-party software.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an IAM user in the enterprise account that has permissions allowing only the actions required by the third-party application. Also generate a new access key and secret key from the user to be given to the third-party provider.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Connect to allow the third-party application to access your AWS resources. In the AWS Connect configuration, input the <code>ExternalId</code> context key to ensure that it matches the unique customer ID of the 3rd party vendor.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a new IAM role for the 3rd-party vendor. Add a permission policy that only allows the actions required by the third party application. Also, add a trust policy with a <code>Condition</code> element for the <code>ExternalId</code> context key. The Condition must test the <code>ExternalId</code> context key to ensure that it matches the unique customer ID from the 3rd party vendor.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>At times, you need to give third party access to your AWS resources (delegate access). One important aspect of this scenario is the External ID, optional information that you can use in an IAM role trust policy to designate who can assume the role.</p><p>To use an external ID, update a role trust policy with the external ID of your choice. Then, when someone uses the AWS CLI or AWS API to assume that role, they must provide the external ID.</p><p>For example, let's say that you decide to hire a third-party company called Boracay Corp to monitor your AWS account and help optimize costs. In order to track your daily spending, Boracay Corp needs to access your AWS resources. Boracay Corp also monitors many other AWS accounts for other customers.</p><p><img src=\"https://media.tutorialsdojo.com/sap_iam_assume_role_cross_account.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_iam_assume_role_cross_account.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Do not give Boracay Corp access to an IAM user and its long-term credentials in your AWS account. Instead, use an IAM role and its temporary security credentials. An IAM role provides a mechanism to allow a third party to access your AWS resources without needing to share long-term credentials (for example, an IAM user's access key).</p><p>You can use an IAM role to establish a trusted relationship between your AWS account and the Boracay Corp account. After this relationship is established, a member of the Boracay Corp account can call the AWS STS AssumeRole API to obtain temporary security credentials. The Boracay Corp members can then use the credentials to access AWS resources in your account.</p><p>When a user, a resource, an application, or any service needs to access any AWS service or resource, always opt to create an appropriate role that has the least privileged access or only the required access, rather than using any other credentials such as keys.</p><p>Therefore, the correct answer is: <strong>Create a new IAM role for the 3rd-party vendor. Add a permission policy that only allows the actions required by the third party application. Also, add a trust policy with a </strong><code><strong>Condition</strong></code><strong> element for the </strong><code><strong>ExternalId</strong></code><strong> context key. The Condition must test the </strong><code><strong>ExternalId</strong></code><strong> context key to ensure that it matches the unique customer ID from the 3rd party vendor.</strong></p><p>The option that says: <strong>Use Amazon Connect to allow the third-party application to access your AWS resources. In the AWS Connect configuration, input the </strong><code><strong>ExternalId</strong></code><strong> context key to ensure that it matches the unique customer ID of the 3rd party vendor </strong>is incorrect because Amazon Connect is simply an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost. You should use an IAM Role in this scenario instead of Amazon Connect.</p><p>The option that says: <strong>Provide your own access key and secret key to the third-party software</strong> is incorrect because you should never share your access and secret keys.</p><p>The option that says: <strong>Create an IAM user in the enterprise account that has permissions allowing only the actions required by the third-party application and generating a new access key and secret key from the user to be given to the third-party provider</strong> is incorrect because when a user is created, its security credentials are stored in the EC2 which can be compromised, and the creation of the appropriate role is always the better solution rather than creating a user.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy"
    ]
  },
  {
    "id": 67,
    "question": "<p>A company has a critical application running on an Auto Scaling group of Amazon EC2 instances. The application CI/CD pipelines are created on AWS CodePipeline and all of the relevant AWS resources are defined in AWS CloudFormation templates. During deployments, the Auto Scaling group spawns new instances and the user data script downloads the new artifact from a central Amazon S3 bucket. With several code updates during the development cycle, a recent update on the CloudFormation templates has caused a major application downtime.</p><p>Which of the following solutions should the Solutions Architect implement to reduce the chances of downtime during deployments?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add an AWS CodeBuild stage on the deployment pipeline to automatically test on a non-production environment. Leverage change sets on AWS CloudFormation to preview changes before applying to production. Set up a blue/green deployment pattern on AWS CodeDeploy to deploy changes on a separate environment and to quickly rollback if needed.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Update the CloudFormation templates to include cfn helper scripts. This will detect and report conditions during deployments to ensure that only healthy deployments are continued. Create test plans for the quality assurance team to ensure that changes are tested on a non-production environment before applying to production.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Check the CloudFormation templates for errors with the help of plugins on the integrated development environment (IDE). Ensure that the templates are valid using AWS CLI. Include cfn helper scripts on the deployment code to detect and report for errors. Deploy on a non-production environment and perform manual testing before applying changes to production.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a blue/green deployment pattern on AWS CodeDeploy using CloudFormation to update the user data deployment scripts. Manually login to the instances and perform tests to verify that the deployment is successful and the application is running as expected.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p><strong>AWS CodeBuild</strong> is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more. You can also customize build environments in CodeBuild to use your own build tools.</p><p>You can automate your release process by using <strong>AWS CodePipeline</strong> to test your code and run your builds with AWS CodeBuild. This involves two main steps:</p><p>- Create a continuous delivery (CD) pipeline with CodePipeline that automates builds with CodeBuild.</p><p>- Add test and build automation with CodeBuild to an existing pipeline in CodePipeline.</p><p>When you need to update a <strong>CloudFormation</strong> stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. <strong>Change sets</strong> allow you to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set.</p><p><strong>AWS CodeDeploy</strong> is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it possible to automate the deployment of code to either Amazon EC2 or on-premises instances. AWS CodeDeploy supports blue/green deployments. AWS CodeDeploy offers two ways to perform blue/green deployments:</p><p>- In the first approach, AWS CodeDeploy makes a copy of an Auto Scaling group. It, in turn, provisions new Amazon EC2 instances, deploys the application to these new instances, and then redirects traffic to the newly deployed code.</p><p>- In the second approach, you use instance tags or an Auto Scaling group to select the instances that will be used for the green environment. AWS CodeDeploy then deploys the code to the tagged instances.</p><p>In the following figure, the release manager uses the workstation instance to push a new version of the application to AWS CodeDeploy and starts a blue-green deployment. AWS CodeDeploy creates a copy of the Auto Scaling group. It launches two new web server instances just like the original two. AWS CodeDeploy installs the new version of the application and then redirects the load balancer to the new instances.</p><p><img src=\"https://media.tutorialsdojo.com/sap_codebuild_blue_gree_deploy.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_codebuild_blue_gree_deploy.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Add an AWS CodeBuild stage on the deployment pipeline to automatically test on a non-production environment. Leverage change sets on AWS CloudFormation to preview changes before applying to production. Set up a blue/green deployment pattern on AWS CodeDeploy to deploy changes on a separate environment and to quickly rollback if needed.</strong> With AWS CodeBuild on your pipeline, you can add automated tests to verify that the artifact is working as expected. CloudFormation change sets allow you to preview proposed changes on templates before you apply them. AWS CodeDeploy can use a blue/green deployment strategy to have a separate deployment environment and easy rollback procedures.</p><p>The option that says: <strong>Update the CloudFormation templates to include cfn helper scripts. This will detect and report conditions during deployments to ensure that only healthy deployments are continued. Create test plans for the quality assurance team to ensure that changes are tested on a non-production environment before applying to production</strong> is incorrect. This may be possible, however, it will rely on another team to do the testing. It is better to set up AWS CodeBuild to automate this verification which also reduces any human intervention or errors.</p><p>The option that says: <strong>Check the CloudFormation templates for errors with the help of plugins on the integrated development environment (IDE). Ensure that the templates are valid using AWS CLI. Include cfn helper scripts on the deployment code to detect and report for errors. Deploy on a non-production environment and perform manual testing before applying changes to production</strong> is incorrect. Using plugins on IDEs only detects syntax errors on your CloudFormation codes. It won't prevent the user from creating logical errors on the template that may have drastic effects on the current environment.</p><p>The option that says: <strong>Set up a blue/green deployment pattern on AWS CodeDeploy using CloudFormation to update the user data deployment scripts. Manually login to the instances and perform tests to verify that the deployment is successful and the application is running as expected</strong> is incorrect. This is possible but not recommended since manual intervention is prone to human error. You should create automated testing instead to verify the application for every deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/performing-bluegreen-deployments-with-aws-codedeploy-and-auto-scaling-groups/\">https://aws.amazon.com/blogs/devops/performing-bluegreen-deployments-with-aws-codedeploy-and-auto-scaling-groups/</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p><p><a href=\"https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/\">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p><p><br></p><p><strong>Check out these AWS CodeBuild, AWS CodeDeploy, and AWS CloudFormation Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codebuild/?src=udemy\">https://tutorialsdojo.com/aws-codebuild/</a></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/devops/performing-bluegreen-deployments-with-aws-codedeploy-and-auto-scaling-groups/",
      "https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html",
      "https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/",
      "https://tutorialsdojo.com/aws-codebuild/?src=udemy",
      "https://tutorialsdojo.com/aws-codedeploy/?src=udemy",
      "https://tutorialsdojo.com/aws-cloudformation/?src=udemy"
    ]
  },
  {
    "id": 68,
    "question": "<p>A company is building an innovative AI-powered traffic monitoring portal and uses AWS to host its cloud infrastructure. For the initial deployment, the application would be used by an entire city. The application should be highly available and fault-tolerant to avoid unnecessary downtime.</p><p>Which of the following options is the MOST suitable architecture that you should implement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch an Auto Scaling group of EC2 instances on two Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use a MySQL RDS instance with Multi-AZ deployments configuration. Use Route 53 and create an A record that points to the ELB.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use ElastiCache for the database caching of the portal. Launch an Auto Scaling group of EC2 instances on four Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use a MySQL RDS instance with Multi-AZ deployments configuration and Read Replicas. Use Route 53 and create a CNAME that points to the ELB.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use DynamoDB as the database of the portal. Launch an Auto Scaling group of EC2 instances on four Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use Route 53 and create an A record that points to the ELB.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Launch an Auto Scaling group of EC2 instances on three Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use Amazon Aurora with Aurora Replicas as the database tier. Use Route 53 and create an Alias record that points to the ELB.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>Suppose that you start out running your app or website on a single EC2 instance, and over time, traffic increases to the point that you require more than one instance to meet the demand. You can launch multiple EC2 instances from your AMI and then use Elastic Load Balancing to distribute incoming traffic for your application across these EC2 instances. This increases the availability of your application. Placing your instances in multiple Availability Zones also improves the fault tolerance in your application. If one Availability Zone experiences an outage, traffic is routed to the other Availability Zone.</p><p><img src=\"https://media.tutorialsdojo.com/sap_alb_cross_zone.PNG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_alb_cross_zone.PNG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>You can use<strong> Amazon EC2 Auto Scaling</strong> to maintain a minimum number of running instances for your application at all times. Amazon EC2 Auto Scaling can detect when your instance or application is unhealthy and replace it automatically to maintain the availability of your application. You can also use Amazon EC2 Auto Scaling to scale your Amazon EC2 capacity up or down automatically based on demand, using criteria that you specify.</p><p>In this scenario, all of the options are highly available architectures. The main difference here is how they use Route 53.</p><p>The correct answer is the option that says: <strong>Launch an Auto Scaling group of EC2 instances on three Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use Amazon Aurora with Aurora Replicas as the database tier. Use Route 53 and create an Alias record that points to the ELB. </strong>It uses the correct type of record in Route 53, which is an alias record, to point to the ELB.</p><p>The option that says: <strong>Launch an Auto Scaling group of EC2 instances on two Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use a MySQL RDS instance with Multi-AZ deployments configuration. Use Route 53 and create an A record that points to the ELB</strong> is incorrect. You need to create an Alias record with the DNS name and not a normal A-record that points to an IP address.</p><p>The option that says: <strong>Use DynamoDB as the database of the portal. Launch an Auto Scaling group of EC2 instances on four Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use Route 53 and create an A record that points to the ELB </strong>is incorrect. To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias resource record set that points to your load balancer.</p><p>The option that says: <strong>Use ElastiCache for the database caching of the portal. Launch an Auto Scaling group of EC2 instances on four Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use a MySQL RDS instance with Multi-AZ deployments configuration and Read Replicas. Use Route 53 and create a CNAME that point to the ELB</strong> is incorrect. You have to use an Alias record to route to the ELB and not a CNAME record.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html",
      "https://tutorialsdojo.com/amazon-route-53/?src=udemy",
      "https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy"
    ]
  },
  {
    "id": 69,
    "question": "<p>A multinational corporation has recently acquired a smaller company. The solutions architect was instructed to consolidate the multiple AWS accounts of both entities using AWS Organizations. The solutions architect has set up the required service control policies (SCPs) to simplify the process of controlling access permissions for each individual account and Organizational Units (OUs). However, one account is having trouble creating a new S3 bucket, and it is required to investigate the cause of this issue. The account has the following SCP attached:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"Version\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"2012-10-17\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Statement\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"pln\">  </span><span class=\"pun\">{</span></li><li class=\"L4\"><span class=\"pln\">    </span><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"pln\">    </span><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"cloudtrail:*\"</span><span class=\"pun\">,</span></li><li class=\"L6\"><span class=\"pln\">    </span><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"*\"</span></li><li class=\"L7\"><span class=\"pln\">  </span><span class=\"pun\">},</span></li><li class=\"L8\"><span class=\"pln\">  </span><span class=\"pun\">{</span></li><li class=\"L9\"><span class=\"pln\">    </span><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L0\"><span class=\"pln\">    </span><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"iam:*\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"pln\">    </span><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"*\"</span></li><li class=\"L2\"><span class=\"pln\">  </span><span class=\"pun\">}</span></li><li class=\"L3\"><span class=\"pln\"> </span><span class=\"pun\">]</span></li><li class=\"L4\"><span class=\"pun\">}</span></li></ol></pre></div></div><p><br></p><p>Each IAM user of the account has the following IAM policy attached:</p><p><br></p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"Version\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"2012-10-17\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Statement\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"pln\">  </span><span class=\"pun\">{</span></li><li class=\"L4\"><span class=\"pln\">    </span><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"pln\">    </span><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"s3:*\"</span><span class=\"pun\">,</span></li><li class=\"L6\"><span class=\"pln\">    </span><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L7\"><span class=\"pln\">     </span><span class=\"str\">\"arn:aws:s3:::*\"</span></li><li class=\"L8\"><span class=\"pun\">]</span></li><li class=\"L9\"><span class=\"pun\">},</span></li><li class=\"L0\"><span class=\"pln\">  </span><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"pln\">    </span><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Deny\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"pln\">    </span><span class=\"str\">\"NotAction\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"s3:*\"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"pln\">    </span><span class=\"str\">\"NotResource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L4\"><span class=\"pln\">     </span><span class=\"str\">\"arn:aws:s3:::*\"</span></li><li class=\"L5\"><span class=\"pln\">   </span><span class=\"pun\">]</span></li><li class=\"L6\"><span class=\"pln\">  </span><span class=\"pun\">}</span></li><li class=\"L7\"><span class=\"pln\"> </span><span class=\"pun\">]</span></li><li class=\"L8\"><span class=\"pun\">}</span></li></ol></pre></div></div><p><br></p><p>Based on the provided SCP and IAM policy, which of the following options could be the possible root cause of this problem?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The SCP is the root cause since it does not explicitly allow the required action that would enable the account to create an S3 bucket.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Both the IAM policy and the SCP are the problem. The SCP should explicitly allow S3 bucket creation in its policy and the IAM policy should exactly match the permissions of the SCP.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The IAM policy is the root cause because you have denied user permissions to execute any S3-related actions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The SCP is the root cause because it does not support whitelisting actions of the AWS resources.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p>A <strong>service control policy (SCP)</strong> is a policy that specifies the services and actions that users and roles can use in the specified AWS accounts. SCPs are similar to IAM permission policies except that they don't grant any permissions. Instead, SCPs specify the maximum permissions for an organization, organizational unit (OU), or account. When you attach an SCP to your organization root or an OU, the SCP limits permissions for entities in member accounts. Even if a user is granted full administrator permissions with an IAM permission policy, any access that is not explicitly allowed or that is explicitly denied by the SCPs affecting that account is blocked.</p><p>For example, if you assign an SCP that allows only database service access to your \"database\" account, then any user, group, or role in that account is denied access to any other service's operations. SCPs are available only when you enable all features in your organization.</p><p><img src=\"https://media.tutorialsdojo.com/sap_scp_organization_s3.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_scp_organization_s3.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>By default, an SCP named <strong>FullAWSAccess</strong> is attached to every root, OU, and account. This default SCP allows all actions and all services. So in a new organization, until you start creating or manipulating the SCPs, all of your existing IAM permissions continue to operate as they did. As soon as you apply a new or modified SCP to a root or OU that contains an account, the permissions that your users have in that account become filtered by the SCP. Permissions that used to work might now be denied if they're not allowed by the SCP at every level of the hierarchy down to the specified account.</p><p>Therefore, the correct answer is: <strong>The SCP is the root cause since it does not explicitly allow the required action that would enable the account to create an S3 bucket.</strong> The default service policy was changed which means that you would need to explicitly allow your account access to S3 to be able to create buckets. By removing the default FullAWSAccess SCP, all actions for all services are now implicitly denied. To use SCPs as a whitelist, you must replace the AWS-managed <strong>FullAWSAccess</strong> SCP with an SCP that explicitly permits only those services and actions that you want to allow. Your custom SCP then overrides the implicit Deny with an explicit Allow for only those actions that you want to permit.</p><p>The option that says: <strong>The SCP is the root cause because it does not support whitelisting actions of the AWS resources</strong> is incorrect. The SCP format is correct, and it definitely does support the whitelisting feature.</p><p>The option that says: <strong>The IAM policy is the root cause because you have denied user permissions to execute any S3-related actions</strong> is incorrect. The IAM policy allows the user to perform all actions under Amazon S3. The included Deny policy only affects actions on AWS resources that are not of S3, and therefore, should not hinder you from creating S3 buckets. Take note that the <code>NotAction/NotResource</code> is an advanced policy element that explicitly matches everything except the list of action/resources that you specified.</p><p>The option that says:<strong> Both the IAM policy and the SCP are the problem. The SCP should explicitly allow S3 bucket creation in its policy and the IAM policy should exactly match the permissions of the SCP</strong> is incorrect. The IAM policy does not necessarily need to match the service control policy. Although it is true that you would have to explicitly allow S3 bucket creation on the SCP, an SCP does not grant any permissions. Users and roles must still be granted permissions with appropriate IAM permission policies. A user without any IAM permission policies has no access at all, even if the applicable SCPs allow all services and all actions.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_deny-except-bucket.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_deny-except-bucket.html</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_deny-except-bucket.html",
      "https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy"
    ]
  },
  {
    "id": 70,
    "question": "<p>A supermarket chain has a team that handles branded credit card transactions from major card schemes such as Mastercard, Visa, Discover, and AMEX. The company requested an external auditor to audit its AWS environment as part of the Payment Card Industry Data Security Standard (PCI DSS) security compliance. The auditor, operating from their own AWS account, has requested read-only access to the AWS resources across all the company's accounts in order to conduct the necessary checks.</p><p>Which of the following options is the recommended action to give the auditor the required access?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a new IAM User which has an access key ID and a secret access key for API calls that can be used by the auditor. Attach a read-only permissions to the IAM user.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an IAM role in each AWS account that requires auditing, with a trust policy that lists the auditor's ARN as a principal. Assign this role read-only permissions to access necessary resources.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Give the auditor each of your AWS users' username and password in your VPC and let the auditor use those credentials to login to a specific account and conduct the audit.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Active Directory account for the auditor and use identity federation for SSO to let the auditor log in to your AWS environment and conduct the audit.",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>In this scenario, it is recommended that you create an IAM Role that contains the required permissions needed by the auditor. This specific role can be revoked from the user once the compliance activities end.</p><p>An <strong>IAM role</strong> is similar to a user, in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have standard long-term credentials (password or access keys) associated with it. Instead, if a user assumes a role, temporary security credentials are created dynamically and provided to the user.</p><p><img src=\"https://media.tutorialsdojo.com/sap_iamRole.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_iamRole.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is:<strong> Create an IAM role in each AWS account that requires auditing, with a trust policy that lists the auditor's ARN as a principal. Assign this role read-only permissions to access necessary resources.</strong></p><p>The options that says: <strong>Give the auditor each of your AWS users' username and password in your VPC and let the auditor use those credentials to login to a specific account and conduct the audit</strong> is incorrect. Providing the username and password is in itself a major security breach that will result in compliance failure.</p><p>The options that says: <strong>Create an Active Directory account for the auditor and use identity federation for SSO to let the auditor log in to your AWS environment and conduct the audit</strong> is incorrect. While using Active Directory and SSO is a secure approach, this method is typically used for integrating an organization's internal user management system with AWS, which is not necessary for an external auditor.</p><p>The options that says: <strong>Create a new IAM User which has an access key ID and a secret access key for API calls that can be used by the auditor. Attach a read-only permissions to the IAM user</strong> is incorrect. Creating a new IAM user for the auditor with read-only permissions is a valid approach, but it's not as secure and manageable as using roles. IAM users with long-term credentials can pose a security risk if those credentials are compromised.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html",
      "https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy"
    ]
  },
  {
    "id": 71,
    "question": "<p>A company stores confidential files on an Amazon S3 bucket. There was a recent production incident in the company in which the files that are stored in an S3 bucket were accidentally made public. This has caused data leakage that affected the company revenue. The management has instructed the solutions architect to come up with a solution to safeguard the S3 bucket. The solution should only allow private files to be uploaded to the S3 bucket and no file should have a public read or public write access.</p><p>Which of the following options should the solutions architect implement to meet the above requirements with MINIMAL effort?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up AWS Organizations and create a new Service Control Policy (SCP) that will deny public objects from being uploaded to the Amazon S3 bucket. Attach the SCP to the AWS account.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up a policy that restricts all <code>s3:PutObject</code> actions of the user to have a <code>private</code> canned ACL only which prohibits any public access to the uploaded objects.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the <code>s3-bucket-public-read-prohibited</code> and <code>s3-bucket-public-write-prohibited</code> managed rules in AWS Config to restrict all users from uploading publicly accessible and writable files to the S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable Amazon S3 Block Public Access in the S3 bucket.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p>Amazon S3 provides Block Public Access settings for buckets and accounts to help you manage public access to Amazon S3 resources. By default, new buckets and objects don't allow public access, but users can modify bucket policies or object permissions to allow public access. Amazon S3 Block Public Access provides settings that override these policies and permissions so that you can limit public access to these resources.</p><p>With Amazon S3 Block Public Access, account administrators and bucket owners can easily set up centralized controls to limit public access to their Amazon S3 resources that are enforced regardless of how the resources are created.</p><p><img src=\"https://media.tutorialsdojo.com/sap_s3_disable_public_access.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_s3_disable_public_access.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Enable Amazon S3 Block Public Access in the S3 bucket.</strong> It provides a way to meet the requirements with minimal effort.</p><p>When Amazon S3 receives a request to access a bucket or an object, it determines whether the bucket or the bucket owner's account has a Block Public Access setting. If there is an existing Block Public Access setting that prohibits the requested access, then Amazon S3 rejects the request. Amazon S3 Block Public Access provides four settings. These settings are independent and can be used in any combination, and each setting can be applied to a bucket or to an entire AWS account.</p><p>If a bucket has Block Public Access settings that are different from its owner's account, Amazon S3 applies the most restrictive combination of the bucket-level and account-level settings. Thus, when Amazon S3 evaluates whether an operation is prohibited by a Block Public Access setting, it rejects any request that would violate either a bucket-level or an account-level setting.</p><p>The option that says: <strong>Set up a policy that restricts all </strong><code><strong>s3:PutObject</strong></code><strong> actions of the user to have a </strong><code><strong>private</strong></code><strong> canned ACL only which prohibits any public access to the uploaded objects</strong> is incorrect. Although this solution is possible, it entails a lot of effort to set up an IAM policy that restricts the user from uploading public objects. Using the Amazon Block Public Access is a more suitable solution for this scenario.</p><p>The option that says: <strong>Use the </strong><code><strong>s3-bucket-public-read-prohibited</strong></code><strong> and </strong><code><strong>s3-bucket-public-write-prohibited</strong></code><strong> managed rules in AWS Config to restrict all users from uploading publicly accessible and writable files to the S3 bucket</strong> is incorrect. This solution with AWS Config will only notify you and your team of public objects in the S3 bucket. It would not be able to restrict any user from uploading public objects.</p><p>The option that says: <strong>Set up AWS Organizations and create a new Service Control Policy (SCP) that will deny public objects from being uploaded to the Amazon S3 bucket, then attaching the SCP to the AWS account</strong> is incorrect. Although you can satisfy the requirement using a service control policy (SCP), it still entails a lot of effort to implement. Remember that the scenario asks you to meet the requirements with minimal effort. Enabling the Amazon S3 Block Public Access in the S3 bucket is still the easiest one to implement. An SCP is primarily used to determine what services and actions can be delegated by administrators to the users and roles in the accounts that the SCP is applied to.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html",
      "https://tutorialsdojo.com/amazon-s3/?src=udemy"
    ]
  },
  {
    "id": 72,
    "question": "<p>A world-renowned logistics company runs its global enterprise e-commerce platform on the AWS cloud. The company has built a multi-tier web application running in a VPC that uses an Elastic Load Balancer in front of both the web tier and the app tier, with static assets served directly from an Amazon S3 bucket. It uses a combination of Amazon RDS and DynamoDB for the dynamic data and then archiving nightly into an Amazon S3 bucket for further processing with Amazon Elastic MapReduce. After a routine audit, the company found questionable log entries and suspected that someone is attempting to gain unauthorized access to the system. The solutions architect has been tasked to improve the security of the architecture from DDoS, SQL injection, and HTTP flood attacks as well as from bad bots (content scrapers).</p><p>Which of the following approach provides the MOST suitable and scalable solution to protect the infrastructure from these kinds of security attacks?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an identical application stack that acts as a standby environment in another AWS region by using an AWS CloudFormation template. Use AWS CloudFormation StackSets to deploy the new stack and configure the security groups as well as network ACLs of the EC2 instances. Use Amazon Macie to protect the data stored in the Amazon S3 bucket. Create a Route 53 failover routing policy and configure an active-passive failover.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Insert the identified suspect's source IP as an explicit inbound deny to the network ACL rules of the web tier's subnet. Set up AWS Config to periodically audit the network ACLs and ensure that the blacklisted IP addresses are always in place.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Establish an AWS Direct Connect (DX) connection to the VPC through a Direct Connect partner. Configure Internet connectivity to filter the traffic in hardware Web Application Firewall (WAF) and then reroute the traffic through the DX connection into the application. Use the company's wide area network (WAN) to send traffic over the DX connection.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up AWS WAF and AWS Shield Advanced on all web endpoints. Launch AWS WAF rules against SQL injection and other common web exploits.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>AWS Shield</strong> is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection.</p><p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. AWS WAF gives you control over which traffic to allow or block to your web applications by defining customizable web security rules. You can use AWS WAF to create custom rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that are designed for your specific application. New rules can be deployed within minutes, letting you respond quickly to changing traffic patterns. Also, AWS WAF includes a full-featured API that you can use to automate the creation, deployment, and maintenance of web security rules.</p><p><img src=\"https://media.tutorialsdojo.com/sap_waf_shield_cloudfront.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_waf_shield_cloudfront.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Hence, the correct answer is: <strong>Set up AWS WAF and AWS Shield Advanced on all web endpoints. Launch AWS WAF rules against SQL injection and other common web exploits.</strong></p><p>The option that says:<strong> Create an identical application stack that acts as a standby environment in another AWS region by using an AWS CloudFormation template. Use AWS CloudFormation StackSets to deploy the new stack and configure the security groups as well as network ACLs of the EC2 instances. Use Amazon Macie to protect the data stored in the Amazon S3 bucket. Create a Route 53 failover routing policy and configure an active-passive failover</strong> is incorrect because this solution doesn't provide the necessary security protection against DDoS and other web vulnerability attacks. It only provides a disaster recovery plan in the event that your primary environment goes down. You have to set up AWS WAF and AWS Shield Advanced instead.</p><p>The option that says:<strong> Insert the identified suspect's source IP as an explicit inbound deny to the network ACL rules of the web tier's subnet. Set up AWS Config to periodically audit the network ACLs and ensure that the blacklisted IP addresses are always in place</strong> is incorrect. Even though blocking certain IPs will mitigate the risk, the attacker could maneuver the IP address and circumvent the IP check by NACL, and it does not prevent attacks from new sources of threat.</p><p>The option that says: <strong>Establish an AWS Direct Connect (DX) connection to the VPC through a Direct Connect partner. Configure Internet connectivity to filter the traffic in hardware Web Application Firewall (WAF) and then reroute the traffic through the DX connection into the application. Use the company's wide area network (WAN) to send traffic over the DX connection </strong>is incorrect. Although this option could work, the setup is very complex and it is not a cost-effective solution. Using the AWS Shield Advanced and AWS WAF combination is still the better solution.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p><p><a href=\"https://www.slideshare.net/AmazonWebServices/aws-august-webinar-series-ddos-resiliency\">https://www.slideshare.net/AmazonWebServices/aws-august-webinar-series-ddos-resiliency</a></p><p><br></p><p><strong>Check out these AWS WAF and Shield Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p><p><a href=\"https://tutorialsdojo.com/aws-shield/?src=udemy\">https://tutorialsdojo.com/shield/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/waf/",
      "https://www.slideshare.net/AmazonWebServices/aws-august-webinar-series-ddos-resiliency",
      "https://tutorialsdojo.com/aws-waf/?src=udemy",
      "https://tutorialsdojo.com/aws-shield/?src=udemy"
    ]
  },
  {
    "id": 73,
    "question": "<p>A company has recently migrated its core application to the AWS Cloud. The application allows users to upload scanned forms through a web application hosted on a fleet of Amazon EC2 instances. The application connects to a backend database hosted on Amazon RDS for PostgreSQL. The user metadata are stored on the database while the scanned forms are stored on an Amazon S3 bucket.</p><p>For each uploaded form, the application sends a notification to an Amazon SNS topic to which the team members are subscribed. Then, one of the team members will log in, validate the forms, and manually extracts relevant data from the scanned forms. This information is then submitted to another system using an API. The management wants to improve this process by automation to reduce human effort, increase efficiency and maintain high accuracy.</p><p>Which of the following options is the recommended solution to meet the company's requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add another tier to the application by using AWS Step Functions and AWS Lambda to facilitate the different stages of processing. Implement an artificial intelligence and machine learning (AL/ML) service using Amazon Rekognition and Amazon Transcribe to parse information from the scanned forms. Store the output in Amazon DocumentDB. Update the application to parse data from the DocumentDB table and send it to the other system via API call.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add another tier to the application by deploying an artificial intelligence and machine learning (AL/ML) model trained using Amazon SageMaker service. Use this model to perform optical character recognition (OCR) on the uploaded forms. Store the output in another Amazon S3 bucket. Update the application to parse data from the Amazon S3 bucket and send it to the other system via API call.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add another tier to the application by using AWS Step Functions and AWS Lambda to facilitate the different stages of processing. Use a combination of Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) and parse data from the scanned forms. Store the output in another Amazon S3 bucket. Update the application to parse data from the Amazon S3 bucket and send it to the other system via API call.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Add another tier to the application by deploying an open-source optical character recognition (OCR) software on Amazon Elastic Kubernetes (Amazon EKS) to process the uploaded forms. Store the output in another Amazon S3 bucket. Parse the output and send it to an Amazon DynamoDB table. Update the application to get data from the DynamoDB table and send it to the other system via API call.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Continuous Improvement for Existing Solutions",
    "explanation": "<p><strong>Amazon Textract</strong> is a fully managed machine learning service that automatically extracts printed text, handwriting, and other data from scanned documents that goes beyond simple optical character recognition (OCR) to identify, understand, and extract data from forms and tables.</p><p>Amazon Textract helps you add document text detection and analysis to your applications. Using Amazon Textract, you can do the following:</p><p>-Detect typed and handwritten text in a variety of documents, including financial reports, medical records, and tax forms.</p><p>-Extract text, forms, and tables from documents with structured data using the Amazon Textract Document Analysis API.</p><p>-Specify and extract information from documents using the Queries feature within the Amazon Textract Analyze Document API.</p><p>-Process invoices and receipts with the AnalyzeExpense API.</p><p>-Process ID documents, such as driver's licenses and passports issued by the U.S. government, using the AnalyzeID API.</p><p>-Upload and process mortgage loan packages through automatic routing of the document pages to the appropriate Amazon Textract analysis operations using the Analyze Lending workflow.</p><p><strong>Amazon Comprehend</strong> uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents.</p><p>Amazon Comprehend uses machine learning to help you uncover the insights and relationships in your unstructured data. The service identifies the language of the text; extracts key phrases, places, people, brands, or events; understands how positive or negative the text is; analyzes text using tokenization and parts of speech; and automatically organizes a collection of text files by topic.</p><p><img src=\"https://media.tutorialsdojo.com/sap_textract_comprehend.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_textract_comprehend.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is: <strong>Add another tier to the application by using AWS Step Functions and AWS Lambda to facilitate the different stages of processing. Use a combination of Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) and parse data from the scanned forms. Store the output in another Amazon S3 bucket. Update the application to parse data from the Amazon S3 bucket and send it to the other system via API call.</strong> Amazon Textract uses machine learning service that automatically extracts printed text, handwriting, and other data from scanned documents. Amazon Comprehend uses machine learning to find insights and relationships in text.</p><p>The option that says: <strong>Add another tier to the application by deploying an open-source optical character recognition (OCR) software on Amazon Elastic Kubernetes (Amazon EKS) to process the uploaded forms. Store the output in another Amazon S3 bucket. Parse the output and send it to an Amazon DynamoDB table. Update the application to get data from the DynamoDB table and send it to the other system via API call </strong>is incorrect. This may be possible but is not recommended as it will add operation overhead. The Amazon Textract and Comprehend services are better suited for this scenario.</p><p>The option that says: <strong>Add another tier to the application by using AWS Step Functions and AWS Lambda to facilitate the different stages of processing. Implement an artificial intelligence and machine learning (AL/ML) service using Amazon Rekognition and Amazon Transcribe to parse information from the scanned forms. Store the output in Amazon DocumentDB. Update the application to parse data from the DocumentDB table and send it to the other system via API call </strong>is incorrect. Amazon Rekognition is used for computer vision to extract information from images and videos, not for scanned text forms. Amazon Transcribe is used for automatic speech recognition or adding speech-to-text capabilities to applications.</p><p>The option that says: <strong>Add another tier to the application by deploying an artificial intelligence and machine learning (AL/ML) model trained using Amazon SageMaker service. Use this model to perform optical character recognition (OCR) on the uploaded forms. Store the output in another Amazon S3 bucket. Update the application to parse data from the Amazon S3 bucket and send it to the other system via API call</strong> is incorrect. This may be possible, however, this will require you to create and train a model for performing OCR. AWS already offers Textract and Comprehend services which are better suited for this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/textract/latest/dg/what-is.html\">https://docs.aws.amazon.com/textract/latest/dg/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/comprehend/latest/dg/what-is.html\">https://docs.aws.amazon.com/comprehend/latest/dg/what-is.html</a></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/building-an-nlp-powered-search-index-with-amazon-textract-and-amazon-comprehend/\">https://aws.amazon.com/blogs/machine-learning/building-an-nlp-powered-search-index-with-amazon-textract-and-amazon-comprehend/</a></p><p><br></p><p><strong>Check out these AWS Textract and Amazon Comprehend Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-textract/?src=udemy\">https://tutorialsdojo.com/amazon-textract/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-comprehend/?src=udemy\">https://tutorialsdojo.com/amazon-comprehend/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/textract/latest/dg/what-is.html",
      "https://docs.aws.amazon.com/comprehend/latest/dg/what-is.html",
      "https://aws.amazon.com/blogs/machine-learning/building-an-nlp-powered-search-index-with-amazon-textract-and-amazon-comprehend/",
      "https://tutorialsdojo.com/amazon-textract/?src=udemy",
      "https://tutorialsdojo.com/amazon-comprehend/?src=udemy"
    ]
  },
  {
    "id": 74,
    "question": "<p>A company has recently finished developing a web application that will soon be put into production. Before it is transferred into the production environment, a final test run must be conducted. Only the employees can access the web app - either from the corporate network or from the Internet. The manager instructed the solutions architect to ensure that the EC2 instance hosting the application server will not be exposed to the Internet.</p><p>Which of the following options is the recommended implementation to fulfill the company requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>1. Configure SSL VPN on the public subnet of your VPC. </p><p>2. Install an SSL VPN client software on all employee workstations. </p><p>3. Create a private subnet in your VPC and place your application servers in it. </p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>1. Launch an Elastic Load Balancer for your EC2 instances that terminates SSL to them. </p><p>2. Create a public subnet in your VPC and launch your application servers in it.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>1. Use AWS Direct Connect to hook up your employee workstations to the VPC via a private interface. </p><p>2. Create a public subnet and place your application servers in it.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>1. Use IPsec VPN that would allow your employees to access the network of your application servers.&nbsp;</p><p>2. Create a public subnet in your VPC and launch your application servers in it.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design for New Solutions",
    "explanation": "<p>In this scenario, you have a web application which is still under development but you want to enable access to it only for the employees via public Internet. In this scenario, you can implement an SSL VPN solution in which the employees can connect first and once they are authenticated, they will be granted access to the online portal. In this way, you can launch the web servers in the private subnet and still access it over the Internet via the VPN.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_vpn.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_aws_vpn.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the correct answer is:</p><p><strong>1. Configure SSL VPN on the public subnet of your VPC.</strong></p><p><strong>2. Install an SSL VPN client software on all employee workstations.</strong></p><p><strong>3. Create a private subnet in your VPC and place your application servers in it.</strong></p><p>The following option is incorrect. Even though an IPSec VPN may work, your application servers are still exposed since you launched them in a public subnet:</p><p><strong>1. Use IPsec VPN that would allow your employees to access the network of your application servers.</strong></p><p><strong>2. Create a public subnet in your VPC and launch your application servers in it.</strong></p><p>The following option is incorrect because you don't need to set up a DirectConnect connection in order to meet the requirement. Additionally, it is costly to maintain this connection considering that it is not required to have a high bandwidth connection between the customer and AWS:</p><p><strong>1. Use AWS Direct Connect to hook up your employee workstations to the VPC via a private interface.</strong></p><p><strong>2. Create a public subnet and place your application servers in it.</strong></p><p>The following option is incorrect because terminating the SSL on the EC2 instance does not meet the requirement.</p><p><strong>1. Launch an Elastic Load Balancer for your EC2 instances that terminates SSL to them.</strong></p><p><strong>2. Create a public subnet in your VPC and launch your application servers in it.</strong></p><p>The application servers are still exposed as it is deployed to a public subnet.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html</a></p><p><a href=\"https://aws.amazon.com/marketplace/pp/B00MI40CAE?qid=1532168886060&amp;sr=0-1&amp;ref_=srh_res_product_title\">https://aws.amazon.com/marketplace/pp/B00MI40CAE?qid=1532168886060&amp;sr=0-1&amp;ref_=srh_res_product_title</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html",
      "https://aws.amazon.com/marketplace/pp/B00MI40CAE?qid=1532168886060&amp;sr=0-1&amp;ref_=srh_res_product_title",
      "https://tutorialsdojo.com/amazon-vpc/?src=udemy"
    ]
  },
  {
    "id": 75,
    "question": "<p>A company uses Amazon WorkSpaces to improve the productivity and security of its remote workers. Hundreds of remote workers log in to the virtual desktop service using the Amazon WorkSpaces client application on a regular basis. Users have reported that they cannot log in to their virtual desktops even though they have the correct credentials.</p><p>Upon investigation, the Solutions Architect discovered that the filesystem storing the user profiles has reached its capacity, which is the reason why users cannot establish a new session in Amazon WorkSpaces. The environment is configured with a 10 TB Amazon FSx for Windows File Server file system to store the user profiles.</p><p>Which of the following options should the Solutions Architect implement to solve the issue and prevent it from happening again?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon CloudWatch Alarm using the <code>FreeStorageCapacity</code> metric to monitor the file system. Once triggered, use AWS Steps Functions as the target. Run the Steps Function to create a new Amazon FSx for Windows File Server file system and migrate the user profiles.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon CloudWatch Alarm to monitor the <code>FreeStorageCapacity</code> metric of the file system. Write an AWS Lambda Function to increase the capacity of the Amazon FSx for Windows File Server file system using the update-file-system command. Utilize Amazon EventBridge to invoke this Lambda function when the metric threshold is reached.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>From the Amazon FSx console, select the desired file system to edit its attributes. Enable the option Dynamically Allocate to allow the file system to scale depending on the size of the data stored. This will present a large capacity drive to Amazon WorkSpaces clients and will grow automatically as users add more data to their profiles.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a new Amazon FSx for Windows File Server file system with a larger capacity. Create a script to copy all user profiles to the new file system. Create an Amazon CloudWatch metric to monitor the FreeStorageCapacity of the filesystem and send a notification via Amazon SNS before it reaches capacity.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain - Design Solutions for Organizational Complexity",
    "explanation": "<p><strong>Amazon FSx for Windows File Server</strong> provides fully managed Microsoft Windows file servers, backed by a fully native Windows file system. With file storage on Amazon FSx, the code, applications, and tools that Windows developers and administrators use today can continue to work unchanged. Windows applications and workloads ideal for Amazon FSx include business applications, home directories, web serving, content management, data analytics, software build setups, and media processing workloads.</p><p>As you need additional storage, you can increase the storage capacity that is configured on your FSx for Windows File Server file system. You can do so using the Amazon FSx console, the Amazon FSx API, or the AWS Command Line Interface (AWS CLI).</p><p>You can only increase the amount of storage capacity for a file system; you cannot decrease storage capacity. When you increase the storage capacity of your Amazon FSx file system, behind the scenes, Amazon FSx adds a new, larger set of disks to your file system. Amazon FSx then runs a storage optimization process in the background to transparently migrate data from the old disks to the new disks.</p><p>The following illustration shows the four main steps of the process that Amazon FSx uses when increasing a file system's storage capacity.</p><p><img src=\"https://media.tutorialsdojo.com/sap_fsx_increase_size.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://media.tutorialsdojo.com/sap_fsx_increase_size.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Hình ảnh lớn hơn\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>EventBridge (CloudWatch Events)</strong> helps you to respond to state changes in your AWS resources. With EventBridge (CloudWatch Events), you can create rules that match selected events in the stream and route them to your AWS Lambda function to take action.</p><p>Using the update-file-system command, you can use AWS SDK or CLI to programmatically increase the size of the FSx file system. You can use Amazon CloudWatch to monitor the metrics of the file system and trigger the Lambda function to perform an action.</p><p>Therefore, the correct answer is: <strong>Create an Amazon CloudWatch Alarm to monitor the FreeStorageCapacity metric of the file system. Write an AWS Lambda Function to increase the capacity of the Amazon FSx for Windows File Server file system using the update-file-system command. Utilize Amazon EventBridge to invoke this Lambda function when the metric threshold is reached.</strong></p><p>The option that says: <strong>Create a new Amazon FSx for Windows File Server file system with a larger capacity. Create a script to copy all user profiles to the new file system. Create an Amazon CloudWatch metric to monitor the FreeStorageCapacity of the filesystem and send a notification via Amazon SNS before it reaches capacity</strong> is incorrect. You don't have to manually copy all user data to a new volume. You can increase the file system and Amazon FSx automatically migrates the data to a larger volume in the background.</p><p>The option that says: <strong>Create an Amazon CloudWatch Alarm using the FreeStorageCapacity metric to monitor the file system. Once triggered, use AWS Steps Functions as the target. Run the Steps Function to create a new Amazon FSx for Windows File Server file system and migrate the user profiles</strong> is incorrect. You don't need to create the Step Function to migrate the user profiles. Amazon FSx automatically migrates the data in the background when you increase the file system size.</p><p>The option that says: <strong>From the Amazon FSx console, select the desired file system to edit its attributes. Enable the option Dynamically Allocate to allow the file system to scale depending on the size of the data stored. This will present a large capacity drive to Amazon WorkSpaces clients and will grow automatically as users add more data to their profiles</strong> is incorrect. There is no option to Dynamically Allocate the file system size. You can manually adjust the file system size using the Amazon FSx console, the Amazon FSx API, or the AWS CLI.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html</a></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html</a></p><p><br></p><p><strong>Check out this Amazon FSx Cheat Sheet and Amazon CloudWatch Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-fsx/?src=udemy\">https://tutorialsdojo.com/amazon-fsx/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html",
      "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html",
      "https://tutorialsdojo.com/amazon-fsx/?src=udemy",
      "https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy"
    ]
  }
]