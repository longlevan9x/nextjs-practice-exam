[
  {
    "id": 1,
    "question": "A company is implementing an application on Amazon EC2 instances. The application needs to process incoming transactions. When the application detects a transaction that is not valid, the application must send a chat message to the company's support team. To send the message, the application needs to retrieve the access token to authenticate by using the chat API.<br>A developer needs to implement a solution to store the access token. The access token must be encrypted at rest and in transit. The access token must also be accessible from other AWS accounts.<br>Which solution will meet these requirements with the LEAST management overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Use an AWS Systems Manager Parameter Store SecureString parameter that uses an AWS Key Management Service (AWS KMS) AWS managed key to store the access token. Add a resource-based policy to the parameter to allow access from other accounts. Update the IAM role of the EC2 instances with permissions to access Parameter Store. Retrieve the token from Parameter Store with the decrypt flag enabled. Use the decrypted access token to send the message to the chat.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Encrypt the access token by using an AWS Key Management Service (AWS KMS) customer managed key. Store the access token in an Amazon DynamoDB table. Update the IAM role of the EC2 instances with permissions to access DynamoDB and AWS KMS. Retrieve the token from DynamoDDecrypt the token by using AWS KMS on the EC2 instances. Use the decrypted access token to send the message to the chat.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS Secrets Manager with an AWS Key Management Service (AWS KMS) customer managed key to store the access token. Add a resource-based policy to the secret to allow access from other accounts. Update the IAM role of the EC2 instances with permissions to access Secrets Manager. Retrieve the token from Secrets Manager. Use the decrypted access token to send the message to the chat.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Encrypt the access token by using an AWS Key Management Service (AWS KMS) AWS managed key. Store the access token in an Amazon S3 bucket. Add a bucket policy to the S3 bucket to allow access from other accounts. Update the IAM role of the EC2 instances with permissions to access Amazon S3 and AWS KMS. Retrieve the token from the S3 bucket. Decrypt the token by using AWS KMS on the EC2 instances. Use the decrypted access token to send the massage to the chat.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 2,
    "question": "A company is running Amazon EC2 instances in multiple AWS accounts. A developer needs to implement an application that collects all the lifecycle events of the EC2 instances. The application needs to store the lifecycle events in a single Amazon Simple Queue Service (Amazon SQS) queue in the company's main AWS account for further processing.<br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure Amazon EC2 to deliver the EC2 instance lifecycle events from all accounts to the Amazon EventBridge event bus of the main account. Add an EventBridge rule to the event bus of the main account that matches all EC2 instance lifecycle events. Add the SQS queue as a target of the rule.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use the resource policies of the SQS queue in the main account to give each account permissions to write to that SQS queue. Add to the Amazon EventBridge event bus of each account an EventBridge rule that matches all EC2 instance lifecycle events. Add the SQS queue in the main account as a target of the rule.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Write an AWS Lambda function that scans through all EC2 instances in the company accounts to detect EC2 instance lifecycle changes. Configure the Lambda function to write a notification message to the SQS queue in the main account if the function detects an EC2 instance lifecycle change. Add an Amazon EventBridge scheduled rule that invokes the Lambda function every minute.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure the permissions on the main account event bus to receive events from all accounts. Create an Amazon EventBridge rule in each account to send all the EC2 instance lifecycle events to the main account event bus. Add an EventBridge rule to the main account event bus that matches all EC2 instance lifecycle events. Set the SQS queue as a target for the rule.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 3,
    "question": "An application is using Amazon Cognito user pools and identity pools for secure access. A developer wants to integrate the user-specific file upload and download features in the application with Amazon S3. The developer must ensure that the files are saved and retrieved in a secure manner and that users can access only their own files. The file sizes range from 3 KB to 300 MB.<br>Which option will meet these requirements with the HIGHEST level of security?",
    "answers": [
      {
        "id": 1,
        "answer": "Use S3 Event Notifications to validate the file upload and download requests and update the user interface (UI).",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Save the details of the uploaded files in a separate Amazon DynamoDB table. Filter the list of files in the user interface (UI) by comparing the current user ID with the user ID associated with the file in the table.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon API Gateway and an AWS Lambda function to upload and download files. Validate each request in the Lambda function before performing the requested operation.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use an IAM policy within the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 4,
    "question": "A company is building a scalable data management solution by using AWS services to improve the speed and agility of development. The solution will ingest large volumes of data from various sources and will process this data through multiple business rules and transformations.<br>The solution requires business rules to run in sequence and to handle reprocessing of data if errors occur when the business rules run. The company needs the solution to be scalable and to require the least possible maintenance.<br>Which AWS service should the company use to manage and automate the orchestration of the data flows to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "AWS Batch",
        "correct": false
      },
      {
        "id": 2,
        "answer": "AWS Step Functions",
        "correct": true
      },
      {
        "id": 3,
        "answer": "AWS Glue",
        "correct": false
      },
      {
        "id": 4,
        "answer": "AWS Lambda",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 5,
    "question": "A developer has created an AWS Lambda function that is written in Python. The Lambda function reads data from objects in Amazon S3 and writes data to an Amazon DynamoDB table. The function is successfully invoked from an S3 event notification when an object is created. However, the function fails when it attempts to write to the DynamoDB table.<br>What is the MOST likely cause of this issue?",
    "answers": [
      {
        "id": 1,
        "answer": "The Lambda function's concurrency limit has been exceeded.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "DynamoDB table requires a global secondary index (GSI) to support writes.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "The Lambda function does not have IAM permissions to write to DynamoDB.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "The DynamoDB table is not running in the same Availability Zone as the Lambda function.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 6,
    "question": "A developer is creating an AWS CloudFormation template to deploy Amazon EC2 instances across multiple AWS accounts. The developer must choose the EC2 instances from a list of approved instance types.<br>How can the developer incorporate the list of approved instance types in the CloudFormation template?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a separate CloudFormation template for each EC2 instance type in the list.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "In the Resources section of the CloudFormation template, create resources for each EC2 instance type in the list.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "In the CloudFormation template, create a separate parameter for each EC2 instance type in the list.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "In the CloudFormation template, create a parameter with the list of EC2 instance types as AllowedValues.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 7,
    "question": "A developer has an application that makes batch requests directly to Amazon DynamoDB by using the BatchGetItem low-level API operation. The responses frequently return values in the UnprocessedKeys element.<br>Which actions should the developer take to increase the resiliency of the application when the batch response includes values in UnprocessedKeys? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Retry the batch operation immediately.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Retry the batch operation with exponential backoff and randomized delay.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Update the application to use an AWS software development kit (AWS SDK) to make the requests.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Increase the provisioned read capacity of the DynamoDB tables that the operation accesses.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Increase the provisioned write capacity of the DynamoDB tables that the operation accesses.",
        "correct": false
      }
    ],
    "corrects": [
      2,
      4
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 8,
    "question": "A company is running a custom application on a set of on-premises Linux servers that are accessed using Amazon API Gateway. AWS X-Ray tracing has been enabled on the API test stage.<br>How can a developer enable X-Ray tracing on the on-premises servers with the LEAST amount of configuration?",
    "answers": [
      {
        "id": 1,
        "answer": "Install and run the X-Ray SDK on the on-premises servers to capture and relay the data to the X-Ray service.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Capture incoming requests on-premises and configure an AWS Lambda function to pull, process, and relay relevant data to X-Ray using the PutTraceSegments API call.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Capture incoming requests on-premises and configure an AWS Lambda function to pull, process, and relay relevant data to X-Ray using the PutTelemetryRecords API call.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 9,
    "question": "A company wants to share information with a third party. The third party has an HTTP API endpoint that the company can use to share the information. The company has the required API key to access the HTTP API.<br>The company needs a way to manage the API key by using code. The integration of the API key with the application code cannot affect application performance.<br>Which solution will meet these requirements MOST securely?",
    "answers": [
      {
        "id": 1,
        "answer": "Store the API credentials in AWS Secrets Manager. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Store the API credentials in a local code variable. Push the code to a secure Git repository. Use the local code variable at runtime to make the API call.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Store the API credentials as an object in a private Amazon S3 bucket. Restrict access to the S3 object by using IAM policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Store the API credentials in an Amazon DynamoDB table. Restrict access to the table by using resource-based policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 10,
    "question": "A developer is deploying a new application to Amazon Elastic Container Service (Amazon ECS). The developer needs to securely store and retrieve different types of variables. These variables include authentication information for a remote API, the URL for the API, and credentials. The authentication information and API URL must be available to all current and future deployed versions of the application across development, testing, and production environments.<br>How should the developer retrieve the variables with the FEWEST application changes?",
    "answers": [
      {
        "id": 1,
        "answer": "Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Update the application to retrieve the variables from AWS Key Management Service (AWS KMS). Store the API URL and credentials as unique keys for each environment.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Update the application to retrieve the variables from an encrypted file that is stored with the application. Store the API URL and credentials in unique files for each environment.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Update the application to retrieve the variables from each of the deployed environments. Define the authentication information and API URL in the ECS task definition as unique names during the deployment process.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 11,
    "question": "A company is migrating legacy internal applications to AWS. Leadership wants to rewrite the internal employee directory to use native AWS services. A developer needs to create a solution for storing employee contact details and high-resolution photos for use with the new application.<br>Which solution will enable the search and retrieval of each employee's individual details and high-resolution photos using AWS APIs?",
    "answers": [
      {
        "id": 1,
        "answer": "Encode each employee's contact information and photos using Base64. Store the information in an Amazon DynamoDB table using a sort key.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Store each employee's contact information in an Amazon DynamoDB table along with the object keys for the photos stored in Amazon S3.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Amazon Cognito user pools to implement the employee directory in a fully managed software-as-a-service (SaaS) method.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Store employee contact information in an Amazon RDS DB instance with the photos stored in Amazon Elastic File System (Amazon EFS).",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 12,
    "question": "A developer is creating an application that will give users the ability to store photos from their cellphones in the cloud. The application needs to support tens of thousands of users. The application uses an Amazon API Gateway REST API that is integrated with AWS Lambda functions to process the photos. The application stores details about the photos in Amazon DynamoDB.<br>Users need to create an account to access the application. In the application, users must be able to upload photos and retrieve previously uploaded photos. The photos will range in size from 300 KB to 5 MB.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Cognito user pools to manage user accounts. Create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Use the Lambda function to store the photos and details in the DynamoDB table. Retrieve previously uploaded photos directly from the DynamoDB table.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon Cognito user pools to manage user accounts. Create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an IAM user for each user of the application during the sign-up process. Use IAM authentication to access the API Gateway API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a users table in DynamoDB. Use the table to manage user accounts. Create a Lambda authorizer that validates user credentials against the users table. Integrate the Lambda authorizer with API Gateway to control access to the API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as par of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 13,
    "question": "A company receives food orders from multiple partners. The company has a microservices application that uses Amazon API Gateway APIs with AWS Lambda integration. Each partner sends orders by calling a customized API that is exposed through API Gateway. The API call invokes a shared Lambda function to process the orders.<br>Partners need to be notified after the Lambda function processes the orders. Each partner must receive updates for only the partner's own orders. The company wants to add new partners in the future with the fewest code changes possible.<br>Which solution will meet these requirements in the MOST scalable way?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a different Amazon Simple Notification Service (Amazon SNS) topic for each partner. Configure the Lambda function to publish messages for each partner to the partner's SNS topic.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a different Lambda function for each partner. Configure the Lambda function to notify each partner's service endpoint directly.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure the Lambda function to publish messages with specific attributes to the SNS topic. Subscribe each partner to the SNS topic. Apply the appropriate filter policy to the topic subscriptions.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create one Amazon Simple Notification Service (Amazon SNS) topic. Subscribe all partners to the SNS topic.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 14,
    "question": "A financial company must store original customer records for 10 years for legal reasons. A complete record contains personally identifiable information (PII). According to local regulations, PII is available to only certain people in the company and must not be shared with third parties. The company needs to make the records available to third-party organizations for statistical analysis without sharing the PII.<br>A developer wants to store the original immutable record in Amazon S3. Depending on who accesses the S3 document, the document should be returned as is or with all the PII removed. The developer has written an AWS Lambda function to remove the PII from the document. The function is named removePii.<br>What should the developer do so that the company can meet the PII requirements while maintaining only one copy of the document?",
    "answers": [
      {
        "id": 1,
        "answer": "Set up an S3 event notification that invokes the removePii function when an S3 GET request is made. Call Amazon S3 by using a GET request to access the object without PII.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up an S3 event notification that invokes the removePii function when an S3 PUT request is made. Call Amazon S3 by using a PUT request to access the object without PII.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an S3 Object Lambda access point from the S3 console. Select the removePii function. Use S3 Access Points to access the object without PII.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an S3 access point from the S3 console. Use the access point name to call the GetObjectLegalHold S3 API function. Pass in the removePii function name to access the object without PII.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 15,
    "question": "A developer is deploying an AWS Lambda function The developer wants the ability to return to older versions of the function quickly and seamlessly.<br>How can the developer achieve this goal with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS OpsWorks to perform blue/green deployments.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use a function alias with different versions.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Maintain deployment packages for older versions in Amazon S3.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS CodePipeline for deployments and rollbacks.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 16,
    "question": "A developer has written an AWS Lambda function. The function is CPU-bound. The developer wants to ensure that the function returns responses quickly.<br>How can the developer improve the function's performance?",
    "answers": [
      {
        "id": 1,
        "answer": "Increase the function's CPU core count.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Increase the function's memory.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Increase the function's reserved concurrency.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Increase the function's timeout.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 17,
    "question": "For a deployment using AWS Code Deploy, what is the run order of the hooks for in-place deployments?",
    "answers": [
      {
        "id": 1,
        "answer": "BeforeInstall -&gt; ApplicationStop -&gt; ApplicationStart -&gt; AfterInstall",
        "correct": false
      },
      {
        "id": 2,
        "answer": "ApplicationStop -&gt; BeforeInstall -&gt; AfterInstall -&gt; ApplicationStart",
        "correct": true
      },
      {
        "id": 3,
        "answer": "BeforeInstall -&gt; ApplicationStop -&gt; ValidateService -&gt; ApplicationStart",
        "correct": false
      },
      {
        "id": 4,
        "answer": "ApplicationStop -&gt; BeforeInstall -&gt; ValidateService -&gt; ApplicationStart",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 18,
    "question": "A company is building a serverless application on AWS. The application uses an AWS Lambda function to process customer orders 24 hours a day, 7 days a week. The Lambda function calls an external vendor's HTTP API to process payments.<br>During load tests, a developer discovers that the external vendor payment processing API occasionally times out and returns errors. The company expects that some payment processing API calls will return errors.<br>The company wants the support team to receive notifications in near real time only when the payment processing external API error rate exceed 5% of the total number of transactions in an hour. Developers need to use an existing Amazon Simple Notification Service (Amazon SNS) topic that is configured to notify the support team.<br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Write the results of payment processing API calls to Amazon CloudWatch. Use Amazon CloudWatch Logs Insights to query the CloudWatch logs. Schedule the Lambda function to check the CloudWatch logs and notify the existing SNS topic.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Publish custom metrics to CloudWatch that record the failures of the external payment processing API calls. Configure a CloudWatch alarm to notify the existing SNS topic when error rate exceeds the specified rate.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Publish the results of the external payment processing API calls to a new Amazon SNS topic. Subscribe the support team members to the new SNS topic.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Write the results of the external payment processing API calls to Amazon S3. Schedule an Amazon Athena query to run at regular intervals. Configure Athena to send notifications to the existing SNS topic when the error rate exceeds the specified rate.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 19,
    "question": "A company is offering APIs as a service over the internet to provide unauthenticated read access to statistical information that is updated daily. The company uses Amazon API Gateway and AWS Lambda to develop the APIs. The service has become popular, and the company wants to enhance the responsiveness of the APIs.<br>Which action can help the company achieve this goal?",
    "answers": [
      {
        "id": 1,
        "answer": "Enable API caching in API Gateway.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Configure API Gateway to use an interface VPC endpoint.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Enable cross-origin resource sharing (CORS) for the APIs.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure usage plans and API keys in API Gateway.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 20,
    "question": "A developer wants to store information about movies. Each movie has a title, release year, and genre. The movie information also can include additional properties about the cast and production crew. This additional information is inconsistent across movies. For example, one movie might have an assistant director, and another movie might have an animal trainer.<br>The developer needs to implement a solution to support the following use cases:<br>For a given title and release year, get all details about the movie that has that title and release year.<br>For a given title, get all details about all movies that have that title.<br>For a given genre, get all details about all movies in that genre.<br>Which data store configuration will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the title as the partition key and the release year as the sort key. Create a global secondary index that uses the genre as the partition key and the title as the sort key.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the genre as the partition key and the release year as the sort key. Create a global secondary index that uses the title as the partition key.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "On an Amazon RDS DB instance, create a table that contains columns for title, release year, and genre. Configure the title as the primary key.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "On an Amazon RDS DB instance, create a table where the primary key is the title and all other data is encoded into JSON format as one additional column.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 21,
    "question": "A developer maintains an Amazon API Gateway REST API. Customers use the API through a frontend UI and Amazon Cognito authentication.<br>The developer has a new version of the API that contains new endpoints and backward-incompatible interface changes. The developer needs to provide beta access to other developers on the team without affecting customers.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Define a development stage on the API Gateway API. Instruct the other developers to point the endpoints to the development stage.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Define a new API Gateway API that points to the new API application code. Instruct the other developers to point the endpoints to the new API.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Implement a query parameter in the API application code that determines which code version to call.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Specify new API Gateway endpoints for the API endpoints that the developer wants to add.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 22,
    "question": "A developer is creating an application that will store personal health information (PHI). The PHI needs to be encrypted at all times. An encrypted Amazon RDS for MySQL DB instance is storing the data. The developer wants to increase the performance of the application by caching frequently accessed data while adding the ability to sort or rank the cached datasets.<br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon ElastiCache for Redis instance. Enable encryption of data in transit and at rest. Store frequently accessed data in the cache.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an Amazon ElastiCache for Memcached instance. Enable encryption of data in transit and at rest. Store frequently accessed data in the cache.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon RDS for MySQL read replica. Connect to the read replica by using SSL. Configure the read replica to store frequently accessed data.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon DynamoDB table and a DynamoDB Accelerator (DAX) cluster for the table. Store frequently accessed data in the DynamoDB table.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 23,
    "question": "A company has a multi-node Windows legacy application that runs on premises. The application uses a network shared folder as a centralized configuration repository to store configuration files in .xml format. The company is migrating the application to Amazon EC2 instances. As part of the migration to AWS, a developer must identify a solution that provides high availability for the repository.<br>Which solution will meet this requirement MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Mount an Amazon Elastic Block Store (Amazon EBS) volume onto one of the EC2 instances. Deploy a file system on the EBS volume. Use the host operating system to share a folder. Update the application code to read and write configuration files from the shared folder.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy a micro EC2 instance with an instance store volume. Use the host operating system to share a folder. Update the application code to read and write configuration files from the shared folder.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon S3 bucket to host the repository. Migrate the existing .xml files to the S3 bucket. Update the application code to use the AWS SDK to read and write configuration files from Amazon S3.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an Amazon S3 bucket to host the repository. Migrate the existing .xml files to the S3 bucket. Mount the S3 bucket to the EC2 instances as a local volume. Update the application code to read and write configuration files from the disk.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 24,
    "question": "A company wants to deploy and maintain static websites on AWS. Each website's source code is hosted in one of several version control systems, including AWS CodeCommit, Bitbucket, and GitHub.<br>The company wants to implement phased releases by using development, staging, user acceptance testing, and production environments in the AWS Cloud. Deployments to each environment must be started by code merges on the relevant Git branch. The company wants to use HTTPS for all data exchange. The company needs a solution that does not require servers to run continuously.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Host each website by using AWS Amplify with a serverless backend. Conned the repository branches that correspond to each of the desired environments. Start deployments by merging code changes to a desired branch.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Host each website in AWS Elastic Beanstalk with multiple environments. Use the EB CLI to link each repository branch. Integrate AWS CodePipeline to automate deployments from version control code merges.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Host each website in different Amazon S3 buckets for each environment. Configure AWS CodePipeline to pull source code from version control. Add an AWS CodeBuild stage to copy source code to Amazon S3.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Host each website on its own Amazon EC2 instance. Write a custom deployment script to bundle each website's static assets. Copy the assets to Amazon EC2. Set up a workflow to run the script when code is merged.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 25,
    "question": "A company is migrating an on-premises database to Amazon RDS for MySQL. The company has read-heavy workloads. The company wants to refactor the code to achieve optimum read performance for queries.<br>Which solution will meet this requirement with LEAST current and future effort?",
    "answers": [
      {
        "id": 1,
        "answer": "Use a multi-AZ Amazon RDS deployment. Increase the number of connections that the code makes to the database or increase the connection pool size if a connection pool is in use.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use a multi-AZ Amazon RDS deployment. Modify the code so that queries access the secondary RDS instance.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy Amazon RDS with one or more read replicas. Modify the application code so that queries use the URL for the read replicas.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use open source replication software to create a copy of the MySQL database on an Amazon EC2 instance. Modify the application code so that queries use the IP address of the EC2 instance.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 26,
    "question": "A developer is creating an application that will be deployed on IoT devices. The application will send data to a RESTful API that is deployed as an AWS Lambda function. The application will assign each API request a unique identifier. The volume of API requests from the application can randomly increase at any given time of day.<br>During periods of request throttling, the application might need to retry requests. The API must be able to handle duplicate requests without inconsistencies or data loss.<br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon RDS for MySQL DB instance. Store the unique identifier for each request in a database table. Modify the Lambda function to check the table for the identifier before processing the request.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon DynamoDB table. Store the unique identifier for each request in the table. Modify the Lambda function to check the table for the identifier before processing the request.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an Amazon DynamoDB table. Store the unique identifier for each request in the table. Modify the Lambda function to return a client error response when the function receives a duplicate request.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon ElastiCache for Memcached instance. Store the unique identifier for each request in the cache. Modify the Lambda function to check the cache for the identifier before processing the request.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 27,
    "question": "A developer wants to expand an application to run in multiple AWS Regions. The developer wants to copy Amazon Machine Images (AMIs) with the latest changes and create a new application stack in the destination Region. According to company requirements, all AMIs must be encrypted in all Regions. However, not all the AMIs that the company uses are encrypted.<br>How can the developer expand the application to run in the destination Region while meeting the encryption requirement?",
    "answers": [
      {
        "id": 1,
        "answer": "Create new AMIs, and specify encryption parameters. Copy the encrypted AMIs to the destination Region. Delete the unencrypted AMIs.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use AWS Key Management Service (AWS KMS) to enable encryption on the unencrypted AMIs. Copy the encrypted AMIs to the destination Region.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS Certificate Manager (ACM) to enable encryption on the unencrypted AMIs. Copy the encrypted AMIs to the destination Region.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Copy the unencrypted AMIs to the destination Region. Enable encryption by default in the destination Region.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 28,
    "question": "A company hosts a client-side web application for one of its subsidiaries on Amazon S3. The web application can be accessed through Amazon CloudFront from https://www.example.com. After a successful rollout, the company wants to host three more client-side web applications for its remaining subsidiaries on three separate S3 buckets.<br>To achieve this goal, a developer moves all the common JavaScript files and web fonts to a central S3 bucket that serves the web applications. However, during testing, the developer notices that the browser blocks the JavaScript files and web fonts.<br>What should the developer do to prevent the browser from blocking the JavaScript files and web fonts?",
    "answers": [
      {
        "id": 1,
        "answer": "Create four access points that allow access to the central S3 bucket. Assign an access point to each web application bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a bucket policy that allows access to the central S3 bucket. Attach the bucket policy to the central S3 bucket",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a cross-origin resource sharing (CORS) configuration that allows access to the central S3 bucket. Add the CORS configuration to the central S3 bucket.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create a Content-MD5 header that provides a message integrity check for the central S3 bucket. Insert the Content-MD5 header for each web application request.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 29,
    "question": "An application is processing clickstream data using Amazon Kinesis. The clickstream data feed into Kinesis experiences periodic spikes. The PutRecords API call occasionally fails and the logs show that the failed call returns the response shown below:<br><img title=\"image1\" src=\"https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image1.png\"><br>Which techniques will help mitigate this exception? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Implement retries with exponential backoff.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use a PutRecord API instead of PutRecords.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Reduce the frequency and/or size of the requests.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use Amazon SNS instead of Kinesis.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Reduce the number of KCL consumers.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 30,
    "question": "A company has an application that uses Amazon Cognito user pools as an identity provider. The company must secure access to user records. The company has set up multi-factor authentication (MFA). The company also wants to send a login activity notification by email every time a user logs in.<br>What is the MOST operationally efficient solution that meets this requirement?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS Lambda function that uses Amazon Simple Email Service (Amazon SES) to send the email notification. Add an Amazon API Gateway API to invoke the function. Call the API from the client side when login confirmation is received.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an AWS Lambda function that uses Amazon Simple Email Service (Amazon SES) to send the email notification. Add an Amazon Cognito post authentication Lambda trigger for the function.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an AWS Lambda function that uses Amazon Simple Email Service (Amazon SES) to send the email notification. Create an Amazon CloudWatch Logs log subscription filter to invoke the function based on the login status.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure Amazon Cognito to stream all logs to Amazon Kinesis Data Firehose. Create an AWS Lambda function to process the streamed logs and to send the email notification based on the login status of each user.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 31,
    "question": "A developer has an application that stores data in an Amazon S3 bucket. The application uses an HTTP API to store and retrieve objects. When the PutObject API operation adds objects to the S3 bucket the developer must encrypt these objects at rest by using server-side encryption with Amazon S3 managed keys (SSE-S3).<br>Which solution will meet this requirement?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS Key Management Service (AWS KMS) key. Assign the KMS key to the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set the x-amz-server-side-encryption header when invoking the PutObject API operation.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Provide the encryption key in the HTTP header of every request.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Apply TLS to encrypt the traffic to the S3 bucket.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 32,
    "question": "A developer needs to perform geographic load testing of an API. The developer must deploy resources to multiple AWS Regions to support the load testing of the API.<br>How can the developer meet these requirements without additional application code?",
    "answers": [
      {
        "id": 1,
        "answer": "Create and deploy an AWS Lambda function in each desired Region. Configure the Lambda function to create a stack from an AWS CloudFormation template in that Region when the function is invoked.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an AWS CloudFormation template that defines the load test resources. Use the AWS CLI create-stack-set command to create a stack set in the desired Regions.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an AWS Systems Manager document that defines the resources. Use the document to create the resources in the desired Regions.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an AWS CloudFormation template that defines the load test resources. Use the AWS CLI deploy command to create a stack from the template in each Region.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 33,
    "question": "A developer is creating an application that includes an Amazon API Gateway REST API in the us-east-2 Region. The developer wants to use Amazon CloudFront and a custom domain name for the API. The developer has acquired an SSL/TLS certificate for the domain from a third-party provider.<br>How should the developer configure the custom domain for the application?",
    "answers": [
      {
        "id": 1,
        "answer": "Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the same Region as the API. Create a DNS A record for the custom domain.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Import the SSL/TLS certificate into CloudFront. Create a DNS CNAME record for the custom domain.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the same Region as the API. Create a DNS CNAME record for the custom domain.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the us-east-1 Region. Create a DNS CNAME record for the custom domain.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 34,
    "question": "A developer is creating a template that uses AWS CloudFormation to deploy an application. The application is serverless and uses Amazon API Gateway, Amazon DynamoDB, and AWS Lambda.<br>Which AWS service or tool should the developer use to define serverless resources in YAML?",
    "answers": [
      {
        "id": 1,
        "answer": "CloudFormation serverless intrinsic functions",
        "correct": false
      },
      {
        "id": 2,
        "answer": "AWS Elastic Beanstalk",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Serverless Application Model (AWS SAM)",
        "correct": true
      },
      {
        "id": 4,
        "answer": "AWS Cloud Development Kit (AWS CDK)",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 35,
    "question": "A developer wants to insert a record into an Amazon DynamoDB table as soon as a new file is added to an Amazon S3 bucket.<br>Which set of steps would be necessary to achieve this?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an event with Amazon EventBridge that will monitor the S3 bucket and then insert the records into DynamoDB.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure an S3 event to invoke an AWS Lambda function that inserts records into DynamoDB.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an AWS Lambda function that will poll the S3 bucket and then insert the records into DynamoDB.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a cron job that will run at a scheduled time and insert the records into DynamoDB.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 36,
    "question": "A development team maintains a web application by using a single AWS CloudFormation template. The template defines web servers and an Amazon RDS database. The team uses the Cloud Formation template to deploy the Cloud Formation stack to different environments.<br>During a recent application deployment, a developer caused the primary development database to be dropped and recreated. The result of this incident was a loss of data. The team needs to avoid accidental database deletion in the future.<br>Which solutions will meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Add a CloudFormation Deletion Policy attribute with the Retain value to the database resource.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Update the CloudFormation stack policy to prevent updates to the database.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Modify the database to use a Multi-AZ deployment.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a CloudFormation stack set for the web application and database deployments.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Add a Cloud Formation DeletionPolicy attribute with the Retain value to the stack.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 37,
    "question": "A company has an Amazon S3 bucket that contains sensitive data. The data must be encrypted in transit and at rest. The company encrypts the data in the S3 bucket by using an AWS Key Management Service (AWS KMS) key. A developer needs to grant several other AWS accounts the permission to use the S3 GetObject operation to retrieve the data from the S3 bucket.<br>How can the developer enforce that all requests to retrieve the data provide encryption in transit?",
    "answers": [
      {
        "id": 1,
        "answer": "Define a resource-based policy on the S3 bucket to deny access when a request meets the condition “aws:SecureTransport”: “false”.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Define a resource-based policy on the S3 bucket to allow access when a request meets the condition “aws:SecureTransport”: “false”.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Define a role-based policy on the other accounts' roles to deny access when a request meets the condition of “aws:SecureTransport”: “false”.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Define a resource-based policy on the KMS key to deny access when a request meets the condition of “aws:SecureTransport”: “false”.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 38,
    "question": "An application that is hosted on an Amazon EC2 instance needs access to files that are stored in an Amazon S3 bucket. The application lists the objects that are stored in the S3 bucket and displays a table to the user. During testing, a developer discovers that the application does not show any objects in the list.<br>What is the MOST secure way to resolve this issue?",
    "answers": [
      {
        "id": 1,
        "answer": "Update the IAM instance profile that is attached to the EC2 instance to include the S3:* permission for the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Update the IAM instance profile that is attached to the EC2 instance to include the S3:ListBucket permission for the S3 bucket.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Update the developer's user permissions to include the S3:ListBucket permission for the S3 bucket.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Update the S3 bucket policy by including the S3:ListBucket permission and by setting the Principal element to specify the account number of the EC2 instance.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 39,
    "question": "A company is planning to securely manage one-time fixed license keys in AWS. The company's development team needs to access the license keys in automaton scripts that run in Amazon EC2 instances and in AWS CloudFormation stacks.<br>Which solution will meet these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Amazon S3 with encrypted files prefixed with “config”",
        "correct": false
      },
      {
        "id": 2,
        "answer": "AWS Secrets Manager secrets with a tag that is named SecretString",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Systems Manager Parameter Store SecureString parameters",
        "correct": true
      },
      {
        "id": 4,
        "answer": "CloudFormation NoEcho parameters",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 40,
    "question": "A company has deployed infrastructure on AWS. A development team wants to create an AWS Lambda function that will retrieve data from an Amazon Aurora database. The Amazon Aurora database is in a private subnet in company's VPC. The VPC is named VPC1. The data is relational in nature. The Lambda function needs to access the data securely.<br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create the Lambda function. Configure VPC1 access for the function. Attach a security group named SG1 to both the Lambda function and the database. Configure the security group inbound and outbound rules to allow TCP traffic on Port 3306.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create and launch a Lambda function in a new public subnet that is in a new VPC named VPC2. Create a peering connection between VPC1 and VPC2.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create the Lambda function. Configure VPC1 access for the function. Assign a security group named SG1 to the Lambda function. Assign a second security group named SG2 to the database. Add an inbound rule to SG1 to allow TCP traffic from Port 3306.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Export the data from the Aurora database to Amazon S3. Create and launch a Lambda function in VPC1. Configure the Lambda function query the data from Amazon S3.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 41,
    "question": "A developer is building a web application that uses Amazon API Gateway to expose an AWS Lambda function to process requests from clients. During testing, the developer notices that the API Gateway times out even though the Lambda function finishes under the set time limit.<br>Which of the following API Gateway metrics in Amazon CloudWatch can help the developer troubleshoot the issue? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "CacheHitCount",
        "correct": false
      },
      {
        "id": 2,
        "answer": "IntegrationLatency",
        "correct": true
      },
      {
        "id": 3,
        "answer": "CacheMissCount",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Latency",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Count",
        "correct": false
      }
    ],
    "corrects": [
      2,
      4
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 42,
    "question": "A development team wants to build a continuous integration/continuous delivery (CI/CD) pipeline. The team is using AWS CodePipeline to automate the code build and deployment. The team wants to store the program code to prepare for the CI/CD pipeline.<br>Which AWS service should the team use to store the program code?",
    "answers": [
      {
        "id": 1,
        "answer": "AWS CodeDeploy",
        "correct": false
      },
      {
        "id": 2,
        "answer": "AWS CodeArtifact",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS CodeCommit",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Amazon CodeGuru",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 43,
    "question": "A developer is designing an AWS Lambda function that creates temporary files that are less than 10 MB during invocation. The temporary files will be accessed and modified multiple times during invocation. The developer has no need to save or retrieve these files in the future.<br>Where should the temporary files be stored?",
    "answers": [
      {
        "id": 1,
        "answer": "the /tmp directory",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Amazon Elastic File System (Amazon EFS)",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon S3",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 44,
    "question": "A developer is designing a serverless application with two AWS Lambda functions to process photos. One Lambda function stores objects in an Amazon S3 bucket and stores the associated metadata in an Amazon DynamoDB table. The other Lambda function fetches the objects from the S3 bucket by using the metadata from the DynamoDB table. Both Lambda functions use the same Python library to perform complex computations and are approaching the quota for the maximum size of zipped deployment packages.<br>What should the developer do to reduce the size of the Lambda deployment packages with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Package each Python library in its own .zip file archive. Deploy each Lambda function with its own copy of the library.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a Lambda layer with the required Python library. Use the Lambda layer in both Lambda functions.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Combine the two Lambda functions into one Lambda function. Deploy the Lambda function as a single .zip file archive.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Download the Python library to an S3 bucket. Program the Lambda functions to reference the object URLs.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 45,
    "question": "A developer is writing an AWS Lambda function. The developer wants to log key events that occur while the Lambda function runs. The developer wants to include a unique identifier to associate the events with a specific function invocation. The developer adds the following code to the Lambda function:<br><img title=\"image2\" src=\"https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image2.png\"><br>Which solution will meet this requirement?",
    "answers": [
      {
        "id": 1,
        "answer": "Obtain the request identifier from the AWS request ID field in the context object. Configure the application to write logs to standard output.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Obtain the request identifier from the AWS request ID field in the event object. Configure the application to write logs to a file.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Obtain the request identifier from the AWS request ID field in the event object. Configure the application to write logs to standard output.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Obtain the request identifier from the AWS request ID field in the context object. Configure the application to write logs to a file.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 46,
    "question": "A developer is working on a serverless application that needs to process any changes to an Amazon DynamoDB table with an AWS Lambda function.<br>How should the developer configure the Lambda function to detect changes to the DynamoDB table?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon Kinesis data stream, and attach it to the DynamoDB table. Create a trigger to connect the data stream to the Lambda function.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon EventBridge rule to invoke the Lambda function on a regular schedule. Conned to the DynamoDB table from the Lambda function to detect changes.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Enable DynamoDB Streams on the table. Create a trigger to connect the DynamoDB stream to the Lambda function.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an Amazon Kinesis Data Firehose delivery stream, and attach it to the DynamoDB table. Configure the delivery stream destination as the Lambda function.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 47,
    "question": "An application uses an Amazon EC2 Auto Scaling group. A developer notices that EC2 instances are taking a long time to become available during scale-out events. The UserData script is taking a long time to run.<br>The developer must implement a solution to decrease the time that elapses before an EC2 instance becomes available. The solution must make the most recent version of the application available at all times and must apply all available security updates. The solution also must minimize the number of images that are created. The images must be validated.<br>Which combination of steps should the developer take to meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Use EC2 Image Builder to create an Amazon Machine Image (AMI). Install all the patches and agents that are needed to manage and run the application. Update the Auto Scaling group launch configuration to use the AMI.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use EC2 Image Builder to create an Amazon Machine Image (AMI). Install the latest version of the application and all the patches and agents that are needed to manage and run the application. Update the Auto Scaling group launch configuration to use the AMI.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up AWS CodeDeploy to deploy the most recent version of the application at runtime.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Set up AWS CodePipeline to deploy the most recent version of the application at runtime.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Remove any commands that perform operating system patching from the UserData script.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 48,
    "question": "A developer is creating an AWS Lambda function that needs credentials to connect to an Amazon RDS for MySQL database. An Amazon S3 bucket currently stores the credentials. The developer needs to improve the existing solution by implementing credential rotation and secure storage. The developer also needs to provide integration with the Lambda function.<br>Which solution should the developer use to store and retrieve the credentials with the LEAST management overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Store the credentials in AWS Systems Manager Parameter Store. Select the database that the parameter will access. Use the default AWS Key Management Service (AWS KMS) key to encrypt the parameter. Enable automatic rotation for the parameter. Use the parameter from Parameter Store on the Lambda function to connect to the database.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Encrypt the credentials with the default AWS Key Management Service (AWS KMS) key. Store the credentials as environment variables for the Lambda function. Create a second Lambda function to generate new credentials and to rotate the credentials by updating the environment variables of the first Lambda function. Invoke the second Lambda function by using an Amazon EventBridge rule that runs on a schedule. Update the database to use the new credentials. On the first Lambda function, retrieve the credentials from the environment variables. Decrypt the credentials by using AWS KMS, Connect to the database.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Store the credentials in AWS Secrets Manager. Set the secret type to Credentials for Amazon RDS database. Select the database that the secret will access. Use the default AWS Key Management Service (AWS KMS) key to encrypt the secret. Enable automatic rotation for the secret. Use the secret from Secrets Manager on the Lambda function to connect to the database.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Encrypt the credentials by using AWS Key Management Service (AWS KMS). Store the credentials in an Amazon DynamoDB table. Create a second Lambda function to rotate the credentials. Invoke the second Lambda function by using an Amazon EventBridge rule that runs on a schedule. Update the DynamoDB table. Update the database to use the generated credentials. Retrieve the credentials from DynamoDB with the first Lambda function. Connect to the database.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 49,
    "question": "A developer has written the following IAM policy to provide access to an Amazon S3 bucket:<br><img title=\"image3\" src=\"https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image3.png\"><br>Which access does the policy allow regarding the s3:GetObject and s3:PutObject actions?",
    "answers": [
      {
        "id": 1,
        "answer": "Access on all buckets except the “DOC-EXAMPLE-BUCKET” bucket",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Access on all buckets that start with “DOC-EXAMPLE-BUCKET” except the “DOC-EXAMPLE-BUCKET/secrets” bucket",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Access on all objects in the “DOC-EXAMPLE-BUCKET” bucket along with access to all S3 actions for objects in the “DOC-EXAMPLE-BUCKET” bucket that start with “secrets”",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Access on all objects in the “DOC-EXAMPLE-BUCKET” bucket except on objects that start with “secrets”",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 50,
    "question": "A developer is creating a mobile app that calls a backend service by using an Amazon API Gateway REST API. For integration testing during the development phase, the developer wants to simulate different backend responses without invoking the backend service.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS Lambda function. Use API Gateway proxy integration to return constant HTTP responses.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon EC2 instance that serves the backend REST API by using an AWS CloudFormation template.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Customize the API Gateway stage to select a response type based on the request.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use a request mapping template to select the mock integration response.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 51,
    "question": "A developer has a legacy application that is hosted on-premises. Other applications hosted on AWS depend on the on-premises application for proper functioning. In case of any application errors, the developer wants to be able to use Amazon CloudWatch to monitor and troubleshoot all applications from one place.<br>How can the developer accomplish this?",
    "answers": [
      {
        "id": 1,
        "answer": "Install an AWS SDK on the on-premises server to automatically send logs to CloudWatch.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Download the CloudWatch agent to the on-premises server. Configure the agent to use IAM user credentials with permissions for CloudWatch.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Upload log files from the on-premises server to Amazon S3 and have CloudWatch read the files.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Upload log files from the on-premises server to an Amazon EC2 instance and have the instance forward the logs to CloudWatch.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 52,
    "question": "An Amazon Kinesis Data Firehose delivery stream is receiving customer data that contains personally identifiable information. A developer needs to remove pattern-based customer identifiers from the data and store the modified data in an Amazon S3 bucket.<br>What should the developer do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Implement Kinesis Data Firehose data transformation as an AWS Lambda function. Configure the function to remove the customer identifiers. Set an Amazon S3 bucket as the destination of the delivery stream.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Launch an Amazon EC2 instance. Set the EC2 instance as the destination of the delivery stream. Run an application on the EC2 instance to remove the customer identifiers. Store the transformed data in an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Amazon OpenSearch Service instance. Set the OpenSearch Service instance as the destination of the delivery stream. Use search and replace to remove the customer identifiers. Export the data to an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an AWS Step Functions workflow to remove the customer identifiers. As the last step in the workflow, store the transformed data in an Amazon S3 bucket. Set the workflow as the destination of the delivery stream.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 53,
    "question": "A developer is using an AWS Lambda function to generate avatars for profile pictures that are uploaded to an Amazon S3 bucket. The Lambda function is automatically invoked for profile pictures that are saved under the /original/ S3 prefix. The developer notices that some pictures cause the Lambda function to time out. The developer wants to implement a fallback mechanism by using another Lambda function that resizes the profile picture.<br>Which solution will meet these requirements with the LEAST development effort?",
    "answers": [
      {
        "id": 1,
        "answer": "Set the image resize Lambda function as a destination of the avatar generator Lambda function for the events that fail processing.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Set the SQS queue as a destination with an on failure condition for the avatar generator Lambda function. Configure the image resize Lambda function to poll from the SQS queue.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an AWS Step Functions state machine that invokes the avatar generator Lambda function and uses the image resize Lambda function as a fallback. Create an Amazon EventBridge rule that matches events from the S3 bucket to invoke the state machine.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Set the SNS topic as a destination with an on failure condition for the avatar generator Lambda function. Subscribe the image resize Lambda function to the SNS topic.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 54,
    "question": "A developer needs to migrate an online retail application to AWS to handle an anticipated increase in traffic. The application currently runs on two servers: one server for the web application and another server for the database. The web server renders webpages and manages session state in memory. The database server hosts a MySQL database that contains order details. When traffic to the application is heavy, the memory usage for the web server approaches 100% and the application slows down considerably.<br>The developer has found that most of the memory increase and performance decrease is related to the load of managing additional user sessions. For the web server migration, the developer will use Amazon EC2 instances with an Auto Scaling group behind an Application Load Balancer.<br>Which additional set of changes should the developer make to the application to improve the application's performance?",
    "answers": [
      {
        "id": 1,
        "answer": "Use an EC2 instance to host the MySQL database. Store the session data and the application data in the MySQL database.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon ElastiCache for Memcached to store and manage the session data. Use an Amazon RDS for MySQL DB instance to store the application data.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Amazon ElastiCache for Memcached to store and manage the session data and the application data.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use the EC2 instance store to manage the session data. Use an Amazon RDS for MySQL DB instance to store the application data.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 55,
    "question": "An application uses Lambda functions to extract metadata from files uploaded to an S3 bucket; the metadata is stored in Amazon DynamoDB. The application starts behaving unexpectedly, and the developer wants to examine the logs of the Lambda function code for errors.<br>Based on this system configuration, where would the developer find the logs?",
    "answers": [
      {
        "id": 1,
        "answer": "Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "answer": "AWS CloudTrail",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon CloudWatch",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Amazon DynamoDB",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 56,
    "question": "A company is using an AWS Lambda function to process records from an Amazon Kinesis data stream. The company recently observed slow processing of the records. A developer notices that the iterator age metric for the function is increasing and that the Lambda run duration is constantly above normal.<br>Which actions should the developer take to increase the processing speed? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Increase the number of shards of the Kinesis data stream.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Decrease the timeout of the Lambda function.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Increase the memory that is allocated to the Lambda function.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Decrease the number of shards of the Kinesis data stream.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Increase the timeout of the Lambda function.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 57,
    "question": "A company needs to harden its container images before the images are in a running state. The company's application uses Amazon Elastic Container Registry (Amazon ECR) as an image registry. Amazon Elastic Kubernetes Service (Amazon EKS) for compute, and an AWS CodePipeline pipeline that orchestrates a continuous integration and continuous delivery (CI/CD) workflow.<br>Dynamic application security testing occurs in the final stage of the pipeline after a new image is deployed to a development namespace in the EKS cluster. A developer needs to place an analysis stage before this deployment to analyze the container image earlier in the CI/CD pipeline.<br>Which solution will meet these requirements with the MOST operational efficiency?",
    "answers": [
      {
        "id": 1,
        "answer": "Build the container image and run the docker scan command locally. Mitigate any findings before pushing changes to the source code repository. Write a pre-commit hook that enforces the use of this workflow before commit.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a new CodePipeline stage that occurs after the container image is built. Configure ECR basic image scanning to scan on image push. Use an AWS Lambda function as the action provider. Configure the Lambda function to check the scan results and to fail the pipeline if there are findings.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a new CodePipeline stage that occurs after source code has been retrieved from its repository. Run a security scanner on the latest revision of the source code. Fail the pipeline if there are findings.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Add an action to the deployment stage of the pipeline so that the action occurs before the deployment to the EKS cluster. Configure ECR basic image scanning to scan on image push. Use an AWS Lambda function as the action provider. Configure the Lambda function to check the scan results and to fail the pipeline if there are findings.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 58,
    "question": "A developer is testing a new file storage application that uses an Amazon CloudFront distribution to serve content from an Amazon S3 bucket. The distribution accesses the S3 bucket by using an origin access identity (OAI). The S3 bucket's permissions explicitly deny access to all other users.<br>The application prompts users to authenticate on a login page and then uses signed cookies to allow users to access their personal storage directories. The developer has configured the distribution to use its default cache behavior with restricted viewer access and has set the origin to point to the S3 bucket. However, when the developer tries to navigate to the login page, the developer receives a 403 Forbidden error.<br>The developer needs to implement a solution to allow unauthenticated access to the login page. The solution also must keep all private content secure.<br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Add a second cache behavior to the distribution with the same origin as the default cache behavior. Set the path pattern for the second cache behavior to the path of the login page, and make viewer access unrestricted. Keep the default cache behavior's settings unchanged.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Add a second cache behavior to the distribution with the same origin as the default cache behavior. Set the path pattern for the second cache behavior to *, and make viewer access restricted. Change the default cache behavior's path pattern to the path of the login page, and make viewer access unrestricted.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Add a second origin as a failover origin to the default cache behavior. Point the failover origin to the S3 bucket. Set the path pattern for the primary origin to *, and make viewer access restricted. Set the path pattern for the failover origin to the path of the login page, and make viewer access unrestricted.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Add a bucket policy to the S3 bucket to allow read access. Set the resource on the policy to the Amazon Resource Name (ARN) of the login page object in the S3 bucket. Add a CloudFront function to the default cache behavior to redirect unauthorized requests to the login page's S3 URL.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 59,
    "question": "A developer is using AWS Amplify Hosting to build and deploy an application. The developer is receiving an increased number of bug reports from users. The developer wants to add end-to-end testing to the application to eliminate as many bugs as possible before the bugs reach production.<br>Which solution should the developer implement to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Run the amplify add test command in the Amplify CLI.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create unit tests in the application. Deploy the unit tests by using the amplify push command in the Amplify CLI.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Add a test phase to the amplify.yml build settings for the application.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Add a test phase to the aws-exports.js file for the application.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 60,
    "question": "An ecommerce company is using an AWS Lambda function behind Amazon API Gateway as its application tier. To process orders during checkout, the application calls a POST API from the frontend. The POST API invokes the Lambda function asynchronously. In rare situations, the application has not processed orders. The Lambda application logs show no errors or failures.<br>What should a developer do to solve this problem?",
    "answers": [
      {
        "id": 1,
        "answer": "Inspect the frontend logs for API failures. Call the POST API manually by using the requests from the log file.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create and inspect the Lambda dead-letter queue. Troubleshoot the failed functions. Reprocess the events.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Inspect the Lambda logs in Amazon CloudWatch for possible errors. Fix the errors.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Make sure that caching is disabled for the POST API in API Gateway.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 61,
    "question": "A company is building a web application on AWS. When a customer sends a request, the application will generate reports and then make the reports available to the customer within one hour. Reports should be accessible to the customer for 8 hours. Some reports are larger than 1 MB. Each report is unique to the customer. The application should delete all reports that are older than 2 days.<br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Generate the reports and then store the reports as Amazon DynamoDB items that have a specified TTL. Generate a URL that retrieves the reports from DynamoDB. Provide the URL to customers through the web application.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Generate the reports and then store the reports in an Amazon S3 bucket that uses server-side encryption. Attach the reports to an Amazon Simple Notification Service (Amazon SNS) message. Subscribe the customer to email notifications from Amazon SNS.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Generate the reports and then store the reports in an Amazon S3 bucket that uses server-side encryption. Generate a presigned URL that contains an expiration date Provide the URL to customers through the web application. Add S3 Lifecycle configuration rules to the S3 bucket to delete old reports.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Generate the reports and then store the reports in an Amazon RDS database with a date stamp. Generate an URL that retrieves the reports from the RDS database. Provide the URL to customers through the web application. Schedule an hourly AWS Lambda function to delete database records that have expired date stamps.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 62,
    "question": "A company has deployed an application on AWS Elastic Beanstalk. The company has configured the Auto Scaling group that is associated with the Elastic Beanstalk environment to have five Amazon EC2 instances. If the capacity is fewer than four EC2 instances during the deployment, application performance degrades. The company is using the all-at-once deployment policy.<br>What is the MOST cost-effective way to solve the deployment issue?",
    "answers": [
      {
        "id": 1,
        "answer": "Change the Auto Scaling group to six desired instances.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Change the deployment policy to traffic splitting. Specify an evaluation time of 1 hour.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Change the deployment policy to rolling with additional batch. Specify a batch size of 1.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Change the deployment policy to rolling. Specify a batch size of 2.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 63,
    "question": "A developer is incorporating AWS X-Ray into an application that handles personal identifiable information (PII). The application is hosted on Amazon EC2 instances. The application trace messages include encrypted PII and go to Amazon CloudWatch. The developer needs to ensure that no PII goes outside of the EC2 instances.<br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Manually instrument the X-Ray SDK in the application code.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use the X-Ray auto-instrumentation agent.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon Macie to detect and hide PII. Call the X-Ray API from AWS Lambda.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Distro for Open Telemetry.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 64,
    "question": "A developer is migrating some features from a legacy monolithic application to use AWS Lambda functions instead. The application currently stores data in an Amazon Aurora DB cluster that runs in private subnets in a VPC. The AWS account has one VPC deployed. The Lambda functions and the DB cluster are deployed in the same AWS Region in the same AWS account.<br>The developer needs to ensure that the Lambda functions can securely access the DB cluster without crossing the public internet.<br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure the DB cluster's public access setting to Yes.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure an Amazon RDS database proxy for he Lambda functions.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure a NAT gateway and a security group for the Lambda functions.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure the VPC, subnets, and a security group for the Lambda functions.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 65,
    "question": "A developer is building a new application on AWS. The application uses an AWS Lambda function that retrieves information from an Amazon DynamoDB table. The developer hard coded the DynamoDB table name into the Lambda function code. The table name might change over time. The developer does not want to modify the Lambda code if the table name changes.<br>Which solution will meet these requirements MOST efficiently?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a Lambda environment variable to store the table name. Use the standard method for the programming language to retrieve the variable.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Store the table name in a file. Store the file in the /tmp folder. Use the SDK for the programming language to retrieve the table name.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a file to store the table name. Zip the file and upload the file to the Lambda layer. Use the SDK for the programming language to retrieve the table name.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a global variable that is outside the handler in the Lambda function to store the table name.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  }
]