[
  {
    "id": 1,
    "question": "<p>A photo-sharing company is storing user profile pictures in an Amazon S3 bucket and an image analysis application is deployed on four Amazon EC2 instances. A solutions architect would like to trigger an image analysis procedure only on one of the four Amazon EC2 instances for each photo uploaded.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon EventBridge event that reacts to objects uploads in Amazon S3 and invokes one of the Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon S3 Event Notification that sends a message to an Amazon SNS topic. Subscribe the Amazon EC2 instances to the Amazon SNS topic",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Subscribe the Amazon EC2 instances to the Amazon S3 Inventory stream",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon S3 Event Notification that sends a message to an Amazon SQS queue. Make the Amazon EC2 instances read from the Amazon SQS queue",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nCreate an Amazon S3 Event Notification that sends a message to an Amazon SQS queue. Make the Amazon EC2 instances read from the Amazon SQS queue\n\nThe Amazon S3 event notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications.\n\nAmazon S3 supports the following destinations where it can publish events:\n\nAmazon Simple Notification Service (Amazon SNS) topic\n\nAmazon Simple Queue Service (Amazon SQS) queue\n\nAWS Lambda\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.\n\nHere we have to use Amazon S3 Event Notifications (which can send a message to either AWS Lambda, Amazon SNS, or Amazon SQS) to send a message to the Amazon SQS queue. By using Amazon SQS, we know only one Amazon EC2 instance among the four will pick up a message and process it.\n\nIncorrect options:\n\nSubscribe the Amazon EC2 instances to the Amazon S3 Inventory stream - Amazon S3 Inventory is a distractor. If you're curious - Amazon S3 inventory helps you manage your storage by creating lists of the objects in an Amazon S3 bucket on a defined schedule.\n\nCreate an Amazon EventBridge event that reacts to objects uploads in Amazon S3 and invokes one of the Amazon EC2 instances- Amazon EventBridge events cannot invoke applications on Amazon EC2 instances, so we have to rule out that answer.\n\nCreate an Amazon S3 Event Notification that sends a message to an Amazon SNS topic. Subscribe the Amazon EC2 instances to the Amazon SNS topic- Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\n\nUsing Amazon SNS would send a message to each Amazon EC2 instance via the Amazon SNS topic, therefore making all of them work for each upload. This is not the intended behavior.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\n\nhttps://aws.amazon.com/sqs/",
    "correctAnswerExplanations": [
      {
        "answer": "Create an Amazon S3 Event Notification that sends a message to an Amazon SQS queue. Make the Amazon EC2 instances read from the Amazon SQS queue",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The Amazon S3 event notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 supports the following destinations where it can publish events:"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Notification Service (Amazon SNS) topic"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (Amazon SQS) queue"
      },
      {
        "answer": "",
        "explanation": "AWS Lambda"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent."
      },
      {
        "answer": "",
        "explanation": "Here we have to use Amazon S3 Event Notifications (which can send a message to either AWS Lambda, Amazon SNS, or Amazon SQS) to send a message to the Amazon SQS queue. By using Amazon SQS, we know only one Amazon EC2 instance among the four will pick up a message and process it."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Subscribe the Amazon EC2 instances to the Amazon S3 Inventory stream</strong> - Amazon S3 Inventory is a distractor. If you're curious - Amazon S3 inventory helps you manage your storage by creating lists of the objects in an Amazon S3 bucket on a defined schedule."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon EventBridge event that reacts to objects uploads in Amazon S3 and invokes one of the Amazon EC2 instances</strong>- Amazon EventBridge events cannot invoke applications on Amazon EC2 instances, so we have to rule out that answer."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon S3 Event Notification that sends a message to an Amazon SNS topic. Subscribe the Amazon EC2 instances to the Amazon SNS topic</strong>- Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications."
      },
      {
        "answer": "",
        "explanation": "Using Amazon SNS would send a message to each Amazon EC2 instance via the Amazon SNS topic, therefore making all of them work for each upload. This is not the intended behavior."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html",
      "https://aws.amazon.com/sqs/"
    ]
  },
  {
    "id": 2,
    "question": "<p>A company has multiple Amazon EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on Amazon S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a Amazon DynamoDB table.</p>\n\n<p>How would you go about providing private access to these AWS resources which are not part of this custom VPC?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nCreate a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC\n\nEndpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.\n\nA VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.\n\nThere are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.\n\nA gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:\n\nAmazon S3\n\nAmazon DynamoDB\n\nIncorrect options:\n\nCreate a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC\n\nCreate a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC\n\nAmazon DynamoDB supports AWS PrivateLink. With AWS PrivateLink, you can simplify private network connectivity between virtual private clouds (VPCs), DynamoDB, and your on-premises data centers using interface VPC endpoints and private IP addresses. So, Amazon DynamoDB supports both interface endpoints as well as gateway endpoints. However, to use the interface endpoints, you need to connect to the given services using the private IP address, instead of creating an entry as a target in the route table of the custom VPC. Therefore, both these options are incorrect.\n\nCreate a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address - Origin Access Identity (OAI) is used within the context of Amazon CloudFront. To restrict access to content that you serve from Amazon S3 buckets, you can create a special Amazon CloudFront user called an origin access identity (OAI) and associate it with your distribution. You cannot use OAI to facilitate access to Amazon S3 from a VPC.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\n\nhttps://aws.amazon.com/about-aws/whats-new/2024/03/amazon-dynamodb-aws-privatelink/",
    "correctAnswerExplanations": [
      {
        "answer": "Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic."
      },
      {
        "answer": "",
        "explanation": "A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network."
      },
      {
        "answer": "",
        "explanation": "There are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service."
      },
      {
        "answer": "",
        "explanation": "A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:"
      },
      {
        "answer": "",
        "explanation": "Amazon S3"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC",
        "explanation": ""
      },
      {
        "answer": "Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB supports AWS PrivateLink. With AWS PrivateLink, you can simplify private network connectivity between virtual private clouds (VPCs), DynamoDB, and your on-premises data centers using interface VPC endpoints and private IP addresses. So, Amazon DynamoDB supports both interface endpoints as well as gateway endpoints. However, to use the interface endpoints, you need to connect to the given services using the private IP address, instead of creating an entry as a target in the route table of the custom VPC. Therefore, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address</strong> - Origin Access Identity (OAI) is used within the context of Amazon CloudFront. To restrict access to content that you serve from Amazon S3 buckets, you can create a special Amazon CloudFront user called an origin access identity (OAI) and associate it with your distribution. You cannot use OAI to facilitate access to Amazon S3 from a VPC."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://aws.amazon.com/about-aws/whats-new/2024/03/amazon-dynamodb-aws-privatelink/"
    ]
  },
  {
    "id": 3,
    "question": "<p>A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround time to set up the connection.</p>\n\n<p>What is the MOST cost-effective way to establish such a connection?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up a bastion host on Amazon EC2",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up an Internet Gateway between the on-premises data center and AWS cloud",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up AWS Direct Connect",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up an AWS Site-to-Site VPN connection",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nSet up an AWS Site-to-Site VPN connection\n\nBy default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network. You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection. A VPN connection refers to the connection between your VPC and your own on-premises network.\n\nAn AWS Site-to-Site VPN connection offers two VPN tunnels between a virtual private gateway or a transit gateway on the AWS side, and a customer gateway (which represents a VPN device) on the remote (on-premises) side.\n\nA virtual private gateway (VGW) is the VPN concentrator on the Amazon side of the AWS Site-to-Site VPN connection. You create a virtual private gateway and attach it to the VPC from which you want to create the AWS Site-to-Site VPN connection.\n\nHow virtual private gateway works: \n via - https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\n\nAn AWS transit gateway is a transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. For more information, see Amazon VPC Transit Gateways. You can create a Site-to-Site VPN connection as an attachment on a transit gateway.\n\nHow AWS transit gateway works: \n via - https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\n\nIncorrect options:\n\nSet up a bastion host on Amazon EC2 - A bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. The bastion host runs on an Amazon EC2 instance that is typically in a public subnet of your Amazon VPC. Other Amazon EC2 instances can be in a subnet that is not publicly accessible, and they are set up with a security group that allows SSH access from the security group attached to the underlying Amazon EC2 instance running the bastion host. A bastion host cannot be used to set up a connection between its on-premises data center and AWS Cloud.\n\nSet up AWS Direct Connect - AWS Direct Connect is a network service that provides an alternative to using the Internet to utilize AWS cloud services. AWS Direct Connect enables customers to have low latency, secure and private connections to AWS for workloads that require higher speed or lower latency than the internet. A Dedicated Connection is made through a 1 Gbps, 10 Gbps, or 100 Gbps Ethernet port dedicated to a single customer. AWS Direct Connect takes about a month to provision the connection, so this option is ruled out for the given use case.\n\nSet up an Internet Gateway between the on-premises data center and AWS cloud - An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An Internet Gateway cannot be used to set up a connection between its on-premises data center and AWS Cloud.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html",
    "correctAnswerExplanations": [
      {
        "answer": "Set up an AWS Site-to-Site VPN connection",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "By default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network. You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection. A VPN connection refers to the connection between your VPC and your own on-premises network."
      },
      {
        "answer": "",
        "explanation": "An AWS Site-to-Site VPN connection offers two VPN tunnels between a virtual private gateway or a transit gateway on the AWS side, and a customer gateway (which represents a VPN device) on the remote (on-premises) side."
      },
      {
        "answer": "",
        "explanation": "A virtual private gateway (VGW) is the VPN concentrator on the Amazon side of the AWS Site-to-Site VPN connection. You create a virtual private gateway and attach it to the VPC from which you want to create the AWS Site-to-Site VPN connection."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q25-i1.jpg",
        "answer": "",
        "explanation": "How virtual private gateway works:"
      },
      {
        "link": "https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html"
      },
      {
        "answer": "",
        "explanation": "An AWS transit gateway is a transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. For more information, see Amazon VPC Transit Gateways. You can create a Site-to-Site VPN connection as an attachment on a transit gateway."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q25-i2.jpg",
        "answer": "",
        "explanation": "How AWS transit gateway works:"
      },
      {
        "link": "https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up a bastion host on Amazon EC2</strong> - A bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. The bastion host runs on an Amazon EC2 instance that is typically in a public subnet of your Amazon VPC. Other Amazon EC2 instances can be in a subnet that is not publicly accessible, and they are set up with a security group that allows SSH access from the security group attached to the underlying Amazon EC2 instance running the bastion host. A bastion host cannot be used to set up a connection between its on-premises data center and AWS Cloud."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up AWS Direct Connect</strong> - AWS Direct Connect is a network service that provides an alternative to using the Internet to utilize AWS cloud services. AWS Direct Connect enables customers to have low latency, secure and private connections to AWS for workloads that require higher speed or lower latency than the internet. A Dedicated Connection is made through a 1 Gbps, 10 Gbps, or 100 Gbps Ethernet port dedicated to a single customer. AWS Direct Connect takes about a month to provision the connection, so this option is ruled out for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an Internet Gateway between the on-premises data center and AWS cloud</strong> - An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An Internet Gateway cannot be used to set up a connection between its on-premises data center and AWS Cloud."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html",
      "https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html",
      "https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html"
    ]
  },
  {
    "id": 4,
    "question": "<p>As a Solutions Architect, you have set up a database on a single Amazon EC2 instance that has an Amazon EBS volume of type gp2. You currently have 300 gigabytes of space on the gp2 device. The Amazon EC2 instance is of type m5.large. The database performance has recently been poor and upon looking at Amazon CloudWatch, you realize the IOPS on the Amazon EBS volume is maxing out. The disk size of the database must not change because of a licensing issue.</p>\n\n<p>How do you troubleshoot this issue?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Convert the gp2 volume to an io1",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Convert the Amazon EC2 instance to an i3.4xlarge",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Increase the IOPS on the gp2 volume",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Stop the Amazon CloudWatch agent to improve performance",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nAmazon EBS provides the following volume types, which differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications. The volumes types fall into two categories:\n\nSSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS\n\nHDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS\n\nConvert the gp2 volume to an io1\n\nProvisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.\n\nThe only solution is to convert the volume into an io1 volume. This will allow us to keep the same disk size while independently increasing the IOPS for that volume.\n\nIncorrect options:\n\nStop the Amazon CloudWatch agent to improve performance - The Amazon CloudWatch agent does not have any impact on the performance of the instance.\n\nIncrease the IOPS on the gp2 volume - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver their provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB.\n\nIOPS cannot be directly increased on a gp2 volume without increasing its size, which is not possible due to the question's constraints.\n\nConvert the Amazon EC2 instance to an i3.4xlarge - Converting the Amazon EC2 instance to i3.4xlarge won't improve the Amazon EBS drive's performance.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon EBS provides the following volume types, which differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications. The volumes types fall into two categories:"
      },
      {
        "answer": "",
        "explanation": "SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS"
      },
      {
        "answer": "",
        "explanation": "HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS"
      },
      {
        "answer": "Convert the gp2 volume to an io1",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time."
      },
      {
        "answer": "",
        "explanation": "The only solution is to convert the volume into an io1 volume. This will allow us to keep the same disk size while independently increasing the IOPS for that volume."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Stop the Amazon CloudWatch agent to improve performance</strong> - The Amazon CloudWatch agent does not have any impact on the performance of the instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Increase the IOPS on the gp2 volume</strong> - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver their provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB."
      },
      {
        "answer": "",
        "explanation": "IOPS cannot be directly increased on a gp2 volume without increasing its size, which is not possible due to the question's constraints."
      },
      {
        "answer": "",
        "explanation": "<strong>Convert the Amazon EC2 instance to an i3.4xlarge</strong> - Converting the Amazon EC2 instance to i3.4xlarge won't improve the Amazon EBS drive's performance."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops"
    ]
  },
  {
    "id": 5,
    "question": "<p>A company has moved its business critical data to Amazon Elastic File System (Amazon EFS) which will be accessed by multiple Amazon EC2 instances.</p>\n\n<p>As an AWS Certified Solutions Architect - Associate, which of the following would you recommend to exercise access control such that only the permitted Amazon EC2 instances can read from the Amazon EFS file system? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use an IAM policy to control access for clients who can mount your file system with the required permissions",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Use VPC security groups to control the network traffic to and from your file system",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct options:\n\nUse VPC security groups to control the network traffic to and from your file system\n\nUse an IAM policy to control access for clients who can mount your file system with the required permissions\n\nYou control which Amazon EC2 instances can access your Amazon EFS file system by using VPC security group rules and AWS Identity and Access Management (IAM) policies. Use VPC security groups to control the network traffic to and from your file system. Attach an IAM policy to your file system to control which clients can mount your file system and with what permissions, and you may use Amazon EFS Access Points to manage application access. Control access to files and directories with POSIX-compliant user and group-level permissions.\n\nFiles and directories in an Amazon EFS file system support standard Unix-style read, write, and execute permissions based on the user ID and group IDs. When an NFS client mounts an Amazon EFS file system without using an access point, the user ID and group ID provided by the client is trusted. You can also use Amazon EFS access points to override user ID and group IDs used by the NFS client. When users attempt to access files and directories, Amazon EFS checks their user IDs and group IDs to verify that each user has permission to access the objects.\n\nIncorrect options:\n\nUse network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance - Network ACLs operate at the subnet level and not at the instance level.\n\nSet up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system - There is no such thing as an IAM policy root credentials and this statement has been added as a distractor.\n\nUse Amazon GuardDuty to curb unwanted access to Amazon EFS file system - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. It cannot be used for access control to the Amazon EFS file system.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison\n\nhttps://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html\n\nhttps://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use VPC security groups to control the network traffic to and from your file system",
        "explanation": ""
      },
      {
        "answer": "Use an IAM policy to control access for clients who can mount your file system with the required permissions",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You control which Amazon EC2 instances can access your Amazon EFS file system by using VPC security group rules and AWS Identity and Access Management (IAM) policies. Use VPC security groups to control the network traffic to and from your file system. Attach an IAM policy to your file system to control which clients can mount your file system and with what permissions, and you may use Amazon EFS Access Points to manage application access. Control access to files and directories with POSIX-compliant user and group-level permissions."
      },
      {
        "answer": "",
        "explanation": "Files and directories in an Amazon EFS file system support standard Unix-style read, write, and execute permissions based on the user ID and group IDs. When an NFS client mounts an Amazon EFS file system without using an access point, the user ID and group ID provided by the client is trusted. You can also use Amazon EFS access points to override user ID and group IDs used by the NFS client. When users attempt to access files and directories, Amazon EFS checks their user IDs and group IDs to verify that each user has permission to access the objects."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance</strong> - Network ACLs operate at the subnet level and not at the instance level."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system</strong> - There is no such thing as an IAM policy root credentials and this statement has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. It cannot be used for access control to the Amazon EFS file system."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison",
      "https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html",
      "https://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html"
    ]
  },
  {
    "id": 6,
    "question": "<p>A company helps its customers legally sign highly confidential contracts. To meet the strong industry requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud using Amazon Simple Storage Service (Amazon S3) and would like you, the solution architect, to advise them on the encryption scheme to adopt.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Server-side encryption with AWS KMS keys (SSE-KMS)",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Client Side Encryption",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Server-side encryption with customer-provided keys (SSE-C)",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nClient Side Encryption\n\nClient-side encryption is the act of encrypting your data locally to help ensure its security in transit and at rest. To encrypt your objects before you send them to Amazon S3, use the Amazon S3 Encryption Client. When your objects are encrypted in this manner, your objects aren't exposed to any third party, including AWS. Amazon S3 receives your objects already encrypted; Amazon S3 does not play a role in encrypting or decrypting your objects. You can use both the Amazon S3 Encryption Client and server-side encryption to encrypt your data. When you send encrypted objects to Amazon S3, Amazon S3 doesn't recognize the objects as being encrypted, it only detects typical objects.\n\nIncorrect options:\n\nServer-side encryption with AWS KMS keys (SSE-KMS) - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom.\n\nServer-side encryption with Amazon S3 managed keys (SSE-S3) - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key.\n\nServer-side encryption with customer-provided keys (SSE-C) - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html",
    "correctAnswerExplanations": [
      {
        "answer": "Client Side Encryption",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Client-side encryption is the act of encrypting your data locally to help ensure its security in transit and at rest. To encrypt your objects before you send them to Amazon S3, use the Amazon S3 Encryption Client. When your objects are encrypted in this manner, your objects aren't exposed to any third party, including AWS. Amazon S3 receives your objects already encrypted; Amazon S3 does not play a role in encrypting or decrypting your objects. You can use both the Amazon S3 Encryption Client and server-side encryption to encrypt your data. When you send encrypted objects to Amazon S3, Amazon S3 doesn't recognize the objects as being encrypted, it only detects typical objects."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Server-side encryption with AWS KMS keys (SSE-KMS)</strong> - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom."
      },
      {
        "answer": "",
        "explanation": "<strong>Server-side encryption with Amazon S3 managed keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key."
      },
      {
        "answer": "",
        "explanation": "<strong>Server-side encryption with customer-provided keys (SSE-C)</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"
    ]
  },
  {
    "id": 7,
    "question": "<p>An application running on an Amazon EC2 instance needs to access a Amazon DynamoDB table in the same AWS account.</p>\n\n<p>Which of the following solutions should a solutions architect configure for the necessary permissions?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nSet up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance\n\nA service role is an IAM role that a service assumes to perform actions on your behalf. Service roles provide access only within your account and cannot be used to grant access to services in other accounts. An IAM administrator can create, modify, and delete a service role from within IAM. When you create the service role, you define the trusted entity in the definition.\n\nIf you are going to use the role with Amazon EC2 or another AWS service that uses Amazon EC2, you must store the role in an instance profile. An instance profile is a container for a role that can be attached to an Amazon EC2 instance when launched. An instance profile can contain only one role, and that limit cannot be increased. If you create the role using the AWS Management Console, the instance profile is created for you with the same name as the role.\n\nIncorrect options:\n\nSet up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly\n\nSet up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly\n\nYou should never store the IAM access credentials for a user in Amazon S3 or local storage or a database. It's a security bad practice. It is always recommended to use IAM roles to configure access to other AWS resources from Amazon EC2 instances. Therefore both these options are incorrect.\n\nSet up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role - There is no need for this option because when you create an IAM service role for Amazon EC2, the role automatically has Amazon EC2 identified as a trusted entity. Therefore this option is not correct.\n\nConfiguring a Service Role: \n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html",
    "correctAnswerExplanations": [
      {
        "answer": "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A service role is an IAM role that a service assumes to perform actions on your behalf. Service roles provide access only within your account and cannot be used to grant access to services in other accounts. An IAM administrator can create, modify, and delete a service role from within IAM. When you create the service role, you define the <code>trusted entity</code> in the definition."
      },
      {
        "answer": "",
        "explanation": "If you are going to use the role with Amazon EC2 or another AWS service that uses Amazon EC2, you must store the role in an instance profile. An instance profile is a container for a role that can be attached to an Amazon EC2 instance when launched. An instance profile can contain only one role, and that limit cannot be increased. If you create the role using the AWS Management Console, the instance profile is created for you with the same name as the role."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly",
        "explanation": ""
      },
      {
        "answer": "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You should never store the IAM access credentials for a user in Amazon S3 or local storage or a database. It's a security bad practice. It is always recommended to use IAM roles to configure access to other AWS resources from Amazon EC2 instances. Therefore both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role</strong> - There is no need for this option because when you create an IAM service role for Amazon EC2, the role automatically has Amazon EC2 identified as a trusted entity. Therefore this option is not correct."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q34-i1.jpg",
        "answer": "",
        "explanation": "Configuring a Service Role:"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html"
    ]
  },
  {
    "id": 8,
    "question": "<p>A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and the company now wants to explore an alternate solution on AWS.</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend that can provide support for quick and easy migration from RabbitMQ?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon SQS FIFO (First-In-First-Out)",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon MQ",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Amazon Simple Queue Service (Amazon SQS) Standard",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nAmazon MQ\n\nAmazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Message brokers allow different software systemsoften using different programming languages, and on different platformsto communicate and exchange information. If an organization is using messaging with existing applications and wants to move the messaging service to the cloud quickly and easily, AWS recommends Amazon MQ for such a use case. So this is the correct option.\n\nIncorrect options:\n\nAmazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS does not provide support for migration from RabbitMQ as its a fully managed pub/sub messaging service. Hence this option is incorrect.\n\nAmazon Simple Queue Service (Amazon SQS) Standard - Amazon SQS Standard offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. SQS Standard does not provide support for migration from RabbitMQ. Hence this option is incorrect.\n\nAmazon SQS FIFO (First-In-First-Out) - Amazon SQS FIFO (First-In-First-Out) has all the capabilities of the standard queue. They are used when the order of operations and events is critical, or where duplicates can't be tolerated. SQS FIFO does not provide support for migration from RabbitMQ. Hence this option is incorrect.\n\nReference:\n\nhttps://aws.amazon.com/amazon-mq/\n\nhttps://aws.amazon.com/blogs/compute/migrating-from-rabbitmq-to-amazon-mq/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon MQ",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Message brokers allow different software systemsoften using different programming languages, and on different platformsto communicate and exchange information. If an organization is using messaging with existing applications and wants to move the messaging service to the cloud quickly and easily, AWS recommends Amazon MQ for such a use case. So this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS does not provide support for migration from RabbitMQ as its a fully managed pub/sub messaging service. Hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (Amazon SQS) Standard</strong> - Amazon SQS Standard offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. SQS Standard does not provide support for migration from RabbitMQ. Hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon SQS FIFO (First-In-First-Out)</strong> - Amazon SQS FIFO (First-In-First-Out) has all the capabilities of the standard queue. They are used when the order of operations and events is critical, or where duplicates can't be tolerated. SQS FIFO does not provide support for migration from RabbitMQ. Hence this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/amazon-mq/",
      "https://aws.amazon.com/blogs/compute/migrating-from-rabbitmq-to-amazon-mq/"
    ]
  },
  {
    "id": 9,
    "question": "<p>A company has media files that need to be shared internally. Users are first authenticated using Active Directory and then they access files on a Microsoft Windows platform. The engineering manager wants to keep the same user permissions but wants the company to migrate the storage layer to AWS Cloud as the company is reaching its storage capacity limit on the on-premises infrastructure.</p>\n\n<p>What should a solutions architect recommend to meet this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up Amazon EFS and move all media files",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Provision Amazon EC2 with Windows OS, attach multiple Amazon EBS volumes, and move all media files",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a corporate Amazon S3 bucket and move all media files",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up Amazon FSx for Windows File Server and move all the media files",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nSet up Amazon FSx for Windows File Server and move all the media files\n\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. To support a wide spectrum of workloads, Amazon FSx provides high levels of throughput and IOPS and consistent sub-millisecond latencies.\n\nAmazon FSx file storage is accessible from Windows, Linux, and macOS compute instances and devices running on AWS or on-premises. Thousands of compute instances and devices can access a file system concurrently. Amazon FSx for Windows File Server supports Microsoft Active Directory (AD) integration so the same user permissions and access credentials can be used to access the files on FSx Windows File Server.\n\nIncorrect options:\n\nCreate a corporate Amazon S3 bucket and move all media files - Amazon S3 is object-based storage and it does not support file storage. Hence S3 is not the correct option.\n\nSet up Amazon EFS and move all media files - Amazon EFS provides scalable file storage for use with Amazon EC2. You can use an EFS file system as a common data source for workloads and applications running on multiple instances. EFS is not compatible with the Windows platform, so this option is ruled out.\n\nProvision Amazon EC2 with Windows OS, attach multiple Amazon EBS volumes, and move all media files - Multi-attach Amazon EBS volumes are supported only for Nitro EC2 instances which are Linux-based. So this option is ruled out.\n\nReference:\n\nhttps://aws.amazon.com/fsx/windows/",
    "correctAnswerExplanations": [
      {
        "answer": "Set up Amazon FSx for Windows File Server and move all the media files",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. To support a wide spectrum of workloads, Amazon FSx provides high levels of throughput and IOPS and consistent sub-millisecond latencies."
      },
      {
        "answer": "",
        "explanation": "Amazon FSx file storage is accessible from Windows, Linux, and macOS compute instances and devices running on AWS or on-premises. Thousands of compute instances and devices can access a file system concurrently. Amazon FSx for Windows File Server supports Microsoft Active Directory (AD) integration so the same user permissions and access credentials can be used to access the files on FSx Windows File Server."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a corporate Amazon S3 bucket and move all media files</strong> - Amazon S3 is object-based storage and it does not support file storage. Hence S3 is not the correct option."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon EFS and move all media files</strong> - Amazon EFS provides scalable file storage for use with Amazon EC2. You can use an EFS file system as a common data source for workloads and applications running on multiple instances. EFS is not compatible with the Windows platform, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision Amazon EC2 with Windows OS, attach multiple Amazon EBS volumes, and move all media files</strong> - Multi-attach Amazon EBS volumes are supported only for Nitro EC2 instances which are Linux-based. So this option is ruled out."
      }
    ],
    "references": [
      "https://aws.amazon.com/fsx/windows/"
    ]
  },
  {
    "id": 10,
    "question": "<p>You are looking to build an index of your files in Amazon S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in Amazon S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50 terabytes of data.</p>\n\n<p>How can you build this index efficiently?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nCreate an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS\n\nAmazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.\n\nUsing the Range HTTP header in a GET Object request, you can fetch a byte-range from an object, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. This helps you achieve higher aggregate throughput versus a single whole-object request. Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted.\n\nA byte-range request is a perfect way to get the beginning of a file and ensuring we remain efficient during our scan of our Amazon S3 bucket. So this is the correct option.\n\nIncorrect options:\n\nUse the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index - You cannot import data from Amazon S3 into Amazon RDS, so this option is incorrect.\n\nCreate an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS - If you build an application that loads all the files from Amazon S3, that would work, but you would read 50TB of data and that may be very expensive and slow. So this option is incorrect.\n\nCreate an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS - Amazon S3 Select is a new Amazon S3 capability designed to pull out only the data you need from an object, which can dramatically improve the performance and reduce the cost of applications that need to access data in Amazon S3. You cannot use Byte Range Fetch parameter with S3 Select to traverse the Amazon S3 bucket and get the first bytes of a file. So this option is incorrect.\n\nExam Alert:\n\nPlease note that with Amazon S3 Select, you can scan a subset of an object by specifying a range of bytes to query using the ScanRange parameter. This capability lets you parallelize scanning the whole object by splitting the work into separate Amazon S3 Select requests for a series of non-overlapping scan ranges. Use the Amazon S3 Select ScanRange parameter and Start at (Byte) and End at (Byte).\n\n via - https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range",
    "correctAnswerExplanations": [
      {
        "answer": "Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance."
      },
      {
        "answer": "",
        "explanation": "Using the Range HTTP header in a GET Object request, you can fetch a byte-range from an object, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. This helps you achieve higher aggregate throughput versus a single whole-object request. Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted."
      },
      {
        "answer": "",
        "explanation": "A byte-range request is a perfect way to get the beginning of a file and ensuring we remain efficient during our scan of our Amazon S3 bucket. So this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index</strong> - You cannot import data from Amazon S3 into Amazon RDS, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS</strong> - If you build an application that loads all the files from Amazon S3, that would work, but you would read 50TB of data and that may be very expensive and slow. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS</strong> - Amazon S3 Select is a new Amazon S3 capability designed to pull out only the data you need from an object, which can dramatically improve the performance and reduce the cost of applications that need to access data in Amazon S3. You cannot use Byte Range Fetch parameter with S3 Select to traverse the Amazon S3 bucket and get the first bytes of a file. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "answer": "",
        "explanation": "Please note that with Amazon S3 Select, you can scan a subset of an object by specifying a range of bytes to query using the ScanRange parameter. This capability lets you parallelize scanning the whole object by splitting the work into separate Amazon S3 Select requests for a series of non-overlapping scan ranges. Use the Amazon S3 Select ScanRange parameter and Start at (Byte) and End at (Byte)."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range"
    ]
  },
  {
    "id": 11,
    "question": "<p>During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used for cryptocurrency mining. The Amazon EC2 instance does not host any authorized application related to cryptocurrency mining.</p>\n\n<p>Which AWS service can be used to protect the Amazon EC2 instances from such unauthorized behavior in the future?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "AWS Web Application Firewall (AWS WAF)",
        "correct": false
      },
      {
        "id": 2,
        "answer": "AWS Firewall Manager",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon GuardDuty",
        "correct": true
      },
      {
        "id": 4,
        "answer": "AWS Shield Advanced",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nAmazon GuardDuty\n\nAmazon GuardDuty continuously monitors for malicious or unauthorized behavior to help protect your AWS resources, including your AWS accounts and access keys. Amazon GuardDuty identifies any unusual or unauthorized activity, like cryptocurrency mining or infrastructure deployments in a region that has never been used. Powered by threat intelligence and machine learning, GuardDuty is continuously evolving to help you protect your AWS environment.\n\nThe cryptocurrency finding expands the services ability to detect Amazon EC2 instances querying IP addresses associated with the cryptocurrency-related activity. The finding type is: CryptoCurrency:EC2/BitcoinTool.B, CryptoCurrency:EC2/BitcoinTool.B!DNS.\n\nThis finding informs you that the listed Amazon EC2 instance in your AWS environment is querying a domain name that is associated with Bitcoin or other cryptocurrency-related activity. Bitcoin is a worldwide cryptocurrency and digital payment system that can be exchanged for other currencies, products, and services. Bitcoin is a reward for bitcoin mining and is highly sought after by threat actors.\n\nIf you use the Amazon EC2 instance to mine or manage cryptocurrency, or this instance is otherwise involved in blockchain activity, this finding could represent expected activity for your environment. If this is the case in your AWS environment, AWS recommends that you set up a suppression rule for this finding.\n\nIncorrect options:\n\nAWS Web Application Firewall (AWS WAF) - AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting.\n\nAWS Shield Advanced - For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall. AWS Shield Advanced also gives you 24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS-related spikes in your Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 charges.\n\nAWS Firewall Manager - AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure, from a central administrator account.\n\nNone of these three services can detect unauthorized cryptocurrency mining activity on EC2 instances, so these options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#cryptocurrency-ec2-bitcointoolbdns",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon GuardDuty",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon GuardDuty continuously monitors for malicious or unauthorized behavior to help protect your AWS resources, including your AWS accounts and access keys. Amazon GuardDuty identifies any unusual or unauthorized activity, like cryptocurrency mining or infrastructure deployments in a region that has never been used. Powered by threat intelligence and machine learning, GuardDuty is continuously evolving to help you protect your AWS environment."
      },
      {
        "answer": "",
        "explanation": "The cryptocurrency finding expands the services ability to detect Amazon EC2 instances querying IP addresses associated with the cryptocurrency-related activity. The finding type is: CryptoCurrency:EC2/BitcoinTool.B, CryptoCurrency:EC2/BitcoinTool.B!DNS."
      },
      {
        "answer": "",
        "explanation": "This finding informs you that the listed Amazon EC2 instance in your AWS environment is querying a domain name that is associated with Bitcoin or other cryptocurrency-related activity. Bitcoin is a worldwide cryptocurrency and digital payment system that can be exchanged for other currencies, products, and services. Bitcoin is a reward for bitcoin mining and is highly sought after by threat actors."
      },
      {
        "answer": "",
        "explanation": "If you use the Amazon EC2 instance to mine or manage cryptocurrency, or this instance is otherwise involved in blockchain activity, this finding could represent expected activity for your environment. If this is the case in your AWS environment, AWS recommends that you set up a suppression rule for this finding."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Web Application Firewall (AWS WAF)</strong> - AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Shield Advanced</strong> - For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall. AWS Shield Advanced also gives you 24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS-related spikes in your Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 charges."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Firewall Manager</strong> - AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure, from a central administrator account."
      },
      {
        "answer": "",
        "explanation": "None of these three services can detect unauthorized cryptocurrency mining activity on EC2 instances, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#cryptocurrency-ec2-bitcointoolbdns"
    ]
  },
  {
    "id": 12,
    "question": "<p>An engineering team wants to orchestrate multiple Amazon ECS task types running on Amazon EC2 instances that are part of the Amazon ECS cluster. The output and state data for all tasks need to be stored. The amount of data output by each task is approximately 20 megabytes and there could be hundreds of tasks running at a time. As old outputs are archived, the storage size is not expected to exceed 1 terabyte.</p>\n\n<p>As a solutions architect, which of the following would you recommend as an optimized solution for high-frequency reading and writing?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon EFS with Provisioned Throughput mode",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon EFS with Bursting Throughput mode",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use an Amazon EBS volume mounted to the Amazon ECS cluster instances",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon DynamoDB table that is accessible by all ECS cluster instances",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nAmazon EFS file systems are distributed across an unconstrained number of storage servers. This distributed data storage design enables file systems to grow elastically to petabyte scale. It also enables massively parallel access from compute instances, including Amazon EC2, Amazon ECS, and AWS Lambda, to your data.\n\nUse Amazon EFS with Provisioned Throughput mode\n\nProvisioned Throughput mode is available for applications with high throughput to storage (MiB/s per TiB) ratios, or with requirements greater than those allowed by the Bursting Throughput mode. For example, say you're using Amazon EFS for development tools, web serving, or content management applications where the amount of data in your file system is low relative to throughput demands. Your file system can now get the high levels of throughput your applications require without having to pad your file system.\n\nIf your file system is in the Provisioned Throughput mode, you can increase the Provisioned Throughput of your file system as often as you want. You can decrease your file system throughput in Provisioned Throughput mode as long as it's been more than 24 hours since the last decrease. Additionally, you can change between Provisioned Throughput mode and the default Bursting Throughput mode as long as its been more than 24 hours since the last throughput mode change.\n\n via - https://docs.aws.amazon.com/efs/latest/ug/performance.html\n\nIncorrect options:\n\nUse Amazon EFS with Bursting Throughput mode - With Bursting Throughput mode, a file system's throughput scales as the amount of data stored in the standard storage class grows. File-based workloads are typically spiky, driving high levels of throughput for short periods of time, and low levels of throughput the rest of the time. To accommodate this, Amazon EFS is designed to burst to high throughput levels for periods of time. By default, AWS recommends that you run your application in the Bursting Throughput mode. But, if you're planning to migrate large amounts of data into your file system, consider switching to Provisioned Throughput mode.\n\nThe use-case mentions that the solution should be optimized for high-frequency reading and writing even when the old outputs are archived, therefore Provisioned Throughput mode is a better fit as it guarantees high levels of throughput your applications require without having to pad your file system.\n\nUse an Amazon EBS volume mounted to the Amazon ECS cluster instances - Amazon EFS has a higher throughput than Amazon EBS. In addition, Amazon EBS can be attached to multiple Amazon EC2 instances when the underlying EBS type is io1/io2 and the instance is of Nitro type. The use-case does not provide any such details, so this option is ruled out.\n\nUse Amazon DynamoDB table that is accessible by all ECS cluster instances - Amazon DynamoDB is not a fit for this scenario as each task output is 20 MB but the storage limit for each item in a Amazon DynamoDB table is 400 KB. You could write custom code to split the task output data into multiple items but it is not an optimal solution compared to using Amazon EFS in Provisioned Throughput mode.\n\nReferences:\n\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon EFS file systems are distributed across an unconstrained number of storage servers. This distributed data storage design enables file systems to grow elastically to petabyte scale. It also enables massively parallel access from compute instances, including Amazon EC2, Amazon ECS, and AWS Lambda, to your data."
      },
      {
        "answer": "Use Amazon EFS with Provisioned Throughput mode",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Provisioned Throughput mode is available for applications with high throughput to storage (MiB/s per TiB) ratios, or with requirements greater than those allowed by the Bursting Throughput mode. For example, say you're using Amazon EFS for development tools, web serving, or content management applications where the amount of data in your file system is low relative to throughput demands. Your file system can now get the high levels of throughput your applications require without having to pad your file system."
      },
      {
        "answer": "",
        "explanation": "If your file system is in the Provisioned Throughput mode, you can increase the Provisioned Throughput of your file system as often as you want. You can decrease your file system throughput in Provisioned Throughput mode as long as it's been more than 24 hours since the last decrease. Additionally, you can change between Provisioned Throughput mode and the default Bursting Throughput mode as long as its been more than 24 hours since the last throughput mode change."
      },
      {
        "link": "https://docs.aws.amazon.com/efs/latest/ug/performance.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EFS with Bursting Throughput mode</strong> - With Bursting Throughput mode, a file system's throughput scales as the amount of data stored in the standard storage class grows. File-based workloads are typically spiky, driving high levels of throughput for short periods of time, and low levels of throughput the rest of the time. To accommodate this, Amazon EFS is designed to burst to high throughput levels for periods of time. By default, AWS recommends that you run your application in the Bursting Throughput mode. But, if you're planning to migrate large amounts of data into your file system, consider switching to Provisioned Throughput mode."
      },
      {
        "answer": "",
        "explanation": "The use-case mentions that the solution should be optimized for high-frequency reading and writing even when the old outputs are archived, therefore Provisioned Throughput mode is a better fit as it guarantees high levels of throughput your applications require without having to pad your file system."
      },
      {
        "answer": "",
        "explanation": "<strong>Use an Amazon EBS volume mounted to the Amazon ECS cluster instances</strong> - Amazon EFS has a higher throughput than Amazon EBS. In addition, Amazon EBS can be attached to multiple Amazon EC2 instances when the underlying EBS type is io1/io2 and the instance is of Nitro type. The use-case does not provide any such details, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon DynamoDB table that is accessible by all ECS cluster instances</strong> - Amazon DynamoDB is not a fit for this scenario as each task output is 20 MB but the storage limit for each item in a Amazon DynamoDB table is 400 KB. You could write custom code to split the task output data into multiple items but it is not an optimal solution compared to using Amazon EFS in Provisioned Throughput mode."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/efs/latest/ug/performance.html",
      "https://docs.aws.amazon.com/efs/latest/ug/performance.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items"
    ]
  },
  {
    "id": 13,
    "question": "<p>Your company has created a data warehouse using Amazon Redshift that is used to analyze data from Amazon S3. From the usage pattern, you have detected that after 30 days, the data is rarely queried in Amazon Redshift and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum amount of cost on Amazon Redshift.</p>\n\n<p>What do you recommend? (Select two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Move the data to Amazon S3 Glacier Deep Archive after 30 days",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Analyze the cold data with Amazon Athena",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Move the data to Amazon S3 Standard IA after 30 days",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Migrate the Amazon Redshift underlying storage to Amazon S3 IA",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Create a smaller Amazon Redshift Cluster with the cold data",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct options:\n\nMove the data to Amazon S3 Standard IA after 30 days\n\nAmazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.\n\nAnalyze the cold data with Amazon Athena\n\nAmazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Amazon Athena to process logs, perform ad-hoc analysis, and run interactive queries.\n\nMoving the data to Amazon S3 glacier will prevent us from being able to query it. Therefore, we should migrate the data to Amazon S3 Standard IA and use Amazon Athena to analyze the cold data.\n\nIncorrect options:\n\nMigrate the Amazon Redshift underlying storage to Amazon S3 IA - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications.\n\nRedshift's internal storage does not have \"tiers\" of storage classes like Amazon S3, so this option is also ruled out.\n\nCreate a smaller Amazon Redshift Cluster with the cold data - Creating a smaller cluster with the cold data would not decrease the storage cost of Amazon Redshift, which will only increase with time as we keep on creating data. Therefore this option is ruled out.\n\nMove the data to Amazon S3 Glacier Deep Archive after 30 days - Amazon S3 Glacier Deep Archive (GDA) is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. GDA has a first-byte latency of several hours, so this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/athena/\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html",
    "correctAnswerExplanations": [
      {
        "answer": "Move the data to Amazon S3 Standard IA after 30 days",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days."
      },
      {
        "answer": "Analyze the cold data with Amazon Athena",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Amazon Athena to process logs, perform ad-hoc analysis, and run interactive queries."
      },
      {
        "answer": "",
        "explanation": "Moving the data to Amazon S3 glacier will prevent us from being able to query it.  Therefore, we should migrate the data to Amazon S3 Standard IA and use Amazon Athena to analyze the cold data."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Migrate the Amazon Redshift underlying storage to Amazon S3 IA</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications."
      },
      {
        "answer": "",
        "explanation": "Redshift's internal storage does not have \"tiers\" of storage classes like Amazon S3, so this option is also ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a smaller Amazon Redshift Cluster with the cold data</strong> - Creating a smaller cluster with the cold data would not decrease the storage cost of Amazon Redshift, which will only increase with time as we keep on creating data. Therefore this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Move the data to Amazon S3 Glacier Deep Archive after 30 days</strong> - Amazon S3 Glacier Deep Archive (GDA) is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. GDA has a first-byte latency of several hours, so this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/athena/",
      "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html"
    ]
  },
  {
    "id": 14,
    "question": "<p>A company uses a legacy on-premises reporting application that operates on gigabytes of .json files and represents years of data. The legacy application cannot handle the growing size of .json files. New .json files are added daily from various data sources to a central on-premises storage location. The company wants to continue to support the legacy application. The company has hired you as a solutions architect to build a solution that can manage ongoing data updates from your on-premises application to Amazon S3.</p>\n\n<p>Which of the following solutions would you suggest to address the given requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files between on-premises and Amazon Elastic File System (Amazon EFS). Enable replication from Amazon EFS to the company's Amazon S3 bucket",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up an on-premises volume gateway. Configure data sources to write the .json files to the volume gateway. Point the legacy analytics application to the volume gateway. The volume gateway should replicate data to Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up an on-premises file gateway. Configure data sources to write the .json files to the file gateway. Point the legacy analytics application to the file gateway. The file gateway should replicate the .json files to Amazon S3",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Set up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files between the company's on-premises storage and the company's Amazon S3 bucket",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nSet up an on-premises file gateway. Configure data sources to write the .json files to the file gateway. Point the legacy analytics application to the file gateway. The file gateway should replicate the .json files to Amazon S3\n\nA file gateway provides a simple solution for presenting one or more Amazon S3 buckets and their objects as a mountable NFS or SMB file share to one or more clients on-premises.\n\nThe file gateway is deployed as a virtual machine in VMware ESXi or Microsoft Hyper-V environments on-premises, or in an Amazon Elastic Compute Cloud (Amazon EC2) instance in AWS. File gateway can also be deployed in data center and remote office locations on a Storage Gateway hardware appliance. When deployed, file gateway provides a seamless connection between on-premises NFS (v3.0 or v4.1) or SMB (v1 or v2) clientstypically applicationsand Amazon S3 buckets hosted in a given AWS Region. The file gateway employs a local read/write cache to provide low-latency access to data for file share clients in the same local area network (LAN) as the file gateway.\n\nA bucket share consists of a file share hosted from a file gateway across a single Amazon S3 bucket. The file gateway virtual machine appliance currently supports up to 10 bucket shares.\n\nFile Gateway Architecture: \n via - https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html\n\nIncorrect options:\n\nSet up an on-premises volume gateway. Configure data sources to write the .json files to the volume gateway. Point the legacy analytics application to the volume gateway. The volume gateway should replicate data to Amazon S3 - The Volume Gateway provides block storage to your on-premises applications using iSCSI connectivity. Data on the volumes is stored in Amazon S3 and you can take point in time copies of volumes that are stored in AWS as Amazon EBS snapshots. Volume Gateway is for block storage and not for file storage, so it is not the right option.\n\nSet up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files between the company's on-premises storage and the company's Amazon S3 bucket\n\nSet up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files between on-premises and Amazon Elastic File System (Amazon EFS). Enable replication from Amazon EFS to the company's Amazon S3 bucket\n\nAWS recommends that you should use AWS DataSync to migrate existing data to Amazon S3, and subsequently use the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for ongoing updates from your on-premises file-based applications. Therefore, both these options are incorrect, as they use DataSync for ongoing replication.\n\nReference:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html",
    "correctAnswerExplanations": [
      {
        "answer": "Set up an on-premises file gateway. Configure data sources to write the .json files to the file gateway. Point the legacy analytics application to the file gateway. The file gateway should replicate the .json files to Amazon S3",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A file gateway provides a simple solution for presenting one or more Amazon S3 buckets and their objects as a mountable NFS or SMB file share to one or more clients on-premises."
      },
      {
        "answer": "",
        "explanation": "The file gateway is deployed as a virtual machine in VMware ESXi or Microsoft Hyper-V environments on-premises, or in an Amazon Elastic Compute Cloud (Amazon EC2) instance in AWS. File gateway can also be deployed in data center and remote office locations on a Storage Gateway hardware appliance. When deployed, file gateway provides a seamless connection between on-premises NFS (v3.0 or v4.1) or SMB (v1 or v2) clientstypically applicationsand Amazon S3 buckets hosted in a given AWS Region. The file gateway employs a local read/write cache to provide low-latency access to data for file share clients in the same local area network (LAN) as the file gateway."
      },
      {
        "answer": "",
        "explanation": "A bucket share consists of a file share hosted from a file gateway across a single Amazon S3 bucket. The file gateway virtual machine appliance currently supports up to 10 bucket shares."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q31-i1.jpg",
        "answer": "",
        "explanation": "File Gateway Architecture:"
      },
      {
        "link": "https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an on-premises volume gateway. Configure data sources to write the .json files to the volume gateway. Point the legacy analytics application to the volume gateway. The volume gateway should replicate data to Amazon S3</strong> - The Volume Gateway provides block storage to your on-premises applications using iSCSI connectivity. Data on the volumes is stored in Amazon S3 and you can take point in time copies of volumes that are stored in AWS as Amazon EBS snapshots. Volume Gateway is for block storage and not for file storage, so it is not the right option."
      },
      {
        "answer": "Set up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files between the company's on-premises storage and the company's Amazon S3 bucket",
        "explanation": ""
      },
      {
        "answer": "Set up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files between on-premises and Amazon Elastic File System (Amazon EFS). Enable replication from Amazon EFS to the company's Amazon S3 bucket",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS recommends that you should use AWS DataSync to migrate existing data to Amazon S3, and subsequently use the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for ongoing updates from your on-premises file-based applications. Therefore, both these options are incorrect, as they use DataSync for ongoing replication."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html",
      "https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html"
    ]
  },
  {
    "id": 15,
    "question": "<p>A development team has noticed that one of the Amazon EC2 instances has been incorrectly configured with the 'DeleteOnTermination' attribute set to True for its root EBS volume.</p>\n\n<p>As a Solution's Architect, can you suggest a way to disable this flag while the instance is still running?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set the DisableApiTermination attribute of the instance using the API",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Update the attribute using AWS management console. Select the Amazon EC2 instance and then uncheck the DeleteOnTermination check box for the root EBS volume",
        "correct": false
      },
      {
        "id": 3,
        "answer": "The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set the DeleteOnTermination attribute to False using the command line",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nWhen an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume and is set to False for all other volume types.\n\nSet the DeleteOnTermination attribute to False using the command line\n\nIf the instance is already running, you can set DeleteOnTermination to False using the command line.\n\nIncorrect options:\n\nUpdate the attribute using AWS management console. Select the Amazon EC2 instance and then uncheck the DeleteOnTermination check box for the root EBS volume - You can set the DeleteOnTermination attribute to False when you launch a new instance. It is not possible to update this attribute of a running instance from the AWS console.\n\nSet the DisableApiTermination attribute of the instance using the API - By default, you can terminate your instance using the Amazon EC2 console, command-line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance. The DisableApiTermination attribute controls whether the instance can be terminated using the console, CLI, or API. This option cannot be used to control the delete status for the EBS volume when the instance terminates.\n\nThe attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag - This statement is wrong and given only as a distractor.\n\nReferences:\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume and is set to False for all other volume types."
      },
      {
        "answer": "Set the DeleteOnTermination attribute to False using the command line",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "If the instance is already running, you can set <code>DeleteOnTermination</code> to False using the command line."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Update the attribute using AWS management console. Select the Amazon EC2 instance and then uncheck the <code>DeleteOnTermination</code> check box for the root EBS volume</strong> - You can set the <code>DeleteOnTermination</code> attribute to False when you launch a new instance. It is not possible to update this attribute of a running instance from the AWS console."
      },
      {
        "answer": "",
        "explanation": "<strong>Set the <code>DisableApiTermination</code> attribute of the instance using the API</strong> - By default, you can terminate your instance using the Amazon EC2 console, command-line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance. The <code>DisableApiTermination</code> attribute controls whether the instance can be terminated using the console, CLI, or API. This option cannot be used to control the delete status for the EBS volume when the instance terminates."
      },
      {
        "answer": "",
        "explanation": "<strong>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</strong> - This statement is wrong and given only as a distractor."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance"
    ]
  },
  {
    "id": 16,
    "question": "<p>A software engineering intern at a company is documenting the features offered by Amazon EC2 Spot instances and Spot fleets.</p>\n\n<p>Can you help the intern by selecting the correct options that identify the key characteristics of these two types of Spot entities? (Select two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
        "correct": false
      },
      {
        "id": 2,
        "answer": "A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity",
        "correct": false
      },
      {
        "id": 3,
        "answer": "A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct options:\n\nSpot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification\n\nSpot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices that Amazon Web Services can interrupt with a 2-minute notification. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted.\n\nA Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity\n\nA Spot fleet is a collection, or fleet, of Spot Instances, and optionally On-Demand Instances. The Spot fleet attempts to launch the number of Spot Instances and On-Demand Instances to meet the target capacity that you specified in the Spot fleet request. A Spot fleet allows you to automatically request and manage multiple Spot instances that provide the lowest price per unit of capacity for your cluster or application, like a batch processing job, a Hadoop workflow, or an HPC grid computing job.\n\n via - https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html\n\nIncorrect options:\n\nA Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity\n\nSpot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification\n\nThese two options contradict the explanation provided above, so these options are incorrect.\n\nSpot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted - You could use Spot blocks (now deprecated) to request Amazon EC2 Spot instances for 1 to 6 hours to avoid being interrupted. So, Spot fleets cannot be used for this purpose.\n\nReferences:\n\nhttps://www.amazonaws.cn/en/ec2/spot-instances/faqs/\n\nhttps://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html",
    "correctAnswerExplanations": [
      {
        "answer": "Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices that Amazon Web Services can interrupt with a 2-minute notification. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted."
      },
      {
        "answer": "A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A Spot fleet is a collection, or fleet, of Spot Instances, and optionally On-Demand Instances. The Spot fleet attempts to launch the number of Spot Instances and On-Demand Instances to meet the target capacity that you specified in the Spot fleet request. A Spot fleet allows you to automatically request and manage multiple Spot instances that provide the lowest price per unit of capacity for your cluster or application, like a batch processing job, a Hadoop workflow, or an HPC grid computing job."
      },
      {
        "link": "https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity",
        "explanation": ""
      },
      {
        "answer": "Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These two options contradict the explanation provided above, so these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted</strong> - You could use Spot blocks (now deprecated) to request Amazon EC2 Spot instances for 1 to 6 hours to avoid being interrupted. So, Spot fleets cannot be used for this purpose."
      }
    ],
    "references": [
      "https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html",
      "https://www.amazonaws.cn/en/ec2/spot-instances/faqs/",
      "https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html"
    ]
  },
  {
    "id": 17,
    "question": "<p>A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines.</p>\n\n<p>Which of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Spot Instances",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Dedicated Instances",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Dedicated Hosts",
        "correct": false
      },
      {
        "id": 4,
        "answer": "On-Demand Instances",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nDedicated Instances\n\nDedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.\n\nA Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.\n\nDifferences between Dedicated Hosts and Dedicated Instances: \n via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances\n\nIncorrect options:\n\nSpot Instances - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option.\n\nDedicated Hosts - An Amazon EC2 Dedicated Host is a physical server with Amazon EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on Amazon EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement.\n\nOn-Demand Instances - With On-Demand Instances, you pay for the compute capacity by the second with no long-term commitments. You have full control over its lifecycleyou decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements.\n\nHigh Level Overview of Amazon EC2 Instance Purchase Options: \n via - https://aws.amazon.com/ec2/pricing/\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html",
    "correctAnswerExplanations": [
      {
        "answer": "Dedicated Instances",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances."
      },
      {
        "answer": "",
        "explanation": "A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q21-i1.jpg",
        "answer": "",
        "explanation": "Differences between Dedicated Hosts and Dedicated Instances:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Spot Instances</strong> -  A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price.  Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option."
      },
      {
        "answer": "",
        "explanation": "<strong>Dedicated Hosts</strong> - An Amazon EC2 Dedicated Host is a physical server with Amazon EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on Amazon EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement."
      },
      {
        "answer": "",
        "explanation": "<strong>On-Demand Instances</strong> - With On-Demand Instances, you pay for the compute capacity by the second with no long-term commitments. You have full control over its lifecycleyou decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q21-i2.jpg",
        "answer": "",
        "explanation": "High Level Overview of Amazon EC2 Instance Purchase Options:"
      },
      {
        "link": "https://aws.amazon.com/ec2/pricing/"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances",
      "https://aws.amazon.com/ec2/pricing/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html"
    ]
  },
  {
    "id": 18,
    "question": "<p>The engineering team at a startup is evaluating the most optimal block storage volume type for the Amazon EC2 instances hosting its flagship application. The storage volume should support very low latency but it does not need to persist the data when the instance terminates. As a solutions architect, you have proposed using Instance Store volumes to meet these requirements.</p>\n\n<p>Which of the following would you identify as the key characteristics of the Instance Store volumes? (Select two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation",
        "correct": false
      },
      {
        "id": 2,
        "answer": "You can't detach an instance store volume from one instance and attach it to a different instance",
        "correct": true
      },
      {
        "id": 3,
        "answer": "If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved",
        "correct": true
      },
      {
        "id": 4,
        "answer": "You can specify instance store volumes for an instance when you launch or restart it",
        "correct": false
      },
      {
        "id": 5,
        "answer": "An instance store is a network storage type",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct options:\n\nYou can't detach an instance store volume from one instance and attach it to a different instance\n\nYou can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance. The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists.\n\nIf you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved\n\nIf you create an AMI from an instance, the data on its instance store volumes isn't preserved and isn't present on the instance store volumes of the instances that you launch from the AMI.\n\nIncorrect options:\n\nInstance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation - When you stop, hibernate, or terminate an instance, every block of storage in the instance store is reset. Therefore, this option is incorrect.\n\nYou can specify instance store volumes for an instance when you launch or restart it - You can specify instance store volumes for an instance only when you launch it.\n\nAn instance store is a network storage type - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
    "correctAnswerExplanations": [
      {
        "answer": "You can't detach an instance store volume from one instance and attach it to a different instance",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance. The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists."
      },
      {
        "answer": "If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "If you create an AMI from an instance, the data on its instance store volumes isn't preserved and isn't present on the instance store volumes of the instances that you launch from the AMI."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation</strong> - When you stop, hibernate, or terminate an instance, every block of storage in the instance store is reset. Therefore, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>You can specify instance store volumes for an instance when you launch or restart it</strong> - You can specify instance store volumes for an instance only when you launch it."
      },
      {
        "answer": "",
        "explanation": "<strong>An instance store is a network storage type</strong> - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html"
    ]
  },
  {
    "id": 19,
    "question": "<p>You have deployed a database technology that has a synchronous replication mode to survive disasters in data centers. The database is therefore deployed on two Amazon EC2 instances in two Availability Zones (AZs). The database must be publicly available so you have deployed the Amazon EC2 instances in public subnets. The replication protocol currently uses the Amazon EC2 public IP addresses.</p>\n\n<p>What can you do to decrease the replication cost?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an Elastic Fabric Adapter (EFA)",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use the Amazon EC2 instances private IP for the replication",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create a Private Link between the two Amazon EC2 instances",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nUse the Amazon EC2 instances private IP for the replication\n\nThe source of the cost is that traffic between two EC2 instances is going over the public internet, thus incurring high costs. Here, the correct answer is to use Private IP, so that the network remains private, for a minimal cost.\n\nIncorrect options:\n\nAssign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication - Using Elastic IPs will not solve the problem as the traffic will still be going over the public internet.\n\nCreate a Private Link between the two Amazon EC2 instances - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network.\n\nPrivate Link is a distractor in this question. Private Link is leveraged to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering and allowing the connections between the two to remain within the AWS network.\n\nUse an Elastic Fabric Adapter (EFA) - The Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run HPC applications requiring high levels of inter-instance communications, like computational fluid dynamics, weather modeling, and reservoir simulation, at scale on AWS. This option is not relevant to the given use-case.\n\nReferences:\n\nhttps://aws.amazon.com/privatelink/\n\nhttps://aws.amazon.com/hpc/efa/",
    "correctAnswerExplanations": [
      {
        "answer": "Use the Amazon EC2 instances private IP for the replication",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The source of the cost is that traffic between two EC2 instances is going over the public internet, thus incurring high costs. Here, the correct answer is to use Private IP, so that the network remains private, for a minimal cost."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication</strong> - Using Elastic IPs will not solve the problem as the traffic will still be going over the public internet."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a Private Link between the two Amazon EC2 instances</strong> - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network."
      },
      {
        "answer": "",
        "explanation": "Private Link is a distractor in this question. Private Link is leveraged to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering and allowing the connections between the two to remain within the AWS network."
      },
      {
        "answer": "",
        "explanation": "<strong>Use an Elastic Fabric Adapter (EFA)</strong> - The Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run HPC applications requiring high levels of inter-instance communications, like computational fluid dynamics, weather modeling, and reservoir simulation, at scale on AWS. This option is not relevant to the given use-case."
      }
    ],
    "references": [
      "https://aws.amazon.com/privatelink/",
      "https://aws.amazon.com/hpc/efa/"
    ]
  },
  {
    "id": 20,
    "question": "<p>A development team is looking for a solution that saves development time and deployment costs for an application that uses a high-throughput request-response message pattern.</p>\n\n<p>Which of the following Amazon SQS queue types is the best fit to meet this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon Simple Queue Service (Amazon SQS) FIFO queues",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon Simple Queue Service (Amazon SQS) delay queues",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon Simple Queue Service (Amazon SQS) dead-letter queues",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon Simple Queue Service (Amazon SQS) temporary queues",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nAmazon Simple Queue Service (Amazon SQS) temporary queues\n\nTemporary queues help you save development time and deployment costs when using common message patterns such as request-response. You can use the Temporary Queue Client to create high-throughput, cost-effective, application-managed temporary queues.\n\nThe client maps multiple temporary queuesapplication-managed queues created on demand for a particular processonto a single Amazon SQS queue automatically. This allows your application to make fewer API calls and have a higher throughput when the traffic to each temporary queue is low. When a temporary queue is no longer in use, the client cleans up the temporary queue automatically, even if some processes that use the client aren't shut down cleanly.\n\nThe following are the benefits of temporary queues:\n\nThey serve as lightweight communication channels for specific threads or processes.\n\nThey can be created and deleted without incurring additional costs.\n\nThey are API-compatible with static (normal) Amazon SQS queues. This means that existing code that sends and receives messages can send messages to and receive messages from virtual queues.\n\nTo better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary Queue Client. This client makes it easy to create and delete many temporary messaging destinations without inflating your AWS bill. The key concept behind the client is the virtual queue. Virtual queues let you multiplex many low-traffic queues onto a single Amazon SQS queue. Creating a virtual queue only instantiates a local buffer to hold messages for consumers as they arrive; there is no API call to SQS and no costs associated with creating a virtual queue.\n\nEnd-to-end process for sending messages through virtual queues: \n via - https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/\n\nIncorrect options:\n\nAmazon Simple Queue Service (Amazon SQS) dead-letter queues - Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. Amazon SQS does not create the dead-letter queue automatically. You must first create the queue before using it as a dead-letter queue.\n\nAmazon Simple Queue Service (Amazon SQS) FIFO queues - Amazon SQS FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated. FIFO queues also provide exactly-once processing but have a limited number of transactions per second (TPS).\n\nAmazon Simple Queue Service (Amazon SQS) delay queues - Delay queues let you postpone the delivery of new messages to a queue for a number of seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-temporary-queues.html\n\nhttps://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon Simple Queue Service (Amazon SQS) temporary queues",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Temporary queues help you save development time and deployment costs when using common message patterns such as request-response. You can use the Temporary Queue Client to create high-throughput, cost-effective, application-managed temporary queues."
      },
      {
        "answer": "",
        "explanation": "The client maps multiple temporary queuesapplication-managed queues created on demand for a particular processonto a single Amazon SQS queue automatically. This allows your application to make fewer API calls and have a higher throughput when the traffic to each temporary queue is low. When a temporary queue is no longer in use, the client cleans up the temporary queue automatically, even if some processes that use the client aren't shut down cleanly."
      },
      {
        "answer": "",
        "explanation": "The following are the benefits of temporary queues:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>They serve as lightweight communication channels for specific threads or processes.</p></li>\n<li><p>They can be created and deleted without incurring additional costs.</p></li>\n<li><p>They are API-compatible with static (normal) Amazon SQS queues. This means that existing code that sends and receives messages can send messages to and receive messages from virtual queues.</p></li>\n</ol>"
      },
      {
        "answer": "",
        "explanation": "To better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary Queue Client. This client makes it easy to create and delete many temporary messaging destinations without inflating your AWS bill. The key concept behind the client is the virtual queue. Virtual queues let you multiplex many low-traffic queues onto a single Amazon SQS queue. Creating a virtual queue only instantiates a local buffer to hold messages for consumers as they arrive; there is no API call to SQS and no costs associated with creating a virtual queue."
      },
      {
        "image": "https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2019/07/26/Selection_015.png",
        "answer": "",
        "explanation": "End-to-end process for sending messages through virtual queues:"
      },
      {
        "link": "https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (Amazon SQS) dead-letter queues</strong> - Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. Amazon SQS does not create the dead-letter queue automatically. You must first create the queue before using it as a dead-letter queue."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (Amazon SQS) FIFO queues</strong> - Amazon SQS FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated. FIFO queues also provide exactly-once processing but have a limited number of transactions per second (TPS)."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (Amazon SQS) delay queues</strong> - Delay queues let you postpone the delivery of new messages to a queue for a number of seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-temporary-queues.html",
      "https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/"
    ]
  },
  {
    "id": 21,
    "question": "<p>The CTO of an online home rental marketplace wants to re-engineer the caching layer of the current architecture for its relational database. The CTO wants the caching layer to have replication and archival support built into the architecture.</p>\n\n<p>Which of the following AWS service offers the capabilities required for the re-engineering of the caching layer?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon DocumentDB",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon DynamoDB Accelerator (DAX)",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon ElastiCache for Redis",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Amazon ElastiCache for Memcached",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nAmazon ElastiCache for Redis\n\nAmazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache for Redis supports replication and archival snapshots right out of the box. Hence this is the correct option.\n\nExam Alert:\n\nPlease review this comparison sheet for Redis vs Memcached features: \n via - https://aws.amazon.com/elasticache/redis-vs-memcached/\n\nIncorrect options:\n\nAmazon ElastiCache for Memcached - Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database. Session stores are easy to create with Amazon ElastiCache for Memcached. ElastiCache for Memcached does not support replication and archival snapshots, so this option is ruled out.\n\nAmazon DynamoDB Accelerator (DAX) - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. DAX cannot be used as a caching layer for a relational database.\n\nAmazon DocumentDB - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. DocumentDB cannot be used as a caching layer for a relational database.\n\nReferences:\n\nhttps://aws.amazon.com/elasticache/redis/\n\nhttps://aws.amazon.com/elasticache/redis-vs-memcached/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon ElastiCache for Redis",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache for Redis supports replication and archival snapshots right out of the box. Hence this is the correct option."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q10-i1.jpg",
        "answer": "",
        "explanation": "Please review this comparison sheet for Redis vs Memcached features:"
      },
      {
        "link": "https://aws.amazon.com/elasticache/redis-vs-memcached/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon ElastiCache for Memcached</strong> - Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database. Session stores are easy to create with Amazon ElastiCache for Memcached. ElastiCache for Memcached does not support replication and archival snapshots, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. DAX cannot be used as a caching layer for a relational database."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon DocumentDB</strong> - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. DocumentDB cannot be used as a caching layer for a relational database."
      }
    ],
    "references": [
      "https://aws.amazon.com/elasticache/redis-vs-memcached/",
      "https://aws.amazon.com/elasticache/redis/",
      "https://aws.amazon.com/elasticache/redis-vs-memcached/"
    ]
  },
  {
    "id": 22,
    "question": "<p>A systems administration team has a requirement to run certain custom scripts only once during the launch of the Amazon Elastic Compute Cloud (Amazon EC2) instances that host their application.</p>\n\n<p>Which of the following represents the best way of configuring a solution for this requirement with minimal effort?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS CLI to run the user data scripts only once while launching the instance",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Run the custom scripts as user data scripts on the Amazon EC2 instances",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Run the custom scripts as instance metadata scripts on the Amazon EC2 instances",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nRun the custom scripts as user data scripts on the Amazon EC2 instances\n\nWhen you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives.\n\nBy default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. Hence, no extra configuration is needed, apart from including the custom scripts in user data scripts.\n\nIncorrect options:\n\nUpdate Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process - You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance. By default, the scripts are run, only once during the boot process while first launching the instance.\n\nRun the custom scripts as instance metadata scripts on the Amazon EC2 instances- Instance metadata is data about your instance that you can use to configure or manage the running instance. Metadata cannot be used to run custom scripts.\n\nUse AWS CLI to run the user data scripts only once while launching the instance - This statement is incorrect and used only as a distractor.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html",
    "correctAnswerExplanations": [
      {
        "answer": "Run the custom scripts as user data scripts on the Amazon EC2 instances",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "When you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives."
      },
      {
        "answer": "",
        "explanation": "By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. Hence, no extra configuration is needed, apart from including the custom scripts in user data scripts."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process</strong> - You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance. By default, the scripts are run, only once during the boot process while first launching the instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Run the custom scripts as instance metadata scripts on the Amazon EC2 instances</strong>- Instance metadata is data about your instance that you can use to configure or manage the running instance. Metadata cannot be used to run custom scripts."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS CLI to run the user data scripts only once while launching the instance</strong> - This statement is incorrect and used only as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html"
    ]
  },
  {
    "id": 23,
    "question": "<p>A startup wants to create a highly available architecture for its multi-tier application. Currently, the startup manages a single Amazon EC2 instance along with a single Amazon RDS MySQL DB instance. The startup has hired you as an AWS Certified Solutions Architect - Associate to build a solution that meets these requirements while minimizing the underlying infrastructure maintenance effort.</p>\n\n<p>What will you recommend?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nCreate an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration\n\nAmazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application.\n\n via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\n\nApplication Load Balancer automatically distributes your incoming traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets.\n\n via - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\n\nIn a multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across Availability Zones to the standby to keep both in sync and protect your latest database updates against DB instance failure.\n\n via - https://aws.amazon.com/rds/features/multi-az/\n\nTo create a highly available architecture for the given use case, you need to set up an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones and then point the Application Load Balancer to the target group having the Amazon EC2 instances.\n\nIncorrect options:\n\nCreate an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone - A read replica cannot be used to enhance the availability of an Amazon RDS MySQL DB. You must use the multi-AZ configuration of Amazon RDS MySQL for this use case.\n\nCreate an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration - Having the Amazon EC2 instances in a single Availability Zone will not create a highly available solution. In the case of an outage for the entire Availability Zone, the Amazon EC2 instances would be unreachable. Hence this option is incorrect.\n\nProvision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs - This option has been added as a distractor. It requires significant monitoring and development effort to keep the Amazon EC2 instances highly available as well as keep the MySQL DBs in sync.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\n\nhttps://aws.amazon.com/rds/features/multi-az/",
    "correctAnswerExplanations": [
      {
        "answer": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application."
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html"
      },
      {
        "answer": "",
        "explanation": "Application Load Balancer automatically distributes your incoming traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets."
      },
      {
        "link": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html"
      },
      {
        "answer": "",
        "explanation": "In a multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across Availability Zones to the standby to keep both in sync and protect your latest database updates against DB instance failure."
      },
      {
        "link": "https://aws.amazon.com/rds/features/multi-az/"
      },
      {
        "answer": "",
        "explanation": "To create a highly available architecture for the given use case, you need to set up an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones and then point the Application Load Balancer to the target group having the Amazon EC2 instances."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone</strong> - A read replica cannot be used to enhance the availability of an Amazon RDS MySQL DB. You must use the multi-AZ configuration of Amazon RDS MySQL for this use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration</strong> - Having the Amazon EC2 instances in a single Availability Zone will not create a highly available solution. In the case of an outage for the entire Availability Zone, the Amazon EC2 instances would be unreachable. Hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs</strong> - This option has been added as a distractor. It requires significant monitoring and development effort to keep the Amazon EC2 instances highly available as well as keep the MySQL DBs in sync."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://aws.amazon.com/rds/features/multi-az/"
    ]
  },
  {
    "id": 24,
    "question": "<p>A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should also support configuring a trust relationship with any existing on-premises Microsoft Active Directory.</p>\n\n<p>Which AWS Directory Service is the best fit for this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Active Directory Connector",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Simple Active Directory (Simple AD)",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Transit Gateway",
        "correct": false
      },
      {
        "id": 4,
        "answer": "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nAWS Directory Service lets you run Microsoft Active Directory (AD) as a managed service. AWS Directory Service for Microsoft Active Directory, also referred to as AWS Managed Microsoft AD, is powered by Windows Server 2012 R2. When you select and launch this directory type, it is created as a highly available pair of domain controllers connected to your virtual private cloud (VPC).\n\nWith AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud, including Microsoft SharePoint and custom .NET and SQL Server-based applications. You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO).\n\nAWS Managed Microsoft AD is your best choice if you need actual Active Directory features to support AWS applications or Windows workloads, including Amazon Relational Database Service for Microsoft SQL Server. It's also best if you want a standalone AD in the AWS Cloud that supports Office 365 or you need an LDAP directory to support your Linux applications.\n\nIncorrect options:\n\nActive Directory Connector - AD Connector is a directory gateway with which you can redirect directory requests to your on-premises Microsoft Active Directory without caching any information in the cloud. AD Connector is your best choice when you want to use your existing on-premises directory with compatible AWS services.\n\nSimple Active Directory (Simple AD) - Simple AD is a standalone directory in the cloud, where you create and manage user identities and manage access to applications. Simple AD provides a subset of the features offered by AWS Managed Microsoft AD. However, note that Simple AD does not support features such as multi-factor authentication (MFA), trust relationships with other domains, Active Directory Administrative Center, PowerShell support, Active Directory recycle bin, group managed service accounts, and schema extensions for POSIX and Microsoft applications.\n\nAWS Transit Gateway - AWS Transit Gateway connects VPCs and on-premises networks through a central hub. Transit Gateway is not an Active Directory service.\n\nReferences:\n\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html\n\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html\n\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html",
    "correctAnswerExplanations": [
      {
        "answer": "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Directory Service lets you run Microsoft Active Directory (AD) as a managed service. AWS Directory Service for Microsoft Active Directory, also referred to as AWS Managed Microsoft AD, is powered by Windows Server 2012 R2. When you select and launch this directory type, it is created as a highly available pair of domain controllers connected to your virtual private cloud (VPC)."
      },
      {
        "answer": "",
        "explanation": "With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud, including Microsoft SharePoint and custom .NET and SQL Server-based applications. You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO)."
      },
      {
        "answer": "",
        "explanation": "AWS Managed Microsoft AD is your best choice if you need actual Active Directory features to support AWS applications or Windows workloads, including Amazon Relational Database Service for Microsoft SQL Server. It's also best if you want a standalone AD in the AWS Cloud that supports Office 365 or you need an LDAP directory to support your Linux applications."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Active Directory Connector</strong> - AD Connector is a directory gateway with which you can redirect directory requests to your on-premises Microsoft Active Directory without caching any information in the cloud. AD Connector is your best choice when you want to use your existing on-premises directory with compatible AWS services."
      },
      {
        "answer": "",
        "explanation": "<strong>Simple Active Directory (Simple AD)</strong> - Simple AD is a standalone directory in the cloud, where you create and manage user identities and manage access to applications. Simple AD provides a subset of the features offered by AWS Managed Microsoft AD. However, note that Simple AD does not support features such as multi-factor authentication (MFA), trust relationships with other domains, Active Directory Administrative Center, PowerShell support, Active Directory recycle bin, group managed service accounts, and schema extensions for POSIX and Microsoft applications."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Transit Gateway</strong> - AWS Transit Gateway connects VPCs and on-premises networks through a central hub. Transit Gateway is not an Active Directory service."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html",
      "https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html",
      "https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html"
    ]
  },
  {
    "id": 25,
    "question": "<p>A healthcare company runs a fleet of Amazon EC2 instances in two private subnets (named PR1 and PR2) across two Availability Zones (AZs) named A1 and A2. The Amazon EC2 instances need access to the internet for operating system patch management and third-party software maintenance. To facilitate this, the engineering team at the company wants to set up two Network Address Translation gateways (NAT gateways) in a highly available configuration.</p>\n\n<p>Which of the following options would you suggest?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nSet up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2\n\nA NAT gateway is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances.\n\nFor the given use case, the Amazon EC2 instances in the private subnets can connect to the internet through public NAT gateways in their respective Availability Zones (AZ). You should create public NAT gateway in the public subnet of each AZ and must associate an elastic IP address with the NAT gateway at creation. Then, you can route traffic from the NAT gateway to the internet gateway for the VPC.\n\nIf you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateways Availability Zone is down, resources in the other Availability Zones lose internet access. To create a highly available or an Availability Zone independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone.\n\n via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\n\nIncorrect options:\n\nSet up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2 - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create NAT gateways in the private subnet for the given use case.\n\nSet up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2 - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create both NAT gateways in a single public subnet, as this configuration would not be highly available.\n\nSet up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2 - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create a single NAT gateway, as this configuration would not be highly available.\n\nReference:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
    "correctAnswerExplanations": [
      {
        "answer": "Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A NAT gateway is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances."
      },
      {
        "answer": "",
        "explanation": "For the given use case, the Amazon EC2 instances in the private subnets can connect to the internet through public NAT gateways in their respective Availability Zones (AZ). You should create public NAT gateway in the public subnet of each AZ and must associate an elastic IP address with the NAT gateway at creation. Then, you can route traffic from the NAT gateway to the internet gateway for the VPC."
      },
      {
        "answer": "",
        "explanation": "If you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateways Availability Zone is down, resources in the other Availability Zones lose internet access. To create a highly available or an Availability Zone independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone."
      },
      {
        "link": "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2</strong> - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create NAT gateways in the private subnet for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2</strong> - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create both NAT gateways in a single public subnet, as this configuration would not be highly available."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2</strong> - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create a single NAT gateway, as this configuration would not be highly available."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"
    ]
  },
  {
    "id": 26,
    "question": "<p>A Big Data consulting company runs large distributed and replicated workloads on the on-premises data center. The company now wants to move these workloads to Amazon EC2 instances by using the placement groups feature and it wants to minimize correlated hardware failures.</p>\n\n<p>Which of the following represents the correct placement group configuration for the given requirement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Partition placement groups",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Spread placement groups",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Multi-AZ placement groups",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Cluster placement groups",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nPartition placement groups\n\nPartition placement groups help reduce the likelihood of correlated hardware failures for your application. When using partition placement groups, Amazon EC2 divides each group into logical segments called partitions. Amazon EC2 ensures that each partition within a placement group has its own set of racks. Each rack has its own network and power source. No two partitions within a placement group share the same racks, allowing you to isolate the impact of a hardware failure within your application.\n\nThe following image is a simple visual representation of a partition placement group in a single Availability Zone. It shows instances that are placed into a partition placement group with three partitionsPartition 1, Partition 2, and Partition 3. Each partition comprises multiple instances. The instances in a partition do not share racks with the instances in the other partitions, allowing you to contain the impact of a single hardware failure to only the associated partition.\n\nPartition placement groups can be used to deploy large distributed and replicated workloads, such as HDFS, HBase, and Cassandra, across distinct racks. When you launch instances into a partition placement group, Amazon EC2 tries to distribute the instances evenly across the number of partitions that you specify. You can also launch instances into a specific partition to have more control over where the instances are placed.\n\nA partition placement group can have partitions in multiple Availability Zones in the same Region. A partition placement group can have a maximum of seven partitions per Availability Zone. The number of instances that can be launched into a partition placement group is limited only by the limits of your account.\n\nPartition placement groups: \n via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition\n\nIncorrect options:\n\nCluster placement groups - A cluster placement group is a logical grouping of instances within a single Availability Zone. A cluster placement group can span peered VPCs in the same Region. Instances in the same cluster placement group enjoy a higher per-flow throughput limit for TCP/IP traffic and are placed in the same high-bisection bandwidth segment of the network. Cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also recommended when the majority of the network traffic is between the instances in the group. As the instances are packed close together inside an Availability Zone, this option is not correct for the given use case.\n\nCluster placement groups: \n via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition\n\nSpread placement groups - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Launching instances in a spread placement group reduces the risk of simultaneous failures that might occur when instances share the same racks. Spread placement groups provide access to distinct racks, and are therefore suitable for mixing instance types or launching instances over time. As the use-case talks about running large distributed and replicated workloads, so it needs more instances, therefore this option is not the right fit for the given use-case.\n\nA spread placement group can span multiple Availability Zones in the same Region. You can have a maximum of seven running instances per Availability Zone per group.\n\nThe following image shows seven instances in a single Availability Zone that are placed into a spread placement group. The seven instances are placed on seven different racks.\n\nSpread placement groups: \n via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition\n\nMulti-AZ placement groups - This is a made-up option, given as a distractor. You should note that the Partition and Spread placement groups can span across multiple Availability Zones in the same Region.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
    "correctAnswerExplanations": [
      {
        "answer": "Partition placement groups",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Partition placement groups help reduce the likelihood of correlated hardware failures for your application. When using partition placement groups, Amazon EC2 divides each group into logical segments called partitions. Amazon EC2 ensures that each partition within a placement group has its own set of racks. Each rack has its own network and power source. No two partitions within a placement group share the same racks, allowing you to isolate the impact of a hardware failure within your application."
      },
      {
        "answer": "",
        "explanation": "The following image is a simple visual representation of a partition placement group in a single Availability Zone. It shows instances that are placed into a partition placement group with three partitionsPartition 1, Partition 2, and Partition 3. Each partition comprises multiple instances. The instances in a partition do not share racks with the instances in the other partitions, allowing you to contain the impact of a single hardware failure to only the associated partition."
      },
      {
        "answer": "",
        "explanation": "Partition placement groups can be used to deploy large distributed and replicated workloads, such as HDFS, HBase, and Cassandra, across distinct racks. When you launch instances into a partition placement group, Amazon EC2 tries to distribute the instances evenly across the number of partitions that you specify. You can also launch instances into a specific partition to have more control over where the instances are placed."
      },
      {
        "answer": "",
        "explanation": "A partition placement group can have partitions in multiple Availability Zones in the same Region. A partition placement group can have a maximum of seven partitions per Availability Zone. The number of instances that can be launched into a partition placement group is limited only by the limits of your account."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q40-i1.jpg",
        "answer": "",
        "explanation": "Partition placement groups:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Cluster placement groups</strong> - A cluster placement group is a logical grouping of instances within a single Availability Zone. A cluster placement group can span peered VPCs in the same Region. Instances in the same cluster placement group enjoy a higher per-flow throughput limit for TCP/IP traffic and are placed in the same high-bisection bandwidth segment of the network. Cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also recommended when the majority of the network traffic is between the instances in the group. As the instances are packed close together inside an Availability Zone, this option is not correct for the given use case."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q40-i2.jpg",
        "answer": "",
        "explanation": "Cluster placement groups:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition"
      },
      {
        "answer": "",
        "explanation": "<strong>Spread placement groups</strong> - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Launching instances in a spread placement group reduces the risk of simultaneous failures that might occur when instances share the same racks. Spread placement groups provide access to distinct racks, and are therefore suitable for mixing instance types or launching instances over time. As the use-case talks about running large distributed and replicated workloads, so it needs more instances, therefore this option is not the right fit for the given use-case."
      },
      {
        "answer": "",
        "explanation": "A spread placement group can span multiple Availability Zones in the same Region. You can have a maximum of seven running instances per Availability Zone per group."
      },
      {
        "answer": "",
        "explanation": "The following image shows seven instances in a single Availability Zone that are placed into a spread placement group. The seven instances are placed on seven different racks."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q40-i3.jpg",
        "answer": "",
        "explanation": "Spread placement groups:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition"
      },
      {
        "answer": "",
        "explanation": "<strong>Multi-AZ placement groups</strong> - This is a made-up option, given as a distractor. You should note that the Partition and Spread placement groups can span across multiple Availability Zones in the same Region."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"
    ]
  },
  {
    "id": 27,
    "question": "<p>An e-commerce company uses Amazon RDS MySQL DB to store the data. The analytics department at the company runs its reports on the same database. The engineering team has noticed sluggish performance on the database when the analytics reporting process is in progress.</p>\n\n<p>As an AWS Certified Solutions Architect - Associate, which of the following would you suggest as the MOST cost-optimal solution to improve the performance?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nCreate a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica\n\nAmazon RDS uses the MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL DB engines' built-in replication functionality to create a special type of database instance called a read replica from a source database instance. The source database instance becomes the primary database instance. Updates made to the primary database instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica.\n\nAmazon RDS Read Replicas: \n via - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\n via - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\nYou can use read replicas to improve the performance of your Amazon RDS MySQL DB by handling business reporting or data warehousing scenarios where you might want business reporting queries to run against your read replica, rather than your production database instance.\n\nYou can create up to five read replicas from one DB instance. For replication to operate effectively, each read replica should have the same amount of compute and storage resources as the source database instance. If you scale the source database instance, also scale the read replicas.\n\n via - https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html\n\nIncorrect options:\n\nCreate a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica - As mentioned in the explanation above, you should create a read-replica with the same compute capacity and the same storage capacity as the primary.\n\nCreate a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance\n\nCreate a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance\n\nMulti-AZ deployments are not a read scaling solution, so you cannot use a standby to serve read traffic. The standby is there just for failover. Hence both these options are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\nhttps://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon RDS uses the MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL DB engines' built-in replication functionality to create a special type of database instance called a read replica from a source database instance. The source database instance becomes the primary database instance. Updates made to the primary database instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica."
      },
      {
        "image": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/read-replica.png",
        "answer": "",
        "explanation": "Amazon RDS Read Replicas:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"
      },
      {
        "answer": "",
        "explanation": "You can use read replicas to improve the performance of your Amazon RDS MySQL DB by handling business reporting or data warehousing scenarios where you might want business reporting queries to run against your read replica, rather than your production database instance."
      },
      {
        "answer": "",
        "explanation": "You can create up to five read replicas from one DB instance. For replication to operate effectively, each read replica should have the same amount of compute and storage resources as the source database instance. If you scale the source database instance, also scale the read replicas."
      },
      {
        "link": "https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica</strong> - As mentioned in the explanation above, you should create a read-replica with the same compute capacity and the same storage capacity as the primary."
      },
      {
        "answer": "Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance",
        "explanation": ""
      },
      {
        "answer": "Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Multi-AZ deployments are not a read scaling solution, so you cannot use a standby to serve read traffic. The standby is there just for failover. Hence both these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
      "https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
      "https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html"
    ]
  },
  {
    "id": 28,
    "question": "<p>An e-commerce website is migrating towards a microservices-based approach for their website and plans to expose their website from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, mycorp.com/products, and mycorp.com/orders. The website would like to use Amazon ECS on the backend to manage these microservices and possibly host the same container of the application multiple times on the same Amazon EC2 instance.</p>\n\n<p>Which feature can help you achieve this with minimal effort?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Classic Load Balancer + dynamic port mapping",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Application Load Balancer + dynamic port mapping",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Application Load Balancer + Reverse Proxy running as a Docker daemon on each Amazon ECS host",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Network Load Balancer + dynamic port mapping",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nApplication Load Balancer + dynamic port mapping\n\nApplication Load Balancer can automatically distribute incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones (AZs).\n\nDynamic port mapping with an Application Load Balancer makes it easier to run multiple tasks on the same Amazon ECS service on an Amazon ECS cluster.\n\nIncorrect option:\n\nApplication Load Balancer + Reverse Proxy running as a Docker daemon on each Amazon ECS host - Dynamic Port Mapping is available for the Application Load Balancer. A reverse proxy solution would work but would be too much work to manage. Here the Application Load Balancer has a feature that provides a direct dynamic port mapping feature and integration with the Amazon ECS service so we will leverage that.\n\nClassic Load Balancer + dynamic port mapping - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the Amazon EC2-Classic network.\n\nWith the Classic Load Balancer, you must statically map port numbers on a container instance. The Classic Load Balancer does not allow you to run multiple copies of a task on the same instance because of the ports conflict. An Application Load Balancer uses dynamic port mapping so that you can run multiple tasks from a single service on the same container instance.\n\nNetwork Load Balancer + dynamic port mapping - Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers  within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.\n\nReferences:\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html",
    "correctAnswerExplanations": [
      {
        "answer": "Application Load Balancer + dynamic port mapping",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Application Load Balancer can automatically distribute incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones (AZs)."
      },
      {
        "answer": "",
        "explanation": "Dynamic port mapping with an Application Load Balancer makes it easier to run multiple tasks on the same Amazon ECS service on an Amazon ECS cluster."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Incorrect option:"
      },
      {
        "answer": "",
        "explanation": "<strong>Application Load Balancer + Reverse Proxy running as a Docker daemon on each Amazon ECS host</strong> - Dynamic Port Mapping is available for the Application Load Balancer. A reverse proxy solution would work but would be too much work to manage. Here the Application Load Balancer has a feature that provides a direct dynamic port mapping feature and integration with the Amazon ECS service so we will leverage that."
      },
      {
        "answer": "",
        "explanation": "<strong>Classic Load Balancer + dynamic port mapping</strong> - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the Amazon EC2-Classic network."
      },
      {
        "answer": "",
        "explanation": "With the Classic Load Balancer, you must statically map port numbers on a container instance. The Classic Load Balancer does not allow you to run multiple copies of a task on the same instance because of the ports conflict. An Application Load Balancer uses dynamic port mapping so that you can run multiple tasks from a single service on the same container instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Network Load Balancer + dynamic port mapping</strong> - Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers  within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html"
    ]
  },
  {
    "id": 29,
    "question": "<p>A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance.</p>\n\n<p>Which combination of steps should the solutions architect take? (Select two)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up AWS Lambda with AWS Step Functions to process the data",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Provision Amazon EC2 instances in an Auto Scaling group to process the data",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up Amazon Kinesis Data Streams to ingest the data",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Set up AWS Database Migration Service (AWS DMS) to ingest the data",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Set up AWS Fargate with Amazon ECS to process the data",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct options:\n\nSet up Amazon Kinesis Data Streams to ingest the data\n\nSet up AWS Fargate with Amazon ECS to process the data\n\nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.\n\nAWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.\n\nFor the given use case, we can use Kinesis Data Streams as the ingestion layer and the containerized ECS application on AWS Fargate as the processing layer. Both these components are serverless and can scale to offer the desired performance.\n\nIncorrect options:\n\nSet up AWS Database Migration Service (AWS DMS) to ingest the data - AWS Database Migration Service helps you migrate databases to AWS quickly and securely. DMS cannot be used for real-time data ingestion. Hence, this option is incorrect.\n\nSet up AWS Lambda with AWS Step Functions to process the data - The maximum timeout value for any AWS Lambda function is 15 minutes. When the specified timeout is reached, AWS Lambda terminates the execution of your Lambda function. Since the use case talks about a job that runs for 30 minutes, Lambda is not an option here.\n\nProvision Amazon EC2 instances in an Auto Scaling group to process the data - The given requirement is for a serverless solution to process the data. Hence, provisioning an Amazon EC2 instance is clearly not the right solution.\n\nReference:\n\nhttps://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/",
    "correctAnswerExplanations": [
      {
        "answer": "Set up Amazon Kinesis Data Streams to ingest the data",
        "explanation": ""
      },
      {
        "answer": "Set up AWS Fargate with Amazon ECS to process the data",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more."
      },
      {
        "answer": "",
        "explanation": "AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design."
      },
      {
        "answer": "",
        "explanation": "For the given use case, we can use Kinesis Data Streams as the ingestion layer and the containerized ECS application on AWS Fargate as the processing layer. Both these components are serverless and can scale to offer the desired performance."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up AWS Database Migration Service (AWS DMS) to ingest the data</strong> - AWS Database Migration Service helps you migrate databases to AWS quickly and securely. DMS cannot be used for real-time data ingestion. Hence, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up AWS Lambda with AWS Step Functions to process the data</strong> - The maximum timeout value for any AWS Lambda function is 15 minutes. When the specified timeout is reached, AWS Lambda terminates the execution of your Lambda function. Since the use case talks about a job that runs for 30 minutes, Lambda is not an option here."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision Amazon EC2 instances in an Auto Scaling group to process the data</strong> - The given requirement is for a serverless solution to process the data. Hence, provisioning an Amazon EC2 instance is clearly not the right solution."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/"
    ]
  },
  {
    "id": 30,
    "question": "<p>A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year.</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon S3 Standard-IA",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon EFS Infrequent Access",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Amazon EFS Standard",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon S3 Standard",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nAmazon EFS Infrequent Access\n\nAmazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed, elastic, NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. The EFS IA storage class costs only $0.025/GB-month. To get started with EFS IA, simply enable EFS Lifecycle Management for your file system by selecting a lifecycle policy that matches your needs.\n\nHow Amazon EFS Infrequent Access Works: \n via - https://aws.amazon.com/efs/features/infrequent-access/\n\nIncorrect options:\n\nAmazon EFS Standard - Amazon EFS Infrequent Access is more cost-effective than EFS Standard for the given use-case, therefore this option is incorrect.\n\nAmazon S3 Standard\n\nAmazon S3 Standard-IA\n\nBoth these options are object-based storage, whereas the given use-case requires a POSIX compliant file storage solution. Hence these two options are incorrect.\n\nReference:\n\nhttps://aws.amazon.com/efs/features/infrequent-access/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon EFS Infrequent Access",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed, elastic, NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. The EFS IA storage class costs only $0.025/GB-month. To get started with EFS IA, simply enable EFS Lifecycle Management for your file system by selecting a lifecycle policy that matches your needs."
      },
      {
        "image": "https://d1.awsstatic.com/EFS/product-page-diagram-Amazon-EFS-Infrequent-Access-How-It-Works.83f88e30a40c27f38abae1ff157712a336dd1320.png",
        "answer": "",
        "explanation": "How Amazon EFS Infrequent Access Works:"
      },
      {
        "link": "https://aws.amazon.com/efs/features/infrequent-access/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EFS Standard</strong> - Amazon EFS Infrequent Access is more cost-effective than EFS Standard for the given use-case, therefore this option is incorrect."
      },
      {
        "answer": "Amazon S3 Standard",
        "explanation": ""
      },
      {
        "answer": "Amazon S3 Standard-IA",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Both these options are object-based storage, whereas the given use-case requires a POSIX compliant file storage solution. Hence these two options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/efs/features/infrequent-access/",
      "https://aws.amazon.com/efs/features/infrequent-access/"
    ]
  },
  {
    "id": 31,
    "question": "<p>A Hollywood production studio is looking at transferring their existing digital media assets of around 20 petabytes to AWS Cloud in the shortest possible timeframe.</p>\n\n<p>Which of the following is an optimal solution for this requirement, given that the studio's data centers are located at a remote location?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "AWS Snowball",
        "correct": false
      },
      {
        "id": 2,
        "answer": "AWS Snowmobile",
        "correct": true
      },
      {
        "id": 3,
        "answer": "AWS Direct Connect",
        "correct": false
      },
      {
        "id": 4,
        "answer": "AWS Storage Gateway",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nAWS Snowmobile\n\nAWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective. AWS recommends using Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball.\n\nIncorrect options:\n\nAWS Snowball - The AWS Snowball service uses physical storage devices to transfer large amounts of data between Amazon Simple Storage Service (Amazon S3) and client's onsite data storage location at faster-than-internet speeds. Snowball provides powerful interfaces that you can use to create jobs, track data, and track the status of your jobs through to completion. AWS recommends snowball only if you want to transfer greater than 10 TB of data between your on-premises data centers and Amazon S3.\n\nAWS Storage Gateway - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. Used for key hybrid storage solutions that include moving tape backups to the cloud, reducing on-premises storage with cloud-backed file shares, providing low latency access to data in AWS for on-premises applications, as well as various migration, archiving, processing, and disaster recovery use cases. This is not an optimal solution since the studio's data centers are in remote locations where internet speed may not optimal, thereby increasing both cost and time for migrating 20TB of data.\n\nAWS Direct Connect - AWS Direct Connect is a network service that provides an alternative to using the Internet to connect a customers on-premises sites to AWS. Data is transmitted through a private network connection between AWS and a customers datacenter or corporate network. Direct Connect connection takes significant cost as well as time to provision. This is not the correct solution since the studio wants the data transfer to be done in the shortest possible time.\n\nReference:\n\nhttps://aws.amazon.com/snowmobile/",
    "correctAnswerExplanations": [
      {
        "answer": "AWS Snowmobile",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective.  AWS recommends using Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Snowball</strong> - The AWS Snowball service uses physical storage devices to transfer large amounts of data between Amazon Simple Storage Service (Amazon S3) and client's onsite data storage location at faster-than-internet speeds. Snowball provides powerful interfaces that you can use to create jobs, track data, and track the status of your jobs through to completion. AWS recommends snowball only if you want to transfer greater than 10 TB of data between your on-premises data centers and Amazon S3."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Storage Gateway</strong> - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage.  Used for key hybrid storage solutions that include moving tape backups to the cloud, reducing on-premises storage with cloud-backed file shares, providing low latency access to data in AWS for on-premises applications, as well as various migration, archiving, processing, and disaster recovery use cases. This is not an optimal solution since the studio's data centers are in remote locations where internet speed may not optimal, thereby increasing both cost and time for migrating 20TB of data."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Direct Connect</strong> - AWS Direct Connect is a network service that provides an alternative to using the Internet to connect a customers on-premises sites to AWS. Data is transmitted through a private network connection between AWS and a customers datacenter or corporate network. Direct Connect connection takes significant cost as well as time to provision. This is not the correct solution since the studio wants the data transfer to be done in the shortest possible time."
      }
    ],
    "references": [
      "https://aws.amazon.com/snowmobile/"
    ]
  },
  {
    "id": 32,
    "question": "<p>A company is transferring a significant volume of data from on-site storage to AWS, where it will be accessed by Windows, Mac, and Linux-based Amazon EC2 instances within the same AWS region using both SMB and NFS protocols. Part of this data will be accessed regularly, while the rest will be accessed less frequently. The company requires a hosting solution for this data that minimizes operational overhead.</p>\n\n<p>What solution would best meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nSet up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume\n\nAmazon FSx for NetApp ONTAP is a storage service that allows customers to launch and run fully managed ONTAP file systems in the cloud. ONTAP is NetApps file system technology that provides a widely adopted set of data access and data management capabilities.\n\nAmazon FSx for NetApp ONTAP Overview via - https://aws.amazon.com/fsx/netapp-ontap/\n\nThe given use case mandates that the storage on AWS will be accessed by Windows, Mac, and Linux-based Amazon EC2 instances within the same AWS region using both SMB and NFS protocols. Amongst the Amazon FSx family, FSx for ONTAP is the only file system that supports this key requirement.\n\n via - https://aws.amazon.com/fsx/when-to-choose-fsx/\n\nIncorrect options:\n\nSet up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume\n\nSet up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume\n\nAmazon EFS is not supported on Windows instances. So, both these options are incorrect.\n\nSet up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume - Amazon FSx for OpenZFS is a fully managed file storage service that lets you launch, run, and scale fully managed file systems built on the open-source OpenZFS file system. FSx for OpenZFS makes it easy to migrate your on-premises file servers without changing your applications or how you manage data, and to build new high-performance, data-intensive applications on the cloud. FSx for OpenZFS is compatible with Windows, Linux, macOS clients. It supports NFS 3, 4.0, 4.1, 4.2 protocols, however, it does NOT support the SMB protocol.\n\nReferences:\n\nhttps://aws.amazon.com/fsx/netapp-ontap/\n\nhttps://aws.amazon.com/fsx/when-to-choose-fsx/\n\nhttps://aws.amazon.com/fsx/openzfs/faqs/\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html",
    "correctAnswerExplanations": [
      {
        "answer": "Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon FSx for NetApp ONTAP is a storage service that allows customers to launch and run fully managed ONTAP file systems in the cloud. ONTAP is NetApps file system technology that provides a widely adopted set of data access and data management capabilities."
      },
      {
        "link": "https://d1.awsstatic.com/FSXN%402x.72d7f1b119ec9438a370775830648c5f1f362db7.png",
        "answer": "",
        "explanation": "<a href=\"https://d1.awsstatic.com/FSXN%402x.72d7f1b119ec9438a370775830648c5f1f362db7.png\">Amazon FSx for NetApp ONTAP Overview</a>\nvia - <a href=\"https://aws.amazon.com/fsx/netapp-ontap/\">https://aws.amazon.com/fsx/netapp-ontap/</a>"
      },
      {
        "answer": "",
        "explanation": "The given use case mandates that the storage on AWS will be accessed by Windows, Mac, and Linux-based Amazon EC2 instances within the same AWS region using both SMB and NFS protocols. Amongst the Amazon FSx family, FSx for ONTAP is the only file system that supports this key requirement."
      },
      {
        "link": "https://aws.amazon.com/fsx/when-to-choose-fsx/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume",
        "explanation": ""
      },
      {
        "answer": "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon EFS is not supported on Windows instances. So, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume</strong> - Amazon FSx for OpenZFS is a fully managed file storage service that lets you launch, run, and scale fully managed file systems built on the open-source OpenZFS file system. FSx for OpenZFS makes it easy to migrate your on-premises file servers without changing your applications or how you manage data, and to build new high-performance, data-intensive applications on the cloud. FSx for OpenZFS is compatible with Windows, Linux, macOS clients. It supports NFS 3, 4.0, 4.1, 4.2 protocols, however, it does NOT support the SMB protocol."
      }
    ],
    "references": [
      "https://d1.awsstatic.com/FSXN%402x.72d7f1b119ec9438a370775830648c5f1f362db7.png",
      "https://aws.amazon.com/fsx/netapp-ontap/",
      "https://aws.amazon.com/fsx/when-to-choose-fsx/",
      "https://aws.amazon.com/fsx/netapp-ontap/",
      "https://aws.amazon.com/fsx/when-to-choose-fsx/",
      "https://aws.amazon.com/fsx/openzfs/faqs/",
      "https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html"
    ]
  },
  {
    "id": 33,
    "question": "<p>The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Amazon Route 53 to accomplish this.</p>\n\n<p>Which of the following settings of the VPC need to be enabled? (Select two)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "enableDnsSupport",
        "correct": true
      },
      {
        "id": 2,
        "answer": "enableVpcSupport",
        "correct": false
      },
      {
        "id": 3,
        "answer": "enableDnsHostnames",
        "correct": true
      },
      {
        "id": 4,
        "answer": "enableVpcHostnames",
        "correct": false
      },
      {
        "id": 5,
        "answer": "enableDnsDomain",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct options:\n\nenableDnsHostnames\n\nenableDnsSupport\n\nA private hosted zone is a container for records for a domain that you host in one or more Amazon virtual private clouds (VPCs). You create a hosted zone for a domain (such as example.com), and then you create records to tell Amazon Route 53 how you want traffic to be routed for that domain within and among your VPCs.\n\nFor each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to true:\n\nenableDnsHostnames\n\nenableDnsSupport\n\nIncorrect options:\n\nenableVpcSupport\n\nenableVpcHostnames\n\nenableDnsDomain\n\nThe options enableVpcSupport, enableVpcHostnames and enableDnsDomain have been added as distractors.\n\nReference:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-creating.html",
    "correctAnswerExplanations": [
      {
        "answer": "enableDnsHostnames",
        "explanation": ""
      },
      {
        "answer": "enableDnsSupport",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A private hosted zone is a container for records for a domain that you host in one or more Amazon virtual private clouds (VPCs). You create a hosted zone for a domain (such as example.com), and then you create records to tell Amazon Route 53 how you want traffic to be routed for that domain within and among your VPCs."
      },
      {
        "answer": "",
        "explanation": "For each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to true:"
      },
      {
        "answer": "enableDnsHostnames",
        "explanation": ""
      },
      {
        "answer": "enableDnsSupport",
        "explanation": ""
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "enableVpcSupport",
        "explanation": ""
      },
      {
        "answer": "enableVpcHostnames",
        "explanation": ""
      },
      {
        "answer": "enableDnsDomain",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The options enableVpcSupport, enableVpcHostnames and enableDnsDomain have been added as distractors."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-creating.html"
    ]
  },
  {
    "id": 34,
    "question": "<p>The development team at a company manages a Python based nightly process with a runtime of 30 minutes. The process can withstand any interruptions in its execution and start over again. The process currently runs on the on-premises infrastructure and it needs to be migrated to AWS.</p>\n\n<p>Which of the following options do you recommend as the MOST cost-effective solution?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Run on AWS Lambda",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Run on an Application Load Balancer",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Run on a Spot Instance with a persistent request type",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Run on Amazon EMR",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nRun on a Spot Instance with a persistent request type\n\nA Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The request type (one-time or persistent) determines whether the request is opened again when Amazon EC2 interrupts a Spot Instance or if you stop a Spot Instance. If the request is persistent, the request is opened again after your Spot Instance is interrupted. If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance.\n\nIncorrect options:\n\nRun on an Application Load Balancer - Application Load Balancer operates at the request level (layer 7), routing traffic to targets  Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.\n\nApplication Load Balancer helps distribute load for HTTP(S) requests. This option has been added as a distractor.\n\nRun on Amazon EMR - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\n\nAmazon EMR is to run Big Data load that is meant to be run on Hadoop, this is also a distractor.\n\nRun on AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\n\nAWS Lambda would be the perfect fit if our script could run in less than 15 minutes, as this is the maximum timeout for AWS Lambda.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html",
    "correctAnswerExplanations": [
      {
        "answer": "Run on a Spot Instance with a persistent request type",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The request type (one-time or persistent) determines whether the request is opened again when Amazon EC2 interrupts a Spot Instance or if you stop a Spot Instance. If the request is persistent, the request is opened again after your Spot Instance is interrupted. If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Run on an Application Load Balancer</strong> - Application Load Balancer operates at the request level (layer 7), routing traffic to targets  Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications."
      },
      {
        "answer": "",
        "explanation": "Application Load Balancer helps distribute load for HTTP(S) requests. This option has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Run on Amazon EMR</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances."
      },
      {
        "answer": "",
        "explanation": "Amazon EMR is to run Big Data load that is meant to be run on Hadoop, this is also a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Run on AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume."
      },
      {
        "answer": "",
        "explanation": "AWS Lambda would be the perfect fit if our script could run in less than 15 minutes, as this is the maximum timeout for AWS Lambda."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html"
    ]
  },
  {
    "id": 35,
    "question": "<p>The engineering team at a company wants to create a daily big data analysis job leveraging Spark for analyzing online/offline sales and customer loyalty data to create customized reports on a client-by-client basis. The big data analysis job needs to read the data from Amazon S3 and output it back to Amazon S3.</p>\n\n<p>Which technology do you recommend to run the Big Data analysis job? (Select two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "AWS Batch",
        "correct": false
      },
      {
        "id": 2,
        "answer": "AWS Glue",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Amazon EMR",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Amazon Athena",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Amazon Redshift",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct options:\n\nAmazon EMR\n\nAmazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. EMR is used for launching Hadoop / Spark clusters. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\n\nAWS Glue\n\nAWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. AWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. AWS Glue jobs extract data, transform it, and load the resulting data back to S3, data stores in a VPC, or on-premises JDBC data stores as a target.\n\nIncorrect options:\n\nAmazon Redshift - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications.\n\nAmazon Athena - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.\n\nAWS Batch - AWS Batch can be used to plan, schedule, and execute your batch computing workloads on Amazon EC2 Instances. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted.\n\nReferences:\n\nhttps://aws.amazon.com/emr/\n\nhttps://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon EMR",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. EMR is used for launching Hadoop / Spark clusters. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances."
      },
      {
        "answer": "AWS Glue",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. AWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. AWS Glue jobs extract data, transform it, and load the resulting data back to S3, data stores in a VPC, or on-premises JDBC data stores as a target."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Batch</strong> - AWS Batch can be used to plan, schedule, and execute your batch computing workloads on Amazon EC2 Instances. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted."
      }
    ],
    "references": [
      "https://aws.amazon.com/emr/",
      "https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/"
    ]
  },
  {
    "id": 36,
    "question": "<p>You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't been created to work in distributed mode. Still, you want to make sure your setup can automatically recover from the failure of an Availability Zone (AZ).</p>\n\n<p>Which of the following options should be combined to form the MOST cost-efficient solution? (Select three)</p>",
    "corrects": [
      2,
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Assign an Amazon EC2 Instance Role to perform the necessary API calls",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Create a Spot Fleet request",
        "correct": false
      },
      {
        "id": 6,
        "answer": "Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct options:\n\nCreate an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1\n\nAmazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.\n\nSo we have an Auto Scaling Group with desired=1, across two AZ, so that if an instance goes down, it is automatically recreated in another AZ. So this option is correct.\n\nCreate an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it\n\nApplication Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets  Amazon EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.\n\nAn Elastic IP address is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.\n\nNow, between the ALB and the Elastic IP. If we use an ALB, things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one Amazon EC2 instance. Instead, to minimize costs, we must use an Elastic IP.\n\nAssign an Amazon EC2 Instance Role to perform the necessary API calls\n\nFor that Elastic IP to be attached to our Amazon EC2 instance, we must use an EC2 user data script, and our Amazon EC2 instance must have the correct IAM permissions to perform the API call, so we need an Amazon EC2 instance role.\n\nIncorrect options:\n\nCreate a Spot Fleet request - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price.\n\nThe Spot Fleet selects the Spot Instance pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated.\n\nSpot Fleets requests would not fit our purpose as we are looking at a critical application. Spot instances can be terminated. So this option is incorrect.\n\nCreate an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2 - An Auto Scaling Group with desired=2 would create two instances, and this won't work for us as our monolith application is not made to work with two instances as per the given use-case.\n\nCreate an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group - If we use an Application Load Balancer (ALB), things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one Amazon EC2 instance. So this option is not correct.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size."
      },
      {
        "answer": "",
        "explanation": "So we have an Auto Scaling Group with desired=1, across two AZ, so that if an instance goes down, it is automatically recreated in another AZ. So this option is correct."
      },
      {
        "answer": "Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets  Amazon EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications."
      },
      {
        "answer": "",
        "explanation": "An Elastic IP address is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account."
      },
      {
        "answer": "",
        "explanation": "Now, between the ALB and the Elastic IP. If we use an ALB, things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one Amazon EC2 instance. Instead, to minimize costs, we must use an Elastic IP."
      },
      {
        "answer": "Assign an Amazon EC2 Instance Role to perform the necessary API calls",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "For that Elastic IP to be attached to our Amazon EC2 instance, we must use an EC2 user data script, and our Amazon EC2 instance must have the correct IAM permissions to perform the API call, so we need an Amazon EC2 instance role."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a Spot Fleet request</strong> - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price."
      },
      {
        "answer": "",
        "explanation": "The Spot Fleet selects the Spot Instance pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated."
      },
      {
        "answer": "",
        "explanation": "Spot Fleets requests would not fit our purpose as we are looking at a critical application. Spot instances can be terminated. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2</strong> - An Auto Scaling Group with desired=2 would create two instances, and this won't work for us as our monolith application is not made to work with two instances as per the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group</strong> - If we use an Application Load Balancer (ALB), things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one Amazon EC2 instance. So this option is not correct."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html"
    ]
  },
  {
    "id": 37,
    "question": "<p>A Big Data company wants to optimize its daily Extract-Transform-Load (ETL) process that migrates and transforms data from its Amazon S3 based data lake to an Amazon Redshift cluster. The team wants to manage this daily job in a serverless environment.</p>\n\n<p>Which AWS service is the best fit to manage this process without the need to configure or manage the underlying compute resources?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon EMR",
        "correct": false
      },
      {
        "id": 2,
        "answer": "AWS Data Pipeline",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Database Migration Service (DMS)",
        "correct": false
      },
      {
        "id": 4,
        "answer": "AWS Glue",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nAWS Glue\n\nAWS Glue provides a managed ETL service that runs on a serverless Apache Spark environment. This allows you to focus on your ETL job and not worry about configuring and managing the underlying compute resources. AWS Glue takes a data-first approach and allows you to focus on the data properties and data manipulation to transform the data to a form where you can derive business insights. It provides an integrated data catalog that makes metadata available for ETL as well as querying via Amazon Athena and Amazon Redshift Spectrum.\n\nCreate a unified catalog to find data across multiple data stores using AWS Glue: \n via - https://aws.amazon.com/glue/\n\nAWS Glue automates much of the effort required for data integration. AWS Glue crawls your data sources, identifies data formats, and suggests schemas to store your data. It automatically generates the code to run your data transformations and loading processes. You can use AWS Glue to easily run and manage thousands of ETL jobs or to combine and replicate data across multiple data stores using SQL.\n\nAWS Glue runs in a serverless environment. There is no infrastructure to manage, and AWS Glue provisions, configures, and scales the resources required to run your data integration jobs. You pay only for the resources your jobs use while running.\n\nAWS Glue is the right fit since the company is looking at a managed ETL service without having the overhead of configuring, maintaining, or managing any servers.\n\n via - https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html\n\nIncorrect options:\n\nAWS Data Pipeline - AWS Data Pipeline provides a managed orchestration service that gives you greater flexibility in terms of the execution environment, access and control over the compute resources that run your code, as well as the code itself that does data processing. AWS Data Pipeline launches compute resources in your account allowing you direct access to the Amazon EC2 instances or Amazon EMR clusters. As this option provides access to the underlying EC2 instances so it's not a serverless solution. Therefore this option is incorrect for the given use case.\n\nAmazon EMR - EMR is a web service to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). As this option provides access to the underlying Amazon EC2 instances so it's not a serverless solution. Therefore this option is incorrect for the given use case.\n\nAWS Database Migration Service (DMS) - AWS Database Migration Service (DMS) helps you migrate databases to AWS easily and securely. For use cases that require a database migration from on-premises to AWS or database replication between on-premises sources and sources on AWS, AWS recommends you use AWS DMS. Once your data is in AWS, you can use AWS Glue to move, combine, replicate, and transform data from your data source into another database or data warehouse, such as Amazon Redshift. As the use-case talks about data migration and transformation between AWS services, so AWS Glue is a better fit than DMS.\n\nReferences:\n\nhttps://aws.amazon.com/glue/faqs/\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html",
    "correctAnswerExplanations": [
      {
        "answer": "AWS Glue",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Glue provides a managed ETL service that runs on a serverless Apache Spark environment. This allows you to focus on your ETL job and not worry about configuring and managing the underlying compute resources. AWS Glue takes a data-first approach and allows you to focus on the data properties and data manipulation to transform the data to a form where you can derive business insights. It provides an integrated data catalog that makes metadata available for ETL as well as querying via Amazon Athena and Amazon Redshift Spectrum."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q48-i1.jpg",
        "answer": "",
        "explanation": "Create a unified catalog to find data across multiple data stores using AWS Glue:"
      },
      {
        "link": "https://aws.amazon.com/glue/"
      },
      {
        "answer": "",
        "explanation": "AWS Glue automates much of the effort required for data integration. AWS Glue crawls your data sources, identifies data formats, and suggests schemas to store your data. It automatically generates the code to run your data transformations and loading processes. You can use AWS Glue to easily run and manage thousands of ETL jobs or to combine and replicate data across multiple data stores using SQL."
      },
      {
        "answer": "",
        "explanation": "AWS Glue runs in a serverless environment. There is no infrastructure to manage, and AWS Glue provisions, configures, and scales the resources required to run your data integration jobs. You pay only for the resources your jobs use while running."
      },
      {
        "answer": "",
        "explanation": "AWS Glue is the right fit since the company is looking at a managed ETL service without having the overhead of configuring, maintaining, or managing any servers."
      },
      {
        "link": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Data Pipeline</strong> - AWS Data Pipeline provides a managed orchestration service that gives you greater flexibility in terms of the execution environment, access and control over the compute resources that run your code, as well as the code itself that does data processing. AWS Data Pipeline launches compute resources in your account allowing you direct access to the Amazon EC2 instances or Amazon EMR clusters. As this option provides access to the underlying EC2 instances so it's not a serverless solution. Therefore this option is incorrect for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EMR</strong> - EMR is a web service to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). As this option provides access to the underlying Amazon EC2 instances so it's not a serverless solution. Therefore this option is incorrect for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Database Migration Service (DMS)</strong> - AWS Database Migration Service (DMS) helps you migrate databases to AWS easily and securely. For use cases that require a database migration from on-premises to AWS or database replication between on-premises sources and sources on AWS, AWS recommends you use AWS DMS. Once your data is in AWS, you can use AWS Glue to move, combine, replicate, and transform data from your data source into another database or data warehouse, such as Amazon Redshift. As the use-case talks about data migration and transformation between AWS services, so AWS Glue is a better fit than DMS."
      }
    ],
    "references": [
      "https://aws.amazon.com/glue/",
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html",
      "https://aws.amazon.com/glue/faqs/",
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html"
    ]
  },
  {
    "id": 38,
    "question": "<p>A digital media streaming company wants to use Amazon CloudFront to distribute its content only to its service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver restricted content to the bona fide end users? (Select two)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon CloudFront signed cookies",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon CloudFront signed URLs",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Require HTTPS for communication between Amazon CloudFront and your S3 origin",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Require HTTPS for communication between Amazon CloudFront and your custom origin",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct options:\n\nUse Amazon CloudFront signed URLs\n\nMany companies that distribute content over the internet want to restrict access to documents, business data, media streams, or content that is intended for selected users, for example, users who have paid a fee.\n\nTo securely serve this private content by using Amazon CloudFront, you can do the following:\n\nRequire that your users access your private content by using special Amazon CloudFront signed URLs or signed cookies.\n\nA signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content. So this is a correct option.\n\nUse Amazon CloudFront signed cookies\n\nAmazon CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. So this is also a correct option.\n\nIncorrect options:\n\nRequire HTTPS for communication between Amazon CloudFront and your custom origin\n\nRequire HTTPS for communication between Amazon CloudFront and your S3 origin\n\nRequiring HTTPS for communication between Amazon CloudFront and your custom origin (or S3 origin) only enables secure access to the underlying content. You cannot use HTTPS to restrict access to your private content. So both these options are incorrect.\n\nForward HTTPS requests to the origin server by using the ECDSA or RSA ciphers - This option is just added as a distractor. You cannot use HTTPS to restrict access to your private content.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon CloudFront signed URLs",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Many companies that distribute content over the internet want to restrict access to documents, business data, media streams, or content that is intended for selected users, for example, users who have paid a fee."
      },
      {
        "answer": "",
        "explanation": "To securely serve this private content by using Amazon CloudFront, you can do the following:"
      },
      {
        "answer": "",
        "explanation": "Require that your users access your private content by using special Amazon CloudFront signed URLs or signed cookies."
      },
      {
        "answer": "",
        "explanation": "A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content. So this is a correct option."
      },
      {
        "answer": "Use Amazon CloudFront signed cookies",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. So this is also a correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Require HTTPS for communication between Amazon CloudFront and your custom origin",
        "explanation": ""
      },
      {
        "answer": "Require HTTPS for communication between Amazon CloudFront and your S3 origin",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Requiring HTTPS for communication between Amazon CloudFront and your custom origin (or S3 origin) only enables secure access to the underlying content. You cannot use HTTPS to restrict access to your private content. So both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers</strong> - This option is just added as a distractor. You cannot use HTTPS to restrict access to your private content."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html"
    ]
  },
  {
    "id": 39,
    "question": "<p>The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application that needs to support relational queries. The application will have unpredictable spikes of usage that the team does not know in advance.</p>\n\n<p>Which database would you recommend using?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon Aurora Serverless",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon DynamoDB with Provisioned Capacity and Auto Scaling",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon DynamoDB with On-Demand Capacity",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nAmazon Aurora Serverless\n\nAmazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. The database design for an OLTP application fits the relational model, therefore you can infer an OLTP system as a Relational Database.\n\nAmazon Aurora Serverless is the perfect way to create a database that can scale down to 0 servers, and scale up to many servers, as an OLTP database. So this is the correct option.\n\nIncorrect options:\n\nAmazon DynamoDB with Provisioned Capacity and Auto Scaling\n\nAmazon DynamoDB with On-Demand Capacity\n\nAmazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.\n\nAmazon DynamoDB is a NoSQL database and doesn't do relational queries, therefore it's a choice we have to eliminate, even though the two modes proposed here help us cope with an unpredictable amount of usage. So both these options are incorrect.\n\nAmazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Amazon Elasticache is used as a caching layer in front of relational databases. Amazon ElastiCache is a NoSQL database and doesn't facilitate relational queries, so this option is ruled out.\n\nReferences:\n\nhttps://aws.amazon.com/rds/aurora/serverless/\n\nhttps://aws.amazon.com/rds/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon Aurora Serverless",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. The database design for an OLTP application fits the relational model, therefore you can infer an OLTP system as a Relational Database."
      },
      {
        "answer": "",
        "explanation": "Amazon Aurora Serverless is the perfect way to create a database that can scale down to 0 servers, and scale up to many servers, as an OLTP database. So this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Amazon DynamoDB with Provisioned Capacity and Auto Scaling",
        "explanation": ""
      },
      {
        "answer": "Amazon DynamoDB with On-Demand Capacity",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications."
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a NoSQL database and doesn't do relational queries, therefore it's a choice we have to eliminate, even though the two modes proposed here help us cope with an unpredictable amount of usage. So both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Amazon Elasticache is used as a caching layer in front of relational databases. Amazon ElastiCache is a NoSQL database and doesn't facilitate relational queries, so this option is ruled out."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/aurora/serverless/",
      "https://aws.amazon.com/rds/"
    ]
  },
  {
    "id": 40,
    "question": "<p>A social media application lets users upload photos and perform image editing operations. The application offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be processed before those submitted by lite users. Photos are uploaded to Amazon S3 and the job information is sent to Amazon SQS.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nCreate two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue\n\nAWS recommends using separate queues to provide prioritization of work. Therefore, for the given use case, you need to create an Amazon SQS standard queue for processing pro users' photos and another Amazon SQS standard queue for processing lite users' photos. Then you can configure Amazon EC2 instances to prioritize polling for the pro queue over the lite queue.\n\n via - https://aws.amazon.com/sqs/features/\n\nIncorrect options:\n\nCreate two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling\n\nCreate two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling\n\nAmazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. While the regular short polling returns immediately, even if the message queue being polled is empty, long-polling doesnt return a response until a message arrives in the message queue, or the long poll times out. Since long polling or short polling cannot impact the priority of processing for the two queues, so both these options are incorrect.\n\nCreate one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first - To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. Setting visibility timeout to zero can result in the same pro photo being processed by more than one consumer. This does not help in prioritizing the processing of pro photos over the lite photos.\n\n via - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\n\nReferences:\n\nhttps://aws.amazon.com/sqs/features/\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS recommends using separate queues to provide prioritization of work. Therefore, for the given use case, you need to create an Amazon SQS standard queue for processing pro users' photos and another Amazon SQS standard queue for processing lite users' photos. Then you can configure Amazon EC2 instances to prioritize polling for the pro queue over the lite queue."
      },
      {
        "link": "https://aws.amazon.com/sqs/features/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling",
        "explanation": ""
      },
      {
        "answer": "Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. While the regular short polling returns immediately, even if the message queue being polled is empty, long-polling doesnt return a response until a message arrives in the message queue, or the long poll times out. Since long polling or short polling cannot impact the priority of processing for the two queues, so both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first</strong> - To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. Setting visibility timeout to zero can result in the same pro photo being processed by more than one consumer. This does not help in prioritizing the processing of pro photos over the lite photos."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html"
      }
    ],
    "references": [
      "https://aws.amazon.com/sqs/features/",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html",
      "https://aws.amazon.com/sqs/features/",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html"
    ]
  },
  {
    "id": 41,
    "question": "<p>A company is developing a document management application on AWS. The application runs on Amazon EC2 instances in multiple Availability Zones (AZs). The company requires the document store to be highly available and the documents need to be returned immediately when requested. The engineering team has configured the application to use Amazon Elastic Block Store (Amazon EBS) to store the documents but the team is willing to consider other options to meet the availability requirement.</p>\n\n<p>As a solutions architect, which of the following will you recommend?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nSet up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store\n\nInstances that use Amazon EBS for the root device automatically have an Amazon EBS volume attached. When you launch an Amazon EBS-backed instance, AWS creates an Amazon EBS volume for each Amazon EBS snapshot referenced by the AMI you use. An Amazon EBS-backed instance can be stopped and later restarted without affecting data stored in the attached volumes.\n\nAmazon S3 provides access to reliable, fast, and inexpensive data storage infrastructure. It is designed to make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from within Amazon EC2 or anywhere on the web. S3 is highly available and can be configured to work as a document store for the given use case.\n\nIncorrect options:\n\nSet up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store - As the documents need to be returned immediately when requested, Amazon S3 Glacier is not the right fit, since there is a lag of several minutes/hours when you want to read data from Glacier.\n\nCreate snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones - You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. Hence, using Amazon EBS volumes as a primary storage solution is ineffective, and creating recurring snapshots is a management nightmare for the current use case.\n\nProvision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances - You cannot mount Instance Store volumes to multiple Amazon EC2 instances. An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\n\nhttps://aws.amazon.com/s3/storage-classes/",
    "correctAnswerExplanations": [
      {
        "answer": "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Instances that use Amazon EBS for the root device automatically have an Amazon EBS volume attached. When you launch an Amazon EBS-backed instance, AWS creates an Amazon EBS volume for each Amazon EBS snapshot referenced by the AMI you use. An Amazon EBS-backed instance can be stopped and later restarted without affecting data stored in the attached volumes."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 provides access to reliable, fast, and inexpensive data storage infrastructure. It is designed to make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from within Amazon EC2 or anywhere on the web. S3 is highly available and can be configured to work as a document store for the given use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store</strong> - As the documents need to be returned immediately when requested, Amazon S3 Glacier is not the right fit, since there is a lag of several minutes/hours when you want to read data from Glacier."
      },
      {
        "answer": "",
        "explanation": "<strong>Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones</strong> - You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. Hence, using Amazon EBS volumes as a primary storage solution is ineffective, and creating recurring snapshots is a management nightmare for the current use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances</strong> - You cannot mount Instance Store volumes to multiple Amazon EC2 instances. An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "id": 42,
    "question": "<p>A company manages a High Performance Computing (HPC) application that needs to be deployed on Amazon EC2 instances. The application requires high levels of inter-node communications and high network traffic between the instances.</p>\n\n<p>As a solutions architect, which of the following options would you recommend to the engineering team at the company? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Deploy Amazon EC2 instances in a cluster placement group",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Deploy Amazon EC2 instances in a spread placement group",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy Amazon EC2 instances behind a Network Load Balancer",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy Amazon EC2 instances in a partition placement group",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Deploy Amazon EC2 instances with Elastic Fabric Adapter (EFA)",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct options:\n\nDeploy Amazon EC2 instances with Elastic Fabric Adapter (EFA)\n\nElastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at scale on AWS. Its custom-built operating system (OS) bypass hardware interface enhances the performance of inter-instance communications, which is critical to scaling these applications. Therefore this option is correct.\n\nDeploy Amazon EC2 instances in a cluster placement group\n\nCluster placement groups pack instances close together inside an Availability Zone. They are recommended when the majority of the network traffic is between the instances in the group. These are also recommended for applications that benefit from low network latency, high network throughput, or both. Therefore this option is one of the correct answers.\n\nIncorrect options:\n\nDeploy Amazon EC2 instances in a spread placement group - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. You can have a maximum of seven running instances per Availability Zone per group. Since the spread placement group can span across multiple Availability Zones in the same Region, it cannot support high levels of inter-node communications and high network traffic. So this option is incorrect.\n\nDeploy Amazon EC2 instances in a partition placement group - A partition placement group spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement group can have a maximum of seven partitions per Availability Zone. Since the partition placement group can have partitions in multiple Availability Zones in the same Region, it cannot support high levels of inter-node communications and high network traffic. So this option is incorrect.\n\nDeploy Amazon EC2 instances behind a Network Load Balancer - A load balancer serves as the single point of contact for clients. The load balancer distributes incoming traffic across multiple targets, such as Amazon EC2 instances. A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. Network Load Balancer cannot facilitate high network traffic between instances. Network Load Balancer cannot support high levels of inter-node communication between EC2 instances. This option just serves as a distractor.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nhttps://aws.amazon.com/hpc/efa/\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
    "correctAnswerExplanations": [
      {
        "answer": "Deploy Amazon EC2 instances with Elastic Fabric Adapter (EFA)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at scale on AWS. Its custom-built operating system (OS) bypass hardware interface enhances the performance of inter-instance communications, which is critical to scaling these applications. Therefore this option is correct."
      },
      {
        "answer": "Deploy Amazon EC2 instances in a cluster placement group",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Cluster placement groups pack instances close together inside an Availability Zone. They are recommended when the majority of the network traffic is between the instances in the group. These are also recommended for applications that benefit from low network latency, high network throughput, or both. Therefore this option is one of the correct answers."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy Amazon EC2 instances in a spread placement group</strong> - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. You can have a maximum of seven running instances per Availability Zone per group. Since the spread placement group can span across multiple Availability Zones in the same Region, it cannot support high levels of inter-node communications and high network traffic. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy Amazon EC2 instances in a partition placement group</strong> - A partition placement group spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement group can have a maximum of seven partitions per Availability Zone. Since the partition placement group can have partitions in multiple Availability Zones in the same Region, it cannot support high levels of inter-node communications and high network traffic. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy Amazon EC2 instances behind a Network Load Balancer</strong> - A load balancer serves as the single point of contact for clients. The load balancer distributes incoming traffic across multiple targets, such as Amazon EC2 instances. A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. Network Load Balancer cannot facilitate high network traffic between instances. Network Load Balancer cannot support high levels of inter-node communication between EC2 instances. This option just serves as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "https://aws.amazon.com/hpc/efa/",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html"
    ]
  },
  {
    "id": 43,
    "question": "<p>The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage firewall rules across its accounts and applications using AWS Organizations.</p>\n\n<p>Which of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)</p>",
    "corrects": [
      1,
      3,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "AWS Shield Advanced",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Amazon GuardDuty",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Web Application Firewall (AWS WAF)",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Amazon Inspector",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Network access control list (network ACL)",
        "correct": false
      },
      {
        "id": 6,
        "answer": "VPC Security Groups",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct options:\n\nAWS Web Application Firewall (AWS WAF)\n\nAWS Shield Advanced\n\nVPC Security Groups\n\nAWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure.\n\nUsing AWS Firewall Manager, you can centrally configure AWS WAF rules, AWS Shield Advanced protection, Amazon Virtual Private Cloud (VPC) security groups, AWS Network Firewalls, and Amazon Route 53 Resolver DNS Firewall rules across accounts and resources in your organization. It does not support Network ACLs as of today.\n\n via - https://aws.amazon.com/firewall-manager/faqs/\n\nIncorrect options:\n\nAmazon GuardDuty - Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. Amazon GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs.\n\nHow Amazon GuardDuty Works: \n\nAmazon Inspector - Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.\n\nNetwork access control list (network ACL) - A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.\n\nThese three options are not in the list of AWS resources supported by AWS Firewall Manager, so these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/firewall-manager/faqs/\n\nhttps://aws.amazon.com/guardduty/\n\nhttps://aws.amazon.com/inspector/",
    "correctAnswerExplanations": [
      {
        "answer": "AWS Web Application Firewall (AWS WAF)",
        "explanation": ""
      },
      {
        "answer": "AWS Shield Advanced",
        "explanation": ""
      },
      {
        "answer": "VPC Security Groups",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure."
      },
      {
        "answer": "",
        "explanation": "Using AWS Firewall Manager, you can centrally configure AWS WAF rules, AWS Shield Advanced protection, Amazon Virtual Private Cloud (VPC) security groups, AWS Network Firewalls, and Amazon Route 53 Resolver DNS Firewall rules across accounts and resources in your organization. It does not support Network ACLs as of today."
      },
      {
        "link": "https://aws.amazon.com/firewall-manager/faqs/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon GuardDuty</strong> - Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. Amazon GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs."
      },
      {
        "image": "https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png",
        "answer": "",
        "explanation": "How Amazon GuardDuty Works:"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances."
      },
      {
        "answer": "",
        "explanation": "<strong>Network access control list (network ACL)</strong> - A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets."
      },
      {
        "answer": "",
        "explanation": "These three options are not in the list of AWS resources supported by AWS Firewall Manager, so these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/firewall-manager/faqs/",
      "https://aws.amazon.com/firewall-manager/faqs/",
      "https://aws.amazon.com/guardduty/",
      "https://aws.amazon.com/inspector/"
    ]
  },
  {
    "id": 44,
    "question": "<p>A financial services company stores confidential data on an Amazon Simple Storage Service (S3) bucket. The compliance guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage the encryption keys.</p>\n\n<p>Which of the following options represents the most cost-optimal solution for the given use case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Client Side Encryption",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Server-side encryption with AWS KMS keys (SSE-KMS)",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Server-side encryption with customer-provided keys (SSE-C)",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nServer-side encryption with Amazon S3 managed keys (SSE-S3)\n\nUsing Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. There are no additional fees for using server-side encryption with Amazon S3-managed keys (SSE-S3).\n\nIncorrect options:\n\nServer-side encryption with customer-provided keys (SSE-C) - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects.\n\nClient Side Encryption - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.\n\nServer-side encryption with AWS KMS keys (SSE-KMS) - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself. Although SSE-KMS provides an option where AWS manages the encryption key on your behalf, however, this entails a usage fee for the KMS key. So this option is not the best fit for the given use case.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html",
    "correctAnswerExplanations": [
      {
        "answer": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. There are no additional fees for using server-side encryption with Amazon S3-managed keys (SSE-S3)."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Server-side encryption with customer-provided keys (SSE-C)</strong> - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects."
      },
      {
        "answer": "",
        "explanation": "<strong>Client Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools."
      },
      {
        "answer": "",
        "explanation": "<strong>Server-side encryption with AWS KMS keys (SSE-KMS)</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself. Although SSE-KMS provides an option where AWS manages the encryption key on your behalf, however, this entails a usage fee for the KMS key. So this option is not the best fit for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html"
    ]
  },
  {
    "id": 45,
    "question": "<p>A company is deploying a publicly accessible web application. To accomplish this, the engineering team has designed the VPC with a public subnet and a private subnet. The application will be hosted on several Amazon EC2 instances in an Auto Scaling group. The team also wants Transport Layer Security (TLS) termination to be offloaded from the Amazon EC2 instances.</p>\n\n<p>Which solution should a solutions architect implement to address these requirements in the most secure manner?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nSet up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer\n\nA load balancer serves as the single point of contact for clients. The load balancer distributes incoming traffic across multiple targets, such as Amazon EC2 instances. This increases the availability of your application. You add one or more listeners to your load balancer.\n\nWith a Network Load Balancer, you can offload the decryption/encryption of Transport Layer Security (TLS) traffic from your application servers to the Network Load Balancer, which helps you optimize the performance of your backend application servers while keeping your workloads secure. Additionally, Network Load Balancers preserve the source IP of the clients to the back-end applications, while terminating Transport Layer Security (TLS) on the load balancer.\n\nAn Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.\n\nThe NLB has to be accessible over the internet and hence has to be in a public subnet and will act as a single point-of-contact for all incoming traffic. NLB will forward the incoming traffic to the Amazon EC2 instances managed by the ASG in the private subnet.\n\nExam Alert:\n\nYou should note that the Application Load Balancer also supports Transport Layer Security (TLS) offloading. The Classic Load Balancer supports SSL offloading.\n\nIncorrect options:\n\nSet up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer - The Auto Scaling group with its target EC2 instances should be in the private subnet to avoid access to EC2 instances over the public internet. Having EC2 instances in the public subnet would weaken the security posture of the application. Hence, this option is incorrect.\n\nSet up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer\n\nSet up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer\n\nNLB should be in the public subnet as it represents the internet-facing component of the web tier. Therefore, both these options are incorrect.\n\nReference:\n\nhttps://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/",
    "correctAnswerExplanations": [
      {
        "answer": "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A load balancer serves as the single point of contact for clients. The load balancer distributes incoming traffic across multiple targets, such as Amazon EC2 instances. This increases the availability of your application. You add one or more listeners to your load balancer."
      },
      {
        "answer": "",
        "explanation": "With a Network Load Balancer, you can offload the decryption/encryption of Transport Layer Security (TLS) traffic from your application servers to the Network Load Balancer, which helps you optimize the performance of your backend application servers while keeping your workloads secure. Additionally, Network Load Balancers preserve the source IP of the clients to the back-end applications, while terminating Transport Layer Security (TLS) on the load balancer."
      },
      {
        "answer": "",
        "explanation": "An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service."
      },
      {
        "answer": "",
        "explanation": "The NLB has to be accessible over the internet and hence has to be in a public subnet and will act as a single point-of-contact for all incoming traffic. NLB will forward the incoming traffic to the Amazon EC2 instances managed by the ASG in the private subnet."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "answer": "",
        "explanation": "You should note that the Application Load Balancer also supports Transport Layer Security (TLS) offloading. The Classic Load Balancer supports SSL offloading."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer</strong> - The Auto Scaling group with its target EC2 instances should be in the private subnet to avoid access to EC2 instances over the public internet. Having EC2 instances in the public subnet would weaken the security posture of the application. Hence, this option is incorrect."
      },
      {
        "answer": "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer",
        "explanation": ""
      },
      {
        "answer": "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "NLB should be in the public subnet as it represents the internet-facing component of the web tier. Therefore, both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/"
    ]
  },
  {
    "id": 46,
    "question": "<p>An application is hosted on multiple Amazon EC2 instances in the same Availability Zone (AZ). The engineering team wants to set up shared data access for these Amazon EC2 instances using Amazon EBS Multi-Attach volumes.</p>\n\n<p>Which Amazon EBS volume type is the correct choice for these Amazon EC2 instances?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Cold HDD Amazon EBS volumes",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Throughput Optimized HDD Amazon EBS volumes",
        "correct": false
      },
      {
        "id": 3,
        "answer": "General-purpose SSD-based Amazon EBS volumes",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Provisioned IOPS SSD Amazon EBS volumes",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nProvisioned IOPS SSD Amazon EBS volumes\n\nAmazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD (io1 or io2) volume to multiple instances that are in the same Availability Zone. You can attach multiple Multi-Attach enabled volumes to an instance or set of instances. Each instance to which the volume is attached has full read and write permission to the shared volume. Multi-Attach makes it easier for you to achieve higher application availability in clustered Linux applications that manage concurrent write operations.\n\nMulti-Attach is supported exclusively on Provisioned IOPS SSD volumes.\n\nIncorrect options:\n\nGeneral-purpose SSD-based Amazon EBS volumes - These SSD-backed Amazon EBS volumes provide a balance of price and performance. AWS recommends these volumes for most workloads. These volume types are not supported for Multi-Attach functionality.\n\nThroughput Optimized HDD Amazon EBS volumes - These HDD-backed volumes provide a low-cost HDD designed for frequently accessed, throughput-intensive workloads. These volume types are not supported for Multi-Attach functionality.\n\nCold HDD Amazon EBS volumes - These HDD-backed volumes provide a lowest-cost HDD design for less frequently accessed workloads. These volume types are not supported for Multi-Attach functionality.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html",
    "correctAnswerExplanations": [
      {
        "answer": "Provisioned IOPS SSD Amazon EBS volumes",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD (io1 or io2) volume to multiple instances that are in the same Availability Zone. You can attach multiple Multi-Attach enabled volumes to an instance or set of instances. Each instance to which the volume is attached has full read and write permission to the shared volume. Multi-Attach makes it easier for you to achieve higher application availability in clustered Linux applications that manage concurrent write operations."
      },
      {
        "answer": "",
        "explanation": "Multi-Attach is supported exclusively on Provisioned IOPS SSD volumes."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>General-purpose SSD-based Amazon EBS volumes</strong> - These SSD-backed Amazon EBS volumes provide a balance of price and performance. AWS recommends these volumes for most workloads. These volume types are not supported for Multi-Attach functionality."
      },
      {
        "answer": "",
        "explanation": "<strong>Throughput Optimized HDD Amazon EBS volumes</strong> - These HDD-backed volumes provide a low-cost HDD designed for frequently accessed, throughput-intensive workloads. These volume types are not supported for Multi-Attach functionality."
      },
      {
        "answer": "",
        "explanation": "<strong>Cold HDD Amazon EBS volumes</strong> - These HDD-backed volumes provide a lowest-cost HDD design for less frequently accessed workloads. These volume types are not supported for Multi-Attach functionality."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html"
    ]
  },
  {
    "id": 47,
    "question": "<p>Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second.</p>\n\n<p>How can you efficiently prevent attackers from overwhelming your application?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure Sticky Sessions on the Application Load Balancer",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS Shield Advanced and setup a rate-based rule",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Define a network access control list (network ACL) on your Application Load Balancer",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nUse an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule\n\nAWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.\n\nThe correct answer is to use WAF (which has integration on top of your ALB) and define a rate-based rule.\n\nIncorrect options:\n\nConfigure Sticky Sessions on the Application Load Balancer - Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets  Amazon EC2 instances, containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.\n\nSticky Sessions on your Application Load Balancer is a distractor here. Sticky sessions are a mechanism to route requests from the same client to the same target. Application Load Balancer supports sticky sessions using load balancer generated cookies. If you enable sticky sessions, the same target receives the request and can use the cookie to recover the session context.\n\nDefine a network access control list (network ACL) on your Application Load Balancer - A network access control list (network ACL) does not work, as this only helps to block specific IPs. On top of things, network access control list (network ACL) is defined at the subnet level, and not for an Application Load Balancer.\n\nUse AWS Shield Advanced and setup a rate-based rule - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. There are two tiers of AWS Shield - Standard and Advanced.\n\nAWS Shield Advanced provides enhanced resource-specific detection and employs advanced mitigation and routing techniques for sophisticated or larger attacks.\n\nAWS Shield Advanced will give you DDoS protection overall, and you cannot set up rate-based rules in Shield.\n\nReferences:\n\nhttps://aws.amazon.com/waf/\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\n\nhttps://aws.amazon.com/shield/\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions",
    "correctAnswerExplanations": [
      {
        "answer": "Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define."
      },
      {
        "answer": "",
        "explanation": "The correct answer is to use WAF (which has integration on top of your ALB) and define a rate-based rule."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Sticky Sessions on the Application Load Balancer</strong> - Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets  Amazon EC2 instances, containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications."
      },
      {
        "answer": "",
        "explanation": "Sticky Sessions on your Application Load Balancer is a distractor here. Sticky sessions are a mechanism to route requests from the same client to the same target. Application Load Balancer supports sticky sessions using load balancer generated cookies. If you enable sticky sessions, the same target receives the request and can use the cookie to recover the session context."
      },
      {
        "answer": "",
        "explanation": "<strong>Define a network access control list (network ACL) on your Application Load Balancer</strong> - A network access control list (network ACL) does not work, as this only helps to block specific IPs. On top of things, network access control list (network ACL) is defined at the subnet level, and not for an Application Load Balancer."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Shield Advanced and setup a rate-based rule</strong> - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. There are two tiers of AWS Shield - Standard and Advanced."
      },
      {
        "answer": "",
        "explanation": "AWS Shield Advanced provides enhanced resource-specific detection and employs advanced mitigation and routing techniques for sophisticated or larger attacks."
      },
      {
        "answer": "",
        "explanation": "AWS Shield Advanced will give you DDoS protection overall, and you cannot set up rate-based rules in Shield."
      }
    ],
    "references": [
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html",
      "https://aws.amazon.com/shield/",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions"
    ]
  },
  {
    "id": 48,
    "question": "<p>The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon Relational Database Service (Amazon RDS) as the database tier for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on Amazon RDS Multi-AZ capabilities.</p>\n\n<p>Which of the following would you identify as correct for Amazon RDS Multi-AZ? (Select two)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason",
        "correct": true
      },
      {
        "id": 2,
        "answer": "For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby",
        "correct": true
      },
      {
        "id": 4,
        "answer": "To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct options:\n\nAmazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby\n\nRunning a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because Amazon RDS applies operating system updates by following these steps:\n\nPerform maintenance on the standby.\n\nPromote the standby to primary.\n\nPerform maintenance on the old primary, which becomes the new standby.\n\nWhen you modify the database engine for your DB instance in a Multi-AZ deployment, then Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade.\n\nAmazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason\n\nYou also benefit from enhanced database availability when running your DB instance as a Multi-AZ deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is limited to the time automatic failover takes to complete.\n\nAnother implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover is automatic and requires no administration. In an Amazon RDS context, this means you are not required to monitor DB instance events and initiate manual DB instance recovery in the event of an Availability Zone failure or DB instance failure.\n\nIncorrect options:\n\nFor automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database - The availability benefits of Multi-AZ also extend to planned maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby.\n\nTo enhance read scalability, a Multi-AZ standby instance can be used to serve read requests - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. As such, the feature uses synchronous replication between primary and standby. AWS implementation makes sure the primary and the standby are constantly in sync, but precludes using the standby for read or write operations.\n\nUpdates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync - When you create your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across the Availability Zone to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.\n\nReference:\n\nhttps://aws.amazon.com/rds/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Running a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because Amazon RDS applies operating system updates by following these steps:"
      },
      {
        "answer": "",
        "explanation": "Perform maintenance on the standby."
      },
      {
        "answer": "",
        "explanation": "Promote the standby to primary."
      },
      {
        "answer": "",
        "explanation": "Perform maintenance on the old primary, which becomes the new standby."
      },
      {
        "answer": "",
        "explanation": "When you modify the database engine for your DB instance in a Multi-AZ deployment, then Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade."
      },
      {
        "answer": "Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You also benefit from enhanced database availability when running your DB instance as a Multi-AZ deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is limited to the time automatic failover takes to complete."
      },
      {
        "answer": "",
        "explanation": "Another implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover is automatic and requires no administration. In an Amazon RDS context, this means you are not required to monitor DB instance events and initiate manual DB instance recovery in the event of an Availability Zone failure or DB instance failure."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database</strong> - The availability benefits of Multi-AZ also extend to planned maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby."
      },
      {
        "answer": "",
        "explanation": "<strong>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</strong> - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. As such, the feature uses synchronous replication between primary and standby. AWS implementation makes sure the primary and the standby are constantly in sync, but precludes using the standby for read or write operations."
      },
      {
        "answer": "",
        "explanation": "<strong>Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</strong> - When you create your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across the Availability Zone to the standby in order to keep both in sync and protect your latest database updates against DB instance failure."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/faqs/"
    ]
  },
  {
    "id": 49,
    "question": "<p>A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets.</p>\n\n<p>How can you provide these users access in the least possible time, with minimal changes?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create a group, attach the policy to the group and place the users in the group",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a policy and assign it manually to the 50 users",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Update the Amazon S3 bucket policy",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nCreate a group, attach the policy to the group and place the users in the group\n\nAn IAM group is a collection of IAM users. You can use groups to specify permissions for a collection of users, which can make those permissions easier to manage for those users. For example, you could have a group called Admins and give that group the types of permissions that administrators typically need. Any user in that group automatically has the permissions that are assigned to the group. If a new user joins your organization and should have administrator privileges, you can assign the appropriate permissions by adding the user to that group.\n\nHere creating a group, assigning users to that group and attaching policies to that group is the best way.\n\nIncorrect options:\n\nUpdate the Amazon S3 bucket policy - Updating the Amazon S3 bucket policy could work but would not scale, as the size of the S3 bucket policy is limited (Bucket policies are limited to 20 KB in size).\n\nCreate a policy and assign it manually to the 50 users - An IAM user is an entity that you create in AWS. The IAM user represents the person or service who uses the IAM user to interact with AWS. Primary use for IAM users is to give people the ability to sign in to the AWS Management Console for interactive tasks and to make programmatic requests to AWS services using the API or CLI. A user in AWS consists of a name, a password to sign in to the AWS Management Console, and up to two access keys that can be used with the API or CLI.\n\nA policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied.\n\nIdentity-based policies  Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity.\n\nResource-based policies  Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts.\n\nCreating a policy and assigning it manually to users would work but would be hard to scale and manage.\n\nCreate an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA - AWS MFA adds extra security because it requires users to provide unique authentication from an AWS supported MFA mechanism in addition to their regular sign-in credentials when they access AWS websites or services. AWS MFA cannot help in terms of granting read/write access to only 50 of the IAM users.\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a group, attach the policy to the group and place the users in the group",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An IAM group is a collection of IAM users. You can use groups to specify permissions for a collection of users, which can make those permissions easier to manage for those users. For example, you could have a group called Admins and give that group the types of permissions that administrators typically need. Any user in that group automatically has the permissions that are assigned to the group. If a new user joins your organization and should have administrator privileges, you can assign the appropriate permissions by adding the user to that group."
      },
      {
        "answer": "",
        "explanation": "Here creating a group, assigning users to that group and attaching policies to that group is the best way."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Update the Amazon S3 bucket policy</strong> - Updating the Amazon S3 bucket policy could work but would not scale, as the size of the S3 bucket policy is limited (Bucket policies are limited to 20 KB in size)."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a policy and assign it manually to the 50 users</strong> - An IAM user is an entity that you create in AWS. The IAM user represents the person or service who uses the IAM user to interact with AWS. Primary use for IAM users is to give people the ability to sign in to the AWS Management Console for interactive tasks and to make programmatic requests to AWS services using the API or CLI. A user in AWS consists of a name, a password to sign in to the AWS Management Console, and up to two access keys that can be used with the API or CLI."
      },
      {
        "answer": "",
        "explanation": "A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied."
      },
      {
        "answer": "",
        "explanation": "Identity-based policies  Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity."
      },
      {
        "answer": "",
        "explanation": "Resource-based policies  Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts."
      },
      {
        "answer": "",
        "explanation": "Creating a policy and assigning it manually to users would work but would be hard to scale and manage."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA</strong> - AWS MFA adds extra security because it requires users to provide unique authentication from an AWS supported MFA mechanism in addition to their regular sign-in credentials when they access AWS websites or services. AWS MFA cannot help in terms of granting read/write access to only 50 of the IAM users."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html"
    ]
  },
  {
    "id": 50,
    "question": "<p>The data engineering team at a company wants to analyze Amazon S3 storage access patterns to decide when to transition the right data to the right storage class.</p>\n\n<p>Which of the following represents a correct option regarding the capabilities of Amazon S3 Analytics storage class analysis?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Storage class analysis only provides recommendations for Standard to Standard IA classes",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nStorage class analysis only provides recommendations for Standard to Standard IA classes\n\nBy using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class.\n\nStorage class analysis only provides recommendations for Standard to Standard IA classes.\n\nAfter storage class analysis observes the infrequent access patterns of a filtered set of data over a period of time, you can use the analysis results to help you improve your lifecycle configurations. You can configure storage class analysis to analyze all the objects in a bucket. Or, you can configure filters to group objects together for analysis by common prefix (that is, objects that have names that begin with a common string), by object tags, or by both prefix and tags.\n\nIncorrect options:\n\nStorage class analysis only provides recommendations for Standard to Standard One-Zone IA classes\n\nStorage class analysis only provides recommendations for Standard to Glacier Deep Archive classes\n\nStorage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html",
    "correctAnswerExplanations": [
      {
        "answer": "Storage class analysis only provides recommendations for Standard to Standard IA classes",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "By using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class."
      },
      {
        "answer": "",
        "explanation": "Storage class analysis only provides recommendations for Standard to Standard IA classes."
      },
      {
        "answer": "",
        "explanation": "After storage class analysis observes the infrequent access patterns of a filtered set of data over a period of time, you can use the analysis results to help you improve your lifecycle configurations. You can configure storage class analysis to analyze all the objects in a bucket. Or, you can configure filters to group objects together for analysis by common prefix (that is, objects that have names that begin with a common string), by object tags, or by both prefix and tags."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes",
        "explanation": ""
      },
      {
        "answer": "Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes",
        "explanation": ""
      },
      {
        "answer": "Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html"
    ]
  },
  {
    "id": 51,
    "question": "<p>A startup uses a fleet of Amazon EC2 servers to manage its CRM application. These Amazon EC2 servers are behind Elastic Load Balancing (ELB). Which of the following configurations are NOT allowed for Elastic Load Balancing?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nUse the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region\n\nElastic Load Balancer automatically distributes incoming traffic across multiple targets  Amazon EC2 instances, containers, IP addresses, and Lambda functions  in multiple Availability Zones and ensures only healthy targets receive traffic. ELB cannot distribute incoming traffic for targets deployed in different regions. This configuration is NOT allowed for the Elastic Load Balancer and therefore this is the correct option.\n\nIncorrect options:\n\nUse the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region\n\nUse the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region\n\nUse the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region\n\nThese three options are valid configurations for the Elastic Load Balancing to distribute traffic (either within an Availability Zone or between two Availability Zones).\n\nReference:\n\nhttps://aws.amazon.com/elasticloadbalancing/",
    "correctAnswerExplanations": [
      {
        "answer": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Elastic Load Balancer automatically distributes incoming traffic across multiple targets  Amazon EC2 instances, containers, IP addresses, and Lambda functions  in multiple Availability Zones and ensures only healthy targets receive traffic.\nELB cannot distribute incoming traffic for targets deployed in different regions. This configuration is NOT allowed for the Elastic Load Balancer and therefore this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region",
        "explanation": ""
      },
      {
        "answer": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region",
        "explanation": ""
      },
      {
        "answer": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options are valid configurations for the Elastic Load Balancing to distribute traffic (either within an Availability Zone or between two Availability Zones)."
      }
    ],
    "references": [
      "https://aws.amazon.com/elasticloadbalancing/"
    ]
  },
  {
    "id": 52,
    "question": "<p>A company has noticed several provisioned throughput exceptions on its Amazon DynamoDB database due to major spikes in the writes to the database. The development team wants to decouple the application layer from the database layer and dedicate a worker process to writing the data to Amazon DynamoDB.</p>\n\n<p>Which middleware do you recommend on using that can scale infinitely and meet these requirements in the most cost effective way?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon Kinesis Data Streams",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon DynamoDB DAX",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon Simple Queue Service (Amazon SQS)",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nAmazon Simple Queue Service (Amazon SQS)\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.\n\nUsing Amazon SQS as a middleware will help us sustain the write throughput during write peaks and therefore this option is the best fit for the given use-case.\n\nIncorrect options:\n\nAmazon DynamoDB DAX - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement  from milliseconds to microseconds  even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.\n\nDAX is used for caching reads, not to help with writes. So this option is ruled out.\n\nAmazon Kinesis Data Streams - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Kinesis is used to process consistent real-time data and does not scale as cost effectively as SQS to handle spikes in traffic.\n\nAmazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS won't keep our data if it cannot be delivered, so this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/sqs/\n\nhttps://aws.amazon.com/kinesis/data-streams/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon Simple Queue Service (Amazon SQS)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent."
      },
      {
        "answer": "",
        "explanation": "Using Amazon SQS as a middleware will help us sustain the write throughput during write peaks and therefore this option is the best fit for the given use-case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon DynamoDB DAX</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement  from milliseconds to microseconds  even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management."
      },
      {
        "answer": "",
        "explanation": "DAX is used for caching reads, not to help with writes. So this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service.  KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Kinesis is used to process consistent real-time data and does not scale as cost effectively as SQS to handle spikes in traffic."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS won't keep our data if it cannot be delivered, so this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 53,
    "question": "<p>Your e-commerce application is using an Amazon RDS PostgreSQL database and an analytics workload also runs on the same database. When the analytics workload is run, your e-commerce application slows down which further affects your sales.</p>\n\n<p>Which of the following is the MOST cost-optimal solution to fix this issue?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a Read Replica in another Region as the Master database and point the analytics workload there",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a Read Replica in the same Region as the Master database and point the analytics workload there",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Migrate the analytics application to AWS Lambda",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nCreate a Read Replica in the same Region as the Master database and point the analytics workload there\n\nAmazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source database instance. Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.\n\nCreating a Read Replica is the answer. As we want to minimize the costs, we need to launch the Read Replica in the same Region as you are not charged for the data transfer incurred in replicating data between your source database instance and read replica within the same AWS Region.\n\nExam Alert:\n\nPlease review this comparison vis-a-vis Multi-AZ vs Read Replica for Amazon RDS: \n via - https://aws.amazon.com/rds/features/multi-az/\n\nIncorrect options:\n\nEnable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region.\n\nEnabling Multi-AZ helps make our database highly-available, but the standby database is not accessible and cannot be used for reads or write. It's just a database that will become primary when the other database encounters a failure. So this option is not correct.\n\nMigrate the analytics application to AWS Lambda- AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\n\nRunning the application on AWS Lambda will not help, as it will still run against the main database and slow down our e-commerce application.\n\nCreate a Read Replica in another Region as the Master database and point the analytics workload there - This is incorrect because we have to pay for inter-Region data replication charges for the Read Replica, whereas the replication of data within a single Region is free.\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/multi-az/\n\nhttps://aws.amazon.com/rds/features/read-replicas/",
    "correctAnswerExplanations": [
      {
        "answer": "Create a Read Replica in the same Region as the Master database and point the analytics workload there",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source database instance. Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region."
      },
      {
        "answer": "",
        "explanation": "Creating a Read Replica is the answer. As we want to minimize the costs, we need to launch the Read Replica in the same Region as you are not charged for the data transfer incurred in replicating data between your source database instance and read replica within the same AWS Region."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q19-i1.jpg",
        "answer": "",
        "explanation": "Please review this comparison vis-a-vis Multi-AZ vs Read Replica for Amazon RDS:"
      },
      {
        "link": "https://aws.amazon.com/rds/features/multi-az/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region."
      },
      {
        "answer": "",
        "explanation": "Enabling Multi-AZ helps make our database highly-available, but the standby database is not accessible and cannot be used for reads or write. It's just a database that will become primary when the other database encounters a failure. So this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Migrate the analytics application to AWS Lambda</strong>- AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume."
      },
      {
        "answer": "",
        "explanation": "Running the application on AWS Lambda will not help, as it will still run against the main database and slow down our e-commerce application."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a Read Replica in another Region as the Master database and point the analytics workload there</strong> - This is incorrect because we have to pay for inter-Region data replication charges for the Read Replica, whereas the replication of data within a single Region is free."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://aws.amazon.com/rds/features/read-replicas/"
    ]
  },
  {
    "id": 54,
    "question": "<p>A security consultant is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the consultant wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified.</p>\n\n<p>Which of the following actions meets the given requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to the developer accounts",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Configure a new trail in AWS CloudTrail from within the developer accounts with the organization trails option enabled",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nSet up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to the developer accounts\n\nService control policy (SCP) is a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organizations access control guidelines.\n\nAn SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user. Any account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the AdministratorAccess IAM policy with / permissions to the user.\n\nSCPs don't affect users or roles in the management account. They affect only the member accounts in your organization.\n\nIncorrect options:\n\nConfigure a new trail in AWS CloudTrail from within the developer accounts with the organization trails option enabled - Configuring each developer account individually is not a viable solution to start with. In addition, any configuration changes can be undone by the user once they are logged into their individual accounts as root users.\n\nSet up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user - The root user can modify this IAM policy itself, so this option is not correct.\n\nSet up a service-linked role for AWS CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account - A service-linked role is a unique type of IAM role that is linked directly to an AWS service. Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf. The linked service also defines how you create, modify, and delete a service-linked role.\n\nThe linked service defines the permissions of its service-linked roles, and unless defined otherwise, only that service can assume the roles. The defined permissions include the trust policy and the permissions policy, and that permissions policy cannot be attached to any other entity such as the ARN in the master account.\n\nReference:\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
    "correctAnswerExplanations": [
      {
        "answer": "Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to the developer accounts",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Service control policy (SCP) is a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organizations access control guidelines."
      },
      {
        "answer": "",
        "explanation": "An SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user. Any account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the AdministratorAccess IAM policy with <em>/</em> permissions to the user."
      },
      {
        "answer": "",
        "explanation": "SCPs don't affect users or roles in the management account. They affect only the member accounts in your organization."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure a new trail in AWS CloudTrail from within the developer accounts with the organization trails option enabled</strong> - Configuring each developer account individually is not a viable solution to start with. In addition, any configuration changes can be undone by the user once they are logged into their individual accounts as root users."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user</strong> - The root user can modify this IAM policy itself, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account</strong> - A service-linked role is a unique type of IAM role that is linked directly to an AWS service. Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf. The linked service also defines how you create, modify, and delete a service-linked role."
      },
      {
        "answer": "",
        "explanation": "The linked service defines the permissions of its service-linked roles, and unless defined otherwise, only that service can assume the roles. The defined permissions include the trust policy and the permissions policy, and that permissions policy cannot be attached to any other entity such as the ARN in the master account."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
    ]
  },
  {
    "id": 55,
    "question": "<p>To support critical production workloads that require maximum resiliency, a company wants to configure network connections between its Amazon VPC and the on-premises infrastructure. The company needs AWS Direct Connect connections with speeds greater than 1 Gbps.</p>\n\n<p>As a solutions architect, which of the following will you suggest as the best architecture for this requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Managed VPN as a backup for AWS Direct Connect connections to ensure maximum resiliency",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Opt for two separate AWS Direct Connect connections terminating on separate devices in more than one Direct Connect location",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Opt for at least two AWS Direct Connect connections terminating on different devices at a single Direct Connect location",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Opt for one AWS Direct Connect connection at each of the multiple Direct Connect locations",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nOpt for two separate AWS Direct Connect connections terminating on separate devices in more than one Direct Connect location\n\nMaximum resilience is achieved by separate connections terminating on separate devices in more than one location. This configuration offers customers maximum resilience to failure. As shown in the figure above, such a topology provides resilience to device failure, connectivity failure, and complete location failure. You can use Direct Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS Direct Connect locations.\n\nMaximum Resiliency for Critical Workloads: \n via - https://aws.amazon.com/directconnect/resiliency-recommendation/\n\nIncorrect options:\n\nOpt for one AWS Direct Connect connection at each of the multiple Direct Connect locations - For critical production workloads that require high resiliency, it is recommended to have one connection at multiple locations. As shown in the figure below, such a topology ensures resilience to connectivity failure due to a fiber cut or a device failure as well as a complete location failure. You can use Direct Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS Direct Connect location.\n\nHigh Resiliency for Critical Workloads: \n via - https://aws.amazon.com/directconnect/resiliency-recommendation/\n\nOpt for at least two AWS Direct Connect connections terminating on different devices at a single Direct Connect location - For non-critical production workloads and development workloads that do not require high resiliency, it is recommended to have at least two connections terminating on different devices at a single location. As shown in the figure above, such a topology helps in the case of the device failure at a location but does not help in the event of a total location failure.\n\nNon Critical Production Workloads or Development Workloads: \n via - https://aws.amazon.com/directconnect/resiliency-recommendation/\n\nUse AWS Managed VPN as a backup for AWS Direct Connect connections to ensure maximum resiliency - It is important to understand that AWS Managed VPN supports up to 1.25 Gbps throughput per VPN tunnel and does not support Equal Cost Multi-Path (ECMP) for egress data path in the case of multiple AWS Managed VPN tunnels terminating on the same VGW. Thus, AWS does not recommend customers use AWS Managed VPN as a backup for AWS Direct Connect connections with speeds greater than 1 Gbps.\n\nReference:\n\nhttps://aws.amazon.com/directconnect/resiliency-recommendation/",
    "correctAnswerExplanations": [
      {
        "answer": "Opt for two separate AWS Direct Connect connections terminating on separate devices in more than one Direct Connect location",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Maximum resilience is achieved by separate connections terminating on separate devices in more than one location. This configuration offers customers maximum resilience to failure. As shown in the figure above, such a topology provides resilience to device failure, connectivity failure, and complete location failure. You can use Direct Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS Direct Connect locations."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q22-i1.jpg",
        "answer": "",
        "explanation": "Maximum Resiliency for Critical Workloads:"
      },
      {
        "link": "https://aws.amazon.com/directconnect/resiliency-recommendation/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Opt for one AWS Direct Connect connection at each of the multiple Direct Connect locations</strong> - For critical production workloads that require high resiliency, it is recommended to have one connection at multiple locations. As shown in the figure below, such a topology ensures resilience to connectivity failure due to a fiber cut or a device failure as well as a complete location failure. You can use Direct Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS Direct Connect location."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q22-i2.jpg",
        "answer": "",
        "explanation": "High Resiliency for Critical Workloads:"
      },
      {
        "link": "https://aws.amazon.com/directconnect/resiliency-recommendation/"
      },
      {
        "answer": "",
        "explanation": "<strong>Opt for at least two AWS Direct Connect connections terminating on different devices at a single Direct Connect location</strong> - For non-critical production workloads and development workloads that do not require high resiliency, it is recommended to have at least two connections terminating on different devices at a single location. As shown in the figure above, such a topology helps in the case of the device failure at a location but does not help in the event of a total location failure."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q22-i3.jpg",
        "answer": "",
        "explanation": "Non Critical Production Workloads or Development Workloads:"
      },
      {
        "link": "https://aws.amazon.com/directconnect/resiliency-recommendation/"
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Managed VPN as a backup for AWS Direct Connect connections to ensure maximum resiliency</strong> - It is important to understand that AWS Managed VPN supports up to 1.25 Gbps throughput per VPN tunnel and does not support Equal Cost Multi-Path (ECMP) for egress data path in the case of multiple AWS Managed VPN tunnels terminating on the same VGW. Thus, AWS does not recommend customers use AWS Managed VPN as a backup for AWS Direct Connect connections with speeds greater than 1 Gbps."
      }
    ],
    "references": [
      "https://aws.amazon.com/directconnect/resiliency-recommendation/",
      "https://aws.amazon.com/directconnect/resiliency-recommendation/",
      "https://aws.amazon.com/directconnect/resiliency-recommendation/",
      "https://aws.amazon.com/directconnect/resiliency-recommendation/"
    ]
  },
  {
    "id": 56,
    "question": "<p>An e-commerce application uses a relational database that runs several queries that perform joins on multiple tables. The development team has found that these queries are slow and expensive, therefore these are a good candidate for caching. The application needs to use a caching service that supports multi-threading.</p>\n\n<p>As a solutions architect, which of the following services would you recommend for the given use case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon ElastiCache for Memcached",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Amazon ElastiCache for Redis",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Global Accelerator",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon DynamoDB Accelerator (DAX)",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nAmazon ElastiCache for Memcached\n\nAmazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store and cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.\n\nMemcached is an open-source, distributed, in-memory key-value store that can retrieve data in milliseconds. Caching site information with Memcached can help you improve the performance and scalability of your site while controlling cost.\n\nChoose Memcached if the following apply to you:\n\nYou need the simplest model possible.\n\nYou need to run large nodes with multiple cores or threads (support for multi-threading).\n\nYou need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases.\n\nYou need to cache objects.\n\n via - https://aws.amazon.com/elasticache/redis-vs-memcached/\n\nIncorrect options:\n\nAmazon ElastiCache for Redis - Redis, which stands for Remote Dictionary Server, is a fast, open-source, in-memory key-value data store for use as a database, cache, message broker, and queue. Redis now delivers sub-millisecond response times enabling millions of requests per second for real-time applications in Gaming, Ad-Tech, Financial Services, Healthcare, and IoT. Redis is a popular choice for caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing, chat/messaging, media streaming, and pub/sub apps.\n\nRedis does not support multi-threading, so this option is not the right fit for the given use case.\n\nAmazon DynamoDB Accelerator (DAX) - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB. DAX does not support relational databases.\n\nAWS Global Accelerator - AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. This option has been added as a distractor, it has nothing to do with database caching.\n\nReferences:\n\nhttps://aws.amazon.com/caching/aws-caching/\n\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\n\nhttps://aws.amazon.com/elasticache/redis-vs-memcached/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon ElastiCache for Memcached",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store and cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases."
      },
      {
        "answer": "",
        "explanation": "Memcached is an open-source, distributed, in-memory key-value store that can retrieve data in milliseconds. Caching site information with Memcached can help you improve the performance and scalability of your site while controlling cost."
      },
      {
        "answer": "",
        "explanation": "Choose Memcached if the following apply to you:"
      },
      {
        "answer": "",
        "explanation": "You need the simplest model possible."
      },
      {
        "answer": "",
        "explanation": "You need to run large nodes with multiple cores or threads (support for multi-threading)."
      },
      {
        "answer": "",
        "explanation": "You need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases."
      },
      {
        "answer": "",
        "explanation": "You need to cache objects."
      },
      {
        "link": "https://aws.amazon.com/elasticache/redis-vs-memcached/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon ElastiCache for Redis</strong> - Redis, which stands for Remote Dictionary Server, is a fast, open-source, in-memory key-value data store for use as a database, cache, message broker, and queue. Redis now delivers sub-millisecond response times enabling millions of requests per second for real-time applications in Gaming, Ad-Tech, Financial Services, Healthcare, and IoT. Redis is a popular choice for caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing, chat/messaging, media streaming, and pub/sub apps."
      },
      {
        "answer": "",
        "explanation": "Redis does not support multi-threading, so this option is not the right fit for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB. DAX does not support relational databases."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Global Accelerator</strong> - AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. This option has been added as a distractor, it has nothing to do with database caching."
      }
    ],
    "references": [
      "https://aws.amazon.com/elasticache/redis-vs-memcached/",
      "https://aws.amazon.com/caching/aws-caching/",
      "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html",
      "https://aws.amazon.com/elasticache/redis-vs-memcached/"
    ]
  },
  {
    "id": 57,
    "question": "<p>The DevOps team at a major financial services company uses Multi-Availability Zone (Multi-AZ) deployment for its MySQL Amazon RDS database in order to automate its database replication and augment data durability. The DevOps team has scheduled a maintenance window for a database engine level upgrade for the coming weekend.</p>\n\n<p>Which of the following is the correct outcome during the maintenance window?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary database instance. This does not cause any downtime for the duration of the upgrade",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. This causes downtime until the upgrade is complete",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby database instance. This does not cause any downtime for the duration of the upgrade",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nAny database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. This causes downtime until the upgrade is complete\n\nAmazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups.\n\nUpgrades to the database engine level require downtime. Even if your Amazon RDS DB instance uses a Multi-AZ deployment, both the primary and standby DB instances are upgraded at the same time. This causes downtime until the upgrade is complete, and the duration of the downtime varies based on the size of your database instance.\n\nAmazon RDS DB Engine Maintenance: \n via - https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/\n\nIncorrect options:\n\nAny database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete - For Amazon RDS database engine level upgrade, primary and standby database instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect.\n\nAny database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary database instance. This does not cause any downtime for the duration of the upgrade - For Amazon RDS database engine level upgrade, primary and standby database instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect.\n\nAny database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby database instance. This does not cause any downtime for the duration of the upgrade - For Amazon RDS database engine level upgrade, primary and standby database instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect.\n\nReference:\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/",
    "correctAnswerExplanations": [
      {
        "answer": "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. This causes downtime until the upgrade is complete",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups."
      },
      {
        "answer": "",
        "explanation": "Upgrades to the database engine level require downtime. Even if your Amazon RDS DB instance uses a Multi-AZ deployment, both the primary and standby DB instances are upgraded at the same time. This causes downtime until the upgrade is complete, and the duration of the downtime varies based on the size of your database instance."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q8-i1.jpg",
        "answer": "",
        "explanation": "Amazon RDS DB Engine Maintenance:"
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete</strong> - For Amazon RDS database engine level upgrade, primary and standby database instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary database instance. This does not cause any downtime for the duration of the upgrade</strong> - For Amazon RDS database engine level upgrade, primary and standby database instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby database instance. This does not cause any downtime for the duration of the upgrade</strong> - For Amazon RDS database engine level upgrade, primary and standby database instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/",
      "https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/"
    ]
  },
  {
    "id": 58,
    "question": "<p>The engineering team at a social media company has noticed that while some of the images stored in Amazon S3 are frequently accessed, others sit idle for a considerable span of time.</p>\n\n<p>As a solutions architect, what is your recommendation to build the MOST cost-effective solution?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Store the images using the Amazon S3 Intelligent-Tiering storage class",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Store the images using the Amazon S3 Standard-IA storage class",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nStore the images using the Amazon S3 Intelligent-Tiering storage class\n\nThe Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.\n\nFor a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. Therefore using the Amazon S3 Intelligent-Tiering storage class is the correct solution for the given problem statement.\n\nAmazon S3 Storage Classes Overview: \n\nIncorrect options:\n\nStore the images using the Amazon S3 Standard-IA storage class\n\nAmazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. As some of the objects are frequently accessed, the per GB retrieval fee for Amazon S3 Standard-IA can cause the costs to shoot up, hence this option is incorrect.\n\nCreate a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class\n\nCreate a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class\n\nCreating a data monitoring application on an Amazon EC2 instance for managing the desired Amazon S3 storage class entails significant development cost as well as infrastructure maintenance effort. The Amazon S3 Intelligent-Tiering storage class does the job in a cost-effective way. Therefore both these options are incorrect.\n\nReference:\n\nhttps://aws.amazon.com/s3/storage-classes/",
    "correctAnswerExplanations": [
      {
        "answer": "Store the images using the Amazon S3 Intelligent-Tiering storage class",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access."
      },
      {
        "answer": "",
        "explanation": "For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. Therefore using the Amazon S3 Intelligent-Tiering storage class is the correct solution for the given problem statement."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q9-i1.jpg",
        "answer": "",
        "explanation": "Amazon S3 Storage Classes Overview:"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Store the images using the Amazon S3 Standard-IA storage class",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. As some of the objects are frequently accessed, the per GB retrieval fee for Amazon S3 Standard-IA can cause the costs to shoot up, hence this option is incorrect."
      },
      {
        "answer": "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class",
        "explanation": ""
      },
      {
        "answer": "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Creating a data monitoring application on an Amazon EC2 instance for managing the desired Amazon S3 storage class entails significant development cost as well as infrastructure maintenance effort. The Amazon S3 Intelligent-Tiering storage class does the job in a cost-effective way. Therefore both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "id": 59,
    "question": "<p>A retail company's procurement application becomes slow when traffic spikes. The application has a three-tier architecture (web, application and database tier) that uses synchronous transactions. The engineering team at the company has identified certain bottlenecks in the application tier but it does not want to change the underlying application architecture.</p>\n\n<p>As a solutions architect, which of the following solutions would you suggest to meet the required application response times while accounting for any traffic spikes?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Leverage Amazon SQS with asynchronous AWS Lambda calls to decouple the application and data tiers",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nLeverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer\n\nA horizontally scalable system is one that can increase capacity by adding more computers to the system. This is in contrast to a vertically scalable system, which is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory or storage.\n\nHorizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers.\n\nElastic Load Balancing is used to automatically distribute your incoming application traffic across all the Amazon EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed.\n\nTo use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto Scaling group to register the group with the load balancer. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group.\n\nWhen you use Elastic Load Balancing with your Auto Scaling group, it's not necessary to register individual Amazon EC2 instances with the load balancer. Instances that are launched by your Auto Scaling group are automatically registered with the load balancer. Likewise, instances that are terminated by your Auto Scaling group are automatically deregistered from the load balancer.\n\nThis option will require fewer design changes, it's mostly configuration changes and the ability for the web/application tier to be able to communicate across instances. Hence, this is the right solution for the current use case.\n\nIncorrect options:\n\nLeverage Amazon SQS with asynchronous AWS Lambda calls to decouple the application and data tiers - This is incorrect as it uses asynchronous AWS Lambda calls and the application uses synchronous transactions. The question says there should be no change in the application architecture.\n\nLeverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS - The issue is not with the persistence layer at all. This option has only been used as a distractor.\n\nYou can deploy scalable Oracle Real Application Clusters (RAC) on Amazon EC2 using Amazon Machine Images (AMI) on AWS Marketplace. Oracle RAC is a shared-everything database cluster technology from Oracle that allows a single database (a set of data files) to be concurrently accessed and served by one or many database server instances.\n\nLeverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size - Vertical scaling is just a band-aid solution and will not work long term.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\n\nhttps://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/",
    "correctAnswerExplanations": [
      {
        "answer": "Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A horizontally scalable system is one that can increase capacity by adding more computers to the system. This is in contrast to a vertically scalable system, which is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory or storage."
      },
      {
        "answer": "",
        "explanation": "Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers."
      },
      {
        "answer": "",
        "explanation": "Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the Amazon EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed."
      },
      {
        "answer": "",
        "explanation": "To use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto Scaling group to register the group with the load balancer. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group."
      },
      {
        "answer": "",
        "explanation": "When you use Elastic Load Balancing with your Auto Scaling group, it's not necessary to register individual Amazon EC2 instances with the load balancer. Instances that are launched by your Auto Scaling group are automatically registered with the load balancer. Likewise, instances that are terminated by your Auto Scaling group are automatically deregistered from the load balancer."
      },
      {
        "answer": "",
        "explanation": "This option will require fewer design changes, it's mostly configuration changes and the ability for the web/application tier to be able to communicate across instances. Hence, this is the right solution for the current use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</strong> - This is incorrect as it uses asynchronous AWS Lambda calls and the application uses synchronous transactions. The question says there should be no change in the application architecture."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</strong> - The issue is not with the persistence layer at all. This option has only been used as a distractor."
      },
      {
        "answer": "",
        "explanation": "You can deploy scalable Oracle Real Application Clusters (RAC) on Amazon EC2 using Amazon Machine Images (AMI) on AWS Marketplace. Oracle RAC is a shared-everything database cluster technology from Oracle that allows a single database (a set of data files) to be concurrently accessed and served by one or many database server instances."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</strong> - Vertical scaling is just a band-aid solution and will not work long term."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html",
      "https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/"
    ]
  },
  {
    "id": 60,
    "question": "<p>As a Solutions Architect, you would like to completely secure the communications between your Amazon CloudFront distribution and your Amazon S3 bucket which contains the static files for your website. Users should only be able to access the Amazon S3 bucket through Amazon CloudFront and not directly.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Make the Amazon S3 bucket public",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nCreate an origin access identity (OAI) and update the Amazon S3 Bucket Policy\n\nTo restrict access to content that you serve from Amazon S3 buckets, you need to follow the following steps:\n\nCreate a special Amazon CloudFront user called an origin access identity (OAI) and associate it with your distribution.\nConfigure your Amazon S3 bucket permissions so that Amazon CloudFront can use the OAI to access the files in your bucket and serve them to your users. Make sure that users cant use a direct URL to the Amazon S3 bucket to access a file there.\n\nAfter you take these steps, users can only access your files through Amazon CloudFront, not directly from the Amazon S3 bucket.\n\nIn general, if youre using an Amazon S3 bucket as the origin for a Amazon CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you restrict access by using, for example, Amazon CloudFront signed URLs or signed cookies, you also wont want people to be able to view files by simply using the direct Amazon S3 URL for the file. Instead, you want them to only access the files by using the Amazon CloudFront URL, so your content remains protected.\n\nIncorrect options:\n\nUpdate the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group - Amazon S3 buckets don't have security groups, hence this is an incorrect option.\n\nMake the Amazon S3 bucket public - If the Amazon S3 bucket is made public, it can be accessed by anyone directly. This is not the requirement.\n\nCreate a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution - You cannot attach IAM roles to the Amazon CloudFront distribution. Here you need to use an OAI.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "To restrict access to content that you serve from Amazon S3 buckets, you need to follow the following steps:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li>Create a special Amazon CloudFront user called an origin access identity (OAI) and associate it with your distribution.</li>\n<li>Configure your Amazon S3 bucket permissions so that Amazon CloudFront can use the OAI to access the files in your bucket and serve them to your users. Make sure that users cant use a direct URL to the Amazon S3 bucket to access a file there.</li>\n</ol>"
      },
      {
        "answer": "",
        "explanation": "After you take these steps, users can only access your files through Amazon CloudFront, not directly from the Amazon S3 bucket."
      },
      {
        "answer": "",
        "explanation": "In general, if youre using an Amazon S3 bucket as the origin for a Amazon CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you restrict access by using, for example, Amazon CloudFront signed URLs or signed cookies, you also wont want people to be able to view files by simply using the direct Amazon S3 URL for the file. Instead, you want them to only access the files by using the Amazon CloudFront URL, so your content remains protected."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group</strong> - Amazon S3 buckets don't have security groups, hence this is an incorrect option."
      },
      {
        "answer": "",
        "explanation": "<strong>Make the Amazon S3 bucket public</strong> - If the Amazon S3 bucket is made public, it can be accessed by anyone directly. This is not the requirement."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution</strong> -  You cannot attach IAM roles to the Amazon CloudFront distribution. Here you need to use an OAI."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html"
    ]
  },
  {
    "id": 61,
    "question": "<p>The DevOps team at an e-commerce company has deployed a fleet of Amazon EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within the <code>us-east-1</code> region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the Amazon EC2 instances under the Auto Scaling Group. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones (AZ) to have unbalanced resources. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check.</p>\n\n<p>Can you identify the correct outcomes for these events? (Select two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it",
        "correct": false
      },
      {
        "id": 2,
        "answer": "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance",
        "correct": true
      },
      {
        "id": 4,
        "answer": "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct options:\n\nAs the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application\n\nAmazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. Actions such as changing the Availability Zones (AZ) for your group or explicitly terminating or detaching instances can lead to the Auto Scaling group becoming unbalanced between Availability Zones. Amazon EC2 Auto Scaling compensates by rebalancing the Availability Zones.\n\nWhen rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application. Therefore, this option is correct.\n\nAvailability Zone Rebalancing Overview: \n via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\n\nAmazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance\n\nHowever, the scaling activity of Auto Scaling works in a different sequence compared to the rebalancing activity. Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.\n\nIncorrect options:\n\nAmazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it - This option contradicts the correct sequence of events outlined earlier for scaling activity created by Amazon EC2 Auto Scaling. Actually, Auto Scaling first terminates the unhealthy instance and then launches a new instance. Hence this is incorrect.\n\nAs the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched - This option contradicts the correct sequence of events outlined earlier for rebalancing activity. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones. Hence this is incorrect.\n\nAmazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously - This is a made-up option as both the terminate and launch activities can't happen simultaneously. This option has been added as a distractor.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html",
    "correctAnswerExplanations": [
      {
        "answer": "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.\nActions such as changing the Availability Zones (AZ) for your group or explicitly terminating or detaching instances can lead to the Auto Scaling group becoming unbalanced between Availability Zones. Amazon EC2 Auto Scaling compensates by rebalancing the Availability Zones."
      },
      {
        "answer": "",
        "explanation": "When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application. Therefore, this option is correct."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q6-i1.jpg",
        "answer": "",
        "explanation": "Availability Zone Rebalancing Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html"
      },
      {
        "answer": "Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "However, the scaling activity of Auto Scaling works in a different sequence compared to the rebalancing activity. Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it</strong> - This option contradicts the correct sequence of events outlined earlier for scaling activity created by Amazon EC2 Auto Scaling. Actually, Auto Scaling first terminates the unhealthy instance and then launches a new instance. Hence this is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched</strong> - This option contradicts the correct sequence of events outlined earlier for rebalancing activity. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones. Hence this is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously</strong> - This is a made-up option as both the terminate and launch activities can't happen simultaneously. This option has been added as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html"
    ]
  },
  {
    "id": 62,
    "question": "<p>The systems administrator at a company wants to set up a highly available architecture for a bastion host solution.</p>\n\n<p>As a solutions architect, which of the following options would you recommend as the solution?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nCreate a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group\n\nNetwork Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers  within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.\n\nIncluding bastion hosts in your VPC environment enables you to securely connect to your Linux instances without exposing your environment to the Internet. After you set up your bastion hosts, you can access the other instances in your VPC through Secure Shell (SSH) connections on Linux. Bastion hosts are also configured with security groups to provide fine-grained ingress control.\n\nYou need to remember that Bastion Hosts are using the SSH protocol, which is a TCP based protocol on port 22. They must be publicly accessible.\n\nHere, the correct answer is to use a Network Load Balancer, which supports TCP traffic, and will automatically allow you to connect to the Amazon EC2 instance in the backend.\n\nIncorrect options:\n\nCreate an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group - An elastic IP address (EIP) can only be attached to one Amazon EC2 instance at a time, so it won't provide you a highly available setup on its own. Note that if we had two Elastic IPs and two Bastion Hosts, this would work.\n\nCreate a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.\n\nVPC Endpoints are not used on top of Amazon EC2 instances. They're a way to access AWS services privately within your VPC (without using the public internet). This is a distractor.\n\nCreate a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group - Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets  Amazon EC2 instances, containers, IP addresses and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.\n\nAn Application Load Balancer only supports HTTP traffic, which is layer 7, while the SSH protocol is based on TCP and is layer 4. So, the Application Load Balancer doesn't work.\n\nReferences:\n\nhttps://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers  within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data."
      },
      {
        "answer": "",
        "explanation": "Including bastion hosts in your VPC environment enables you to securely connect to your Linux instances without exposing your environment to the Internet. After you set up your bastion hosts, you can access the other instances in your VPC through Secure Shell (SSH) connections on Linux. Bastion hosts are also configured with security groups to provide fine-grained ingress control."
      },
      {
        "answer": "",
        "explanation": "You need to remember that Bastion Hosts are using the SSH protocol, which is a TCP based protocol on port 22. They must be publicly accessible."
      },
      {
        "answer": "",
        "explanation": "Here, the correct answer is to use a Network Load Balancer, which supports TCP traffic, and will automatically allow you to connect to the Amazon EC2 instance in the backend."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group</strong> - An elastic IP address (EIP) can only be attached to one Amazon EC2 instance at a time, so it won't provide you a highly available setup on its own. Note that if we had two Elastic IPs and two Bastion Hosts, this would work."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group</strong> - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network."
      },
      {
        "answer": "",
        "explanation": "VPC Endpoints are not used on top of Amazon EC2 instances. They're a way to access AWS services privately within your VPC (without using the public internet). This is a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group</strong> - Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets  Amazon EC2 instances, containers, IP addresses and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications."
      },
      {
        "answer": "",
        "explanation": "An Application Load Balancer only supports HTTP traffic, which is layer 7, while the SSH protocol is based on TCP and is layer 4. So, the Application Load Balancer doesn't work."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html"
    ]
  },
  {
    "id": 63,
    "question": "<p>A company maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the company has transitioned from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all the data to an Amazon S3 bucket, the company is now looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data.</p>\n\n<p>How will you implement this requirement without adding the overhead of splitting the data into logical groups?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nConfigure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data\n\nServer-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. When you use server-side encryption with Amazon S3 managed keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates.\n\nNote: Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 will be automatically encrypted at no additional cost and with no impact on performance.\n\nIncorrect options:\n\nStore the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data - Server-side encryption with Amazon S3 managed keys (SSE-S3) is the easiest way to implement the given requirement, as there is no additional overhead of splitting data. Multiple S3 buckets are redundant for this requirement.\n\nUse Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data - Server-side encryption is the encryption of data at its destination by the application or service that receives it. The requirement is about server-side encryption and not about client-side encryption, hence this choice is incorrect.\n\nConfigure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket - An encryption context is a set of key-value pairs that contain additional contextual information about the data. When an encryption context is specified for an encryption operation, Amazon S3 must specify the same encryption context for the decryption operation. The encryption context offers another level of security for the encryption key. However, it is not useful for generating unique keys.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. When you use server-side encryption with Amazon S3 managed keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates."
      },
      {
        "answer": "",
        "explanation": "Note: Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 will be automatically encrypted at no additional cost and with no impact on performance."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong> - Server-side encryption with Amazon S3 managed keys (SSE-S3) is the easiest way to implement the given requirement, as there is no additional overhead of splitting data. Multiple S3 buckets are redundant for this requirement."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data</strong> - Server-side encryption is the encryption of data at its destination by the application or service that receives it. The requirement is about server-side encryption and not about client-side encryption, hence this choice is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket</strong> - An encryption context is a set of key-value pairs that contain additional contextual information about the data. When an encryption context is specified for an encryption operation, Amazon S3 must specify the same encryption context for the decryption operation. The encryption context offers another level of security for the encryption key. However, it is not useful for generating unique keys."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html"
    ]
  },
  {
    "id": 64,
    "question": "<p>You are using AWS Lambda to implement a batch job for a big data analytics workflow. Based on historical trends, a similar job runs for 30 minutes on average. The AWS Lambda function pulls data from Amazon S3, processes it, and then writes the results back to Amazon S3. When you deployed your AWS Lambda function, you noticed an issue where the AWS Lambda function abruptly failed after 15 minutes of execution.</p>\n\n<p>As a solutions architect, which of the following would you identify as the root cause of the issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "The AWS Lambda function is missing IAM permissions",
        "correct": false
      },
      {
        "id": 2,
        "answer": "The AWS Lambda function is timing out",
        "correct": true
      },
      {
        "id": 3,
        "answer": "The AWS Lambda function is running out of memory",
        "correct": false
      },
      {
        "id": 4,
        "answer": "The AWS Lambda function chosen runtime is wrong",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nAWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\n\nWith AWS Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app. AWS Lambda functions can be configured to run up to 15 minutes per execution. You can set the timeout to any value between 1 second and 15 minutes.\n\nThe AWS Lambda function is timing out\n\nAWS Lambda functions time out after 15 minutes, and are not usually meant for long-running jobs.\n\nIncorrect options:\n\nThe AWS Lambda function is running out of memory - Memory errors will not result in the abrupt termination of the function with no error message.\n\nThe AWS Lambda function chosen runtime is wrong - AWS Lambda function execution will fail if there is an issue with runtime. So, this is not the issue in the current case.\n\nThe AWS Lambda function is missing IAM permissions - Without enough permissions, AWS Lambda would not have been able to start its execution at all. So, permissions are not an issue here.\n\nReference:\n\nhttps://aws.amazon.com/lambda/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume."
      },
      {
        "answer": "",
        "explanation": "With AWS Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app. AWS Lambda functions can be configured to run up to 15 minutes per execution. You can set the timeout to any value between 1 second and 15 minutes."
      },
      {
        "answer": "The AWS Lambda function is timing out",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Lambda functions time out after 15 minutes, and are not usually meant for long-running jobs."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The AWS Lambda function is running out of memory</strong> - Memory errors will not result in the abrupt termination of the function with no error message."
      },
      {
        "answer": "",
        "explanation": "<strong>The AWS Lambda function chosen runtime is wrong</strong> - AWS Lambda function execution will fail if there is an issue with runtime. So, this is not the issue in the current case."
      },
      {
        "answer": "",
        "explanation": "<strong>The AWS Lambda function is missing IAM permissions</strong> - Without enough permissions, AWS Lambda would not have been able to start its execution at all. So, permissions are not an issue here."
      }
    ],
    "references": [
      "https://aws.amazon.com/lambda/faqs/"
    ]
  },
  {
    "id": 65,
    "question": "<p>A development team wants to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?</p>\n\n<p>Which of the following options represents the correct solution?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nConfigure the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set\n\nAmazon S3 encrypts your data at the object level as it writes to disks in AWS data centers, and decrypts it for you when you access it. You can encrypt objects by using client-side encryption or server-side encryption. Client-side encryption occurs when an object is encrypted before you upload it to Amazon S3, and the keys are not managed by AWS. With server-side encryption, Amazon manages the keys in one of three ways:\n\nServer-side encryption with customer-provided encryption keys (SSE-C).\nSSE-S3.\nSSE-KMS.\n\nServer-side encryption is about data encryption at restthat is, Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects.\n\nTo encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS.\n\nIn order to enforce object encryption, create an Amazon S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header. There are two possible values for the x-amz-server-side-encryption header: AES256, which tells S3 to use S3-managed keys, and aws:kms, which tells Amazon S3 to use AWS KMSmanaged keys.\n\nIncorrect options:\n\nConfigure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private - The x-amz-acl header is used to specify an ACL in the PutObject request. Access permissions are defined using this header.\n\nConfigure the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true - By default, Amazon S3 allows both HTTP and HTTPS requests. aws:SecureTransport key is used to check if the request is sent through HTTP or HTTPS. When this key is true, it means that the request is sent through HTTPS.\n\nConfigure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set - As discussed above, the s3:x-amz-acl header is used to set permissions on the specified S3 bucket and has nothing to do with encryption.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 encrypts your data at the object level as it writes to disks in AWS data centers, and decrypts it for you when you access it. You can encrypt objects by using client-side encryption or server-side encryption. Client-side encryption occurs when an object is encrypted before you upload it to Amazon S3, and the keys are not managed by AWS. With server-side encryption, Amazon manages the keys in one of three ways:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li>Server-side encryption with customer-provided encryption keys (SSE-C).</li>\n<li>SSE-S3.</li>\n<li>SSE-KMS.</li>\n</ol>"
      },
      {
        "answer": "",
        "explanation": "Server-side encryption is about data encryption at restthat is, Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects."
      },
      {
        "answer": "",
        "explanation": "To encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS."
      },
      {
        "answer": "",
        "explanation": "In order to enforce object encryption, create an Amazon S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header. There are two possible values for the x-amz-server-side-encryption header: AES256, which tells S3 to use S3-managed keys, and aws:kms, which tells Amazon S3 to use AWS KMSmanaged keys."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private</strong> - The x-amz-acl header is used to specify an ACL in the PutObject request. Access permissions are defined using this header."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true</strong> - By default, Amazon S3 allows both HTTP and HTTPS requests. aws:SecureTransport key is used to check if the request is sent through HTTP or HTTPS. When this key is true, it means that the request is sent through HTTPS."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set</strong> - As discussed above, the s3:x-amz-acl header is used to set permissions on the specified S3 bucket and has nothing to do with encryption."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html"
    ]
  }
]