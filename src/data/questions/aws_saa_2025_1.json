[
  {
    "id": 1,
    "question": "A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis.\n\nWhich of the following options addresses the given use case?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Leverage Amazon API Gateway with AWS Lambda",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Leverage Amazon API Gateway with Amazon Kinesis Data Analytics",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Leverage Amazon Athena with Amazon S3",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Leverage Amazon QuickSight with Amazon Redshift",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nLeverage Amazon API Gateway with Amazon Kinesis Data Analytics\n\nYou can use Kinesis Data Analytics to transform and analyze streaming data in real-time with Apache Flink. Kinesis Data Analytics enables you to quickly build end-to-end stream processing applications for log analytics, clickstream analytics, Internet of Things (IoT), ad tech, gaming, etc. The four most common use cases are streaming extract-transform-load (ETL), continuous metric generation, responsive real-time analytics, and interactive querying of data streams. Kinesis Data Analytics for Apache Flink applications provides your application 50 GB of running application storage per Kinesis Processing Unit (KPU).\n\nAmazon API Gateway is a fully managed service that allows you to publish, maintain, monitor, and secure APIs at any scale. Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs.\n\nAmazon API Gateway: \n via - https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/\n\nFor the given use case, you can use Amazon API Gateway to create a REST API that handles incoming requests having location data from the trucks and sends it to the Kinesis Data Analytics application on the back end.\n\nAmazon Kinesis Data Analytics: \n via - https://aws.amazon.com/kinesis/data-analytics/\n\nIncorrect options:\n\nLeverage Amazon Athena with Amazon S3 - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena cannot be used to build a REST API to consume data from the source. So this option is incorrect.\n\nLeverage Amazon QuickSight with Amazon Redshift - QuickSight is a cloud-native, serverless business intelligence service. Quicksight cannot be used to build a REST API to consume data from the source. Redshift is a fully managed AWS cloud data warehouse. So this option is incorrect.\n\nLeverage Amazon API Gateway with AWS Lambda - You cannot use Lambda to store and retrieve the location data for analysis, so this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html\n\nhttps://aws.amazon.com/kinesis/data-analytics/\n\nhttps://aws.amazon.com/kinesis/data-analytics/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "Leverage Amazon API Gateway with Amazon Kinesis Data Analytics",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can use Kinesis Data Analytics to transform and analyze streaming data in real-time with Apache Flink. Kinesis Data Analytics enables you to quickly build end-to-end stream processing applications for log analytics, clickstream analytics, Internet of Things (IoT), ad tech, gaming, etc. The four most common use cases are streaming extract-transform-load (ETL), continuous metric generation, responsive real-time analytics, and interactive querying of data streams. Kinesis Data Analytics for Apache Flink applications provides your application 50 GB of running application storage per Kinesis Processing Unit (KPU)."
      },
      {
        "answer": "",
        "explanation": "Amazon API Gateway is a fully managed service that allows you to publish, maintain, monitor, and secure APIs at any scale. Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q57-i1.jpg",
        "answer": "",
        "explanation": "Amazon API Gateway:"
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can use Amazon API Gateway to create a REST API that handles incoming requests having location data from the trucks and sends it to the Kinesis Data Analytics application on the back end."
      },
      {
        "image": "https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Amazon-Kinesis-Data-Analytics_HIW.82e3aa53a5c87db03c766218b3d51f1a110c60eb.png",
        "answer": "",
        "explanation": "Amazon Kinesis Data Analytics:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-analytics/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Athena with Amazon S3</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena cannot be used to build a REST API to consume data from the source. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon QuickSight with Amazon Redshift</strong> - QuickSight is a cloud-native, serverless business intelligence service. Quicksight cannot be used to build a REST API to consume data from the source. Redshift is a fully managed AWS cloud data warehouse. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon API Gateway with AWS Lambda</strong> - You cannot use Lambda to store and retrieve the location data for analysis, so this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/",
      "https://aws.amazon.com/kinesis/data-analytics/",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html",
      "https://aws.amazon.com/kinesis/data-analytics/",
      "https://aws.amazon.com/kinesis/data-analytics/faqs/"
    ]
  },
  {
    "id": 2,
    "question": "A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company.\n\nWhich configuration should be used to meet this changed requirement?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure the security group for the Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure the security group on the Application Load Balancer",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nAWS Web Application Firewall (AWS WAF) is a web application firewall service that lets you monitor web requests and protect your web applications from malicious requests. Use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses. You can also use AWS WAF preconfigured protections to block common attacks like SQL injection or cross-site scripting.\n\nConfigure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)\n\nYou can use AWS WAF with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL). Geographic (Geo) Match Conditions in AWS WAF allows you to use AWS WAF to restrict application access based on the geographic location of your viewers. With geo match conditions you can choose the countries from which AWS WAF should allow access.\n\nGeo match conditions are important for many customers. For example, legal and licensing requirements restrict some customers from delivering their applications outside certain countries. These customers can configure a whitelist that allows only viewers in those countries. Other customers need to prevent the downloading of their encrypted software by users in certain countries. These customers can configure a blacklist so that end-users from those countries are blocked from downloading their software.\n\nIncorrect options:\n\nUse Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC) - Geo Restriction feature of Amazon CloudFront helps in restricting traffic based on the user's geographic location. But, CloudFront works from edge locations and doesn't belong to a VPC. Hence, this option itself is incorrect and given only as a distractor.\n\nConfigure the security group on the Application Load Balancer\n\nConfigure the security group for the Amazon EC2 instances\n\nSecurity Groups cannot restrict access based on the user's geographic location.\n\nReferences:\n\nhttps://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/\n\nhttps://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/\n\nhttps://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS Web Application Firewall (AWS WAF) is a web application firewall service that lets you monitor web requests and protect your web applications from malicious requests. Use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses. You can also use AWS WAF preconfigured protections to block common attacks like SQL injection or cross-site scripting."
      },
      {
        "answer": "Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can use AWS WAF with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL). Geographic (Geo) Match Conditions in AWS WAF allows you to use AWS WAF to restrict application access based on the geographic location of your viewers. With geo match conditions you can choose the countries from which AWS WAF should allow access."
      },
      {
        "answer": "",
        "explanation": "Geo match conditions are important for many customers. For example, legal and licensing requirements restrict some customers from delivering their applications outside certain countries. These customers can configure a whitelist that allows only viewers in those countries. Other customers need to prevent the downloading of their encrypted software by users in certain countries. These customers can configure a blacklist so that end-users from those countries are blocked from downloading their software."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)</strong> - Geo Restriction feature of Amazon CloudFront helps in restricting traffic based on the user's geographic location. But, CloudFront works from edge locations and doesn't belong to a VPC. Hence, this option itself is incorrect and given only as a distractor."
      },
      {
        "answer": "Configure the security group on the Application Load Balancer",
        "explanation": ""
      },
      {
        "answer": "Configure the security group for the Amazon EC2 instances",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Security Groups cannot restrict access based on the user's geographic location."
      }
    ],
    "references": [
      "https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/",
      "https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/",
      "https://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/"
    ]
  },
  {
    "id": 3,
    "question": "A large financial institution operates an on-premises data center with hundreds of petabytes of data managed on Microsoft’s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS.\n\nWhich of the following AWS services can facilitate the migration of these workloads?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Microsoft SQL Server on AWS",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon FSx for Windows File Server",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Amazon FSx for Lustre",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nAmazon FSx for Windows File Server\n\nAmazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. Amazon FSx supports the use of Microsoft’s Distributed File System (DFS) to organize shares into a single folder structure up to hundreds of PB in size. So this option is correct.\n\nHow Amazon FSx for Windows File Server Works: \n via - https://aws.amazon.com/fsx/windows/\n\nIncorrect options:\n\nAmazon FSx for Lustre\n\nAmazon FSx for Lustre makes it easy and cost-effective to launch and run the world’s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. Amazon FSx enables you to use Lustre file systems for any workload where storage speed matters. FSx for Lustre does not support Microsoft’s Distributed File System (DFS), so this option is incorrect.\n\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nAWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use managed Active Directory in the AWS Cloud. AWS Managed Microsoft AD is built on the actual Microsoft Active Directory and does not require you to synchronize or replicate data from your existing Active Directory to the cloud. AWS Managed Microsoft AD does not support Microsoft’s Distributed File System (DFS), so this option is incorrect.\n\nMicrosoft SQL Server on AWS\n\nMicrosoft SQL Server on AWS offers you the flexibility to run Microsoft SQL Server database on AWS Cloud. Microsoft SQL Server on AWS does not support Microsoft’s Distributed File System (DFS), so this option is incorrect.\n\nReference:\n\nhttps://aws.amazon.com/fsx/windows/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon FSx for Windows File Server",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol.  It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.\nAmazon FSx supports the use of Microsoft’s Distributed File System (DFS) to organize shares into a single folder structure up to hundreds of PB in size. So this option is correct."
      },
      {
        "image": "https://d1.awsstatic.com/r2018/b/FSx-Windows/FSx_Windows_File_Server_How-it-Works.9396055e727c3903de991e7f3052ec295c86f274.png",
        "answer": "",
        "explanation": "How Amazon FSx for Windows File Server Works:"
      },
      {
        "link": "https://aws.amazon.com/fsx/windows/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Amazon FSx for Lustre",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon FSx for Lustre makes it easy and cost-effective to launch and run the world’s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. Amazon FSx enables you to use Lustre file systems for any workload where storage speed matters.\nFSx for Lustre does not support Microsoft’s Distributed File System (DFS), so this option is incorrect."
      },
      {
        "answer": "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use managed Active Directory in the AWS Cloud. AWS Managed Microsoft AD is built on the actual Microsoft Active Directory and does not require you to synchronize or replicate data from your existing Active Directory to the cloud. AWS Managed Microsoft AD does not support Microsoft’s Distributed File System (DFS), so this option is incorrect."
      },
      {
        "answer": "Microsoft SQL Server on AWS",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Microsoft SQL Server on AWS offers you the flexibility to run Microsoft SQL Server database on AWS Cloud. Microsoft SQL Server on AWS does not support Microsoft’s Distributed File System (DFS), so this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/fsx/windows/",
      "https://aws.amazon.com/fsx/windows/"
    ]
  },
  {
    "id": 4,
    "question": "Which of the following feature of an Amazon S3 bucket can only be suspended and not disabled once it have been enabled?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Versioning",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Static Website Hosting",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Requester Pays",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Server Access Logging",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nVersioning\n\nOnce you version-enable a bucket, it can never return to an unversioned state. Versioning can only be suspended once it has been enabled.\n\nVersioning Overview: \n via - https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\n\nIncorrect options:\n\nServer Access Logging\n\nStatic Website Hosting\n\nRequester Pays\n\nServer Access Logging, Static Website Hosting and Requester Pays features can be disabled even after they have been enabled.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html",
    "correctAnswerExplanations": [
      {
        "answer": "Versioning",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Once you version-enable a bucket, it can never return to an unversioned state. Versioning can only be suspended once it has been enabled."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q39-i1.jpg",
        "answer": "",
        "explanation": "Versioning Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Server Access Logging",
        "explanation": ""
      },
      {
        "answer": "Static Website Hosting",
        "explanation": ""
      },
      {
        "answer": "Requester Pays",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Server Access Logging, Static Website Hosting and Requester Pays features can be disabled even after they have been enabled."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html"
    ]
  },
  {
    "id": 5,
    "question": "The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B.\n\nAt this point in time, what entities exist in Region B?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "1 Amazon EC2 instance and 2 AMIs exist in Region B",
        "correct": false
      },
      {
        "id": 2,
        "answer": "1 Amazon EC2 instance and 1 AMI exist in Region B",
        "correct": false
      },
      {
        "id": 3,
        "answer": "1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B",
        "correct": true
      },
      {
        "id": 4,
        "answer": "1 Amazon EC2 instance and 1 snapshot exist in Region B",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\n1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B\n\nAn Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. When the new AMI is copied from Region A into Region B, it automatically creates a snapshot in Region B because AMIs are based on the underlying snapshots. Further, an instance is created from this AMI in Region B. Hence, we have 1 Amazon EC2 instance, 1 AMI and 1 snapshot in Region B.\n\nAmazon Machine Image (AMI) Overview: \n via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\n\nIncorrect options:\n\n1 Amazon EC2 instance and 1 AMI exist in Region B\n\n1 Amazon EC2 instance and 1 snapshot exist in Region B\n\n1 Amazon EC2 instance and 2 AMIs exist in Region B\n\nAs mentioned earlier in the explanation, when the new AMI is copied from Region A into Region B, it also creates a snapshot in Region B because AMIs are based on the underlying snapshots. In addition, an instance is created from this AMI in Region B. So, we have 1 Amazon EC2 instance, 1 AMI and 1 snapshot in Region B. Hence all three options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html",
    "correctAnswerExplanations": [
      {
        "answer": "1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance.\nWhen the new AMI is copied from Region A into Region B, it automatically creates a snapshot in Region B because AMIs are based on the underlying snapshots. Further, an instance is created from this AMI in Region B. Hence, we have\n1 Amazon EC2 instance, 1 AMI and 1 snapshot in Region B."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q41-i1.jpg",
        "answer": "",
        "explanation": "Amazon Machine Image (AMI) Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "1 Amazon EC2 instance and 1 AMI exist in Region B",
        "explanation": ""
      },
      {
        "answer": "1 Amazon EC2 instance and 1 snapshot exist in Region B",
        "explanation": ""
      },
      {
        "answer": "1 Amazon EC2 instance and 2 AMIs exist in Region B",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As mentioned earlier in the explanation, when the new AMI is copied from Region A into Region B, it also creates a snapshot in Region B because AMIs are based on the underlying snapshots. In addition, an instance is created from this AMI in Region B. So, we have 1 Amazon EC2 instance, 1 AMI and 1 snapshot in Region B. Hence all three options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"
    ]
  },
  {
    "id": 6,
    "question": "The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3.\n\nCan you spot the INVALID lifecycle transitions from the options below? (Select two)",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon S3 Intelligent-Tiering => Amazon S3 Standard",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Amazon S3 Standard-IA => Amazon S3 One Zone-IA",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon S3 Standard => Amazon S3 Intelligent-Tiering",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon S3 One Zone-IA => Amazon S3 Standard-IA",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct options:\n\nAs the question wants to know about the INVALID lifecycle transitions, the following options are the correct answers -\n\nAmazon S3 Intelligent-Tiering => Amazon S3 Standard\n\nAmazon S3 One Zone-IA => Amazon S3 Standard-IA\n\nFollowing are the unsupported life cycle transitions for S3 storage classes - Any storage class to the Amazon S3 Standard storage class. Any storage class to the Reduced Redundancy storage class. The Amazon S3 Intelligent-Tiering storage class to the Amazon S3 Standard-IA storage class. The Amazon S3 One Zone-IA storage class to the Amazon S3 Standard-IA or Amazon S3 Intelligent-Tiering storage classes.\n\nIncorrect options:\n\nAmazon S3 Standard => Amazon S3 Intelligent-Tiering\n\nAmazon S3 Standard-IA => Amazon S3 Intelligent-Tiering\n\nAmazon S3 Standard-IA => Amazon S3 One Zone-IA\n\nHere are the supported life cycle transitions for S3 storage classes - The S3 Standard storage class to any other storage class. Any storage class to the S3 Glacier or S3 Glacier Deep Archive storage classes. The S3 Standard-IA storage class to the S3 Intelligent-Tiering or S3 One Zone-IA storage classes. The S3 Intelligent-Tiering storage class to the S3 One Zone-IA storage class. The S3 Glacier storage class to the S3 Glacier Deep Archive storage class.\n\nAmazon S3 supports a waterfall model for transitioning between storage classes, as shown in the diagram below: \n via - https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "As the question wants to know about the INVALID lifecycle transitions, the following options are the correct answers -"
      },
      {
        "answer": "Amazon S3 Intelligent-Tiering => Amazon S3 Standard",
        "explanation": ""
      },
      {
        "answer": "Amazon S3 One Zone-IA => Amazon S3 Standard-IA",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Following are the unsupported life cycle transitions for S3 storage classes -\nAny storage class to the Amazon S3 Standard storage class.\nAny storage class to the Reduced Redundancy storage class.\nThe Amazon S3 Intelligent-Tiering storage class to the Amazon S3 Standard-IA storage class.\nThe Amazon S3 One Zone-IA storage class to the Amazon S3 Standard-IA or Amazon S3 Intelligent-Tiering storage classes."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Amazon S3 Standard => Amazon S3 Intelligent-Tiering",
        "explanation": ""
      },
      {
        "answer": "Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering",
        "explanation": ""
      },
      {
        "answer": "Amazon S3 Standard-IA => Amazon S3 One Zone-IA",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Here are the supported life cycle transitions for S3 storage classes -\nThe S3 Standard storage class to any other storage class.\nAny storage class to the S3 Glacier or S3 Glacier Deep Archive storage classes.\nThe S3 Standard-IA storage class to the S3 Intelligent-Tiering or S3 One Zone-IA storage classes.\nThe S3 Intelligent-Tiering storage class to the S3 One Zone-IA storage class.\nThe S3 Glacier storage class to the S3 Glacier Deep Archive storage class."
      },
      {
        "image": "https://docs.aws.amazon.com/images/AmazonS3/latest/userguide/images/lifecycle-transitions-v4.png",
        "answer": "",
        "explanation": "Amazon S3 supports a waterfall model for transitioning between storage classes, as shown in the diagram below:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html"
    ]
  },
  {
    "id": 7,
    "question": "An IT security consultancy is working on a solution to protect data stored in Amazon S3 from any malicious activity as well as check for any vulnerabilities on Amazon EC2 instances.\n\nAs a solutions architect, which of the following solutions would you suggest to help address the given requirement?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nUse Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances\n\nAmazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately.\n\nHow Amazon GuardDuty works: \n via - https://aws.amazon.com/guardduty/\n\nAmazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. Amazon Inspector assessments are offered to you as pre-defined rules packages mapped to common security best practices and vulnerability definitions.\n\nIncorrect options:\n\nUse Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances\n\nUse Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances\n\nUse Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/guardduty/\n\nhttps://aws.amazon.com/inspector/",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately."
      },
      {
        "image": "https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png",
        "answer": "",
        "explanation": "How Amazon GuardDuty works:"
      },
      {
        "link": "https://aws.amazon.com/guardduty/"
      },
      {
        "answer": "",
        "explanation": "Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. Amazon Inspector assessments are offered to you as pre-defined rules packages mapped to common security best practices and vulnerability definitions."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
        "explanation": ""
      },
      {
        "answer": "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
        "explanation": ""
      },
      {
        "answer": "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/guardduty/",
      "https://aws.amazon.com/guardduty/",
      "https://aws.amazon.com/inspector/"
    ]
  },
  {
    "id": 8,
    "question": "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort.\n\nWhich of the following options represents the best solution for this use case?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nLeverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nAmazon RDS is a managed service that makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while managing time-consuming database administration tasks. Amazon RDS can automatically back up your database and keep your database software up to date with the latest version. However, RDS does not allow you to access the host OS of the database.\n\nFor the given use-case, you need to use Amazon RDS Custom for Oracle as it allows you to access and customize your database server host and operating system, for example by applying special patches and changing the database software settings to support third-party applications that require privileged access. Amazon RDS Custom for Oracle facilitates these functionalities with minimum infrastructure maintenance effort. You need to set up the RDS Custom for Oracle in multi-AZ configuration for high availability.\n\nAmazon RDS Custom for Oracle: \n via - https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/\n\nIncorrect options:\n\nLeverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nLeverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nAmazon RDS for Oracle does not allow you to access and customize your database server host and operating system. Therefore, both these options are incorrect.\n\nDeploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system - The use case requires that the best solution should involve minimum infrastructure maintenance effort. When you use Amazon EC2 instances to host the databases, you need to manage the server health, server maintenance, server patching, and database maintenance tasks yourself. In addition, you will also need to manage the multi-AZ configuration by deploying Amazon EC2 instances across two Availability Zones (AZs), perhaps by using an Auto Scaling group. These steps entail significant maintenance effort. Hence this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/\n\nhttps://aws.amazon.com/rds/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon RDS is a managed service that makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while managing time-consuming database administration tasks. Amazon RDS can automatically back up your database and keep your database software up to date with the latest version. However, RDS does not allow you to access the host OS of the database."
      },
      {
        "answer": "",
        "explanation": "For the given use-case, you need to use Amazon RDS Custom for Oracle as it allows you to access and customize your database server host and operating system, for example by applying special patches and changing the database software settings to support third-party applications that require privileged access. Amazon RDS Custom for Oracle facilitates these functionalities with minimum infrastructure maintenance effort. You need to set up the RDS Custom for Oracle in multi-AZ configuration for high availability."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q3-i1.jpg",
        "answer": "",
        "explanation": "Amazon RDS Custom for Oracle:"
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "explanation": ""
      },
      {
        "answer": "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon RDS for Oracle does not allow you to access and customize your database server host and operating system. Therefore, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system</strong> - The use case requires that the best solution should involve minimum infrastructure maintenance effort. When you use Amazon EC2 instances to host the databases, you need to manage the server health, server maintenance, server patching, and database maintenance tasks yourself. In addition, you will also need to manage the multi-AZ configuration by deploying Amazon EC2 instances across two Availability Zones (AZs), perhaps by using an Auto Scaling group. These steps entail significant maintenance effort. Hence this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/",
      "https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/",
      "https://aws.amazon.com/rds/faqs/"
    ]
  },
  {
    "id": 9,
    "question": "As part of a pilot program, a biotechnology company wants to integrate data files from its on-premises analytical application with AWS Cloud via an NFS interface.\n\nWhich of the following AWS service is the MOST efficient solution for the given use-case?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "AWS Storage Gateway - File Gateway",
        "correct": true
      },
      {
        "id": 2,
        "answer": "AWS Storage Gateway - Volume Gateway",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Site-to-Site VPN",
        "correct": false
      },
      {
        "id": 4,
        "answer": "AWS Storage Gateway - Tape Gateway",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nAWS Storage Gateway - File Gateway\n\nAWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access.\n\nAWS Storage Gateway's file interface, or file gateway, offers you a seamless way to connect to the cloud in order to store application data files and backup images as durable objects on Amazon S3 cloud storage. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. As the company wants to integrate data files from its analytical instruments into AWS via an NFS interface, therefore AWS Storage Gateway - File Gateway is the correct answer.\n\nFile Gateway Overview: \n via - https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html\n\nIncorrect options:\n\nAWS Storage Gateway - Volume Gateway - You can configure the AWS Storage Gateway service as a Volume Gateway to present cloud-based iSCSI block storage volumes to your on-premises applications. Volume Gateway does not support NFS interface, so this option is not correct.\n\nAWS Storage Gateway - Tape Gateway - AWS Storage Gateway - Tape Gateway allows moving tape backups to the cloud. Tape Gateway does not support NFS interface, so this option is not correct.\n\nAWS Site-to-Site VPN - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN (Site-to-Site VPN) connection. It uses internet protocol security (IPSec) communications to create encrypted VPN tunnels between two locations. You cannot use AWS Site-to-Site VPN to integrate data files via the NFS interface, so this option is not correct.\n\nReferences:\n\nhttps://aws.amazon.com/storagegateway/\n\nhttps://aws.amazon.com/storagegateway/volume/\n\nhttps://aws.amazon.com/storagegateway/file/\n\nhttps://aws.amazon.com/storagegateway/vtl/",
    "correctAnswerExplanations": [
      {
        "answer": "AWS Storage Gateway - File Gateway",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access."
      },
      {
        "answer": "",
        "explanation": "AWS Storage Gateway's file interface, or file gateway, offers you a seamless way to connect to the cloud in order to store application data files and backup images as durable objects on Amazon S3 cloud storage. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. As the company wants to integrate data files from its analytical instruments into AWS via an NFS interface, therefore AWS Storage Gateway - File Gateway is the correct answer."
      },
      {
        "image": "https://docs.aws.amazon.com/storagegateway/latest/userguide/images/file-gateway-concepts-diagram.png",
        "answer": "",
        "explanation": "File Gateway Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Storage Gateway - Volume Gateway</strong> - You can configure the AWS Storage Gateway service as a Volume Gateway to present cloud-based iSCSI block storage volumes to your on-premises applications. Volume Gateway does not support NFS interface, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Storage Gateway - Tape Gateway</strong> - AWS Storage Gateway - Tape Gateway allows moving tape backups to the cloud. Tape Gateway does not support NFS interface, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Site-to-Site VPN</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN (Site-to-Site VPN) connection. It uses internet protocol security (IPSec) communications to create encrypted VPN tunnels between two locations. You cannot use AWS Site-to-Site VPN to integrate data files via the NFS interface, so this option is not correct."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html",
      "https://aws.amazon.com/storagegateway/",
      "https://aws.amazon.com/storagegateway/volume/",
      "https://aws.amazon.com/storagegateway/file/",
      "https://aws.amazon.com/storagegateway/vtl/"
    ]
  },
  {
    "id": 10,
    "question": "A financial services company recently launched an initiative to improve the security of its AWS resources and it had enabled AWS Shield Advanced across multiple AWS accounts owned by the company. Upon analysis, the company has found that the costs incurred are much higher than expected.\n\nWhich of the following would you attribute as the underlying reason for the unexpectedly high costs for AWS Shield Advanced service?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once",
        "correct": true
      },
      {
        "id": 3,
        "answer": "AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nConsolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once\n\nIf your organization has multiple AWS accounts, then you can subscribe multiple AWS Accounts to AWS Shield Advanced by individually enabling it on each account using the AWS Management Console or API. You will pay the monthly fee once as long as the AWS accounts are all under a single consolidated billing, and you own all the AWS accounts and resources in those accounts.\n\nIncorrect options:\n\nAWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs - AWS Shield Advanced does offer protection to resources outside of AWS. This should not cause unexpected spike in billing costs.\n\nAWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs - AWS Shield Standard is automatically enabled for all AWS customers at no additional cost. AWS Shield Advanced is an optional paid service.\n\nSavings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts - This option has been added as a distractor. Savings Plans is a flexible pricing model that offers low prices on Amazon EC2 instances, AWS Lambda, and AWS Fargate usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a 1 or 3 year term. Savings Plans is not applicable for the AWS Shield Advanced service.\n\nReferences:\n\nhttps://aws.amazon.com/shield/faqs/\n\nhttps://aws.amazon.com/savingsplans/faq/",
    "correctAnswerExplanations": [
      {
        "answer": "Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "If your organization has multiple AWS accounts, then you can subscribe multiple AWS Accounts to AWS Shield Advanced by individually enabling it on each account using the AWS Management Console or API. You will pay the monthly fee once as long as the AWS accounts are all under a single consolidated billing, and you own all the AWS accounts and resources in those accounts."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs</strong> - AWS Shield Advanced does offer protection to resources outside of AWS. This should not cause unexpected spike in billing costs."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs</strong> - AWS Shield Standard is automatically enabled for all AWS customers at no additional cost. AWS Shield Advanced is an optional paid service."
      },
      {
        "answer": "",
        "explanation": "<strong>Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts</strong> - This option has been added as a distractor. Savings Plans is a flexible pricing model that offers low prices on Amazon EC2 instances, AWS Lambda, and AWS Fargate usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a 1 or 3 year term. Savings Plans is not applicable for the AWS Shield Advanced service."
      }
    ],
    "references": [
      "https://aws.amazon.com/shield/faqs/",
      "https://aws.amazon.com/savingsplans/faq/"
    ]
  },
  {
    "id": 11,
    "question": "A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3 gigabytes. The junior scientist is using Amazon S3 Transfer Acceleration (Amazon S3TA) for faster image upload. It turns out that Amazon S3TA did not result in an accelerated transfer.\n\nGiven this scenario, which of the following is correct regarding the charges for this image transfer?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload",
        "correct": false
      },
      {
        "id": 2,
        "answer": "The junior scientist only needs to pay Amazon S3 transfer charges for the image upload",
        "correct": false
      },
      {
        "id": 3,
        "answer": "The junior scientist does not need to pay any transfer charges for the image upload",
        "correct": true
      },
      {
        "id": 4,
        "answer": "The junior scientist only needs to pay S3TA transfer charges for the image upload",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nThe junior scientist does not need to pay any transfer charges for the image upload\n\nThere are no S3 data transfer charges when data is transferred in from the internet. Also with S3TA, you pay only for transfers that are accelerated. Therefore the junior scientist does not need to pay any transfer charges for the image upload because S3TA did not result in an accelerated transfer.\n\nAmazon S3 Transfer Acceleration (S3TA) Overview: \n via - https://aws.amazon.com/s3/transfer-acceleration/\n\nIncorrect options:\n\nThe junior scientist only needs to pay S3TA transfer charges for the image upload - Since S3TA did not result in an accelerated transfer, there are no S3TA transfer charges to be paid.\n\nThe junior scientist only needs to pay Amazon S3 transfer charges for the image upload - There are no S3 data transfer charges when data is transferred in from the internet. So this option is incorrect.\n\nThe junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload - There are no Amazon S3 data transfer charges when data is transferred in from the internet. Since S3TA did not result in an accelerated transfer, there are no S3TA transfer charges to be paid.\n\nReferences:\n\nhttps://aws.amazon.com/s3/transfer-acceleration/\n\nhttps://aws.amazon.com/s3/pricing/",
    "correctAnswerExplanations": [
      {
        "answer": "The junior scientist does not need to pay any transfer charges for the image upload",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "There are no S3 data transfer charges when data is transferred in from the internet. Also with S3TA, you pay only for transfers that are accelerated. Therefore the junior scientist does not need to pay any transfer charges for the image upload because S3TA did not result in an accelerated transfer."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q40-i1.jpg",
        "answer": "",
        "explanation": "Amazon S3 Transfer Acceleration (S3TA) Overview:"
      },
      {
        "link": "https://aws.amazon.com/s3/transfer-acceleration/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The junior scientist only needs to pay S3TA transfer charges for the image upload</strong> - Since S3TA did not result in an accelerated transfer, there are no S3TA transfer charges to be paid."
      },
      {
        "answer": "",
        "explanation": "<strong>The junior scientist only needs to pay Amazon S3 transfer charges for the image upload</strong> - There are no S3 data transfer charges when data is transferred in from the internet. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload</strong> - There are no Amazon S3 data transfer charges when data is transferred in from the internet. Since S3TA did not result in an accelerated transfer, there are no S3TA transfer charges to be paid."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/transfer-acceleration/",
      "https://aws.amazon.com/s3/transfer-acceleration/",
      "https://aws.amazon.com/s3/pricing/"
    ]
  },
  {
    "id": 12,
    "question": "A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. The company is looking for AWS services that can be used for buffering or throttling to handle such traffic variations.\n\nWhich of the following services can be used to support this requirement?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nThrottling is the process of limiting the number of requests an authorized program can submit to a given operation in a given amount of time.\n\nAmazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis\n\nTo prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API using the token bucket algorithm, where a token counts for a request. Specifically, API Gateway sets a limit on a steady-state rate and a burst of request submissions against all APIs in your account. In the token bucket algorithm, the burst is the maximum bucket size.\n\nAmazon Simple Queue Service (Amazon SQS) - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers buffer capabilities to smooth out temporary volume spikes without losing messages or increasing latency.\n\nAmazon Kinesis - Amazon Kinesis is a fully managed, scalable service that can ingest, buffer, and process streaming data in real-time.\n\nIncorrect options:\n\nAmazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda - Amazon SQS has the ability to buffer its messages. Amazon Simple Notification Service (SNS) cannot buffer messages and is generally used with SQS to provide the buffering facility. When requests come in faster than your Lambda function can scale, or when your function is at maximum concurrency, additional requests fail as the Lambda throttles those requests with error code 429 status code. So, this combination of services is incorrect.\n\nAmazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis - A Gateway Endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. This cannot help in throttling or buffering of requests. Amazon SQS and Kinesis can buffer incoming data. Since Gateway Endpoint is an incorrect service for throttling or buffering, this option is incorrect.\n\nElastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda - Elastic Load Balancer cannot throttle requests. Amazon SQS can be used to buffer messages. When requests come in faster than your Lambda function can scale, or when your function is at maximum concurrency, additional requests fail as the Lambda throttles those requests with error code 429 status code. So, this combination of services is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html\n\nhttps://aws.amazon.com/sqs/features/",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Throttling is the process of limiting the number of requests an authorized program can submit to a given operation in a given amount of time."
      },
      {
        "answer": "Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API using the token bucket algorithm, where a token counts for a request. Specifically, API Gateway sets a limit on a steady-state rate and a burst of request submissions against all APIs in your account. In the token bucket algorithm, the burst is the maximum bucket size."
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (Amazon SQS) - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers buffer capabilities to smooth out temporary volume spikes without losing messages or increasing latency."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis - Amazon Kinesis is a fully managed, scalable service that can ingest, buffer, and process streaming data in real-time."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda</strong> - Amazon SQS has the ability to buffer its messages. Amazon Simple Notification Service (SNS) cannot buffer messages and is generally used with SQS to provide the buffering facility. When requests come in faster than your Lambda function can scale, or when your function is at maximum concurrency, additional requests fail as the Lambda throttles those requests with error code 429 status code. So, this combination of services is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis</strong> - A Gateway Endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. This cannot help in throttling or buffering of requests. Amazon SQS and Kinesis can buffer incoming data. Since Gateway Endpoint is an incorrect service for throttling or buffering, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda</strong> - Elastic Load Balancer cannot throttle requests. Amazon SQS can be used to buffer messages. When requests come in faster than your Lambda function can scale, or when your function is at maximum concurrency, additional requests fail as the Lambda throttles those requests with error code 429 status code. So, this combination of services is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html",
      "https://aws.amazon.com/sqs/features/"
    ]
  },
  {
    "id": 13,
    "question": "A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected.\n\nWhich of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two)",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Different versions of a single object can have different retention modes and periods",
        "correct": true
      },
      {
        "id": 2,
        "answer": "The bucket default settings will override any explicit retention mode or period you request on an object version",
        "correct": false
      },
      {
        "id": 3,
        "answer": "When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version",
        "correct": true
      },
      {
        "id": 4,
        "answer": "When you use bucket default settings, you specify a Retain Until Date for the object version",
        "correct": false
      },
      {
        "id": 5,
        "answer": "You cannot place a retention period on an object version through a bucket default setting",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct options:\n\nWhen you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version\n\nYou can place a retention period on an object version either explicitly or through a bucket default setting. When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version. Amazon S3 stores the Retain Until Date setting in the object version's metadata and protects the object version until the retention period expires.\n\nDifferent versions of a single object can have different retention modes and periods\n\nLike all other Object Lock settings, retention periods apply to individual object versions. Different versions of a single object can have different retention modes and periods.\n\nFor example, suppose that you have an object that is 15 days into a 30-day retention period, and you PUT an object into Amazon S3 with the same name and a 60-day retention period. In this case, your PUT succeeds, and Amazon S3 creates a new version of the object with a 60-day retention period. The older version maintains its original retention period and becomes deletable in 15 days.\n\nIncorrect options:\n\nYou cannot place a retention period on an object version through a bucket default setting - You can place a retention period on an object version either explicitly or through a bucket default setting.\n\nWhen you use bucket default settings, you specify a Retain Until Date for the object version - When you use bucket default settings, you don't specify a Retain Until Date. Instead, you specify a duration, in either days or years, for which every object version placed in the bucket should be protected.\n\nThe bucket default settings will override any explicit retention mode or period you request on an object version - If your request to place an object version in a bucket contains an explicit retention mode and period, those settings override any bucket default settings for that object version.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html",
    "correctAnswerExplanations": [
      {
        "answer": "When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can place a retention period on an object version either explicitly or through a bucket default setting. When you apply a retention period to an object version explicitly, you specify a <code>Retain Until Date</code> for the object version. Amazon S3 stores the Retain Until Date setting in the object version's metadata and protects the object version until the retention period expires."
      },
      {
        "answer": "Different versions of a single object can have different retention modes and periods",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Like all other Object Lock settings, retention periods apply to individual object versions. Different versions of a single object can have different retention modes and periods."
      },
      {
        "answer": "",
        "explanation": "For example, suppose that you have an object that is 15 days into a 30-day retention period, and you PUT an object into Amazon S3 with the same name and a 60-day retention period. In this case, your PUT succeeds, and Amazon S3 creates a new version of the object with a 60-day retention period. The older version maintains its original retention period and becomes deletable in 15 days."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You cannot place a retention period on an object version through a bucket default setting</strong> - You can place a retention period on an object version either explicitly or through a bucket default setting."
      },
      {
        "answer": "",
        "explanation": "<strong>When you use bucket default settings, you specify a <code>Retain Until Date</code> for the object version</strong> - When you use bucket default settings, you don't specify a Retain Until Date. Instead, you specify a duration, in either days or years, for which every object version placed in the bucket should be protected."
      },
      {
        "answer": "",
        "explanation": "<strong>The bucket default settings will override any explicit retention mode or period you request on an object version</strong> - If your request to place an object version in a bucket contains an explicit retention mode and period, those settings override any bucket default settings for that object version."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html"
    ]
  },
  {
    "id": 14,
    "question": "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users.\n\nAs a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nEnable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3\n\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second.\n\nAmazon DynamoDB Accelerator (DAX) is tightly integrated with Amazon DynamoDB—you simply provision a DAX cluster, use the DAX client SDK to point your existing Amazon DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with Amazon DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache Amazon DynamoDB reads.\n\nAmazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from S3 directly to your users.\n\nWhen a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the Amazon S3 bucket where you’ve stored your content.\n\nSo, you can use Amazon CloudFront to improve application performance to serve static content from Amazon S3.\n\nIncorrect options:\n\nEnable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3\n\nAmazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.\n\nAmazon ElastiCache for Redis Overview: \n via - https://aws.amazon.com/elasticache/redis/\n\nAlthough you can integrate Redis with DynamoDB, it's much more involved than using DAX which is a much better fit.\n\nEnable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3\n\nEnable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3\n\nAmazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database.\n\nAmazon ElastiCache Memcached cannot be used as a cache to serve static content from Amazon S3, so both these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/dynamodb/dax/\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\n\nhttps://aws.amazon.com/elasticache/redis/",
    "correctAnswerExplanations": [
      {
        "answer": "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second."
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB Accelerator (DAX) is tightly integrated with Amazon DynamoDB—you simply provision a DAX cluster, use the DAX client SDK to point your existing Amazon DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with Amazon DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache Amazon DynamoDB reads."
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from S3 directly to your users."
      },
      {
        "answer": "",
        "explanation": "When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the Amazon S3 bucket where you’ve stored your content."
      },
      {
        "answer": "",
        "explanation": "So, you can use Amazon CloudFront to improve application performance to serve static content from Amazon S3."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store."
      },
      {
        "image": "https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png",
        "answer": "",
        "explanation": "Amazon ElastiCache for Redis Overview:"
      },
      {
        "link": "https://aws.amazon.com/elasticache/redis/"
      },
      {
        "answer": "",
        "explanation": "Although you can integrate Redis with DynamoDB, it's much more involved than using DAX which is a much better fit."
      },
      {
        "answer": "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3",
        "explanation": ""
      },
      {
        "answer": "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database."
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache Memcached cannot be used as a cache to serve static content from Amazon S3, so both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/elasticache/redis/",
      "https://aws.amazon.com/dynamodb/dax/",
      "https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/",
      "https://aws.amazon.com/elasticache/redis/"
    ]
  },
  {
    "id": 15,
    "question": "The engineering team at a Spanish professional football club has built a notification system for its website using Amazon Simple Notification Service (Amazon SNS) notifications which are then handled by an AWS Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website.\n\nAs a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit",
        "correct": true
      },
      {
        "id": 2,
        "answer": "The engineering team needs to provision more servers running the AWS Lambda service",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit",
        "correct": false
      },
      {
        "id": 4,
        "answer": "The engineering team needs to provision more servers running the Amazon SNS service",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nAmazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit\n\nAmazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\n\nHow Amazon SNS Works: \n via - https://aws.amazon.com/sns/\n\nWith AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there’s no charge when your code isn’t running.\n\nAWS Lambda currently supports 1000 concurrent executions per AWS account per region. If your Amazon SNS message deliveries to AWS Lambda contribute to crossing these concurrency quotas, your Amazon SNS message deliveries will be throttled. You need to contact AWS support to raise the account limit. Therefore this option is correct.\n\nIncorrect options:\n\nAmazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit - Amazon SNS leverages the proven AWS cloud to dynamically scale with your application. You don't need to contact AWS support, as SNS is a fully managed service, taking care of the heavy lifting related to capacity planning, provisioning, monitoring, and patching. Therefore, this option is incorrect.\n\nThe engineering team needs to provision more servers running the Amazon SNS service\n\nThe engineering team needs to provision more servers running the AWS Lambda service\n\nAs both AWS Lambda and Amazon SNS are serverless and fully managed services, the engineering team cannot provision more servers. Both of these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/sns/\n\nhttps://aws.amazon.com/sns/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications."
      },
      {
        "image": "https://d1.awsstatic.com/product-marketing/SNS/product-page-diagram_SNS_how-it-works_1.53a464980bf0d5a868b141e9a8b2acf12abc503f.png",
        "answer": "",
        "explanation": "How Amazon SNS Works:"
      },
      {
        "link": "https://aws.amazon.com/sns/"
      },
      {
        "answer": "",
        "explanation": "With AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there’s no charge when your code isn’t running."
      },
      {
        "answer": "",
        "explanation": "AWS Lambda currently supports 1000 concurrent executions per AWS account per region. If your Amazon SNS message deliveries to AWS Lambda contribute to crossing these concurrency quotas, your Amazon SNS message deliveries will be throttled. You need to contact AWS support to raise the account limit. Therefore this option is correct."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit</strong> - Amazon SNS leverages the proven AWS cloud to dynamically scale with your application. You don't need to contact AWS support, as SNS is a fully managed service, taking care of the heavy lifting related to capacity planning, provisioning, monitoring, and patching. Therefore, this option is incorrect."
      },
      {
        "answer": "The engineering team needs to provision more servers running the Amazon SNS service",
        "explanation": ""
      },
      {
        "answer": "The engineering team needs to provision more servers running the AWS Lambda service",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As both AWS Lambda and Amazon SNS are serverless and fully managed services, the engineering team cannot provision more servers. Both of these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/sns/faqs/"
    ]
  },
  {
    "id": 16,
    "question": "A retail company runs a customer management system backed by a Microsoft SQL Server database. The system is tightly integrated with applications that rely on T-SQL queries. The company wants to modernize its infrastructure by migrating to Amazon Aurora PostgreSQL, but it needs to avoid major modifications to the existing application logic.\n\nWhich combination of actions should the company take to achieve this goal with minimal application refactoring? (Select two)",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Aurora Global Database to replicate data across regions for compatibility",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct options:\n\nDeploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands\n\nBabelfish allows Aurora PostgreSQL to understand T-SQL (Microsoft SQL Server's query language) and SQL Server wire protocol, enabling applications to communicate with Aurora using SQL Server-style queries with minimal code changes. This is ideal for minimizing application code refactoring.\n\n via - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish.html\n\nUse AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data\n\nThe AWS Schema Conversion Tool (SCT) helps convert the SQL Server schema to PostgreSQL-compatible syntax, and AWS Database Migration Service (DMS) can move the actual data with minimal downtime. These tools are designed for database migration and are essential for schema and data transfer.\n\nIncorrect options:\n\nConfigure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior - Aurora endpoints do not provide protocol-level emulation of SQL Server unless Babelfish is explicitly enabled. There is no feature to make Aurora natively emulate SQL Server behavior without Babelfish.\n\nUse Amazon Aurora Global Database to replicate data across regions for compatibility - Aurora Global Database helps with cross-region disaster recovery and read scalability, but it does not assist with SQL Server compatibility or reduce application code changes.\n\nUse AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration - AWS Glue is primarily used for ETL and data transformation, not for application SQL query conversion. It cannot translate T-SQL into PostgreSQL syntax.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish-compatibility.html",
    "correctAnswerExplanations": [
      {
        "answer": "Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Babelfish allows Aurora PostgreSQL to understand T-SQL (Microsoft SQL Server's query language) and SQL Server wire protocol, enabling applications to communicate with Aurora using SQL Server-style queries with minimal code changes. This is ideal for minimizing application code refactoring."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish.html"
      },
      {
        "answer": "Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The AWS Schema Conversion Tool (SCT) helps convert the SQL Server schema to PostgreSQL-compatible syntax, and AWS Database Migration Service (DMS) can move the actual data with minimal downtime. These tools are designed for database migration and are essential for schema and data transfer."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior</strong> -  Aurora endpoints do not provide protocol-level emulation of SQL Server unless Babelfish is explicitly enabled. There is no feature to make Aurora natively emulate SQL Server behavior without Babelfish."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Aurora Global Database to replicate data across regions for compatibility</strong> - Aurora Global Database helps with cross-region disaster recovery and read scalability, but it does not assist with SQL Server compatibility or reduce application code changes."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration</strong> - AWS Glue is primarily used for ETL and data transformation, not for application SQL query conversion. It cannot translate T-SQL into PostgreSQL syntax."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish-compatibility.html"
    ]
  },
  {
    "id": 17,
    "question": "The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using Amazon API Gateway.\n\nWhich of the following would you identify as correct?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nAmazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server\n\nAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the front door for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications.\n\nHow Amazon API Gateway Works: \n via - https://aws.amazon.com/api-gateway/\n\nAmazon API Gateway creates RESTful APIs that:\n\nAre HTTP-based.\n\nEnable stateless client-server communication.\n\nImplement standard HTTP methods such as GET, POST, PUT, PATCH, and DELETE.\n\nAmazon API Gateway creates WebSocket APIs that:\n\nAdhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server. Route incoming messages based on message content.\n\nSo Amazon API Gateway supports stateless RESTful APIs as well as stateful WebSocket APIs. Therefore this option is correct.\n\nIncorrect options:\n\nAmazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server\n\nAmazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server\n\nAmazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server\n\nThese three options contradict the earlier details provided in the explanation. To summarize, Amazon API Gateway supports stateless RESTful APIs and stateful WebSocket APIs. Hence these options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the front door for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications."
      },
      {
        "image": "https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png",
        "answer": "",
        "explanation": "How Amazon API Gateway Works:"
      },
      {
        "link": "https://aws.amazon.com/api-gateway/"
      },
      {
        "answer": "",
        "explanation": "Amazon API Gateway creates RESTful APIs that:"
      },
      {
        "answer": "",
        "explanation": "Are HTTP-based."
      },
      {
        "answer": "",
        "explanation": "Enable stateless client-server communication."
      },
      {
        "answer": "",
        "explanation": "Implement standard HTTP methods such as GET, POST, PUT, PATCH, and DELETE."
      },
      {
        "answer": "",
        "explanation": "Amazon API Gateway creates WebSocket APIs that:"
      },
      {
        "answer": "",
        "explanation": "Adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server.\nRoute incoming messages based on message content."
      },
      {
        "answer": "",
        "explanation": "So Amazon API Gateway supports stateless RESTful APIs as well as stateful WebSocket APIs. Therefore this option is correct."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server",
        "explanation": ""
      },
      {
        "answer": "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server",
        "explanation": ""
      },
      {
        "answer": "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the earlier details provided in the explanation. To summarize, Amazon API Gateway supports stateless RESTful APIs and stateful WebSocket APIs. Hence these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/api-gateway/",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html"
    ]
  },
  {
    "id": 18,
    "question": "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in request rates.\n\nWhich of the following solutions would you recommend for the given use-case? (Select two)",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Direct Connect",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS Global Accelerator",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon Aurora Replica",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use Amazon CloudFront distribution in front of the Application Load Balancer",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Use AWS Shield",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct options:\n\nYou can use Amazon Aurora replicas and Amazon CloudFront distribution to make the application more resilient to spikes in request rates.\n\nUse Amazon Aurora Replica\n\nAmazon Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Amazon Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer. Up to 15 Aurora Replicas can be distributed across the Availability Zones (AZs) that a DB cluster spans within an AWS Region.\n\nUse Amazon CloudFront distribution in front of the Application Load Balancer\n\nAmazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. Amazon CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.\n\nAmazon CloudFront offers an origin failover feature to help support your data resiliency needs. Amazon CloudFront is a global service that delivers your content through a worldwide network of data centers called edge locations or points of presence (POPs). If your content is not already cached in an edge location, Amazon CloudFront retrieves it from an origin that you've identified as the source for the definitive version of the content.\n\nIncorrect options:\n\nUse AWS Shield - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency. There are two tiers of AWS Shield - Standard and Advanced. AWS Shield cannot be used to improve application resiliency to handle spikes in traffic.\n\nUse AWS Global Accelerator - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. Amazon Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Since Amazon CloudFront is better for improving application resiliency to handle spikes in traffic, so this option is ruled out.\n\nUse AWS Direct Connect - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. AWS Direct Connect cannot be used to improve application resiliency to handle spikes in traffic.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/disaster-recovery-resiliency.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\nhttps://aws.amazon.com/global-accelerator/faqs/\n\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/disaster-recovery-resiliency.html",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "You can use Amazon Aurora replicas and Amazon CloudFront distribution to make the application more resilient to spikes in request rates."
      },
      {
        "answer": "Use Amazon Aurora Replica",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Amazon Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer. Up to 15 Aurora Replicas can be distributed across the Availability Zones (AZs) that a DB cluster spans within an AWS Region."
      },
      {
        "answer": "Use Amazon CloudFront distribution in front of the Application Load Balancer",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. Amazon CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content."
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront offers an origin failover feature to help support your data resiliency needs. Amazon CloudFront is a global service that delivers your content through a worldwide network of data centers called edge locations or points of presence (POPs). If your content is not already cached in an edge location, Amazon CloudFront retrieves it from an origin that you've identified as the source for the definitive version of the content."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Shield</strong> - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency. There are two tiers of AWS Shield - Standard and Advanced. AWS Shield cannot be used to improve application resiliency to handle spikes in traffic."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Global Accelerator</strong> - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. Amazon Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Since Amazon CloudFront is better for improving application resiliency to handle spikes in traffic, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Direct Connect</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. AWS Direct Connect cannot be used to improve application resiliency to handle spikes in traffic."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/disaster-recovery-resiliency.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html",
      "https://aws.amazon.com/global-accelerator/faqs/",
      "https://docs.aws.amazon.com/global-accelerator/latest/dg/disaster-recovery-resiliency.html"
    ]
  },
  {
    "id": 19,
    "question": "The payroll department at a company initiates several computationally intensive workloads on Amazon EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these Amazon EC2 instances and making sure that 10 Amazon EC2 instances are available during this peak usage hour. For normal operations only 2 Amazon EC2 instances are enough to cater to the workload.\n\nAs a solutions architect, which of the following steps would you recommend to implement the solution?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nConfigure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour\n\nScheduled scaling allows you to set your own scaling schedule. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date.\n\nA scheduled action sets the minimum, maximum, and desired sizes to what is specified by the scheduled action at the time specified by the scheduled action. For the given use case, the correct solution is to set the desired capacity to 10. When we want to specify a range of instances, then we must use min and max values.\n\nIncorrect options:\n\nConfigure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour - As mentioned earlier in the explanation, only when we want to specify a range of instances, then we must use min and max values. As the given use-case requires exactly 10 instances to be available during the peak hour, so we must set the desired capacity to 10. Hence this option is incorrect.\n\nConfigure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour\n\nConfigure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour\n\nTarget tracking policy or simple tracking policy cannot be used to effect a scaling action at a certain designated hour. Both these options have been added as distractors.\n\nReference:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Scheduled scaling allows you to set your own scaling schedule. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date."
      },
      {
        "answer": "",
        "explanation": "A scheduled action sets the minimum, maximum, and desired sizes to what is specified by the scheduled action at the time specified by the scheduled action. For the given use case, the correct solution is to set the desired capacity to 10. When we want to specify a range of instances, then we must use min and max values."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour</strong> - As mentioned earlier in the explanation, only when we want to specify a range of instances, then we must use min and max values. As the given use-case requires exactly 10 instances to be available during the peak hour, so we must set the desired capacity to 10. Hence this option is incorrect."
      },
      {
        "answer": "Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "explanation": ""
      },
      {
        "answer": "Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Target tracking policy or simple tracking policy cannot be used to effect a scaling action at a certain designated hour. Both these options have been added as distractors."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html"
    ]
  },
  {
    "id": 20,
    "question": "A retail company uses Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon API Gateway, Amazon RDS, Elastic Load Balancer and Amazon CloudFront services. To improve the security of these services, the Risk Advisory group has suggested a feasibility check for using the Amazon GuardDuty service.\n\nWhich of the following would you identify as data sources supported by Amazon GuardDuty?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events",
        "correct": false
      },
      {
        "id": 2,
        "answer": "VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs",
        "correct": false
      },
      {
        "id": 3,
        "answer": "VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nVPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events\n\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. With the cloud, the collection and aggregation of account and network activities is simplified, but it can be time-consuming for security teams to continuously analyze event log data for potential threats. With GuardDuty, you now have an intelligent and cost-effective option for continuous threat detection in AWS. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats.\n\nAmazon GuardDuty analyzes tens of billions of events across multiple AWS data sources, such as AWS CloudTrail events, Amazon VPC Flow Logs, and DNS logs.\n\nWith a few clicks in the AWS Management Console, GuardDuty can be enabled with no software or hardware to deploy or maintain. By integrating with Amazon EventBridge Events, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems.\n\nHow Amazon GuardDuty works: \n via - https://aws.amazon.com/guardduty/\n\nIncorrect options:\n\nVPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs\n\nElastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events\n\nAmazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReference:\n\nhttps://aws.amazon.com/guardduty/",
    "correctAnswerExplanations": [
      {
        "answer": "VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. With the cloud, the collection and aggregation of account and network activities is simplified, but it can be time-consuming for security teams to continuously analyze event log data for potential threats. With GuardDuty, you now have an intelligent and cost-effective option for continuous threat detection in AWS. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats."
      },
      {
        "answer": "",
        "explanation": "Amazon GuardDuty analyzes tens of billions of events across multiple AWS data sources, such as AWS CloudTrail events, Amazon VPC Flow Logs, and DNS logs."
      },
      {
        "answer": "",
        "explanation": "With a few clicks in the AWS Management Console, GuardDuty can be enabled with no software or hardware to deploy or maintain. By integrating with Amazon EventBridge Events, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems."
      },
      {
        "image": "https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png",
        "answer": "",
        "explanation": "How Amazon GuardDuty works:"
      },
      {
        "link": "https://aws.amazon.com/guardduty/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs",
        "explanation": ""
      },
      {
        "answer": "Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events",
        "explanation": ""
      },
      {
        "answer": "Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/guardduty/",
      "https://aws.amazon.com/guardduty/"
    ]
  },
  {
    "id": 21,
    "question": "The DevOps team at an e-commerce company wants to perform some maintenance work on a specific Amazon EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately.\n\nAs a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two)",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct options:\n\nPut the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service - You can put an instance that is in the InService state into the Standby state, update some software or troubleshoot the instance, and then return the instance to service. Instances that are on standby are still part of the Auto Scaling group, but they do not actively handle application traffic.\n\nHow Standby State Works: \n via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html\n\nSuspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again - The ReplaceUnhealthy process terminates instances that are marked as unhealthy and then creates new instances to replace them. Amazon EC2 Auto Scaling stops replacing instances that are marked as unhealthy. Instances that fail EC2 or Elastic Load Balancing health checks are still marked as unhealthy. As soon as you resume the ReplaceUnhealthly process, Amazon EC2 Auto Scaling replaces instances that were marked unhealthy while this process was suspended.\n\nIncorrect options:\n\nTake a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue - Taking the snapshot of the existing instance to create a new AMI and then creating a new instance in order to apply the maintenance patch is not time/resource optimal, hence this option is ruled out.\n\nDelete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy - It's not recommended to delete the Auto Scaling group just to apply a maintenance patch on a specific instance.\n\nSuspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again - Amazon EC2 Auto Scaling does not execute scaling actions that are scheduled to run during the suspension period. This option is not relevant to the given use-case.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/health-checks-overview.html",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service</strong> - You can put an instance that is in the <code>InService</code> state into the <code>Standby</code> state, update some software or troubleshoot the instance, and then return the instance to service. Instances that are on standby are still part of the Auto Scaling group, but they do not actively handle application traffic."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q32-i1.jpg",
        "answer": "",
        "explanation": "How Standby State Works:"
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Suspend the <code>ReplaceUnhealthy</code> process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the <code>ReplaceUnhealthy</code> process type again</strong> - The <code>ReplaceUnhealthy</code> process terminates instances that are marked as unhealthy and then creates new instances to replace them. Amazon EC2 Auto Scaling stops replacing instances that are marked as unhealthy. Instances that fail EC2 or Elastic Load Balancing health checks are still marked as unhealthy. As soon as you resume the <code>ReplaceUnhealthly</code> process, Amazon EC2 Auto Scaling replaces instances that were marked unhealthy while this process was suspended."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue</strong> - Taking the snapshot of the existing instance to create a new AMI and then creating a new instance in order to apply the maintenance patch is not time/resource optimal, hence this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy</strong> - It's not recommended to delete the Auto Scaling group just to apply a maintenance patch on a specific instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Suspend the <code>ScheduledActions</code> process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the <code>ScheduledActions</code> process type again</strong> - Amazon EC2 Auto Scaling does not execute scaling actions that are scheduled to run during the suspension period. This option is not relevant to the given use-case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/health-checks-overview.html"
    ]
  },
  {
    "id": 22,
    "question": "A file-hosting service uses Amazon Simple Storage Service (Amazon S3) under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single Amazon S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second.\n\nWhich of the following is the MOST resource efficient and cost-optimal way of addressing this issue?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nChange the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations\n\nAmazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket.\n\nThere are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second. Please see this example for more clarity on prefixes: if you have a file f1 stored in an S3 object path like so s3://your_bucket_name/folder1/sub_folder_1/f1, then /folder1/sub_folder_1/ becomes the prefix for file f1.\n\nSome data lake applications on Amazon S3 scan millions or billions of objects for queries that run over petabytes of data. These data lake applications achieve single-instance transfer rates that maximize the network interface used for their Amazon EC2 instance, which can be up to 100 Gb/s on a single instance. These applications then aggregate throughput across multiple instances to get multiple terabits per second. Therefore creating customer-specific custom prefixes within the single bucket and then uploading the daily files into those prefixed locations is the BEST solution for the given constraints.\n\nOptimizing Amazon S3 Performance: \n via - https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\n\nIncorrect options:\n\nChange the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets - Creating a new Amazon S3 bucket for each new customer is an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some customers may use the service sparingly but the bucket name is locked for them forever. Moreover, this is really not required as we can use S3 prefixes to improve the performance.\n\nChange the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket - Creating a new Amazon S3 bucket for each new day's data is also an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some of the bucket names may not be available for daily data processing. Moreover, this is really not required as we can use S3 prefixes to improve the performance.\n\nChange the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files - Amazon EFS is a costlier storage option compared to Amazon S3, so it is ruled out.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html",
    "correctAnswerExplanations": [
      {
        "answer": "Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket."
      },
      {
        "answer": "",
        "explanation": "There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second. Please see this example for more clarity on prefixes:\nif you have a file f1 stored in an S3 object path like so <code>s3://your_bucket_name/folder1/sub_folder_1/f1</code>, then <code>/folder1/sub_folder_1/</code> becomes the prefix for file f1."
      },
      {
        "answer": "",
        "explanation": "Some data lake applications on Amazon S3 scan millions or billions of objects for queries that run over petabytes of data. These data lake applications achieve single-instance transfer rates that maximize the network interface used for their Amazon EC2 instance, which can be up to 100 Gb/s on a single instance. These applications then aggregate throughput across multiple instances to get multiple terabits per second. Therefore creating customer-specific custom prefixes within the single bucket and then uploading the daily files into those prefixed locations is the BEST solution for the given constraints."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q51-i1.jpg",
        "answer": "",
        "explanation": "Optimizing Amazon S3 Performance:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets</strong> - Creating a new Amazon S3 bucket for each new customer is an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some customers may use the service sparingly but the bucket name is locked for them forever. Moreover, this is really not required as we can use S3 prefixes to improve the performance."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket</strong> - Creating a new Amazon S3 bucket for each new day's data is also an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some of the bucket names may not be available for daily data processing. Moreover, this is really not required as we can use S3 prefixes to improve the performance."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files</strong> - Amazon EFS is a costlier storage option compared to Amazon S3, so it is ruled out."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html"
    ]
  },
  {
    "id": 23,
    "question": "An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. The solution should allow for content-based routing as part of the architecture.\n\nAs a Solutions Architect, which of the following will you suggest for the company?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nUse an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance\n\nThe Application Load Balancer (ALB) is best suited for load balancing HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. Operating at the individual request level (Layer 7), the Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request.\n\nThis is the correct option since the question has a specific requirement for content-based routing which can be configured via the Application Load Balancer. Different Availability Zones (AZs) provide high availability to the overall architecture and Auto Scaling group will help mask any instance failures.\n\nMore info on Application Load Balancer: \n via - https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/\n\nIncorrect options:\n\nUse a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance - Network Load Balancer cannot facilitate content-based routing so this option is incorrect.\n\nUse an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance\n\nUse an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance\n\nBoth these options are incorrect as you cannot use the Auto Scaling group to distribute traffic to the Amazon EC2 instances.\n\nAn elastic IP address (EIP) is a static, public, IPv4 address allocated to your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. Elastic IPs do not change and remain allocated to your account until you delete them.\n\nMore info on Elastic Load Balancing (ELB): \n via - https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf\n\nYou can span your Auto Scaling group across multiple Availability Zones (AZs) within an AWS Region and then attaching a load balancer to distribute incoming traffic across those zones.\n\n via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/\n\nhttps://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The Application Load Balancer (ALB) is best suited for load balancing HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. Operating at the individual request level (Layer 7), the Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request."
      },
      {
        "answer": "",
        "explanation": "This is the correct option since the question has a specific requirement for content-based routing which can be configured via the Application Load Balancer. Different Availability Zones (AZs) provide high availability to the overall architecture and Auto Scaling group will help mask any instance failures."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q2-i1.jpg",
        "answer": "",
        "explanation": "More info on Application Load Balancer:"
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance</strong> - Network Load Balancer cannot facilitate content-based routing so this option is incorrect."
      },
      {
        "answer": "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance",
        "explanation": ""
      },
      {
        "answer": "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Both these options are incorrect as you cannot use the Auto Scaling group to distribute traffic to the Amazon EC2 instances."
      },
      {
        "answer": "",
        "explanation": "An elastic IP address (EIP) is a static, public, IPv4 address allocated to your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. Elastic IPs do not change and remain allocated to your account until you delete them."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q2-i2.jpg",
        "answer": "",
        "explanation": "More info on Elastic Load Balancing (ELB):"
      },
      {
        "link": "https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf"
      },
      {
        "answer": "",
        "explanation": "You can span your Auto Scaling group across multiple Availability Zones (AZs) within an AWS Region and then attaching a load balancer to distribute incoming traffic across those zones."
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html"
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/",
      "https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html",
      "https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/",
      "https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html"
    ]
  },
  {
    "id": 24,
    "question": "An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3, runs into hundreds of Terabytes and should be available with millisecond latency.\n\nAs an AWS Certified Solutions Architect – Associate, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon S3 Glacier Deep Archive",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon S3 Standard",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA)\n\nSince the data is accessed only twice in a financial year but needs rapid access when required, the most cost-effective storage class for this use-case is Amazon S3 Standard-IA. S3 Standard-IA storage class is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. Amazon Standard-IA is designed for 99.9% availability compared to 99.99% availability of Amazon S3 Standard. However, the report creation process has failover and retry scenarios built into the workflow, so in case the data is not available owing to the 99.9% availability of Amazon S3 Standard-IA, the job will be auto re-invoked till data is successfully retrieved. Therefore this is the correct option.\n\nAmazon S3 Storage Classes Overview: \n via - https://aws.amazon.com/s3/storage-classes/\n\nIncorrect options:\n\nAmazon S3 Standard - Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. As described above, Amazon S3 Standard-IA storage is a better fit than Amazon S3 Standard, hence using S3 standard is ruled out for the given use-case.\n\nAmazon S3 Intelligent-Tiering (S3 Intelligent-Tiering) - For a small monthly object monitoring and automation charge, Amazon S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers. The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Intelligent-Tiering, with a low per GB storage price and per GB retrieval fee. Moreover, Standard-IA has the same availability as that of Amazon S3 Intelligent-Tiering. So, it's cost-efficient to use S3 Standard-IA instead of S3 Intelligent-Tiering.\n\nAmazon S3 Glacier Deep Archive - Amazon S3 Glacier Deep Archive is a secure, durable, and low-cost storage class for data archiving. Amazon S3 Glacier Deep Archive does not support millisecond latency, so this option is ruled out.\n\nFor more details on the durability, availability, cost and access latency - please review this reference link: https://aws.amazon.com/s3/storage-classes",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Since the data is accessed only twice in a financial year but needs rapid access when required, the most cost-effective storage class for this use-case is Amazon S3 Standard-IA. S3 Standard-IA storage class is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. Amazon Standard-IA is designed for 99.9% availability compared to 99.99% availability of Amazon S3 Standard. However, the report creation process has failover and retry scenarios built into the workflow, so in case the data is not available owing to the 99.9% availability of Amazon S3 Standard-IA, the job will be auto re-invoked till data is successfully retrieved. Therefore this is the correct option."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q15-i1.jpg",
        "answer": "",
        "explanation": "Amazon S3 Storage Classes Overview:"
      },
      {
        "link": "https://aws.amazon.com/s3/storage-classes/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Standard</strong> - Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. As described above, Amazon S3 Standard-IA storage is a better fit than Amazon S3 Standard, hence using S3 standard is ruled out for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)</strong> - For a small monthly object monitoring and automation charge, Amazon S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers. The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Intelligent-Tiering, with a low per GB storage price and per GB retrieval fee. Moreover, Standard-IA has the same availability as that of Amazon S3 Intelligent-Tiering. So, it's cost-efficient to use S3 Standard-IA instead of S3 Intelligent-Tiering."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Glacier Deep Archive</strong> - Amazon S3 Glacier Deep Archive is a secure, durable, and low-cost storage class for data archiving. Amazon S3 Glacier Deep Archive does not support millisecond latency, so this option is ruled out."
      },
      {
        "link": "https://aws.amazon.com/s3/storage-classes",
        "answer": "",
        "explanation": "For more details on the durability, availability, cost and access latency - please review this reference link:\n<a href=\"https://aws.amazon.com/s3/storage-classes\">https://aws.amazon.com/s3/storage-classes</a>"
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/s3/storage-classes"
    ]
  },
  {
    "id": 25,
    "question": "The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home.\n\nAs a solutions architect, which of the following solutions would you recommend? (Select two)",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct options:\n\nPower the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements\n\nAmazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache for Redis can be used to power the live leaderboard, so this option is correct.\n\nAmazon ElastiCache for Redis Overview: \n\nPower the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements\n\nAmazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. So DynamoDB with DAX can be used to power the live leaderboard.\n\nIncorrect options:\n\nPower the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Neptune is not an in-memory database, so this option is not correct.\n\nPower the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements - DynamoDB is not an in-memory database, so this option is not correct.\n\nPower the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. Aurora is not an in-memory database, so this option is not correct.\n\nReferences:\n\nhttps://aws.amazon.com/elasticache/\n\nhttps://aws.amazon.com/elasticache/redis/\n\nhttps://aws.amazon.com/dynamodb/dax/",
    "correctAnswerExplanations": [
      {
        "answer": "Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.\nElastiCache for Redis can be used to power the live leaderboard, so this option is correct."
      },
      {
        "image": "https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png",
        "answer": "",
        "explanation": "Amazon ElastiCache for Redis Overview:"
      },
      {
        "answer": "Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.\nDAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. So DynamoDB with DAX can be used to power the live leaderboard."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Neptune is not an in-memory database, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements</strong> - DynamoDB is not an in-memory database, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. Aurora is not an in-memory database, so this option is not correct."
      }
    ],
    "references": [
      "https://aws.amazon.com/elasticache/",
      "https://aws.amazon.com/elasticache/redis/",
      "https://aws.amazon.com/dynamodb/dax/"
    ]
  },
  {
    "id": 26,
    "question": "The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection.\n\nAs a solutions architect, which of the following solutions would you recommend to the company?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS Direct Connect to establish a connection between the data center and AWS Cloud",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nUse AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud\n\nAWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations.\n\nWith AWS Direct Connect plus VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections.\n\nThis solution combines the AWS managed benefits of the VPN solution with low latency, increased bandwidth, more consistent benefits of the AWS Direct Connect solution, and an end-to-end, secure IPsec connection. Therefore, AWS Direct Connect plus VPN is the correct solution for this use-case.\n\nAWS Direct Connect Plus VPN: \n via - https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html\n\nIncorrect options:\n\nUse AWS site-to-site VPN to establish a connection between the data center and AWS Cloud - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. However, Site-to-site VPN cannot provide low latency and high throughput connection, therefore this option is ruled out.\n\nUse AWS Transit Gateway to establish a connection between the data center and AWS Cloud - AWS Transit Gateway is a network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. AWS Transit Gateway by itself cannot establish a low latency and high throughput connection between a data center and AWS Cloud. Hence this option is incorrect.\n\nUse AWS Direct Connect to establish a connection between the data center and AWS Cloud - AWS Direct Connect by itself cannot provide an encrypted connection between a data center and AWS Cloud, so this option is ruled out.\n\nReferences:\n\nhttps://aws.amazon.com/directconnect/\n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations."
      },
      {
        "answer": "",
        "explanation": "With AWS Direct Connect plus VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections."
      },
      {
        "answer": "",
        "explanation": "This solution combines the AWS managed benefits of the VPN solution with low latency, increased bandwidth, more consistent benefits of the AWS Direct Connect solution, and an end-to-end, secure IPsec connection. Therefore, AWS Direct Connect plus VPN is the correct solution for this use-case."
      },
      {
        "image": "https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/images/image10.png",
        "answer": "",
        "explanation": "AWS Direct Connect Plus VPN:"
      },
      {
        "link": "https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.\nHowever, Site-to-site VPN cannot provide low latency and high throughput connection, therefore this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud</strong> - AWS Transit Gateway is a network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. AWS Transit Gateway by itself cannot establish a low latency and high throughput connection between a data center and AWS Cloud. Hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Direct Connect to establish a connection between the data center and AWS Cloud</strong> - AWS Direct Connect by itself cannot provide an encrypted connection between a data center and AWS Cloud, so this option is ruled out."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html",
      "https://aws.amazon.com/directconnect/",
      "https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html"
    ]
  },
  {
    "id": 27,
    "question": "A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed.\n\nWhat would you recommend?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nUse Amazon CloudFront with a custom origin pointing to the on-premises servers\n\nAmazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. Amazon CloudFront uses standard cache control headers you set on your files to identify static and dynamic content. You can use different origins for different types of content on a single site – e.g. Amazon S3 for static objects, Amazon EC2 for dynamic content, and custom origins for third-party content.\n\nAmazon CloudFront: \n via - https://aws.amazon.com/cloudfront/\n\nAn origin server stores the original, definitive version of your objects. If you're serving content over HTTP, your origin server is either an Amazon S3 bucket or an HTTP server, such as a web server. Your HTTP server can run on an Amazon Elastic Compute Cloud (Amazon EC2) instance or on a server that you manage; these servers are also known as custom origins.\n\n via - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\n\nAmazon CloudFront employs a global network of edge locations and regional edge caches that cache copies of your content close to your viewers. Amazon CloudFront ensures that end-user requests are served by the closest edge location. As a result, viewer requests travel a short distance, improving performance for your viewers. Therefore for the given use case, the users in Asia will enjoy a low latency experience while using the website even though the on-premises servers continue to be in the US.\n\nIncorrect options:\n\nUse Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53 - This option has been added as a distractor. CloudFront cannot have a custom origin pointing to the DNS record of the website on Route 53.\n\nMigrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia - The use case states that the company operates a dynamic website. You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. By contrast, a dynamic website relies on server-side processing, including server-side scripts, such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting, but AWS has other resources for hosting dynamic websites. So this option is incorrect.\n\nLeverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers - Since the on-premises servers continue to be in the US, so even using a Route 53 geo-proximity routing policy that directs the users in Asia to the on-premises servers in the US would not reduce the latency for the users in Asia. So this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/cloudfront/\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. Amazon CloudFront uses standard cache control headers you set on your files to identify static and dynamic content. You can use different origins for different types of content on a single site – e.g. Amazon S3 for static objects, Amazon EC2 for dynamic content, and custom origins for third-party content."
      },
      {
        "image": "https://d1.awsstatic.com/products/cloudfront/product-page-diagram_CloudFront_HIW.475cd71e52ebbb9acbe55fd1b242c75ebb619a2e.png",
        "answer": "",
        "explanation": "Amazon CloudFront:"
      },
      {
        "link": "https://aws.amazon.com/cloudfront/"
      },
      {
        "answer": "",
        "explanation": "An origin server stores the original, definitive version of your objects. If you're serving content over HTTP, your origin server is either an Amazon S3 bucket or an HTTP server, such as a web server. Your HTTP server can run on an Amazon Elastic Compute Cloud (Amazon EC2) instance or on a server that you manage; these servers are also known as custom origins."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html"
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront employs a global network of edge locations and regional edge caches that cache copies of your content close to your viewers. Amazon CloudFront ensures that end-user requests are served by the closest edge location. As a result, viewer requests travel a short distance, improving performance for your viewers. Therefore for the given use case, the users in Asia will enjoy a low latency experience while using the website even though the on-premises servers continue to be in the US."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53</strong> - This option has been added as a distractor. CloudFront cannot have a custom origin pointing to the DNS record of the website on Route 53."
      },
      {
        "answer": "",
        "explanation": "<strong>Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia</strong> - The use case states that the company operates a dynamic website. You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. By contrast, a dynamic website relies on server-side processing, including server-side scripts, such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting, but AWS has other resources for hosting dynamic websites.  So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers</strong> - Since the on-premises servers continue to be in the US, so even using a Route 53 geo-proximity routing policy that directs the users in Asia to the on-premises servers in the US would not reduce the latency for the users in Asia. So this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/cloudfront/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
      "https://aws.amazon.com/cloudfront/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html"
    ]
  },
  {
    "id": 28,
    "question": "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution.\n\nWhich of the following will you recommend to meet these requirements?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nPush score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB\n\nTo help ingest real-time data or streaming data at large scales, you can use Amazon Kinesis Data Streams (KDS). KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. The data collected is available in milliseconds, enabling real-time analytics. KDS provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications.\n\nAWS Lambda integrates natively with Kinesis Data Streams. The polling, checkpointing, and error handling complexities are abstracted when you use this native integration. The processed data can then be configured to be saved in Amazon DynamoDB.\n\nIncorrect options:\n\nPush score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database\n\nPush score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB\n\nPush score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance\n\nThese three options use Amazon EC2 instances as part of the solution architecture. The use-case seeks to minimize the management overhead required to maintain the solution. However, Amazon EC2 instances involve several maintenance activities such as managing the guest operating system and software deployed to the guest operating system, including updates and security patches, etc. Hence these options are incorrect.\n\nReference:\n\nhttps://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/",
    "correctAnswerExplanations": [
      {
        "answer": "Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "To help ingest real-time data or streaming data at large scales, you can use Amazon Kinesis Data Streams (KDS). KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. The data collected is available in milliseconds, enabling real-time analytics. KDS provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications."
      },
      {
        "answer": "",
        "explanation": "AWS Lambda integrates natively with Kinesis Data Streams. The polling, checkpointing, and error handling complexities are abstracted when you use this native integration. The processed data can then be configured to be saved in Amazon DynamoDB."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database",
        "explanation": ""
      },
      {
        "answer": "Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB",
        "explanation": ""
      },
      {
        "answer": "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options use Amazon EC2 instances as part of the solution architecture. The use-case seeks to minimize the management overhead required to maintain the solution. However, Amazon EC2 instances involve several maintenance activities such as managing the guest operating system and software deployed to the guest operating system, including updates and security patches, etc. Hence these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/"
    ]
  },
  {
    "id": 29,
    "question": "A gaming company uses Amazon Aurora as its primary database service. The company has now deployed 5 multi-AZ read replicas to increase the read throughput and for use as failover target. The replicas have been assigned the following failover priority tiers and corresponding instance sizes are given in parentheses: tier-1 (16 terabytes), tier-1 (32 terabytes), tier-10 (16 terabytes), tier-15 (16 terabytes), tier-15 (32 terabytes).\n\nIn the event of a failover, Amazon Aurora will promote which of the following read replicas?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Tier-1 (16 terabytes)",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Tier-15 (32 terabytes)",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Tier-10 (16 terabytes)",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Tier-1 (32 terabytes)",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nTier-1 (32 terabytes)\n\nAmazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs).\n\nFor Amazon Aurora, each Read Replica is associated with a priority tier (0-15). In the event of a failover, Amazon Aurora will promote the Read Replica that has the highest priority (the lowest numbered tier). If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size. If two or more Aurora Replicas share the same priority and size, then Amazon Aurora promotes an arbitrary replica in the same promotion tier.\n\nTherefore, for this problem statement, the Tier-1 (32 terabytes) replica will be promoted.\n\nIncorrect options:\n\nTier-15 (32 terabytes)\n\nTier-1 (16 terabytes)\n\nTier-10 (16 terabytes)\n\nGiven the failover rules discussed earlier in the explanation, these three options are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html",
    "correctAnswerExplanations": [
      {
        "answer": "Tier-1 (32 terabytes)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs)."
      },
      {
        "answer": "",
        "explanation": "For Amazon Aurora, each Read Replica is associated with a priority tier (0-15).  In the event of a failover, Amazon Aurora will promote the Read Replica that has the highest priority (the lowest numbered tier). If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size. If two or more Aurora Replicas share the same priority and size, then Amazon Aurora promotes an arbitrary replica in the same promotion tier."
      },
      {
        "answer": "",
        "explanation": "Therefore, for this problem statement, the Tier-1 (32 terabytes) replica will be promoted."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Tier-15 (32 terabytes)",
        "explanation": ""
      },
      {
        "answer": "Tier-1 (16 terabytes)",
        "explanation": ""
      },
      {
        "answer": "Tier-10 (16 terabytes)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Given the failover rules discussed earlier in the explanation, these three options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html"
    ]
  },
  {
    "id": 30,
    "question": "A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order.\n\nWhich of the following options can be used to implement this system?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon SQS standard queue to process the messages",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nUse Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues - Standard queues vs FIFO queues.\n\nFor FIFO queues, the order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out). On the other hand, the standard SQS queues offer best-effort ordering. This means that occasionally, messages might be delivered in an order different from which they were sent.\n\nBy default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). When you batch 10 messages per operation (maximum), FIFO queues can support up to 3,000 messages per second. Therefore you need to process 4 messages per operation so that the FIFO queue can support up to 1200 messages per second, which is well within the peak rate.\n\nFIFO Queues Overview: \n via - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\n\nIncorrect options:\n\nUse Amazon SQS standard queue to process the messages - As messages need to be processed in order, therefore standard queues are ruled out.\n\nUse Amazon SQS FIFO (First-In-First-Out) queue to process the messages - By default, FIFO queues support up to 300 messages per second and this is not sufficient to meet the message processing throughput per the given use-case. Hence this option is incorrect.\n\nUse Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate - As mentioned earlier in the explanation, you need to use FIFO queues in batch mode and process 4 messages per operation, so that the FIFO queue can support up to 1200 messages per second. With 2 messages per operation, you can only support up to 600 messages per second.\n\nReferences:\n\nhttps://aws.amazon.com/sqs/\n\nhttps://aws.amazon.com/sqs/features/",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues - Standard queues vs FIFO queues."
      },
      {
        "answer": "",
        "explanation": "For FIFO queues, the order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out). On the other hand, the standard SQS queues offer best-effort ordering. This means that occasionally, messages might be delivered in an order different from which they were sent."
      },
      {
        "answer": "",
        "explanation": "By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). When you batch 10 messages per operation (maximum), FIFO queues can support up to 3,000 messages per second. Therefore you need to process 4 messages per operation so that the FIFO queue can support up to 1200 messages per second, which is well within the peak rate."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q34-i1.jpg",
        "answer": "",
        "explanation": "FIFO Queues Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS standard queue to process the messages</strong> - As messages need to be processed in order, therefore standard queues are ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages</strong> - By default, FIFO queues support up to 300 messages per second and this is not sufficient to meet the message processing throughput per the given use-case. Hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate</strong> - As mentioned earlier in the explanation, you need to use FIFO queues in batch mode and process 4 messages per operation, so that the FIFO queue can support up to 1200 messages per second. With 2 messages per operation, you can only support up to 600 messages per second."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/sqs/features/"
    ]
  },
  {
    "id": 31,
    "question": "A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom.\n\nWhich of the following is the BEST solution for this use-case?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nUse server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3\n\nAWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Therefore SSE-KMS is the correct solution for this use-case.\n\nServer Side Encryption in S3: \n via - https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\n\nIncorrect options:\n\nUse server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3 - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However this option does not provide the ability to audit trail the usage of the encryption keys.\n\nUse server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3 - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. However this option does not provide the ability to audit trail the usage of the encryption keys.\n\nUse client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3 - Using client-side encryption is ruled out as the startup does not want to provide the encryption keys.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created.\nSSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Therefore SSE-KMS is the correct solution for this use-case."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q22-i1.jpg",
        "answer": "",
        "explanation": "Server Side Encryption in S3:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However this option does not provide the ability to audit trail the usage of the encryption keys."
      },
      {
        "answer": "",
        "explanation": "<strong>Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. However this option does not provide the ability to audit trail the usage of the encryption keys."
      },
      {
        "answer": "",
        "explanation": "<strong>Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3</strong> - Using client-side encryption is ruled out as the startup does not want to provide the encryption keys."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"
    ]
  },
  {
    "id": 32,
    "question": "Amazon CloudFront offers a multi-tier cache in the form of regional edge caches that improve latency. However, there are certain content types that bypass the regional edge cache, and go directly to the origin.\n\nWhich of the following content types skip the regional edge cache? (Select two)",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "E-commerce assets such as product photos",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin",
        "correct": true
      },
      {
        "id": 3,
        "answer": "User-generated videos",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Dynamic content, as determined at request time (cache-behavior configured to forward all headers)",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Static content such as style sheets, JavaScript files",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct options:\n\nDynamic content, as determined at request time (cache-behavior configured to forward all headers)\n\nAmazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\n\nCloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.\n\nDynamic content, as determined at request time (cache-behavior configured to forward all headers), does not flow through regional edge caches, but goes directly to the origin. So this option is correct.\n\nProxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin\n\nProxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin from the POPs and do not proxy through the regional edge caches. So this option is also correct.\n\nHow Amazon CloudFront Delivers Content: \n via - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html\n\nIncorrect Options:\n\nE-commerce assets such as product photos\n\nUser-generated videos\n\nStatic content such as style sheets, JavaScript files\n\nThe following type of content flows through the regional edge caches - user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos and static content such as style sheets, JavaScript files. Hence these three options are not correct.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html",
    "correctAnswerExplanations": [
      {
        "answer": "Dynamic content, as determined at request time (cache-behavior configured to forward all headers)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment."
      },
      {
        "answer": "",
        "explanation": "CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content."
      },
      {
        "answer": "",
        "explanation": "Dynamic content, as determined at request time (cache-behavior configured to forward all headers), does not flow through regional edge caches, but goes directly to the origin. So this option is correct."
      },
      {
        "answer": "Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin from the POPs and do not proxy through the regional edge caches. So this option is also correct."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q55-i1.jpg",
        "answer": "",
        "explanation": "How Amazon CloudFront Delivers Content:"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "E-commerce assets such as product photos",
        "explanation": ""
      },
      {
        "answer": "User-generated videos",
        "explanation": ""
      },
      {
        "answer": "Static content such as style sheets, JavaScript files",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The following type of content flows through the regional edge caches - user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos and static content such as style sheets, JavaScript files. Hence these three options are not correct."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html"
    ]
  },
  {
    "id": 33,
    "question": "A research group runs its flagship application on a fleet of Amazon EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset.\n\nWhich of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of Amazon EC2 instances?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Instance Store based Amazon EC2 instances",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon EC2 instances with Amazon EFS mount points",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon EC2 instances with access to Amazon S3 based storage",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nUse Instance Store based Amazon EC2 instances\n\nAn instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host instance. Instance store is ideal for the temporary storage of information that changes frequently such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Instance store volumes are included as part of the instance's usage cost.\n\nAs Instance Store based volumes provide high random I/O performance at low cost (as the storage is part of the instance's usage cost) and the resilient architecture can adjust for the loss of any instance, therefore you should use Instance Store based Amazon EC2 instances for this use-case.\n\nAmazon EC2 Instance Store Overview: \n via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\n\nIncorrect options:\n\nUse Amazon Elastic Block Store (Amazon EBS) based EC2 instances - Amazon Elastic Block Store (Amazon EBS) based volumes would need to use provisioned IOPS (io1) as the storage type and that would incur additional costs. As we are looking for the most cost-optimal solution, this option is ruled out.\n\nUse Amazon EC2 instances with Amazon EFS mount points - Using Amazon Elastic File System (Amazon EFS) implies that extra resources would have to be provisioned (compared to using instance store where the storage is located on disks that are physically attached to the host instance itself). As we are looking for the most resource-efficient solution, this option is also ruled out.\n\nUse Amazon EC2 instances with access to Amazon S3 based storage - Using Amazon EC2 instances with access to Amazon S3 based storage does not deliver high random I/O performance, this option is just added as a distractor.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use Instance Store based Amazon EC2 instances",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host instance. Instance store is ideal for the temporary storage of information that changes frequently such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Instance store volumes are included as part of the instance's usage cost."
      },
      {
        "answer": "",
        "explanation": "As Instance Store based volumes provide high random I/O performance at low cost (as the storage is part of the instance's usage cost) and the resilient architecture can adjust for the loss of any instance, therefore you should use Instance Store based Amazon EC2 instances for this use-case."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q43-i1.jpg",
        "answer": "",
        "explanation": "Amazon EC2 Instance Store Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances</strong> - Amazon Elastic Block Store (Amazon EBS) based volumes would need to use provisioned IOPS (io1) as the storage type and that would incur additional costs. As we are looking for the most cost-optimal solution, this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EC2 instances with Amazon EFS mount points</strong> - Using Amazon Elastic File System (Amazon EFS) implies that extra resources would have to be provisioned (compared to using instance store where the storage is located on disks that are physically attached to the host instance itself). As we are looking for the most resource-efficient solution, this option is also ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EC2 instances with access to Amazon S3 based storage</strong> - Using Amazon EC2 instances with access to Amazon S3 based storage does not deliver high random I/O performance, this option is just added as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html"
    ]
  },
  {
    "id": 34,
    "question": "One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the USA to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the USA are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches.\n\nWhich of the following options would allow the company to enforce these streaming restrictions? (Select two)",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct options:\n\nUse Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights\n\nGeolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights.\n\nUse georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution\n\nYou can use georestriction, also known as geo-blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution. When a user requests your content, Amazon CloudFront typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following: Allow your users to access your content only if they're in one of the countries on a whitelist of approved countries. Prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries. So this option is also correct.\n\nAmazon Route 53 Routing Policy Overview: \n via - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\n\nIncorrect options:\n\nUse Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights - Use latency-based routing when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Amazon Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server.\n\nUse Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software.\n\nUse Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records.\n\nWeighted routing or failover routing or latency routing cannot be used to restrict the distribution of content to only the locations in which you have distribution rights. So all three options above are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights."
      },
      {
        "answer": "Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can use georestriction, also known as geo-blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution. When a user requests your content, Amazon CloudFront typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following:\nAllow your users to access your content only if they're in one of the countries on a whitelist of approved countries.\nPrevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries. So this option is also correct."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q38-i1.jpg",
        "answer": "",
        "explanation": "Amazon Route 53 Routing Policy Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights</strong> - Use latency-based routing when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Amazon Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights</strong> - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights</strong> - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records."
      },
      {
        "answer": "",
        "explanation": "Weighted routing or failover routing or latency routing cannot be used to restrict the distribution of content to only the locations in which you have distribution rights. So all three options above are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html"
    ]
  },
  {
    "id": 35,
    "question": "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type.\n\nWhich of the following is correct regarding the pricing for these two services?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nAmazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests\n\nAmazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. ECS allows you to easily run, scale, and secure Docker container applications on AWS.\n\nAmazon ECS Overview: \n via - https://aws.amazon.com/ecs/\n\nWith the Fargate launch type, you pay for the amount of vCPU and memory resources that your containerized application requests. vCPU and memory resources are calculated from the time your container images are pulled until the Amazon ECS Task terminates, rounded up to the nearest second. With the EC2 launch type, there is no additional charge for the EC2 launch type. You pay for AWS resources (e.g. EC2 instances or EBS volumes) you create to store and run your application.\n\nIncorrect options:\n\nBoth Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests\n\nBoth Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used\n\nAs mentioned above - with the Fargate launch type, you pay for the amount of vCPU and memory resources. With EC2 launch type, you pay for AWS resources (e.g. EC2 instances or EBS volumes). Hence both these options are incorrect.\n\nBoth Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour\n\nThis is a made-up option and has been added as a distractor.\n\nReferences:\n\nhttps://aws.amazon.com/ecs/pricing/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. ECS allows you to easily run, scale, and secure Docker container applications on AWS."
      },
      {
        "image": "https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png",
        "answer": "",
        "explanation": "Amazon ECS Overview:"
      },
      {
        "link": "https://aws.amazon.com/ecs/"
      },
      {
        "answer": "",
        "explanation": "With the Fargate launch type, you pay for the amount of vCPU and memory resources that your containerized application requests. vCPU and memory resources are calculated from the time your container images are pulled until the Amazon ECS Task terminates, rounded up to the nearest second.\nWith the EC2 launch type, there is no additional charge for the EC2 launch type. You pay for AWS resources (e.g. EC2 instances or EBS volumes) you create to store and run your application."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests",
        "explanation": ""
      },
      {
        "answer": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As mentioned above - with the Fargate launch type, you pay for the amount of vCPU and memory resources. With EC2 launch type, you pay for AWS resources (e.g. EC2 instances or EBS volumes). Hence both these options are incorrect."
      },
      {
        "answer": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This is a made-up option and has been added as a distractor."
      }
    ],
    "references": [
      "https://aws.amazon.com/ecs/",
      "https://aws.amazon.com/ecs/pricing/"
    ]
  },
  {
    "id": 36,
    "question": "A financial services company uses Amazon GuardDuty for analyzing its AWS account metadata to meet the compliance guidelines. However, the company has now decided to stop using Amazon GuardDuty service. All the existing findings have to be deleted and cannot persist anywhere on AWS Cloud.\n\nWhich of the following techniques will help the company meet this requirement?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "De-register the service under services tab",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Suspend the service in the general settings",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Disable the service in the general settings",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Raise a service request with Amazon to completely delete the data from all their backups",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nAmazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately.\n\nDisable the service in the general settings\n\nDisabling the service will delete all remaining data, including your findings and configurations before relinquishing the service permissions and resetting the service. So, this is the correct option for our use case.\n\nIncorrect options:\n\nSuspend the service in the general settings - You can stop Amazon GuardDuty from analyzing your data sources at any time by choosing to suspend the service in the general settings. This will immediately stop the service from analyzing data, but does not delete your existing findings or configurations.\n\nDe-register the service under services tab - This is a made-up option, used only as a distractor.\n\nRaise a service request with Amazon to completely delete the data from all their backups - There is no need to create a service request as you can delete the existing findings by disabling the service.\n\nReference:\n\nhttps://aws.amazon.com/guardduty/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately."
      },
      {
        "answer": "Disable the service in the general settings",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Disabling the service will delete all remaining data, including your findings and configurations before relinquishing the service permissions and resetting the service. So, this is the correct option for our use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Suspend the service in the general settings</strong> - You can stop Amazon GuardDuty from analyzing your data sources at any time by choosing to suspend the service in the general settings. This will immediately stop the service from analyzing data, but does not delete your existing findings or configurations."
      },
      {
        "answer": "",
        "explanation": "<strong>De-register the service under services tab</strong> - This is a made-up option, used only as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Raise a service request with Amazon to completely delete the data from all their backups</strong> - There is no need to create a service request as you can delete the existing findings by disabling the service."
      }
    ],
    "references": [
      "https://aws.amazon.com/guardduty/faqs/"
    ]
  },
  {
    "id": 37,
    "question": "A development team requires permissions to list an Amazon S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege.\n\n    \"Version\": \"2021-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::example-bucket\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n\n\nWhich statement should a solutions architect add to the policy to address this issue?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "{\n    \"Action\": [\n        \"s3:DeleteObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}",
        "correct": true
      },
      {
        "id": 2,
        "answer": "{\n    \"Action\": [\n        \"s3:DeleteObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket*\"\n    ],\n    \"Effect\": \"Allow\"\n}",
        "correct": false
      },
      {
        "id": 3,
        "answer": "{\n    \"Action\": [\n        \"s3:*\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}",
        "correct": false
      },
      {
        "id": 4,
        "answer": "{\n    \"Action\": [\n        \"s3:*Object\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\n**\n\n{\n    \"Action\": [\n        \"s3:DeleteObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n\n\n**\n\nThe main elements of a policy statement are:\n\nEffect: Specifies whether the statement will Allow or Deny an action (Allow is the effect defined here).\n\nAction: Describes a specific action or actions that will either be allowed or denied to run based on the Effect entered. API actions are unique to each service (DeleteObject is the action defined here).\n\nResource: Specifies the resources—for example, an Amazon S3 bucket or objects—that the policy applies to in Amazon Resource Name (ARN) format ( example-bucket/* is the resource defined here).\n\nThis policy provides the necessary delete permissions on the resources of the Amazon S3 bucket to the group.\n\nIncorrect options:\n\n**\n\n{\n    \"Action\": [\n        \"s3:*Object\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n\n\n** - This policy is incorrect as the action value is invalid\n\n**\n\n{\n    \"Action\": [\n        \"s3:*\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n\n\n** - This policy is incorrect since it allows all actions on the resource, which violates the principle of least privilege, as required by the given use case.\n\n**\n\n{\n    \"Action\": [\n        \"s3:DeleteObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n\n\n** - This is incorrect, as the resource name is incorrect. It should have a /* after the bucket name.\n\nReference:\n\nhttps://aws.amazon.com/blogs/security/techniques-for-writing-least-privilege-iam-policies/",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "**"
      },
      {},
      {
        "answer": "",
        "explanation": "**"
      },
      {
        "answer": "",
        "explanation": "The main elements of a policy statement are:"
      },
      {},
      {
        "answer": "",
        "explanation": "This policy provides the necessary delete permissions on the resources of the Amazon S3 bucket to the group."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "**"
      },
      {},
      {
        "answer": "",
        "explanation": "** - This policy is incorrect as the action value is invalid"
      },
      {
        "answer": "",
        "explanation": "**"
      },
      {},
      {
        "answer": "",
        "explanation": "** - This policy is incorrect since it allows all actions on the resource, which violates the principle of least privilege, as required by the given use case."
      },
      {
        "answer": "",
        "explanation": "**"
      },
      {},
      {
        "answer": "",
        "explanation": "** - This is incorrect, as the resource name is incorrect. It should have a <code>/*</code> after the bucket name."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/security/techniques-for-writing-least-privilege-iam-policies/"
    ]
  },
  {
    "id": 38,
    "question": "A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management (AWS IAM).\n\nAs an AWS Certified Solutions Architect – Associate, which best practices would you recommend (Select two)?",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use user credentials to provide access specific permissions for Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a minimum number of accounts and share these account credentials among employees",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Grant maximum privileges to avoid assigning privileges again",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct options:\n\nEnable AWS Multi-Factor Authentication (AWS MFA) for privileged users\n\nAs per the AWS best practices, it is better to enable Multi Factor Authentication (MFA) for privileged users via an MFA-enabled mobile device or hardware MFA token.\n\nConfigure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions\n\nAWS recommends to turn on AWS CloudTrail to log all IAM actions for monitoring and audit purposes.\n\nIncorrect options:\n\nCreate a minimum number of accounts and share these account credentials among employees - AWS recommends that user account credentials should not be shared between users. So, this option is incorrect.\n\nGrant maximum privileges to avoid assigning privileges again - AWS recommends granting the least privileges required to complete a certain job and avoid giving excessive privileges which can be misused. So, this option is incorrect.\n\nUse user credentials to provide access specific permissions for Amazon EC2 instances - It is highly recommended to use roles to grant access permissions for EC2 instances working on different AWS services. So, this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/iam/\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\n\nhttps://aws.amazon.com/cloudtrail/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As per the AWS best practices, it is better to enable Multi Factor Authentication (MFA) for privileged users via an MFA-enabled mobile device or hardware MFA token."
      },
      {
        "answer": "Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS recommends to turn on AWS CloudTrail to log all IAM actions for monitoring and audit purposes."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a minimum number of accounts and share these account credentials among employees</strong> - AWS recommends that user account credentials should not be shared between users. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Grant maximum privileges to avoid assigning privileges again</strong> - AWS recommends granting the least privileges required to complete a certain job and avoid giving excessive privileges which can be misused. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use user credentials to provide access specific permissions for Amazon EC2 instances</strong> - It is highly recommended to use roles to grant access permissions for EC2 instances working on different AWS services. So, this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/iam/",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
      "https://aws.amazon.com/cloudtrail/faqs/"
    ]
  },
  {
    "id": 39,
    "question": "A company is in the process of migrating its on-premises SMB file shares to AWS so the company can get out of the business of managing multiple file servers across dozens of offices. The company has 200 terabytes of data in its file servers. The existing on-premises applications and native Windows workloads should continue to have low latency access to this data which needs to be stored on a file system service without any disruptions after the migration. The company also wants any new applications deployed on AWS to have access to this migrated data.\n\nWhich of the following is the best solution to meet this requirement?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon EFS. The applications deployed on AWS can access this data directly from Amazon EFS",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon S3. The applications deployed on AWS can access this data directly from Amazon S3",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nUse Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS\n\nFor user or team file shares, and file-based application migrations, Amazon FSx File Gateway provides low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. For applications deployed on AWS, you may access your file shares directly from Amazon FSx in AWS.\n\nFor your native Windows workloads and users, or your SMB clients, Amazon FSx for Windows File Server provides all of the benefits of a native Windows SMB environment that is fully managed and secured and scaled like any other AWS service. You get detailed reporting, replication, backup, failover, and support for native Windows tools like DFS and Active Directory.\n\nAmazon FSx File Gateway: \n via - https://aws.amazon.com/storagegateway/file/\n\nIncorrect options:\n\nUse Amazon Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS - When you need to access S3 using a file system protocol, you should use File Gateway. You get a local cache in the gateway that provides high throughput and low latency over SMB.\n\nAWS Storage Gateway’s File Gateway does not support file shares in Amazon FSx for Windows File Server, so this option is incorrect.\n\nAWS Storage Gateway’s File Gateway: \n\nUse AWS Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon S3. The applications deployed on AWS can access this data directly from Amazon S3 - When you need to access S3 using a file system protocol, you should use File Gateway. You get a local cache in the gateway that provides high throughput and low latency over SMB.\n\nThe given use case requires low latency access to data which needs to be stored on a file system service after migration. Since S3 is an object storage service, so this option is incorrect.\n\nUse Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon EFS. The applications deployed on AWS can access this data directly from Amazon EFS - Amazon FSx File Gateway provides access to fully managed file shares in Amazon FSx for Windows File Server and it does not support EFS. You should also note that EFS uses the Network File System version 4 (NFS v4) protocol and it does not support SMB protocol. Therefore this option is incorrect for the given use case.\n\nReferences:\n\nhttps://aws.amazon.com/storagegateway/file/fsx/\n\nhttps://aws.amazon.com/storagegateway/faqs/\n\nhttps://aws.amazon.com/blogs/storage/aws-reinvent-recap-choosing-storage-for-on-premises-file-based-workloads/",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "For user or team file shares, and file-based application migrations, Amazon FSx File Gateway provides low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. For applications deployed on AWS, you may access your file shares directly from Amazon FSx in AWS."
      },
      {
        "answer": "",
        "explanation": "For your native Windows workloads and users, or your SMB clients, Amazon FSx for Windows File Server provides all of the benefits of a native Windows SMB environment that is fully managed and secured and scaled like any other AWS service. You get detailed reporting, replication, backup, failover, and support for native Windows tools like DFS and Active Directory."
      },
      {
        "image": "https://d1.awsstatic.com/cloud-storage/Amazon%20FSx%20File%20Gateway%20How%20It%20Works%20Diagram.edbf58e4917d47d04e5a5c22132d44bd92733bf5.png",
        "answer": "",
        "explanation": "Amazon FSx File Gateway:"
      },
      {
        "link": "https://aws.amazon.com/storagegateway/file/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS</strong> - When you need to access S3 using a file system protocol, you should use File Gateway. You get a local cache in the gateway that provides high throughput and low latency over SMB."
      },
      {
        "answer": "",
        "explanation": "AWS Storage Gateway’s File Gateway does not support file shares in Amazon FSx for Windows File Server, so this option is incorrect."
      },
      {
        "image": "https://d1.awsstatic.com/cloud-storage/Amazon%20S3%20File%20Gateway%20How%20It%20Works%20Diagram.96e9f7180c6ec8b6212b4d6fadc4a9ac4507b421.png",
        "answer": "",
        "explanation": "AWS Storage Gateway’s File Gateway:"
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Storage Gateway’s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon S3. The applications deployed on AWS can access this data directly from Amazon S3</strong> - When you need to access S3 using a file system protocol, you should use File Gateway. You get a local cache in the gateway that provides high throughput and low latency over SMB."
      },
      {
        "answer": "",
        "explanation": "The given use case requires low latency access to data which needs to be stored on a file system service after migration. Since S3 is an object storage service, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon EFS. The applications deployed on AWS can access this data directly from Amazon EFS</strong> - Amazon FSx File Gateway provides access to fully managed file shares in Amazon FSx for Windows File Server and it does not support EFS. You should also note that EFS uses the Network File System version 4 (NFS v4) protocol and it does not support SMB protocol. Therefore this option is incorrect for the given use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/storagegateway/file/",
      "https://aws.amazon.com/storagegateway/file/fsx/",
      "https://aws.amazon.com/storagegateway/faqs/",
      "https://aws.amazon.com/blogs/storage/aws-reinvent-recap-choosing-storage-for-on-premises-file-based-workloads/"
    ]
  },
  {
    "id": 40,
    "question": "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature.\n\nWhich is the MOST effective way to address this issue so that such incidents do not recur?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals",
        "correct": true
      },
      {
        "id": 2,
        "answer": "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Remove full database access for all IAM users in the organization",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Only root user should have full database access in the organization",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nUse permissions boundary to control the maximum permissions employees can grant to the IAM principals\n\nA permissions boundary can be used to control the maximum permissions employees can grant to the IAM principals (that is, users and roles) that they create and manage. As the IAM administrator, you can define one or more permissions boundaries using managed policies and allow your employee to create a principal with this boundary. The employee can then attach a permissions policy to this principal. However, the effective permissions of the principal are the intersection of the permissions boundary and permissions policy. As a result, the new principal cannot exceed the boundary that you defined. Therefore, using the permissions boundary offers the right solution for this use-case.\n\nPermission Boundary Example: \n via - https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/\n\nIncorrect options:\n\nRemove full database access for all IAM users in the organization - It is not practical to remove full access for all IAM users in the organization because a select set of users need this access for database administration. So this option is not correct.\n\nThe CTO should review the permissions for each new developer's IAM user so that such incidents don't recur - Likewise the CTO is not expected to review the permissions for each new developer's IAM user, as this is best done via an automated procedure. This option has been added as a distractor.\n\nOnly root user should have full database access in the organization - As a best practice, the root user should not access the AWS account to carry out any administrative procedures. So this option is not correct.\n\nReference:\n\nhttps://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/",
    "correctAnswerExplanations": [
      {
        "answer": "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A permissions boundary can be used to control the maximum permissions employees can grant to the IAM principals (that is, users and roles) that they create and manage. As the IAM administrator, you can define one or more permissions boundaries using managed policies and allow your employee to create a principal with this boundary. The employee can then attach a permissions policy to this principal. However, the effective permissions of the principal are the intersection of the permissions boundary and permissions policy. As a result, the new principal cannot exceed the boundary that you defined. Therefore, using the permissions boundary offers the right solution for this use-case."
      },
      {
        "image": "https://d2908q01vomqb2.cloudfront.net/22d200f8670dbdb3e253a90eee5098477c95c23d/2018/07/03/delegated-admin-02.png",
        "answer": "",
        "explanation": "Permission Boundary Example:"
      },
      {
        "link": "https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Remove full database access for all IAM users in the organization</strong> - It is not practical to remove full access for all IAM users in the organization because a select set of users need this access for database administration. So this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur</strong> -  Likewise the CTO is not expected to review the permissions for each new developer's IAM user, as this is best done via an automated procedure. This option has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Only root user should have full database access in the organization</strong> - As a best practice, the root user should not access the AWS account to carry out any administrative procedures. So this option is not correct."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/",
      "https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/"
    ]
  },
  {
    "id": 41,
    "question": "A software engineering intern at an e-commerce company is documenting the process flow to provision Amazon EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes Human Resources payroll data. He wants to highlight those volume types that cannot be used as a boot volume.\n\nCan you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "General Purpose Solid State Drive (gp2)",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Instance Store",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Provisioned IOPS Solid state drive (io1)",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Cold Hard disk drive (sc1)",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Throughput Optimized Hard disk drive (st1)",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct options:\n\nThroughput Optimized Hard disk drive (st1)\n\nCold Hard disk drive (sc1)\n\nThe Amazon EBS volume types fall into two categories:\n\nSolid state drive (SSD) backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS.\n\nHard disk drive (HDD) backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS.\n\nThroughput Optimized HDD (st1) and Cold HDD (sc1) volume types CANNOT be used as a boot volume, so these two options are correct.\n\nPlease see this detailed overview of the volume types for Amazon EBS volumes. \n via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\n\nIncorrect options:\n\nGeneral Purpose Solid State Drive (gp2)\n\nProvisioned IOPS Solid state drive (io1)\n\nInstance Store\n\nGeneral Purpose SSD (gp2), Provisioned IOPS SSD (io1), and Instance Store can be used as a boot volume.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html",
    "correctAnswerExplanations": [
      {
        "answer": "Throughput Optimized Hard disk drive (st1)",
        "explanation": ""
      },
      {
        "answer": "Cold Hard disk drive (sc1)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The Amazon EBS volume types fall into two categories:"
      },
      {
        "answer": "",
        "explanation": "Solid state drive (SSD) backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS."
      },
      {
        "answer": "",
        "explanation": "Hard disk drive (HDD) backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS."
      },
      {
        "answer": "",
        "explanation": "Throughput Optimized HDD (st1) and Cold HDD (sc1) volume types CANNOT be used as a boot volume, so these two options are correct."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q43-i1.jpg",
        "answer": "",
        "explanation": "Please see this detailed overview of the volume types for Amazon EBS volumes."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "General Purpose Solid State Drive (gp2)",
        "explanation": ""
      },
      {
        "answer": "Provisioned IOPS Solid state drive (io1)",
        "explanation": ""
      },
      {
        "answer": "Instance Store",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "General Purpose SSD (gp2), Provisioned IOPS SSD (io1), and Instance Store can be used as a boot volume."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html"
    ]
  },
  {
    "id": 42,
    "question": "An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites.\n\nWhich of the following Amazon EC2 instance topologies should this application be deployed on?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements",
        "correct": false
      },
      {
        "id": 2,
        "answer": "The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures",
        "correct": false
      },
      {
        "id": 3,
        "answer": "The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput",
        "correct": true
      },
      {
        "id": 4,
        "answer": "The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nThe Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput\n\nThe key thing to understand in this question is that HPC workloads need to achieve low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. Cluster placement groups pack instances close together inside an Availability Zone. These are recommended for applications that benefit from low network latency, high network throughput, or both. Therefore this option is the correct answer.\n\nCluster Placement Group: \n via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nIncorrect options:\n\nThe Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively - A partition placement group spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement group can have a maximum of seven partitions per Availability Zone. Since a partition placement group can have partitions in multiple Availability Zones in the same region, therefore instances will not have low-latency network performance. Hence the partition placement group is not the right fit for HPC applications.\n\nPartition Placement Group: \n via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nThe Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. You can have a maximum of seven running instances per Availability Zone per group. Since a spread placement group can span multiple Availability Zones in the same Region, therefore instances will not have low-latency network performance. Hence spread placement group is not the right fit for HPC applications.\n\nSpread Placement Group: \n via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nThe Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements - An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling. You do not use Auto Scaling groups per se to meet HPC requirements.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
    "correctAnswerExplanations": [
      {
        "answer": "The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The key thing to understand in this question is that HPC workloads need to achieve low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. Cluster placement groups pack instances close together inside an Availability Zone. These are recommended for applications that benefit from low network latency, high network throughput, or both. Therefore this option is the correct answer."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q23-i1.jpg",
        "answer": "",
        "explanation": "Cluster Placement Group:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively</strong> - A partition placement group spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement group can have a maximum of seven partitions per Availability Zone.\nSince a partition placement group can have partitions in multiple Availability Zones in the same region, therefore instances will not have low-latency network performance. Hence the partition placement group is not the right fit for HPC applications."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q23-i2.jpg",
        "answer": "",
        "explanation": "Partition Placement Group:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"
      },
      {
        "answer": "",
        "explanation": "<strong>The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures</strong> - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. You can have a maximum of seven running instances per Availability Zone per group. Since a spread placement group can span multiple Availability Zones in the same Region, therefore instances will not have low-latency network performance. Hence spread placement group is not the right fit for HPC applications."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q23-i3.jpg",
        "answer": "",
        "explanation": "Spread Placement Group:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"
      },
      {
        "answer": "",
        "explanation": "<strong>The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements</strong> - An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling. You do not use Auto Scaling groups per se to meet HPC requirements."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"
    ]
  },
  {
    "id": 43,
    "question": "A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes.\n\nWhich of the following is the fastest way to upload the daily compressed file into Amazon S3?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Upload the compressed file in a single operation",
        "correct": false
      },
      {
        "id": 2,
        "answer": "FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Upload the compressed file using multipart upload",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nUpload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)\n\nAmazon S3 Transfer Acceleration (Amazon S3TA) enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.\n\nMultipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts.\n\nIncorrect options:\n\nUpload the compressed file in a single operation - In general, when your object size reaches 100 megabytes, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput - you can upload parts in parallel to improve throughput. Therefore, this option is not correct.\n\nUpload the compressed file using multipart upload - Although using multipart upload would certainly speed up the process, combining with Amazon S3 Transfer Acceleration (Amazon S3TA) would further improve the transfer speed. Therefore just using multipart upload is not the correct option.\n\nFTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket - This is a roundabout process of getting the file into Amazon S3 and added as a distractor. Although it is technically feasible to follow this process, it would involve a lot of scripting and certainly would not be the fastest way to get the file into Amazon S3.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html",
    "correctAnswerExplanations": [
      {
        "answer": "Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Transfer Acceleration (Amazon S3TA) enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path."
      },
      {
        "answer": "",
        "explanation": "Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Upload the compressed file in a single operation</strong> - In general, when your object size reaches 100 megabytes, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput - you can upload parts in parallel to improve throughput. Therefore, this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Upload the compressed file using multipart upload</strong> - Although using multipart upload would certainly speed up the process, combining with Amazon S3 Transfer Acceleration (Amazon S3TA) would further improve the transfer speed. Therefore just using multipart upload is not the correct option."
      },
      {
        "answer": "",
        "explanation": "<strong>FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket</strong> -  This is a roundabout process of getting the file into Amazon S3 and added as a distractor. Although it is technically feasible to follow this process, it would involve a lot of scripting and certainly would not be the fastest way to get the file into Amazon S3."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html"
    ]
  },
  {
    "id": 44,
    "question": "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible.\n\nAs an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nConfigure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days\n\nAmazon S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single Availability Zone (AZ) and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed and re-creatable data but do not require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. The minimum storage duration is 30 days before you can transition objects from Amazon S3 Standard to Amazon S3 One Zone-IA.\n\nAmazon S3 One Zone-IA offers the same high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. S3 Storage Classes can be configured at the object level, and a single bucket can contain objects stored across Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One Zone-IA. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes.\n\nConstraints for Lifecycle storage class transitions: \n via - https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\n\nSupported Amazon S3 lifecycle transitions: \n via - https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\n\nIncorrect options:\n\nConfigure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days\n\nConfigure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days\n\nAs mentioned earlier, the minimum storage duration is 30 days before you can transition objects from Amazon S3 Standard to Amazon S3 One Zone-IA or Amazon S3 Standard-IA, so both these options are added as distractors.\n\nConfigure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days - Amazon S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. But, it costs more than Amazon S3 One Zone-IA because of the redundant storage across Availability Zones (AZs). As the data is re-creatable, so you don't need to incur this additional cost.\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single Availability Zone (AZ) and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed and re-creatable data but do not require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. The minimum storage duration is 30 days before you can transition objects from Amazon S3 Standard to Amazon S3 One Zone-IA."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 One Zone-IA offers the same high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. S3 Storage Classes can be configured at the object level, and a single bucket can contain objects stored across Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One Zone-IA. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q8-i1.jpg",
        "answer": "",
        "explanation": "Constraints for Lifecycle storage class transitions:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q8-i2.jpg",
        "answer": "",
        "explanation": "Supported Amazon S3 lifecycle transitions:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days",
        "explanation": ""
      },
      {
        "answer": "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As mentioned earlier, the minimum storage duration is 30 days before you can transition objects from Amazon S3 Standard to Amazon S3 One Zone-IA or Amazon S3 Standard-IA, so both these options are added as distractors."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days</strong> - Amazon S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. But, it costs more than Amazon S3 One Zone-IA because of the redundant storage across Availability Zones (AZs). As the data is re-creatable, so you don't need to incur this additional cost."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html",
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html"
    ]
  },
  {
    "id": 45,
    "question": "A social photo-sharing company uses Amazon Simple Storage Service (Amazon S3) to store the images uploaded by the users. These images are kept encrypted in Amazon S3 by using AWS Key Management Service (AWS KMS) and the company manages its own AWS KMS keys for encryption. A member of the DevOps team accidentally deleted the AWS KMS key a day ago, thereby rendering the user's photo data unrecoverable. You have been contacted by the company to consult them on possible solutions to this crisis.\n\nAs a solutions architect, which of the following steps would you recommend to solve this issue?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Contact AWS support to retrieve the AWS KMS key from their backup",
        "correct": false
      },
      {
        "id": 2,
        "answer": "As the AWS KMS key was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the KMS key deletion and recover the key",
        "correct": true
      },
      {
        "id": 3,
        "answer": "The AWS KMS key can be recovered by the AWS root account user",
        "correct": false
      },
      {
        "id": 4,
        "answer": "The company should issue a notification on its web application informing the users about the loss of their data",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nAs the AWS KMS key was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the KMS key deletion and recover the key\n\nAWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. AWS KMS is a secure and resilient service that uses hardware security modules that have been validated under FIPS 140-2.\n\nDeleting an AWS KMS key in AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. Therefore, AWS KMS enforces a waiting period. To delete a KMS key in AWS KMS you schedule key deletion. You can set the waiting period from a minimum of 7 days up to a maximum of 30 days. The default waiting period is 30 days. During the waiting period, the KMS key status and key state is Pending deletion. To recover the KMS key, you can cancel key deletion before the waiting period ends. After the waiting period ends you cannot cancel key deletion, and AWS KMS deletes the KMS key.\n\nHow Deleting AWS KMS keys Works: \n via - https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\n\nIncorrect options:\n\nContact AWS support to retrieve the AWS KMS key from their backup\n\nThe AWS KMS key can be recovered by the AWS root account user\n\nThe AWS root account user cannot recover the AWS KMS key and the AWS support does not have access to KMS keys via any backups. Both these options just serve as distractors.\n\nThe company should issue a notification on its web application informing the users about the loss of their data - This option is not required as the data can be recovered via the cancel key deletion feature.\n\nReference:\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html",
    "correctAnswerExplanations": [
      {
        "answer": "As the AWS KMS key was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the KMS key deletion and recover the key",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. AWS KMS is a secure and resilient service that uses hardware security modules that have been validated under FIPS 140-2."
      },
      {
        "answer": "",
        "explanation": "Deleting an AWS KMS key in AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. Therefore, AWS KMS enforces a waiting period. To delete a KMS key in AWS KMS you schedule key deletion. You can set the waiting period from a minimum of 7 days up to a maximum of 30 days. The default waiting period is 30 days. During the waiting period, the KMS key status and key state is Pending deletion. To recover the KMS key, you can cancel key deletion before the waiting period ends. After the waiting period ends you cannot cancel key deletion, and AWS KMS deletes the KMS key."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q35-i1.jpg",
        "answer": "",
        "explanation": "How Deleting AWS KMS keys Works:"
      },
      {
        "link": "https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Contact AWS support to retrieve the AWS KMS key from their backup",
        "explanation": ""
      },
      {
        "answer": "The AWS KMS key can be recovered by the AWS root account user",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The AWS root account user cannot recover the AWS KMS key and the AWS support does not have access to KMS keys via any backups. Both these options just serve as distractors."
      },
      {
        "answer": "",
        "explanation": "<strong>The company should issue a notification on its web application informing the users about the loss of their data</strong> - This option is not required as the data can be recovered via the cancel key deletion feature."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html"
    ]
  },
  {
    "id": 46,
    "question": "A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times.\n\nWhich is the most cost-effective solution to build a solution for the workflow?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS Lambda function to run the workflow processes",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon EC2 reserved instances to run the workflow processes",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon EC2 spot instances to run the workflow processes",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use Amazon EC2 on-demand instances to run the workflow processes",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nUse Amazon EC2 spot instances to run the workflow processes\n\nAmazon EC2 instance types: \n via - https://aws.amazon.com/ec2/pricing/\n\nAmazon EC2 Spot instances allow you to request spare Amazon EC2 computing capacity for up to 90% off the On-Demand price.\n\nSpot instances are recommended for:\n\nApplications that have flexible start and end times Applications that are feasible only at very low compute prices Users with urgent computing needs for large amounts of additional capacity\n\nFor the given use case, spot instances offer the most cost-effective solution as the workflow can withstand disruptions and can be started and stopped multiple times.\n\nFor example, considering a process that runs for an hour and needs about 1024 MB of memory, spot instance pricing for a t2.micro instance (having 1024 MB of RAM) is $0.0035 per hour.\n\nSpot instance pricing: \n via - https://aws.amazon.com/ec2/spot/pricing/\n\nContrast this with the pricing of a Lambda function (having 1024 MB of allocated memory), which comes out to $0.0000000167 per 1ms or $0.06 per hour ($0.0000000167 * 1000 * 60 * 60 per hour).\n\nAWS Lambda function pricing: \n via - https://aws.amazon.com/lambda/pricing/\n\nThus, a spot instance turns out to be about 20 times cost effective than a Lambda function to meet the requirements of the given use case.\n\nIncorrect options:\n\nUse AWS Lambda function to run the workflow processes - As mentioned in the explanation above, a Lambda function turns out to be 20 times more expensive than a spot instance to meet the workflow requirements of the given use case, so this option is incorrect. You should also note that the maximum execution time of a Lambda function is 15 minutes, so the workflow process would be disrupted for sure. On the other hand, it is certainly possible that the workflow process can be completed in a single run on the spot instance (the average frequency of stop instance interruption across all Regions and instance types is <10%).\n\nUse Amazon EC2 on-demand instances to run the workflow processes\n\nUse Amazon EC2 reserved instances to run the workflow processes\n\nYou should note that both on-demand and reserved instances are more expensive than spot instances. In addition, reserved instances have a term of 1 year or 3 years, so they are not suited for the given workflow. Therefore, both these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/ec2/pricing/\n\nhttps://aws.amazon.com/ec2/spot/pricing/\n\nhttps://aws.amazon.com/lambda/pricing/\n\nhttps://aws.amazon.com/ec2/spot/instance-advisor/",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon EC2 spot instances to run the workflow processes",
        "explanation": ""
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q21-i1.jpg",
        "answer": "",
        "explanation": "Amazon EC2 instance types:"
      },
      {
        "link": "https://aws.amazon.com/ec2/pricing/"
      },
      {
        "answer": "",
        "explanation": "Amazon EC2 Spot instances allow you to request spare Amazon EC2 computing capacity for up to 90% off the On-Demand price."
      },
      {
        "answer": "",
        "explanation": "Spot instances are recommended for:"
      },
      {
        "answer": "",
        "explanation": "Applications that have flexible start and end times\nApplications that are feasible only at very low compute prices\nUsers with urgent computing needs for large amounts of additional capacity"
      },
      {
        "answer": "",
        "explanation": "For the given use case, spot instances offer the most cost-effective solution as the workflow can withstand disruptions and can be started and stopped multiple times."
      },
      {
        "answer": "",
        "explanation": "For example, considering a process that runs for an hour and needs about 1024 MB of memory, spot instance pricing for a t2.micro instance (having 1024 MB of RAM) is $0.0035 per hour."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q21-i2.jpg",
        "answer": "",
        "explanation": "Spot instance pricing:"
      },
      {
        "link": "https://aws.amazon.com/ec2/spot/pricing/"
      },
      {
        "answer": "",
        "explanation": "Contrast this with the pricing of a Lambda function (having 1024 MB of allocated memory), which comes out to $0.0000000167 per 1ms or $0.06 per hour ($0.0000000167 * 1000 * 60  * 60 per hour)."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q21-i3.jpg",
        "answer": "",
        "explanation": "AWS Lambda function pricing:"
      },
      {
        "link": "https://aws.amazon.com/lambda/pricing/"
      },
      {
        "answer": "",
        "explanation": "Thus, a spot instance turns out to be about 20 times cost effective than a Lambda function to meet the requirements of the given use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Lambda function to run the workflow processes</strong> - As mentioned in the explanation above, a Lambda function turns out to be 20 times more expensive than a spot instance to meet the workflow requirements of the given use case, so this option is incorrect. You should also note that the maximum execution time of a Lambda function is 15 minutes, so the workflow process would be disrupted for sure. On the other hand, it is certainly possible that the workflow process can be completed in a single run on the spot instance (the average frequency of stop instance interruption across all Regions and instance types is &lt;10%)."
      },
      {
        "answer": "Use Amazon EC2 on-demand instances to run the workflow processes",
        "explanation": ""
      },
      {
        "answer": "Use Amazon EC2 reserved instances to run the workflow processes",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You should note that both on-demand and reserved instances are more expensive than spot instances. In addition, reserved instances have a term of 1 year or 3 years, so they are not suited for the given workflow. Therefore, both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/ec2/pricing/",
      "https://aws.amazon.com/ec2/spot/pricing/",
      "https://aws.amazon.com/lambda/pricing/",
      "https://aws.amazon.com/ec2/pricing/",
      "https://aws.amazon.com/ec2/spot/pricing/",
      "https://aws.amazon.com/lambda/pricing/",
      "https://aws.amazon.com/ec2/spot/instance-advisor/"
    ]
  },
  {
    "id": 47,
    "question": "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account.\n\nAs a solutions architect, which of the following steps would you recommend?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "It is not possible to access cross-account resources",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Both IAM roles and IAM users can be used interchangeably for cross-account access",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nCreate a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment\n\nIAM roles allow you to delegate access to users or services that normally don't have access to your organization's AWS resources. IAM users or AWS services can assume a role to obtain temporary security credentials that can be used to make AWS API calls. Consequently, you don't have to share long-term credentials for access to a resource. Using IAM roles, it is possible to access cross-account resources.\n\nIncorrect options:\n\nCreate new IAM user credentials for the production environment and share these credentials with the set of users from the development environment - There is no need to create new IAM user credentials for the production environment, as you can use IAM roles to access cross-account resources.\n\nIt is not possible to access cross-account resources - You can use IAM roles to access cross-account resources.\n\nBoth IAM roles and IAM users can be used interchangeably for cross-account access - IAM roles and IAM users are separate IAM entities and should not be mixed. Only IAM roles can be used to access cross-account resources.\n\nReference:\n\nhttps://aws.amazon.com/iam/features/manage-roles/",
    "correctAnswerExplanations": [
      {
        "answer": "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "IAM roles allow you to delegate access to users or services that normally don't have access to your organization's AWS resources. IAM users or AWS services can assume a role to obtain temporary security credentials that can be used to make AWS API calls. Consequently, you don't have to share long-term credentials for access to a resource. Using IAM roles, it is possible to access cross-account resources."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment</strong> - There is no need to create new IAM user credentials for the production environment, as you can use IAM roles to access cross-account resources."
      },
      {
        "answer": "",
        "explanation": "<strong>It is not possible to access cross-account resources</strong> - You can use IAM roles to access cross-account resources."
      },
      {
        "answer": "",
        "explanation": "<strong>Both IAM roles and IAM users can be used interchangeably for cross-account access</strong> - IAM roles and IAM users are separate IAM entities and should not be mixed. Only IAM roles can be used to access cross-account resources."
      }
    ],
    "references": [
      "https://aws.amazon.com/iam/features/manage-roles/"
    ]
  },
  {
    "id": 48,
    "question": "The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances have a CPU utilization of about 50%. The application is built on a fleet of Amazon EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances.\n\nAs a solutions architect, what would you recommend so that the application runs near its peak performance state?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nConfigure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%\n\nAn Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies.\n\nWith target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value.\n\nFor example, you can use target tracking scaling to:\n\nConfigure a target tracking scaling policy to keep the average aggregate CPU utilization of your Auto Scaling group at 50 percent. This meets the requirements specified in the given use-case and therefore, this is the correct option.\n\nTarget Tracking Policy Overview: \n via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\n\nIncorrect options:\n\nConfigure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%\n\nConfigure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%\n\nWith step scaling and simple scaling, you choose scaling metrics and threshold values for the Amazon CloudWatch alarms that trigger the scaling process. Neither step scaling nor simple scaling can be configured to use a target metric for CPU utilization, hence both these options are incorrect.\n\nConfigure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50% - An Auto Scaling group cannot directly use a Cloudwatch alarm as the source for a scale-in or scale-out event, hence this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html",
    "correctAnswerExplanations": [
      {
        "answer": "Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies."
      },
      {
        "answer": "",
        "explanation": "With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value."
      },
      {
        "answer": "",
        "explanation": "For example, you can use target tracking scaling to:"
      },
      {
        "answer": "",
        "explanation": "Configure a target tracking scaling policy to keep the average aggregate CPU utilization of your Auto Scaling group at 50 percent. This meets the requirements specified in the given use-case and therefore, this is the correct option."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q30-i1.jpg",
        "answer": "",
        "explanation": "Target Tracking Policy Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%",
        "explanation": ""
      },
      {
        "answer": "Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "With step scaling and simple scaling, you choose scaling metrics and threshold values for the Amazon CloudWatch alarms that trigger the scaling process.\nNeither step scaling nor simple scaling can be configured to use a target metric for CPU utilization, hence both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%</strong> - An Auto Scaling group cannot directly use a Cloudwatch alarm as the source for a scale-in or scale-out event, hence this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html"
    ]
  },
  {
    "id": 49,
    "question": "A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for Amazon RDS Multi-AZ deployment as well as Amazon RDS Read-replicas.\n\nWhich of the following correctly summarizes these capabilities for the given database?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nMulti-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nAmazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones (AZs) within a single region.\n\nAmazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance.\n\nAmazon RDS replicates all databases in the source DB instance. Read replicas can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region.\n\nExam Alert:\n\nPlease review this comparison vis-a-vis Multi-AZ vs Read Replica for Amazon RDS: \n via - https://aws.amazon.com/rds/features/multi-az/\n\nIncorrect Options:\n\nMulti-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nMulti-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nMulti-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nThese three options contradict the earlier details provided in the explanation. To summarize, Multi-AZ deployment follows synchronous replication for Amazon RDS. Hence these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/multi-az/\n\nhttps://aws.amazon.com/rds/features/read-replicas/",
    "correctAnswerExplanations": [
      {
        "answer": "Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones (AZs) within a single region."
      },
      {
        "answer": "",
        "explanation": "Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance."
      },
      {
        "answer": "",
        "explanation": "Amazon RDS replicates all databases in the source DB instance. Read replicas can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q52-i1.jpg",
        "answer": "",
        "explanation": "Please review this comparison vis-a-vis Multi-AZ vs Read Replica for Amazon RDS:"
      },
      {
        "link": "https://aws.amazon.com/rds/features/multi-az/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "explanation": ""
      },
      {
        "answer": "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "explanation": ""
      },
      {
        "answer": "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the earlier details provided in the explanation. To summarize, Multi-AZ deployment follows synchronous replication for Amazon RDS. Hence these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://aws.amazon.com/rds/features/read-replicas/"
    ]
  },
  {
    "id": 50,
    "question": "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost.\n\nWhich of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon FSx for Lustre",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Amazon FSx for Windows File Server",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Glue",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon EMR",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nAmazon FSx for Lustre\n\nAmazon FSx for Lustre makes it easy and cost-effective to launch and run the world’s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. The open-source Lustre file system is designed for applications that require fast storage – where you want your storage to keep up with your compute. FSx for Lustre integrates with Amazon S3, making it easy to process data sets with the Lustre file system. When linked to an S3 bucket, an FSx for Lustre file system transparently presents S3 objects as files and allows you to write changed data back to S3.\n\nFSx for Lustre provides the ability to both process the 'hot data' in a parallel and distributed fashion as well as easily store the 'cold data' on Amazon S3. Therefore this option is the BEST fit for the given problem statement.\n\nIncorrect options:\n\nAmazon FSx for Windows File Server - Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. FSx for Windows does not allow you to present S3 objects as files and does not allow you to write changed data back to S3. Therefore you cannot reference the \"cold data\" with quick access for reads and updates at low cost. Hence this option is not correct.\n\nAmazon EMR - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. EMR does not offer the same storage and processing speed as FSx for Lustre. So it is not the right fit for the given high-performance workflow scenario.\n\nAWS Glue - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. AWS Glue does not offer the same storage and processing speed as FSx for Lustre. So it is not the right fit for the given high-performance workflow scenario.\n\nReferences:\n\nhttps://aws.amazon.com/fsx/lustre/\n\nhttps://aws.amazon.com/fsx/windows/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon FSx for Lustre",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon FSx for Lustre makes it easy and cost-effective to launch and run the world’s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. The open-source Lustre file system is designed for applications that require fast storage – where you want your storage to keep up with your compute. FSx for Lustre integrates with Amazon S3, making it easy to process data sets with the Lustre file system. When linked to an S3 bucket, an FSx for Lustre file system transparently presents S3 objects as files and allows you to write changed data back to S3."
      },
      {
        "answer": "",
        "explanation": "FSx for Lustre provides the ability to both process the 'hot data' in a parallel and distributed fashion as well as easily store the 'cold data' on Amazon S3. Therefore this option is the BEST fit for the given problem statement."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon FSx for Windows File Server</strong> - Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol.  It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.\nFSx for Windows does not allow you to present S3 objects as files and does not allow you to write changed data back to S3. Therefore you cannot reference the \"cold data\" with quick access for reads and updates at low cost. Hence this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EMR</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nEMR does not offer the same storage and processing speed as FSx for Lustre. So it is not the right fit for the given high-performance workflow scenario."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Glue</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing.\nAWS Glue does not offer the same storage and processing speed as FSx for Lustre. So it is not the right fit for the given high-performance workflow scenario."
      }
    ],
    "references": [
      "https://aws.amazon.com/fsx/lustre/",
      "https://aws.amazon.com/fsx/windows/faqs/"
    ]
  },
  {
    "id": 51,
    "question": "A gaming company is looking at improving the availability and performance of its global flagship application which utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service.\n\nWhich of the following AWS services represents the best solution for this use-case?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon Route 53",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon CloudFront",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Elastic Load Balancing (ELB)",
        "correct": false
      },
      {
        "id": 4,
        "answer": "AWS Global Accelerator",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nAWS Global Accelerator\n\nAWS Global Accelerator utilizes the Amazon global network, allowing you to improve the performance of your applications by lowering first-byte latency (the round trip time for a packet to go from a client to your endpoint and back again) and jitter (the variation of latency), and increasing throughput (the amount of time it takes to transfer data) as compared to the public internet.\n\nAWS Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover.\n\nIncorrect options:\n\nAmazon CloudFront - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\n\nAWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery), while Global Accelerator improves performance for a wide range of applications over TCP or UDP.\n\nAWS Elastic Load Balancing (ELB) - Both of the services, ELB and Global Accelerator solve the challenge of routing user requests to healthy application endpoints. AWS Global Accelerator relies on ELB to provide the traditional load balancing features such as support for internal and non-AWS endpoints, pre-warming, and Layer 7 routing. However, while ELB provides load balancing within one Region, AWS Global Accelerator provides traffic management across multiple Regions.\n\nA regional ELB load balancer is an ideal target for AWS Global Accelerator. By using a regional ELB load balancer, you can precisely distribute incoming application traffic across backends, such as Amazon EC2 instances or Amazon ECS tasks, within an AWS Region.\n\nIf you have workloads that cater to a global client base, AWS recommends that you use AWS Global Accelerator. If you have workloads hosted in a single AWS Region and used by clients in and around the same Region, you can use an Application Load Balancer or Network Load Balancer to manage your resources.\n\nAmazon Route 53 - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. Route 53 is ruled out as the company wants to continue using its own custom DNS service.\n\nReference:\n\nhttps://aws.amazon.com/global-accelerator/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "AWS Global Accelerator",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Global Accelerator utilizes the Amazon global network, allowing you to improve the performance of your applications by lowering first-byte latency (the round trip time for a packet to go from a client to your endpoint and back again) and jitter (the variation of latency), and increasing throughput (the amount of time it takes to transfer data) as compared to the public internet."
      },
      {
        "answer": "",
        "explanation": "AWS Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon CloudFront</strong> - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment."
      },
      {
        "answer": "",
        "explanation": "AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery), while Global Accelerator improves performance for a wide range of applications over TCP or UDP."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Elastic Load Balancing (ELB)</strong> - Both of the services, ELB and Global Accelerator solve the challenge of routing user requests to healthy application endpoints. AWS Global Accelerator relies on ELB to provide the traditional load balancing features such as support for internal and non-AWS endpoints, pre-warming, and Layer 7 routing. However, while ELB provides load balancing within one Region, AWS Global Accelerator provides traffic management across multiple Regions."
      },
      {
        "answer": "",
        "explanation": "A regional ELB load balancer is an ideal target for AWS Global Accelerator. By using a regional ELB load balancer, you can precisely distribute incoming application traffic across backends, such as Amazon EC2 instances or Amazon ECS tasks, within an AWS Region."
      },
      {
        "answer": "",
        "explanation": "If you have workloads that cater to a global client base, AWS recommends that you use AWS Global Accelerator. If you have workloads hosted in a single AWS Region and used by clients in and around the same Region, you can use an Application Load Balancer or Network Load Balancer to manage your resources."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Route 53</strong> - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. Route 53 is ruled out as the company wants to continue using its own custom DNS service."
      }
    ],
    "references": [
      "https://aws.amazon.com/global-accelerator/faqs/"
    ]
  },
  {
    "id": 52,
    "question": "The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an Amazon Elastic File System (Amazon EFS) created in us-east-1 region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet.\n\nAs a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region",
        "correct": false
      },
      {
        "id": 2,
        "answer": "The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection",
        "correct": true
      },
      {
        "id": 3,
        "answer": "The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region",
        "correct": false
      },
      {
        "id": 4,
        "answer": "The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nThe spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection\n\nAmazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.\n\nAmazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability. Amazon EC2 instances can access your file system across AZs, regions, and VPCs, while on-premises servers can access using AWS Direct Connect or AWS VPN.\n\nYou can connect to Amazon EFS file systems from EC2 instances in other AWS regions using an inter-region VPC peering connection, and from on-premises servers using an AWS VPN connection. So this is the correct option.\n\nIncorrect options:\n\nThe spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region\n\nThe spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region\n\nCopying the spreadsheet into Amazon S3 or Amazon RDS for MySQL database is not the correct solution as it involves a lot of operational overhead. For Amazon RDS, one would need to write custom code to replicate the spreadsheet functionality running off of the database. S3 does not allow in-place edit of an object. Additionally, it's also not POSIX compliant. So one would need to develop a custom application to \"simulate in-place edits\" to support collabaration as per the use-case. So both these options are ruled out.\n\nThe spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions - Creating copies of the spreadsheet into Amazon EFS file systems of other AWS regions would mean no collaboration would be possible between the teams. In this case, each team would work on \"its own file\" instead of a single file accessed and updated by all teams. Hence this option is incorrect.\n\nReference:\n\nhttps://aws.amazon.com/efs/",
    "correctAnswerExplanations": [
      {
        "answer": "The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources."
      },
      {
        "answer": "",
        "explanation": "Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability. Amazon EC2 instances can access your file system across AZs, regions, and VPCs, while on-premises servers can access using AWS Direct Connect or AWS VPN."
      },
      {
        "answer": "",
        "explanation": "You can connect to Amazon EFS file systems from EC2 instances in other AWS regions using an inter-region VPC peering connection, and from on-premises servers using an AWS VPN connection. So this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region",
        "explanation": ""
      },
      {
        "answer": "The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Copying the spreadsheet into Amazon S3 or Amazon RDS for MySQL database is not the correct solution as it involves a lot of operational overhead. For Amazon RDS, one would need to write custom code to replicate the spreadsheet functionality running off of the database. S3 does not allow in-place edit of an object. Additionally, it's also not POSIX compliant. So one would need to develop a custom application to \"simulate in-place edits\" to support collabaration as per the use-case. So both these options are ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions</strong> - Creating copies of the spreadsheet into Amazon EFS file systems of other AWS regions would mean no collaboration would be possible between the teams. In this case, each team would work on \"its own file\" instead of a single file accessed and updated by all teams. Hence this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/efs/"
    ]
  },
  {
    "id": 53,
    "question": "A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1 gigabytes with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an Amazon EBS volume (General Purpose SSD (gp2)) with 100 gigabytes of provisioned storage and copies the test file into the Amazon EBS volume, and lastly copies the test file into an Amazon EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file.\n\nWhat is the correct order of the storage charges incurred for the test file on these three storage types?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nCost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS\n\nWith Amazon EBS Elastic Volumes, you pay only for the resources that you use. The Amazon EFS Standard Storage pricing is $0.30 per GB per month. Therefore the cost for storing the test file on EFS is $0.30 for the month.\n\nFor Amazon EBS General Purpose SSD (gp2) volumes, the charges are $0.10 per GB-month of provisioned storage. Therefore, for a provisioned storage of 100GB for this use-case, the monthly cost on EBS is $0.10*100 = $10. This cost is irrespective of how much storage is actually consumed by the test file.\n\nFor S3 Standard storage, the pricing is $0.023 per GB per month. Therefore, the monthly storage cost on S3 for the test file is $0.023.\n\nTherefore this is the correct option.\n\nIncorrect options:\n\nCost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS\n\nCost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS\n\nCost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS\n\nFollowing the computations shown earlier in the explanation, these three options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/ebs/pricing/\n\nhttps://aws.amazon.com/s3/pricing/(https://aws.amazon.com/s3/pricing/)\n\nhttps://aws.amazon.com/efs/pricing/",
    "correctAnswerExplanations": [
      {
        "answer": "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "With Amazon EBS Elastic Volumes, you pay only for the resources that you use. The Amazon EFS Standard Storage pricing is $0.30 per GB per month. Therefore the cost for storing the test file on EFS is $0.30 for the month."
      },
      {
        "answer": "",
        "explanation": "For Amazon EBS General Purpose SSD (gp2) volumes, the charges are $0.10 per GB-month of provisioned storage. Therefore, for a provisioned storage of 100GB for this use-case, the monthly cost on EBS is $0.10*100 = $10. This cost is irrespective of how much storage is actually consumed by the test file."
      },
      {
        "answer": "",
        "explanation": "For S3 Standard storage, the pricing is $0.023 per GB per month. Therefore, the monthly storage cost on S3 for the test file is $0.023."
      },
      {
        "answer": "",
        "explanation": "Therefore this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS",
        "explanation": ""
      },
      {
        "answer": "Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS",
        "explanation": ""
      },
      {
        "answer": "Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Following the computations shown earlier in the explanation, these three options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/ebs/pricing/",
      "https://aws.amazon.com/efs/pricing/"
    ]
  },
  {
    "id": 54,
    "question": "A video analytics organization has been acquired by a leading media company. The analytics organization has 10 independent applications with an on-premises data footprint of about 70 Terabytes for each application. The CTO of the media company has set a timeline of two weeks to carry out the data migration from on-premises data center to AWS Cloud and establish connectivity.\n\nWhich of the following are the MOST cost-effective options for completing the data transfer and establishing connectivity? (Select two)",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Setup AWS Site-to-Site VPN to establish on-going connectivity between the on-premises data center and AWS Cloud",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Order 10 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Setup AWS Direct Connect to establish connectivity between the on-premises data center and AWS Cloud",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Order 1 AWS Snowmobile to complete the one-time data transfer",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Order 70 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct options:\n\nOrder 10 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer\n\nAWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 Terabytes of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gigabytes network connectivity to address large scale data transfer and pre-processing use cases.\n\nAs each Snowball Edge Storage Optimized device can handle 80 Terabytes of data, you can order 10 such devices to take care of the data transfer for all applications.\n\nExam Alert:\n\nThe original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80 Terabytes of storage space.\n\nSetup AWS Site-to-Site VPN to establish on-going connectivity between the on-premises data center and AWS Cloud\n\nAWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.\n\nTherefore this option is the right fit for the given use-case as the connectivity can be easily established within the given timeframe.\n\nIncorrect options:\n\nOrder 1 AWS Snowmobile to complete the one-time data transfer - Each AWS Snowmobile has a total capacity of up to 100 petabytes. To migrate large datasets of 10 petabytes or more in a single location, you should use AWS Snowmobile. For datasets less than 10 petabytes or distributed in multiple locations, you should use Snowball. So AWS Snowmobile is not the right fit for this use-case.\n\nSetup AWS Direct Connect to establish connectivity between the on-premises data center and AWS Cloud - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. Direct Connect involves significant monetary investment and takes at least a month to set up, therefore it's not the correct fit for this use-case.\n\nOrder 70 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer - As the data-transfer can be completed with just 10 AWS Snowball Edge Storage Optimized devices, there is no need to order 70 devices.\n\nReferences:\n\nhttps://aws.amazon.com/snowball/faqs/\n\nhttps://aws.amazon.com/vpn/\n\nhttps://aws.amazon.com/snowmobile/faqs/\n\nhttps://aws.amazon.com/directconnect/",
    "correctAnswerExplanations": [
      {
        "answer": "Order 10 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 Terabytes of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gigabytes network connectivity to address large scale data transfer and pre-processing use cases."
      },
      {
        "answer": "",
        "explanation": "As each Snowball Edge Storage Optimized device can handle 80 Terabytes of data, you can order 10 such devices to take care of the data transfer for all applications."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "answer": "",
        "explanation": "The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80 Terabytes of storage space."
      },
      {
        "answer": "Setup AWS Site-to-Site VPN to establish on-going connectivity between the on-premises data center and AWS Cloud",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity."
      },
      {
        "answer": "",
        "explanation": "Therefore this option is the right fit for the given use-case as the connectivity can be easily established within the given timeframe."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Order 1 AWS Snowmobile to complete the one-time data transfer</strong> -  Each AWS Snowmobile has a total capacity of up to 100 petabytes. To migrate large datasets of 10 petabytes or more in a single location, you should use AWS Snowmobile. For datasets less than 10 petabytes or distributed in multiple locations, you should use Snowball. So AWS Snowmobile is not the right fit for this use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Setup AWS Direct Connect to establish connectivity between the on-premises data center and AWS Cloud</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. Direct Connect involves significant monetary investment and takes at least a month to set up, therefore it's not the correct fit for this use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Order 70 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer</strong> - As the data-transfer can be completed with just 10 AWS Snowball Edge Storage Optimized devices, there is no need to order 70 devices."
      }
    ],
    "references": [
      "https://aws.amazon.com/snowball/faqs/",
      "https://aws.amazon.com/vpn/",
      "https://aws.amazon.com/snowmobile/faqs/",
      "https://aws.amazon.com/directconnect/"
    ]
  },
  {
    "id": 55,
    "question": "The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs the games table to be accessible globally but needs the users and games_played tables to be regional only.\n\nHow would you implement this with minimal application refactoring?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "Correct option:\n\nUse an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables\n\nAmazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. Aurora is not an in-memory database.\n\nAmazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case.\n\nFor the given use-case, we, therefore, need to have two Aurora clusters, one for the global table (games table) and the other one for the local tables (users and games_played tables).\n\nIncorrect options:\n\nUse an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables\n\nUse a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables\n\nUse a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables\n\nHere, we want minimal application refactoring. Amazon DynamoDB and Amazon Aurora have a completely different APIs, due to Amazon Aurora being SQL and Amazon DynamoDB being NoSQL. So all three options are incorrect, as they have Amazon DynamoDB as one of the components.\n\nReference:\n\nhttps://aws.amazon.com/rds/aurora/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. Aurora is not an in-memory database."
      },
      {
        "answer": "",
        "explanation": "Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case."
      },
      {
        "answer": "",
        "explanation": "For the given use-case, we, therefore, need to have two Aurora clusters, one for the global table (games table) and the other one for the local tables (users and games_played tables)."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables",
        "explanation": ""
      },
      {
        "answer": "Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables",
        "explanation": ""
      },
      {
        "answer": "Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Here, we want minimal application refactoring. Amazon DynamoDB and Amazon Aurora have a completely different APIs, due to Amazon Aurora being SQL and Amazon DynamoDB being NoSQL. So all three options are incorrect, as they have Amazon DynamoDB as one of the components."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/aurora/faqs/"
    ]
  },
  {
    "id": 56,
    "question": "An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two)",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create AWS account root user access keys and share those keys only with the business owner",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Encrypt the access keys and save them on Amazon S3",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Enable Multi Factor Authentication (MFA) for the AWS account root user account",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Create a strong password for the AWS account root user",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "Correct options:\n\nCreate a strong password for the AWS account root user\n\nEnable Multi Factor Authentication (MFA) for the AWS account root user account\n\nHere are some of the best practices while creating an AWS account root user:\n\n1) Use a strong password to help protect account-level access to the AWS Management Console. 2) Never share your AWS account root user password or access keys with anyone. 3) If you do have an access key for your AWS account root user, delete it. If you must keep it, rotate (change) the access key regularly. You should not encrypt the access keys and save them on Amazon S3. 4) If you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. 5) Enable AWS multi-factor authentication (MFA) on your AWS account root user account.\n\nAWS Root Account Security Best Practices: \n via - https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\n\nIncorrect options:\n\nEncrypt the access keys and save them on Amazon S3 - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Even an encrypted access key for the root user poses a significant security risk. Therefore, this option is incorrect.\n\nCreate AWS account root user access keys and share those keys only with the business owner - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Hence, this option is incorrect.\n\nSend an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future - AWS recommends that you should never share your AWS account root user password or access keys with anyone. Sending an email with AWS account root user credentials creates a security risk as it can be misused by anyone reading the email. Hence, this option is incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#create-iam-users",
    "correctAnswerExplanations": [
      {
        "answer": "Create a strong password for the AWS account root user",
        "explanation": ""
      },
      {
        "answer": "Enable Multi Factor Authentication (MFA) for the AWS account root user account",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Here are some of the best practices while creating an AWS account root user:"
      },
      {
        "answer": "",
        "explanation": "1) Use a strong password to help protect account-level access to the AWS Management Console.\n2) Never share your AWS account root user password or access keys with anyone.\n3) If you do have an access key for your AWS account root user, delete it. If you must keep it, rotate (change) the access key regularly. You should not encrypt the access keys and save them on Amazon S3.\n4) If you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to.\n5) Enable AWS multi-factor authentication (MFA) on your AWS account root user account."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q31-i1.jpg",
        "answer": "",
        "explanation": "AWS Root Account Security Best Practices:"
      },
      {
        "link": "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Encrypt the access keys and save them on Amazon S3</strong> - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Even an encrypted access key for the root user poses a significant security risk. Therefore, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Create AWS account root user access keys and share those keys only with the business owner</strong> - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Hence, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future</strong> - AWS recommends that you should never share your AWS account root user password or access keys with anyone. Sending an email with AWS account root user credentials creates a security risk as it can be misused by anyone reading the email. Hence, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#create-iam-users"
    ]
  },
  {
    "id": 57,
    "question": "A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data.\n\nGiven these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nIngest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing\n\nAWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.\n\nAWS manages all ongoing operations and underlying infrastructure needed to provide a highly available and scalable message queuing service. With SQS, there is no upfront cost, no need to acquire, install, and configure messaging software, and no time-consuming build-out and maintenance of supporting infrastructure. SQS queues are dynamically created and scale automatically so you can build and grow applications quickly and efficiently.\n\nAs there is no need to manually provision the capacity, so this is the correct option.\n\nIncorrect options:\n\nIngest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing -Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic.\n\nFirehose cannot directly write into a DynamoDB table, so this option is incorrect.\n\nIngest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing\n\nIngest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing\n\nUsing an application on an Amazon EC2 instance is ruled out as the carmaker wants to use fully serverless components. So both these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/sqs/\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\n\nhttps://aws.amazon.com/kinesis/data-streams/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent."
      },
      {
        "answer": "",
        "explanation": "AWS manages all ongoing operations and underlying infrastructure needed to provide a highly available and scalable message queuing service. With SQS, there is no upfront cost, no need to acquire, install, and configure messaging software, and no time-consuming build-out and maintenance of supporting infrastructure. SQS queues are dynamically created and scale automatically so you can build and grow applications quickly and efficiently."
      },
      {
        "answer": "",
        "explanation": "As there is no need to manually provision the capacity, so this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing</strong> -Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic."
      },
      {
        "answer": "",
        "explanation": "Firehose cannot directly write into a DynamoDB table, so this option is incorrect."
      },
      {
        "answer": "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "explanation": ""
      },
      {
        "answer": "Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Using an application on an Amazon EC2 instance is ruled out as the carmaker wants to use fully serverless components. So both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/sqs/",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 58,
    "question": "While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs.\n\nWhich of the following represents the best solution for the given scenario?",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nCreate an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team\n\nAWS CloudTrail log data can be ingested into Amazon CloudWatch to monitor and identify your AWS account activity against security threats, and create a governance framework for security best practices. You can analyze log trail event data in CloudWatch using features such as Logs Insight, Contributor Insights, Metric filters, and CloudWatch Alarms.\n\nAWS CloudTrail integrates with the Amazon CloudWatch service to publish the API calls being made to resources or services in the AWS account. The published event has invaluable information that can be used for compliance, auditing, and governance of your AWS accounts. Below we introduce several features available in CloudWatch to monitor API activity, analyze the logs at scale, and take action when malicious activity is discovered, without provisioning your infrastructure.\n\nFor the AWS Cloudtrail logs available in Amazon CloudWatch Logs, you can begin searching and filtering the log data by creating one or more metric filters. Use these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set a CloudWatch Alarm on.\n\nNote: AWS CloudTrail Insights helps AWS users identify and respond to unusual activity associated with write API calls by continuously analyzing CloudTrail management events.\n\nInsights events are logged when AWS CloudTrail detects unusual write management API activity in your account. If you have AWS CloudTrail Insights enabled and CloudTrail detects unusual activity, Insights events are delivered to the destination Amazon S3 bucket for your trail. You can also see the type of insight and the incident time when you view Insights events on the CloudTrail console. Unlike other types of events captured in a CloudTrail trail, Insights events are logged only when CloudTrail detects changes in your account's API usage that differ significantly from the account's typical usage patterns.\n\nIncorrect options:\n\nConfigure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow - AWS CloudTrail cannot stream data to Amazon Kinesis. Amazon S3 buckets and Amazon CloudWatch logs are the only destinations possible.\n\nRun Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards - Generating reports and visualizations help in understanding and analyzing patterns but is not useful as a near-real-time automatic solution for the given problem.\n\nAWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded - When AWS Trusted Advisor refreshes your checks, Trusted Advisor publishes metrics about your check results to Amazon CloudWatch. You can view the metrics in CloudWatch. You can also create alarms to detect status changes to Trusted Advisor checks and status changes for resources, and service quota usage (formerly referred to as limits). The alarm will then notify you when you reach or exceed a service quota for your AWS account. However, the alarm is triggered only when the service limit is reached. We need a solution that raises an alarm when the number of API calls randomly increases or an abnormal pattern is detected. Hence, this option is not the right fit for the given use case.\n\nReferences:\n\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-authorization-failures\n\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html\n\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-insights-events-with-cloudtrail.html\n\nhttps://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-metrics-ta.html",
    "correctAnswerExplanations": [
      {
        "answer": "Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS CloudTrail log data can be ingested into Amazon CloudWatch to monitor and identify your AWS account activity against security threats, and create a governance framework for security best practices. You can analyze log trail event data in CloudWatch using features such as Logs Insight, Contributor Insights, Metric filters, and CloudWatch Alarms."
      },
      {
        "answer": "",
        "explanation": "AWS CloudTrail integrates with the Amazon CloudWatch service to publish the API calls being made to resources or services in the AWS account. The published event has invaluable information that can be used for compliance, auditing, and governance of your AWS accounts. Below we introduce several features available in CloudWatch to monitor API activity, analyze the logs at scale, and take action when malicious activity is discovered, without provisioning your infrastructure."
      },
      {
        "answer": "",
        "explanation": "For the AWS Cloudtrail logs available in Amazon CloudWatch Logs, you can begin searching and filtering the log data by creating one or more metric filters. Use these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set a CloudWatch Alarm on."
      },
      {
        "answer": "",
        "explanation": "Note: AWS CloudTrail Insights helps AWS users identify and respond to unusual activity associated with <code>write</code> API calls by continuously analyzing CloudTrail management events."
      },
      {
        "answer": "",
        "explanation": "Insights events are logged when AWS CloudTrail detects unusual <code>write</code> management API activity in your account. If you have AWS CloudTrail Insights enabled and CloudTrail detects unusual activity, Insights events are delivered to the destination Amazon S3 bucket for your trail. You can also see the type of insight and the incident time when you view Insights events on the CloudTrail console. Unlike other types of events captured in a CloudTrail trail, Insights events are logged only when CloudTrail detects changes in your account's API usage that differ significantly from the account's typical usage patterns."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow</strong> -  AWS CloudTrail cannot stream data to Amazon Kinesis. Amazon S3 buckets and Amazon CloudWatch logs are the only destinations possible."
      },
      {
        "answer": "",
        "explanation": "<strong>Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards</strong> - Generating reports and visualizations help in understanding and analyzing patterns but is not useful as a near-real-time automatic solution for the given problem."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded</strong> - When AWS Trusted Advisor refreshes your checks, Trusted Advisor publishes metrics about your check results to Amazon CloudWatch. You can view the metrics in CloudWatch. You can also create alarms to detect status changes to Trusted Advisor checks and status changes for resources, and service quota usage (formerly referred to as limits). The alarm will then notify you when you reach or exceed a service quota for your AWS account. However, the alarm is triggered only when the service limit is reached. We need a solution that raises an alarm when the number of API calls randomly increases or an abnormal pattern is detected. Hence, this option is not the right fit for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-authorization-failures",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-insights-events-with-cloudtrail.html",
      "https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-metrics-ta.html"
    ]
  },
  {
    "id": 59,
    "question": "A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline.\n\nWhich of the following is the MOST cost-effective strategy for storing this intermediary query data?",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Store the intermediary query results in Amazon S3 Standard storage class",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nStore the intermediary query results in Amazon S3 Standard storage class\n\nAmazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics. As there is no minimum storage duration charge and no retrieval fee (remember that intermediary query results are heavily referenced by other parts of the analytics pipeline), this is the MOST cost-effective storage class amongst the given options.\n\nIncorrect options:\n\nStore the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class - Amazon S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives.\n\nThe minimum storage duration charge is 90 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.\n\nStore the intermediary query results in Amazon S3 Standard-Infrequent Access storage class - Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.\n\nStore the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class - Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. The minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.\n\nTo summarize again, S3 Standard-IA and S3 One Zone-IA have a minimum storage duration charge of 30 days (so instead of 24 hours, you end up paying for 30 days). S3 Standard-IA and S3 One Zone-IA also have retrieval charges (as the results are heavily referenced by other parts of the analytics pipeline, so the retrieval costs would be pretty high). Therefore, these storage classes are not cost optimal for the given use-case.\n\nReference:\n\nhttps://aws.amazon.com/s3/storage-classes/",
    "correctAnswerExplanations": [
      {
        "answer": "Store the intermediary query results in Amazon S3 Standard storage class",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\nAs there is no minimum storage duration charge and no retrieval fee (remember that intermediary query results are heavily referenced by other parts of the analytics pipeline), this is the MOST cost-effective storage class amongst the given options."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class</strong> - Amazon S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives."
      },
      {
        "answer": "",
        "explanation": "The minimum storage duration charge is 90 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class</strong> - Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class</strong> - Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct."
      },
      {
        "answer": "",
        "explanation": "To summarize again, S3 Standard-IA and S3 One Zone-IA have a minimum storage duration charge of 30 days (so instead of 24 hours, you end up paying for 30 days). S3 Standard-IA and S3 One Zone-IA also have retrieval charges (as the results are heavily referenced by other parts of the analytics pipeline, so the retrieval costs would be pretty high). Therefore, these storage classes are not cost optimal for the given use-case."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "id": 60,
    "question": "A telecom company operates thousands of hardware devices like switches, routers, cables, etc. The real-time status data for these devices must be fed into a communications application for notifications. Simultaneously, another analytics application needs to read the same real-time status data and analyze all the connecting lines that may go down because of any device failures.\n\nAs an AWS Certified Solutions Architect – Associate, which of the following solutions would you suggest, so that both the applications can consume the real-time status data concurrently?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon Kinesis Data Streams",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon Simple Notification Service (SNS)",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon Simple Queue Service (SQS) with Amazon Simple Email Service (Amazon SES)",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct option:\n\nAmazon Kinesis Data Streams\n\nAmazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\n\nAWS recommends Amazon Kinesis Data Streams for use cases with requirements that are similar to the following:\n\nRouting related records to the same record processor (as in streaming MapReduce). For example, counting and aggregation are simpler when all records for a given key are routed to the same record processor.\nOrdering of records. For example, you want to transfer log data from the application host to the processing/archival host while maintaining the order of log statements.\nAbility for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.\nAbility to consume records in the same order a few hours later. For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for up to 365 days, you can run the audit application up to 365 days behind the billing application.\n\nIncorrect options:\n\nAmazon Simple Notification Service (SNS) - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Amazon SNS is a notification service and cannot be used for real-time processing of data.\n\nAmazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS) - Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. Since multiple applications need to consume the same data stream concurrently, Kinesis is a better choice when compared to the combination of SQS with SNS.\n\nAmazon Simple Queue Service (SQS) with Amazon Simple Email Service (Amazon SES) - As discussed above, Amazon Kinesis is a better option for this use case in comparison to Amazon SQS. Also, Amazon SES does not fit this use-case. Hence, this option is an incorrect answer.\n\nReference:\n\nhttps://aws.amazon.com/kinesis/data-streams/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon Kinesis Data Streams",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering)."
      },
      {
        "answer": "",
        "explanation": "AWS recommends Amazon Kinesis Data Streams for use cases with requirements that are similar to the following:"
      },
      {}
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Amazon SNS is a notification service and cannot be used for real-time processing of data."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. Since multiple applications need to consume the same data stream concurrently, Kinesis is a better choice when compared to the combination of SQS with SNS."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (SQS) with Amazon Simple Email Service (Amazon SES)</strong> - As discussed above, Amazon Kinesis is a better option for this use case in comparison to Amazon SQS. Also, Amazon SES does not fit this use-case. Hence, this option is an incorrect answer."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 61,
    "question": "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket.\n\nWhich of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct options:\n\nUse Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket\n\nAmazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Amazon S3TA takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.\n\nUse multipart uploads for faster file uploads into the destination Amazon S3 bucket\n\nMultipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput, therefore it facilitates faster file uploads.\n\nIncorrect options:\n\nCreate multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3 - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Direct connect takes significant time (several months) to be provisioned and is an overkill for the given use-case.\n\nCreate multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3 - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections are a good solution if you have low to modest bandwidth requirements and can tolerate the inherent variability in Internet-based connectivity. Site-to-site VPN will not help in accelerating the file transfer speeds into S3 for the given use-case.\n\nUse AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. AWS Global Accelerator will not help in accelerating the file transfer speeds into S3 for the given use-case.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html",
    "correctAnswerExplanations": [
      {
        "answer": "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Amazon S3TA takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path."
      },
      {
        "answer": "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput, therefore it facilitates faster file uploads."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3</strong> - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations.\nDirect connect takes significant time (several months) to be provisioned and is an overkill for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet.\nVPN Connections are a good solution if you have low to modest bandwidth requirements and can tolerate the inherent variability in Internet-based connectivity. Site-to-site VPN will not help in accelerating the file transfer speeds into S3 for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket</strong> - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. AWS Global Accelerator will not help in accelerating the file transfer speeds into S3 for the given use-case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html"
    ]
  },
  {
    "id": 62,
    "question": "A geological research agency maintains the seismological data for the last 100 years. The data has a velocity of 1GB per minute. You would like to store the data with only the most relevant attributes to build a predictive model for earthquakes.\n\nWhat AWS services would you use to build the most cost-effective solution with the LEAST amount of infrastructure maintenance?",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Ingest the data in Amazon Kinesis Data Firehose and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on Amazon S3",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Ingest the data in Amazon Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Ingest the data in a Spark Streaming Cluster on Amazon EMR and use Spark Streaming transformations before writing to Amazon S3",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Ingest the data in Amazon Kinesis Data Streams and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on Amazon S3",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "Correct option:\n\nIngest the data in Amazon Kinesis Data Firehose and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on Amazon S3\n\nAmazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.\n\nAmazon Kinesis Data Firehose Overview \n via - https://aws.amazon.com/kinesis/data-firehose/\n\nThe correct option is to ingest the data in Amazon Kinesis Data Firehose and use a AWS Lambda function to filter and transform the incoming data before the output is dumped on Amazon S3. This way you only need to store a sliced version of the data with only the relevant data attributes required for your model. Also it should be noted that this solution is entirely serverless and requires no infrastructure maintenance.\n\nAmazon Kinesis Data Firehose to Amazon S3: \n via - https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\n\nIncorrect options:\n\nIngest the data in Amazon Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to Amazon S3 - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications, and set up your destination for processed data. Kinesis Data Analytics cannot directly ingest data from the source as it ingests data either from Kinesis Data Streams or Kinesis Data Firehose, so this option is ruled out.\n\nIngest the data in Amazon Kinesis Data Streams and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on Amazon S3 - Amazon Kinesis Data Streams (KDS) is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. Amazon Kinesis Data Streams is integrated with a number of AWS services, including Amazon Kinesis Data Firehose for near real-time transformation.\n\nKinesis Data Streams cannot directly write the output to Amazon S3. Unlike Amazon Kinesis Data Firehose, KDS does not offer a ready-made integration via an intermediary AWS Lambda function to reliably dump data into Amazon S3. You will need to do a lot of custom coding to get the AWS Lambda function to process the incoming stream and then store the transformed output to Amazon S3 with the constraint that the buffer is maintained reliably and no transformed data is lost. So this option is incorrect.\n\nIngest the data in a Spark Streaming Cluster on Amazon EMR and use Spark Streaming transformations before writing to Amazon S3 - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. Using an EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the correct solution for the given use-case should require the least amount of infrastructure maintenance.\n\nReference:\n\nhttps://aws.amazon.com/kinesis/data-firehose/",
    "correctAnswerExplanations": [
      {
        "answer": "Ingest the data in Amazon Kinesis Data Firehose and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on Amazon S3",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security."
      },
      {
        "image": "https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png",
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose Overview"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-firehose/"
      },
      {
        "answer": "",
        "explanation": "The correct option is to ingest the data in Amazon Kinesis Data Firehose and use a AWS Lambda function to filter and transform the incoming data before the output is dumped on Amazon S3. This way you only need to store a sliced version of the data with only the relevant data attributes required for your model. Also it should be noted that this solution is entirely serverless and requires no infrastructure maintenance."
      },
      {
        "image": "https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-s3.png",
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose to Amazon S3:"
      },
      {
        "link": "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Ingest the data in Amazon Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to Amazon S3</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications, and set up your destination for processed data. Kinesis Data Analytics cannot directly ingest data from the source as it ingests data either from Kinesis Data Streams or Kinesis Data Firehose, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Ingest the data in Amazon Kinesis Data Streams and use an intermediary AWS Lambda function to filter and transform the incoming stream before the output is dumped on Amazon S3</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. Amazon Kinesis Data Streams is integrated with a number of AWS services, including Amazon Kinesis Data Firehose for near real-time transformation."
      },
      {
        "answer": "",
        "explanation": "Kinesis Data Streams cannot directly write the output to Amazon S3. Unlike Amazon Kinesis Data Firehose, KDS does not offer a ready-made integration via an intermediary AWS Lambda function to reliably dump data into Amazon S3. You will need to do a lot of custom coding to get the AWS Lambda function to process the incoming stream and then store the transformed output to Amazon S3 with the constraint that the buffer is maintained reliably and no transformed data is lost. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Ingest the data in a Spark Streaming Cluster on Amazon EMR and use Spark Streaming transformations before writing to Amazon S3</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing an EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the correct solution for the given use-case should require the least amount of infrastructure maintenance."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html",
      "https://aws.amazon.com/kinesis/data-firehose/"
    ]
  },
  {
    "id": 63,
    "question": "The development team at an e-commerce startup has set up multiple microservices running on Amazon EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice.\n\nWhich of the following features of Application Load Balancers can be used for this use-case?",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Query string parameter-based routing",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Path-based Routing",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Host-based Routing",
        "correct": false
      },
      {
        "id": 4,
        "answer": "HTTP header-based routing",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct option:\n\nPath-based Routing\n\nElastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions.\n\nIf your application is composed of several individual services, an Application Load Balancer can route a request to a service based on the content of the request. Here are the different types -\n\nHost-based Routing:\n\nYou can route a client request based on the Host field of the HTTP header allowing you to route to multiple domains from the same load balancer.\n\nPath-based Routing:\n\nYou can route a client request based on the URL path of the HTTP header.\n\nHTTP header-based routing:\n\nYou can route a client request based on the value of any standard or custom HTTP header.\n\nHTTP method-based routing:\n\nYou can route a client request based on any standard or custom HTTP method.\n\nQuery string parameter-based routing:\n\nYou can route a client request based on the query string or query parameters.\n\nSource IP address CIDR-based routing:\n\nYou can route a client request based on source IP address CIDR from where the request originates.\n\nPath-based Routing Overview:\n\nYou can use path conditions to define rules that route requests based on the URL in the request (also known as path-based routing).\n\nThe path pattern is applied only to the path of the URL, not to its query parameters. \n via - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#path-conditions\n\nIncorrect options:\n\nQuery string parameter-based routing\n\nHTTP header-based routing\n\nHost-based Routing\n\nAs mentioned earlier in the explanation, none of these three types of routing support requests based on the URL path of the HTTP header. Hence these three are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html",
    "correctAnswerExplanations": [
      {
        "answer": "Path-based Routing",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions."
      },
      {
        "answer": "",
        "explanation": "If your application is composed of several individual services, an Application Load Balancer can route a request to a service based on the content of the request. Here are the different types -"
      },
      {
        "answer": "",
        "explanation": "Host-based Routing:"
      },
      {
        "answer": "",
        "explanation": "You can route a client request based on the Host field of the HTTP header allowing you to route to multiple domains from the same load balancer."
      },
      {
        "answer": "",
        "explanation": "Path-based Routing:"
      },
      {
        "answer": "",
        "explanation": "You can route a client request based on the URL path of the HTTP header."
      },
      {
        "answer": "",
        "explanation": "HTTP header-based routing:"
      },
      {
        "answer": "",
        "explanation": "You can route a client request based on the value of any standard or custom HTTP header."
      },
      {
        "answer": "",
        "explanation": "HTTP method-based routing:"
      },
      {
        "answer": "",
        "explanation": "You can route a client request based on any standard or custom HTTP method."
      },
      {
        "answer": "",
        "explanation": "Query string parameter-based routing:"
      },
      {
        "answer": "",
        "explanation": "You can route a client request based on the query string or query parameters."
      },
      {
        "answer": "",
        "explanation": "Source IP address CIDR-based routing:"
      },
      {
        "answer": "",
        "explanation": "You can route a client request based on source IP address CIDR from where the request originates."
      },
      {
        "answer": "",
        "explanation": "Path-based Routing Overview:"
      },
      {
        "answer": "",
        "explanation": "You can use path conditions to define rules that route requests based on the URL in the request (also known as path-based routing)."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q48-i1.jpg",
        "answer": "",
        "explanation": "The path pattern is applied only to the path of the URL, not to its query parameters."
      },
      {
        "link": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#path-conditions"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "Query string parameter-based routing",
        "explanation": ""
      },
      {
        "answer": "HTTP header-based routing",
        "explanation": ""
      },
      {
        "answer": "Host-based Routing",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As mentioned earlier in the explanation, none of these three types of routing support requests based on the URL path of the HTTP header. Hence these three are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#path-conditions",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html"
    ]
  },
  {
    "id": 64,
    "question": "A company uses Amazon DynamoDB as a data store for various kinds of customer data, such as user profiles, user events, clicks, and visited links. Some of these use-cases require a high request rate (millions of requests per second), low predictable latency, and reliability. The company now wants to add a caching layer to support high read volumes.\n\nAs a solutions architect, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Amazon OpenSearch Service",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon Redshift",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Amazon Relational Database Service (Amazon RDS)",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon ElastiCache",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Amazon DynamoDB Accelerator (DAX)",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "Correct options:\n\nAmazon DynamoDB Accelerator (DAX)\n\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. Therefore, this is a correct option.\n\nDAX Overview: \n via - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\n\nAmazon ElastiCache\n\nAmazon ElastiCache for Memcached is an ideal front-end for data stores like Amazon RDS or Amazon DynamoDB, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. Therefore, this is also a correct option.\n\nIncorrect options:\n\nAmazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Amazon RDS cannot be used as a caching layer for Amazon DynamoDB.\n\nAmazon OpenSearch Service - Amazon OpenSearch Service is a managed service that makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. It cannot be used as a caching layer for Amazon DynamoDB.\n\nAmazon Redshift - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used as a caching layer for Amazon DynamoDB.\n\nReferences:\n\nhttps://aws.amazon.com/dynamodb/dax/\n\nhttps://aws.amazon.com/elasticache/faqs/",
    "correctAnswerExplanations": [
      {
        "answer": "Amazon DynamoDB Accelerator (DAX)",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. Therefore, this is a correct option."
      },
      {
        "image": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png",
        "answer": "",
        "explanation": "DAX Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html"
      },
      {
        "answer": "Amazon ElastiCache",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache for Memcached is an ideal front-end for data stores like Amazon RDS or Amazon DynamoDB, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. Therefore, this is also a correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Relational Database Service (Amazon RDS)</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Amazon RDS cannot be used as a caching layer for Amazon DynamoDB."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon OpenSearch Service</strong> - Amazon OpenSearch Service is a managed service that makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. It cannot be used as a caching layer for Amazon DynamoDB."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used as a caching layer for Amazon DynamoDB."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html",
      "https://aws.amazon.com/dynamodb/dax/",
      "https://aws.amazon.com/elasticache/faqs/"
    ]
  },
  {
    "id": 65,
    "question": "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects.\n\nAs a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Enable versioning on the Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Establish a process to get managerial approval for deleting Amazon S3 objects",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "Correct options:\n\nEnable versioning on the Amazon S3 bucket\n\nVersioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. Versioning-enabled buckets enable you to recover objects from accidental deletion or overwrite.\n\nFor example:\n\nIf you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version. If you delete an object, instead of removing it permanently, Amazon S3 inserts a delete marker, which becomes the current object version. You can always restore the previous version. Hence, this is the correct option.\n\nVersioning Overview: \n via - https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\n\nEnable multi-factor authentication (MFA) delete on the Amazon S3 bucket\n\nTo provide additional protection, multi-factor authentication (MFA) delete can be enabled. MFA delete requires secondary authentication to take place before objects can be permanently deleted from an Amazon S3 bucket. Hence, this is the correct option.\n\nIncorrect options:\n\nCreate an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager - Sending an event trigger after object deletion does not meet the objective of preventing object deletion by mistake because the object has already been deleted. So, this option is incorrect.\n\nEstablish a process to get managerial approval for deleting Amazon S3 objects - This option for getting managerial approval is just a distractor.\n\nChange the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object - There is no provision to set up Amazon S3 configuration to ask for additional confirmation before deleting an object. This option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html",
    "correctAnswerExplanations": [
      {
        "answer": "Enable versioning on the Amazon S3 bucket",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket.\nVersioning-enabled buckets enable you to recover objects from accidental deletion or overwrite."
      },
      {
        "answer": "",
        "explanation": "For example:"
      },
      {
        "answer": "",
        "explanation": "If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version.\nIf you delete an object, instead of removing it permanently, Amazon S3 inserts a delete marker, which becomes the current object version. You can always restore the previous version. Hence, this is the correct option."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q20-i1.jpg",
        "answer": "",
        "explanation": "Versioning Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html"
      },
      {
        "answer": "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "To provide additional protection, multi-factor authentication (MFA) delete can be enabled. MFA delete requires secondary authentication to take place before objects can be permanently deleted from an Amazon S3 bucket. Hence, this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager</strong> - Sending an event trigger after object deletion does not meet the objective of preventing object deletion by mistake because the object has already been deleted. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Establish a process to get managerial approval for deleting Amazon S3 objects</strong> - This option for getting managerial approval is just a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object</strong> - There is no provision to set up Amazon S3 configuration to ask for additional confirmation before deleting an object. This option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html"
    ]
  }
]