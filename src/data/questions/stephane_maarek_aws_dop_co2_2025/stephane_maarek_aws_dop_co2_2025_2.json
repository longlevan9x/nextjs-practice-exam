[
  {
    "id": 1,
    "question": "<p>An Aurora cluster is configured with a single DB instance for a web application. The application uses the instance endpoint to read/write data to the database. The operations team has scheduled an update on the cluster during the upcoming maintenance window. The application support team has requested help to ensure uninterrupted access to the application during the maintenance window.</p>\n\n<p>Which step should a DevOps Engineer take so that the users experience the least possible interruption during the maintenance window?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster custom endpoints by creating two groups of DB instances, one for read and the other for write requests. Update the Aurora cluster's reader endpoint to point to the read DB group of instances</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Turn on the Multi-AZ option on the Aurora cluster. Update the application configuration to use the Aurora cluster Multi-AZ instance endpoint for read/write operations</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Turn on the Multi-AZ option on the Aurora cluster write operations. Update the application configuration to use the Aurora cluster endpoint for write operations. Read operations will automatically be served since its a  Multi-AZ cluster configuration</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 5: Incident and Event Response",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations</strong></p>\n\n<p>Aurora Replicas also referred to as reader instances have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability.</p>\n\n<p>By adding a reader instance to the Aurora cluster, the read-only traffic requests can be served from the reader instance, greatly reducing the traffic to the primary DB and avoiding interruption.</p>\n\n<p>An instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its unique instance endpoint. A cluster endpoint (or writer endpoint) for an Aurora DB cluster connects to the current primary DB instance for that DB cluster. This endpoint is the only one that can perform write operations such as DDL statements.</p>\n\n<p>The cluster endpoint provides failover support for read/write connections to the DB cluster. If the current primary DB instance of a DB cluster fails, Aurora automatically fails over to a new primary DB instance. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption of service. This is the reason we need to change the application configuration to point to cluster endpoint and not to instance endpoint, in the current scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Turn on the Multi-AZ option on the Aurora cluster. Update the application configuration to use the Aurora cluster Multi-AZ instance endpoint for read/write operations</strong></p>\n\n<p><strong>Turn on the Multi-AZ option on the Aurora cluster write operations. Update the application configuration to use the Aurora cluster endpoint for write operations. Read operations will automatically be served since its a Multi-AZ cluster configuration</strong></p>\n\n<p>Once you have created an Aurora cluster, you cannot change its configuration to Multi-AZ, so both of these options are incorrect.</p>\n\n<p><strong>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster custom endpoints by creating two groups of DB instances, one for read and the other for write requests. Update the Aurora cluster's reader endpoint to point to the read DB group of instances</strong> - The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read/write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. Leveraging a custom endpoint for this use case is overkill, so this option is not the best fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations</strong>"
      },
      {
        "answer": "",
        "explanation": "Aurora Replicas also referred to as reader instances have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability."
      },
      {
        "answer": "",
        "explanation": "By adding a reader instance to the Aurora cluster, the read-only traffic requests can be served from the reader instance, greatly reducing the traffic to the primary DB and avoiding interruption."
      },
      {
        "answer": "",
        "explanation": "An instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its unique instance endpoint. A cluster endpoint (or writer endpoint) for an Aurora DB cluster connects to the current primary DB instance for that DB cluster. This endpoint is the only one that can perform write operations such as DDL statements."
      },
      {
        "answer": "",
        "explanation": "The cluster endpoint provides failover support for read/write connections to the DB cluster. If the current primary DB instance of a DB cluster fails, Aurora automatically fails over to a new primary DB instance. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption of service. This is the reason we need to change the application configuration to point to cluster endpoint and not to instance endpoint, in the current scenario."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Turn on the Multi-AZ option on the Aurora cluster. Update the application configuration to use the Aurora cluster Multi-AZ instance endpoint for read/write operations</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Turn on the Multi-AZ option on the Aurora cluster write operations. Update the application configuration to use the Aurora cluster endpoint for write operations. Read operations will automatically be served since its a Multi-AZ cluster configuration</strong>"
      },
      {
        "answer": "",
        "explanation": "Once you have created an Aurora cluster, you cannot change its configuration to Multi-AZ, so both of these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster custom endpoints by creating two groups of DB instances, one for read and the other for write requests. Update the Aurora cluster's reader endpoint to point to the read DB group of instances</strong> - The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read/write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. Leveraging a custom endpoint for this use case is overkill, so this option is not the best fit."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html"
    ]
  },
  {
    "id": 2,
    "question": "<p>A company uses an AWS CodePipeline pipeline to deploy updates to the API several times a month. As part of this process, the DevOps team exports the JavaScript SDK for the API from the API Gateway console and uploads it to an Amazon S3 bucket, which is being used as an origin for an Amazon CloudFront distribution. Web clients access the SDK through the CloudFront distribution's endpoint. The goal is to have an automated solution that ensures the latest SDK is always available to clients whenever there's a new API deployment.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you suggest?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure this rule to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up an Amazon EventBridge rule on a schedule that is invoked every 5 minutes. Configure this rule to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: SDLC Automation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</strong></p>\n\n<p>AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. By creating a CodePipeline action with an AWS Lambda function immediately after the API deployment stage, the DevOps team can automate the process of downloading the SDK from API Gateway and uploading it to the S3 bucket. Additionally, the Lambda function can create a CloudFront invalidation for the SDK path, ensuring that web clients get the latest SDK without any caching issues.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure this rule to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</strong></p>\n\n<p><strong>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</strong></p>\n\n<p>You cannot use any S3 API to invalidate the CloudFront cache, so both of these options are incorrect.</p>\n\n<p><strong>Set up an Amazon EventBridge rule on a schedule that is invoked every 5 minutes. Configure this rule to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</strong> - It is wasteful to invoke an EventBridge rule every 5 minutes to invalidate the CloudFront cache. You should only invalidate the cache when the API has actually been updated.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/\">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. By creating a CodePipeline action with an AWS Lambda function immediately after the API deployment stage, the DevOps team can automate the process of downloading the SDK from API Gateway and uploading it to the S3 bucket. Additionally, the Lambda function can create a CloudFront invalidation for the SDK path, ensuring that web clients get the latest SDK without any caching issues."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure this rule to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</strong>"
      },
      {
        "answer": "",
        "explanation": "You cannot use any S3 API to invalidate the CloudFront cache, so both of these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon EventBridge rule on a schedule that is invoked every 5 minutes. Configure this rule to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</strong> - It is wasteful to invoke an EventBridge rule every 5 minutes to invalidate the CloudFront cache. You should only invalidate the cache when the API has actually been updated."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html",
      "https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/"
    ]
  },
  {
    "id": 3,
    "question": "<p>A company having hundreds of AWS accounts manages its operations and security through a single organization created in AWS Organizations. As per the company's policy, AWS Config and AWS CloudTrail are enabled for all accounts. The security policy mandates configuring AWS Web Application Firewall (AWS WAF) web ACLs for all internet-facing Application Load Balancers (ALBs) and Amazon API Gateway APIs. However, monthly audit reports consistently report unsecured ALBs and API Gateway APIs.</p>\n\n<p>As a DevOps engineer, the security team has requested you to automate these configurations for all accounts to avoid oversight. What steps will you recommend?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Designate one of the AWS accounts in your organization as the administrator for Firewall Manager in AWS Organizations. Create an AWS Firewall Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure a managed rule in AWS Config to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon Systems Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 6: Security and Compliance",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS Firewall Manager offers the freedom to use multiple AWS accounts and to host applications in any desired region while maintaining centralized control over their organization’s security settings and profile. Developers can develop and innovators can innovate, while the security team gains the ability to respond quickly, uniformly, and globally to potential threats and actual attacks.</p>\n\n<p>Firewall Manager is built around named policies that contain WAF rule sets and optional AWS Shield advanced protection. Each policy applies to a specific set of AWS resources, specified by account, resource type, resource identifier, or tag. Policies can be applied automatically to all matching resources, or to a subset that you select. Policies can include WAF rules drawn from within the organization, and also those created by AWS Partners such as Imperva, F5, Trend Micro, and other AWS Marketplace vendors. This gives your security team the power to duplicate their existing on-premises security posture in the cloud.</p>\n\n<p>Firewall Manager has three prerequisites:</p>\n\n<ol>\n<li><p>AWS Organizations – Your organization must be using AWS Organizations to manage your accounts and all features must be enabled.</p></li>\n<li><p>Firewall Administrator – You must designate one of the AWS accounts in your organization as the administrator for Firewall Manager. This gives the account permission to deploy AWS WAF rules across the organization.</p></li>\n<li><p>AWS Config – You must enable AWS Config for all of the accounts in the Organization so that Firewall Manager can detect newly created resources.</p></li>\n</ol>\n\n<p>Using AWS Firewall Manager to centrally manage your Web Application Portfolio:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q27-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q27-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/aws/aws-firewall-manager-central-management-for-your-web-application-portfolio/\">https://aws.amazon.com/blogs/aws/aws-firewall-manager-central-management-for-your-web-application-portfolio/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - Amazon GuardDuty can automatically update the AWS Web Application Firewall Web Access Control Lists (WebACLs) and VPC Network Access Control Lists (NACLs) in response to GuardDuty findings. But, Amazon GuardDuty is a continuous security monitoring and threat detection service and not a security management service like AWS Firewall Manager. Even though GuardDuty can update web ACLs, it's a reaction to a threat. It cannot be used to proactively define rules for web ACLs across accounts to centrally manage the security infrastructure of an organization.</p>\n\n<p><strong>Configure a managed rule in AWS Config to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - Enabling AWS Config is a prerequisite for using AWS Firewall Manager. AWS Config can track the status of resources, but AWS Firewall Manager is needed for centrally managing the security infrastructure.</p>\n\n<p>AWS Firewall Manager and AWS Config:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q27-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q27-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/firewall-manager/\">https://aws.amazon.com/firewall-manager/</a><p></p>\n\n<p><strong>Create an Amazon Systems Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - This option has been added as a distractor and is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html\">https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html\">https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS Firewall Manager offers the freedom to use multiple AWS accounts and to host applications in any desired region while maintaining centralized control over their organization’s security settings and profile. Developers can develop and innovators can innovate, while the security team gains the ability to respond quickly, uniformly, and globally to potential threats and actual attacks."
      },
      {
        "answer": "",
        "explanation": "Firewall Manager is built around named policies that contain WAF rule sets and optional AWS Shield advanced protection. Each policy applies to a specific set of AWS resources, specified by account, resource type, resource identifier, or tag. Policies can be applied automatically to all matching resources, or to a subset that you select. Policies can include WAF rules drawn from within the organization, and also those created by AWS Partners such as Imperva, F5, Trend Micro, and other AWS Marketplace vendors. This gives your security team the power to duplicate their existing on-premises security posture in the cloud."
      },
      {
        "answer": "",
        "explanation": "Firewall Manager has three prerequisites:"
      },
      {},
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q27-i1.jpg",
        "answer": "",
        "explanation": "Using AWS Firewall Manager to centrally manage your Web Application Portfolio:"
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/aws-firewall-manager-central-management-for-your-web-application-portfolio/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - Amazon GuardDuty can automatically update the AWS Web Application Firewall Web Access Control Lists (WebACLs) and VPC Network Access Control Lists (NACLs) in response to GuardDuty findings. But, Amazon GuardDuty is a continuous security monitoring and threat detection service and not a security management service like AWS Firewall Manager. Even though GuardDuty can update web ACLs, it's a reaction to a threat. It cannot be used to proactively define rules for web ACLs across accounts to centrally manage the security infrastructure of an organization."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a managed rule in AWS Config to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - Enabling AWS Config is a prerequisite for using AWS Firewall Manager. AWS Config can track the status of resources, but AWS Firewall Manager is needed for centrally managing the security infrastructure."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q27-i2.jpg",
        "answer": "",
        "explanation": "AWS Firewall Manager and AWS Config:"
      },
      {
        "link": "https://aws.amazon.com/firewall-manager/"
      },
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon Systems Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - This option has been added as a distractor and is irrelevant to the given use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/aws-firewall-manager-central-management-for-your-web-application-portfolio/",
      "https://aws.amazon.com/firewall-manager/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html"
    ]
  },
  {
    "id": 4,
    "question": "<p>An AWS managed <code>cloudformation-stack-drift-detection-check</code> rule is defined in AWS Config for drift detection in AWS CloudFormation resources. The DevOps team is facing two issues:</p>\n\n<p>a) How to detect drifts of Cloudformation custom resources\nb) Drift status of the stack shows as IN_SYNC in the CloudFormation console, the following is the drift detection error  - 'While AWS CloudFormation failed to detect drift, defaulting to NON_COMPLIANT. Re-evaluate the rule and try again. If the problem persists contact AWS CloudFormation support'</p>\n\n<p>As a DevOps Engineer, which steps will you combine to fix the aforementioned issues? (Select two)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS CloudFormation only determines drift for property values that are explicitly set. Explicitly set the property values for your custom resource to be included in drift</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>This error is a false positive and can be ignored for this scenario</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Config rule depends on the availability of <code>DetectStackDrift</code> action of CloudFormation API. AWS Config defaults the rule to NON_COMPLIANT when throttling occurs</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>You receive the error when the AWS Identity and Access Management (IAM) role for the required <code>cloudformationRoleArn</code> parameter doesn't have sufficient service permissions</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>AWS CloudFormation does not support drift detection of custom resources</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 6: Security and Compliance",
    "explanation": "<p>Correct options:</p>\n\n<p>The <code>cloudformation-stack-drift-detection-check</code> rule checks if the actual configuration of a Cloud Formation stack differs, or has drifted, from the expected configuration. A stack is considered to have drifted if one or more of its resources differ from their expected configuration. The rule and the stack are COMPLIANT when the stack drift status is IN_SYNC. The rule is NON_COMPLIANT if the stack drift status is DRIFTED.</p>\n\n<p>CloudFormation offers a drift detection feature to detect unmanaged configuration changes to stacks and resources. This will let you take corrective action to put the stack resources back in sync with their definitions in the stack template. To return a resource to compliance, the resource definition changes can be reverted directly.</p>\n\n<p><strong>AWS Config rule depends on the availability of <code>DetectStackDrift</code> action of CloudFormation API. AWS Config defaults the rule to NON_COMPLIANT when throttling occurs</strong></p>\n\n<p>AWS Config rule depends on the availability of <code>DetectStackDrift</code>. You receive a throttling or \"Rate Exceeded\" error because AWS Config defaults the rule to NON_COMPLIANT when throttling occurs.</p>\n\n<p>Resolve the error resulting from the availability of DetectStackDrift:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q22-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q22-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://repost.aws/knowledge-center/config-cloudformation-drift-detection\">https://repost.aws/knowledge-center/config-cloudformation-drift-detection</a><p></p>\n\n<p><strong>AWS CloudFormation does not support drift detection of custom resources</strong></p>\n\n<p>AWS CloudFormation supports resource import and drift detection operations for only supported resource types. Custom resource types are not currently supported.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You receive the error when the AWS Identity and Access Management (IAM) role for the required <code>cloudformationRoleArn</code> parameter doesn't have sufficient service permissions</strong> - Any issues with permissions will not result in the shown error. Permissions error looks something like this: \"Your stack drift detection operation for the specific stack has failed. Check your existing AWS CloudFormation role permissions and add the missing permissions.\"</p>\n\n<p><strong>This error is a false positive and can be ignored for this scenario</strong> - This option just acts as a distractor.</p>\n\n<p><strong>AWS CloudFormation only determines drift for property values that are explicitly set. Explicitly set the property values for your custom resource to be included in drift</strong> - It is true that CloudFormation only determines drift for property values that are explicitly set, either through the stack template or by specifying template parameters. However, custom resources are not currently supported for drift.</p>\n\n<p>References:</p>\n\n<p>[[https://repost.aws/questions/QUolH-EWnHRNGDbnUiu3chXw/how-to-detect-drifts-of-cloudformation-custom-resource](https://repost.aws/questions/QUolH-EWnHRNGDbnUiu3chXw/how-to-detect-drifts-of-cloudformation-custom-resource)]</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/config-cloudformation-drift-detection\">https://repost.aws/knowledge-center/config-cloudformation-drift-detection</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "The <code>cloudformation-stack-drift-detection-check</code> rule checks if the actual configuration of a Cloud Formation stack differs, or has drifted, from the expected configuration. A stack is considered to have drifted if one or more of its resources differ from their expected configuration. The rule and the stack are COMPLIANT when the stack drift status is IN_SYNC. The rule is NON_COMPLIANT if the stack drift status is DRIFTED."
      },
      {
        "answer": "",
        "explanation": "CloudFormation offers a drift detection feature to detect unmanaged configuration changes to stacks and resources. This will let you take corrective action to put the stack resources back in sync with their definitions in the stack template. To return a resource to compliance, the resource definition changes can be reverted directly."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Config rule depends on the availability of <code>DetectStackDrift</code> action of CloudFormation API. AWS Config defaults the rule to NON_COMPLIANT when throttling occurs</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Config rule depends on the availability of <code>DetectStackDrift</code>. You receive a throttling or \"Rate Exceeded\" error because AWS Config defaults the rule to NON_COMPLIANT when throttling occurs."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q22-i1.jpg",
        "answer": "",
        "explanation": "Resolve the error resulting from the availability of DetectStackDrift:"
      },
      {
        "link": "https://repost.aws/knowledge-center/config-cloudformation-drift-detection"
      },
      {
        "answer": "",
        "explanation": "<strong>AWS CloudFormation does not support drift detection of custom resources</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS CloudFormation supports resource import and drift detection operations for only supported resource types. Custom resource types are not currently supported."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You receive the error when the AWS Identity and Access Management (IAM) role for the required <code>cloudformationRoleArn</code> parameter doesn't have sufficient service permissions</strong> - Any issues with permissions will not result in the shown error. Permissions error looks something like this: \"Your stack drift detection operation for the specific stack has failed. Check your existing AWS CloudFormation role permissions and add the missing permissions.\""
      },
      {
        "answer": "",
        "explanation": "<strong>This error is a false positive and can be ignored for this scenario</strong> - This option just acts as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS CloudFormation only determines drift for property values that are explicitly set. Explicitly set the property values for your custom resource to be included in drift</strong> - It is true that CloudFormation only determines drift for property values that are explicitly set, either through the stack template or by specifying template parameters. However, custom resources are not currently supported for drift."
      }
    ],
    "references": [
      "https://repost.aws/knowledge-center/config-cloudformation-drift-detection",
      "https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html"
    ]
  },
  {
    "id": 5,
    "question": "<p>A video-sharing application stores its files on an Amazon S3 bucket. During the last year, the user traffic has multiplied by thousands and the company is planning on introducing subscription services for its video sharing application. The company needs the access pattern of the video files to identify the most viewed and downloaded videos.</p>\n\n<p>Which of the following would you identify as the MOST cost-effective solution that can be implemented at the earliest?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage Amazon S3 request metrics from CloudWatch for analysis via an AWS Lambda function. The <code>AWS/S3</code> namespace includes the request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> that can be passed to the Lambda function for analyzing the access pattern of the S3 bucket. The output of the Lambda function can be stored in an S3 bucket and visually analyzed using Amazon QuickSight</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable server access logging on the S3 bucket. Use Amazon Redshift Spectrum to efficiently query large datasets leveraging the massive parallelism offered by dedicated Amazon Redshift servers</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable server access logging on the S3 bucket. Configure Amazon Athena to create an external table with the log files. Use SQL query to analyze the access patterns from Athena</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Enable server access logging on the S3 bucket. Configure file create event notification on Access log S3 bucket to trigger an AWS Lambda function. Configure Lambda to write the data to Kinesis Firehose which then writes the log data to Amazon OpenSearch Service. Use Elasticsearch search and analytics engine for analyzing usage patterns of the S3 access logs</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Monitoring and Logging",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable server access logging on the S3 bucket. Configure Amazon Athena to create an external table with the log files. Use SQL query to analyze the access patterns from Athena</strong></p>\n\n<p>Server access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.</p>\n\n<p>Amazon S3 periodically collects access log records, consolidates the records in log files, and then uploads log files to your target bucket as log objects. If you enable logging on multiple source buckets that identify the same target bucket, the target bucket will have access logs for all those source buckets. However, each log object reports access log records for a specific source bucket.</p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly from Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage and you can start analyzing your data immediately. Athena uses an approach known as schema-on-read, which allows you to project your schema onto your data at the time you execute a query. This eliminates the need for any data loading or ETL.</p>\n\n<p>Amazon Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q6-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q6-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\n<a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a><p></p>\n\n<p><code>CREATE EXTERNAL TABLE [IF NOT EXISTS]</code> syntax is used to create the table in Athena. EXTERNAL keyword specifies that the table is based on an underlying data file that exists in Amazon S3, in the location that you specify.</p>\n\n<p>You can use queries on Amazon S3 server access logs to identify Amazon S3 object access requests, for operations such as GET, PUT, and DELETE, and discover further information about those requests.</p>\n\n<p>Analysing object access requests using SQL queries in Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q6-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q6-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon S3 request metrics from CloudWatch for analysis via an AWS Lambda function. The <code>AWS/S3</code> namespace includes the request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> that can be passed to the Lambda function for analyzing the access pattern of the S3 bucket. The output of the Lambda function can be stored in an S3 bucket and visually analyzed using Amazon QuickSight</strong> - The request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> will give the overall count of all Get requests and Bytes downloaded through these requests. It does not help in answering the access pattern questions, such as, which files have been viewed/downloaded the most.</p>\n\n<p><strong>Enable server access logging on the S3 bucket. Use Amazon Redshift Spectrum to efficiently query large datasets leveraging the massive parallelism offered by dedicated Amazon Redshift servers</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to run very fast against large datasets. Amazon Redshift Spectrum has a dependency on provisioned Amazon Redshift servers, which represents a cost-intensive solution. Therefore, Amazon Redshift Spectrum is a cost overkill for the simple access pattern analysis that the current use case needs.</p>\n\n<p><strong>Enable server access logging on the S3 bucket. Configure file create event notification on Access log S3 bucket to trigger an AWS Lambda function. Configure Lambda to write the data to Kinesis Firehose which then writes the log data to Amazon OpenSearch Service. Use Elasticsearch search and analytics engine for analyzing usage patterns of the S3 access logs</strong> -  Amazon OpenSearch is a managed service that makes it easier to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. The service provides support for open-source Elasticsearch APIs, managed Kibana, and integration with other AWS services such as Amazon S3 and Amazon Kinesis for loading streaming data into Amazon ES. OpenSearch is well-suited for real-time analysis of logs or clickstreams. This is not a cost-effective solution as it uses AWS Lambda, Kinesis Data Firehose, and Amazon OpenSearch services for the simple access pattern analysis that the current use case needs.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/metrics-dimensions.html#s3-cloudwatch-metrics\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/metrics-dimensions.html#s3-cloudwatch-metrics</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/analyzing-amazon-s3-server-access-logs-using-amazon-opensearch-service/\">https://aws.amazon.com/blogs/big-data/analyzing-amazon-s3-server-access-logs-using-amazon-opensearch-service/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable server access logging on the S3 bucket. Configure Amazon Athena to create an external table with the log files. Use SQL query to analyze the access patterns from Athena</strong>"
      },
      {
        "answer": "",
        "explanation": "Server access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 periodically collects access log records, consolidates the records in log files, and then uploads log files to your target bucket as log objects. If you enable logging on multiple source buckets that identify the same target bucket, the target bucket will have access logs for all those source buckets. However, each log object reports access log records for a specific source bucket."
      },
      {
        "answer": "",
        "explanation": "Amazon Athena is an interactive query service that makes it easy to analyze data directly from Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage and you can start analyzing your data immediately. Athena uses an approach known as schema-on-read, which allows you to project your schema onto your data at the time you execute a query. This eliminates the need for any data loading or ETL."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q6-i1.jpg",
        "answer": "",
        "explanation": "Amazon Athena:"
      },
      {
        "link": "https://aws.amazon.com/athena/"
      },
      {
        "answer": "",
        "explanation": "<code>CREATE EXTERNAL TABLE [IF NOT EXISTS]</code> syntax is used to create the table in Athena. EXTERNAL keyword specifies that the table is based on an underlying data file that exists in Amazon S3, in the location that you specify."
      },
      {
        "answer": "",
        "explanation": "You can use queries on Amazon S3 server access logs to identify Amazon S3 object access requests, for operations such as GET, PUT, and DELETE, and discover further information about those requests."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q6-i2.jpg",
        "answer": "",
        "explanation": "Analysing object access requests using SQL queries in Athena:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon S3 request metrics from CloudWatch for analysis via an AWS Lambda function. The <code>AWS/S3</code> namespace includes the request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> that can be passed to the Lambda function for analyzing the access pattern of the S3 bucket. The output of the Lambda function can be stored in an S3 bucket and visually analyzed using Amazon QuickSight</strong> - The request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> will give the overall count of all Get requests and Bytes downloaded through these requests. It does not help in answering the access pattern questions, such as, which files have been viewed/downloaded the most."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable server access logging on the S3 bucket. Use Amazon Redshift Spectrum to efficiently query large datasets leveraging the massive parallelism offered by dedicated Amazon Redshift servers</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to run very fast against large datasets. Amazon Redshift Spectrum has a dependency on provisioned Amazon Redshift servers, which represents a cost-intensive solution. Therefore, Amazon Redshift Spectrum is a cost overkill for the simple access pattern analysis that the current use case needs."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable server access logging on the S3 bucket. Configure file create event notification on Access log S3 bucket to trigger an AWS Lambda function. Configure Lambda to write the data to Kinesis Firehose which then writes the log data to Amazon OpenSearch Service. Use Elasticsearch search and analytics engine for analyzing usage patterns of the S3 access logs</strong> -  Amazon OpenSearch is a managed service that makes it easier to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. The service provides support for open-source Elasticsearch APIs, managed Kibana, and integration with other AWS services such as Amazon S3 and Amazon Kinesis for loading streaming data into Amazon ES. OpenSearch is well-suited for real-time analysis of logs or clickstreams. This is not a cost-effective solution as it uses AWS Lambda, Kinesis Data Firehose, and Amazon OpenSearch services for the simple access pattern analysis that the current use case needs."
      }
    ],
    "references": [
      "https://aws.amazon.com/athena/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/metrics-dimensions.html#s3-cloudwatch-metrics",
      "https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html",
      "https://aws.amazon.com/blogs/big-data/analyzing-amazon-s3-server-access-logs-using-amazon-opensearch-service/"
    ]
  },
  {
    "id": 6,
    "question": "<p>A company wants to enforce regulations to prevent frequent logins by DevOps engineers to the Amazon EC2 instances, with the added condition that immediate notification must be sent to the security team if any login occurs.</p>\n\n<p>What solution would you suggest to meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure AWS CloudTrail to track AWS API calls and log them to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Set up an AWS Lambda function as a consumer for the Kinesis stream to process the logs and detect any user logins. Use Amazon SNS to send notifications to the security team upon detecting a login event</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up the Amazon Inspector Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch subscription filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Monitoring and Logging",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong></p>\n\n<p>The Amazon CloudWatch agent can be installed on EC2 instances to collect and send log data to CloudWatch Logs. By setting up a metric filter within CloudWatch Logs, it is possible to search for specific patterns, such as user login events. If a user login is found in the log data, Amazon SNS is used to send an immediate notification to the security team.</p>\n\n<p>You can use CloudWatch Logs to monitor applications and systems using log data in near real-time. For example, CloudWatch Logs can track the number of errors that occur in your application logs and send you a notification whenever the rate of errors exceeds a threshold you specify. The CloudWatch Logs Agent will send log data every five seconds by default and is configurable by the user. You can monitor log events as they are sent to CloudWatch Logs by creating Metric Filters. Metric Filters turn log data into Amazon CloudWatch Metrics for graphing or alarming.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q33-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q33-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/cloudwatch/faqs/\">https://aws.amazon.com/cloudwatch/faqs/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch subscription filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong> - You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. A subscription filter defines the filter pattern to use for filtering which logs events get delivered to your AWS resource, as well as information about where to send matching log events to. You need to use metric Filters (not subscription filters) to turn log data into Amazon CloudWatch Metrics for graphing or alarming. Therefore, this option is incorrect.</p>\n\n<p><strong>Configure AWS CloudTrail to track AWS API calls and log them to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Set up an AWS Lambda function as a consumer for the Kinesis stream to process the logs and detect any user logins. Use Amazon SNS to send notifications to the security team upon detecting a login event</strong> - AWS CloudTrail enables the logging of AWS API calls, including login events, which are then sent to CloudWatch Logs. By subscribing CloudWatch Logs to Amazon Kinesis, the logs can be processed in real-time. An AWS Lambda function attached to Kinesis can parse the logs and identify user logins. If a login is detected, Amazon SNS is used to immediately notify the security team. While AWS CloudTrail captures login events, it typically delivers logs within an average of about 5 minutes of an API call. However, this time is not guaranteed. Therefore, this option is incorrect.</p>\n\n<p><strong>Set up the Amazon Inspector Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong> - Amazon Inspector is a vulnerability management service that continuously scans your AWS workloads for software vulnerabilities and unintended network exposure. Amazon Inspector automatically discovers and scans running Amazon EC2 instances, container images in Amazon Elastic Container Registry (Amazon ECR), and AWS Lambda functions for known software vulnerabilities and unintended network exposure. You cannot use Amazon Inspector to detect user login events in real time, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/faqs/\">https://aws.amazon.com/cloudwatch/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/features/#CloudWatch_Logs\">https://aws.amazon.com/cloudwatch/features/#CloudWatch_Logs</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong>"
      },
      {
        "answer": "",
        "explanation": "The Amazon CloudWatch agent can be installed on EC2 instances to collect and send log data to CloudWatch Logs. By setting up a metric filter within CloudWatch Logs, it is possible to search for specific patterns, such as user login events. If a user login is found in the log data, Amazon SNS is used to send an immediate notification to the security team."
      },
      {
        "answer": "",
        "explanation": "You can use CloudWatch Logs to monitor applications and systems using log data in near real-time. For example, CloudWatch Logs can track the number of errors that occur in your application logs and send you a notification whenever the rate of errors exceeds a threshold you specify. The CloudWatch Logs Agent will send log data every five seconds by default and is configurable by the user. You can monitor log events as they are sent to CloudWatch Logs by creating Metric Filters. Metric Filters turn log data into Amazon CloudWatch Metrics for graphing or alarming."
      },
      {
        "link": "https://aws.amazon.com/cloudwatch/faqs/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch subscription filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong> - You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. A subscription filter defines the filter pattern to use for filtering which logs events get delivered to your AWS resource, as well as information about where to send matching log events to. You need to use metric Filters (not subscription filters) to turn log data into Amazon CloudWatch Metrics for graphing or alarming. Therefore, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure AWS CloudTrail to track AWS API calls and log them to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Set up an AWS Lambda function as a consumer for the Kinesis stream to process the logs and detect any user logins. Use Amazon SNS to send notifications to the security team upon detecting a login event</strong> - AWS CloudTrail enables the logging of AWS API calls, including login events, which are then sent to CloudWatch Logs. By subscribing CloudWatch Logs to Amazon Kinesis, the logs can be processed in real-time. An AWS Lambda function attached to Kinesis can parse the logs and identify user logins. If a login is detected, Amazon SNS is used to immediately notify the security team. While AWS CloudTrail captures login events, it typically delivers logs within an average of about 5 minutes of an API call. However, this time is not guaranteed. Therefore, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up the Amazon Inspector Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong> - Amazon Inspector is a vulnerability management service that continuously scans your AWS workloads for software vulnerabilities and unintended network exposure. Amazon Inspector automatically discovers and scans running Amazon EC2 instances, container images in Amazon Elastic Container Registry (Amazon ECR), and AWS Lambda functions for known software vulnerabilities and unintended network exposure. You cannot use Amazon Inspector to detect user login events in real time, so this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/cloudwatch/faqs/",
      "https://aws.amazon.com/cloudwatch/features/#CloudWatch_Logs",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html"
    ]
  },
  {
    "id": 7,
    "question": "<p>In a multinational company, various AWS accounts are efficiently managed using AWS Control Tower. The company operates both internal and public applications across its infrastructure. To streamline operations, each application team is assigned a dedicated AWS account responsible for hosting their respective applications. These accounts are consolidated under an organization in AWS Organizations. Additionally, a specific AWS Control Tower member account acts as a centralized DevOps hub, offering Continuous Integration/Continuous Deployment (CI/CD) pipelines that application teams utilize to deploy applications to their designated AWS accounts. A specialized IAM role for deployment is available within this central DevOps account.</p>\n\n<p>Currently, a particular application team is facing challenges while attempting to deploy its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster situated in their application-specific AWS account. They have an existing IAM role for deployment within the application AWS account. The deployment process relies on an AWS CodeBuild project, configured within the centralized DevOps account, and utilizes an IAM service role for CodeBuild. However, the deployment process is encountering an Unauthorized error when trying to establish connections to the cross-account EKS cluster from the CodeBuild environment.</p>\n\n<p>To resolve this error and facilitate a successful deployment, what solution would you recommend?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account's deployment IAM role the required access to CodeBuild and the EKS cluster</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Establish a trust relationship in the centralized DevOps account for the application account's deployment IAM role, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Establish a trust relationship in the centralized DevOps account's deployment IAM role for the application account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account's deployment IAM role the required access to CodeBuild</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Configuration Management and IaC",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</strong></p>\n\n<p>By configuring a trust relationship in the application account's deployment IAM role for the centralized DevOps account, using the sts:AssumeRole action, CodeBuild in the centralized DevOps account will be allowed to assume the IAM role in the application account. This will enable CodeBuild to access the EKS cluster in the application account. Additionally, granting the application account's deployment IAM role the necessary access to the EKS cluster ensures that it has the required permissions to perform the deployment tasks. Finally, configuring the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions ensures that the IAM role has the required permissions within the EKS cluster.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q32-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q32-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account's deployment IAM role the required access to CodeBuild and the EKS cluster</strong></p>\n\n<p><strong>Establish a trust relationship in the centralized DevOps account's deployment IAM role for the application account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account's deployment IAM role the required access to CodeBuild</strong></p>\n\n<p>AssumeRoleWithSAML is used for federation scenarios involving SAML-based identity providers, which is not relevant to the given use case, so both these options are incorrect.</p>\n\n<p><strong>Establish a trust relationship in the centralized DevOps account for the application account's deployment IAM role, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</strong></p>\n\n<p>You need to configure a trust relationship in the application account's deployment IAM role for the centralized DevOps account, so that CodeBuild in the centralized DevOps account can assume the IAM role in the application account, by using the sts:AssumeRole action. However, this option sets up the trust relationship in the reverse direction, so it's incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html\">https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</strong>"
      },
      {
        "answer": "",
        "explanation": "By configuring a trust relationship in the application account's deployment IAM role for the centralized DevOps account, using the sts:AssumeRole action, CodeBuild in the centralized DevOps account will be allowed to assume the IAM role in the application account. This will enable CodeBuild to access the EKS cluster in the application account. Additionally, granting the application account's deployment IAM role the necessary access to the EKS cluster ensures that it has the required permissions to perform the deployment tasks. Finally, configuring the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions ensures that the IAM role has the required permissions within the EKS cluster."
      },
      {
        "link": "https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account's deployment IAM role the required access to CodeBuild and the EKS cluster</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Establish a trust relationship in the centralized DevOps account's deployment IAM role for the application account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account's deployment IAM role the required access to CodeBuild</strong>"
      },
      {
        "answer": "",
        "explanation": "AssumeRoleWithSAML is used for federation scenarios involving SAML-based identity providers, which is not relevant to the given use case, so both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Establish a trust relationship in the centralized DevOps account for the application account's deployment IAM role, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</strong>"
      },
      {
        "answer": "",
        "explanation": "You need to configure a trust relationship in the application account's deployment IAM role for the centralized DevOps account, so that CodeBuild in the centralized DevOps account can assume the IAM role in the application account, by using the sts:AssumeRole action. However, this option sets up the trust relationship in the reverse direction, so it's incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html",
      "https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html",
      "https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html"
    ]
  },
  {
    "id": 8,
    "question": "<p>A social media company has its web application hosted on Amazon EC2 instances that are deployed in a single AWS Region. The company has now expanded its operations into new geographies and the company wants to offer low-latency access for the application to its customers. To comply with different financial regulations of each geography, the application needs to operate in silos and the underlying instances in one region should not interact with instances running in other regions.</p>\n\n<p>Which of the following represents the most optimal solution to automate the application deployment to different AWS regions?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a CloudFormation template describing the application infrastructure in the Resources section. Create a CloudFormation stack from the template by using the AWS CLI, specify multiple regions using the --regions parameter to deploy the application</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation change set from an administrator account to launch stack instances that deploy the application to various other regions</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation stack set from an administrator account to launch stack instances that deploy the application to various other regions</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create a shell script that uses the AWS CLI to query the current state in one region and output an AWS CloudFormation template. Create a CloudFormation stack from the template by using the AWS CLI, specifying the --region parameter to deploy the application to other regions</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Configuration Management and IaC",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation stack set from an administrator account to launch stack instances that deploy the application to various other regions</strong></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</a><p></p>\n\n<p>Here are the relevant CloudFormation concepts:</p>\n\n<p>Template: A CloudFormation template is a JSON or YAML formatted text file. You can save these files with any extension, such as .json, .yaml, .template, or .txt. CloudFormation uses these templates as blueprints for building your AWS resources.</p>\n\n<p>Stack: When you use CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack's CloudFormation template.</p>\n\n<p>Change set: If you need to make changes to the running resources in a stack, you update the stack. Before making changes to your resources, you can generate a change set, which is a summary of your proposed changes. Change sets allow you to see how your changes might impact your running resources, especially critical resources, before implementing them.</p>\n\n<p>Stack set: A stack set lets you create stacks in AWS accounts across regions by using a single CloudFormation template. All the resources included in each stack are defined by the stack set's CloudFormation template. As you create the stack set, you specify the template to use, in addition to any parameters and capabilities that the template requires.</p>\n\n<p>After you've defined a stack set, you can create, update, or delete stacks in the target accounts and AWS Regions you specify. When you create, update, or delete stacks, you can also specify operation preferences, such as the order of Regions in which you want the operation to be performed, the failure tolerance beyond which stack operations stop, and the number of accounts in which operations are performed on stacks concurrently. A stack set is a regional resource. If you create a stack set in one AWS Region, you can't see it or change it in other Regions.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a><p></p>\n\n<p>For the given use case, you need to create a stack set that includes the CloudFormation template that you want to use to create stacks and then identify the AWS Regions in which you want to deploy stacks in your target accounts. A stack set ensures consistent deployment of the same stack resources, with the same settings, to all specified target accounts within the Regions you choose.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Create a CloudFormation stack from the template by using the AWS CLI, specify multiple regions using the --regions parameter to deploy the application</strong> - You can create a stack using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. The <code>aws cloudformation create-stack</code> command supports only a <code>region</code> parameter for a single region and not <code>regions</code> parameter for multiple regions.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/multiple-account-multiple-region-aws-cloudformation/\">https://aws.amazon.com/blogs/infrastructure-and-automation/multiple-account-multiple-region-aws-cloudformation/</a><p></p>\n\n<p><strong>Create a shell script that uses the AWS CLI to query the current state in one region and output an AWS CloudFormation template. Create a CloudFormation stack from the template by using the AWS CLI, specifying the --region parameter to deploy the application to other regions</strong> - Creating a script is not an elegant solution to provision the same infrastructure in multiple AWS regions since this functionality is directly offered by AWS in the form of stack sets.</p>\n\n<p><strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation change set from an administrator account to launch stack instances that deploy the application to various other regions</strong> - When you need to update a stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. You cannot use change sets to deploy an application to multiple AWS regions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation stack set from an administrator account to launch stack instances that deploy the application to various other regions</strong>"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html"
      },
      {
        "answer": "",
        "explanation": "Here are the relevant CloudFormation concepts:"
      },
      {
        "answer": "",
        "explanation": "Template: A CloudFormation template is a JSON or YAML formatted text file. You can save these files with any extension, such as .json, .yaml, .template, or .txt. CloudFormation uses these templates as blueprints for building your AWS resources."
      },
      {
        "answer": "",
        "explanation": "Stack: When you use CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack's CloudFormation template."
      },
      {
        "answer": "",
        "explanation": "Change set: If you need to make changes to the running resources in a stack, you update the stack. Before making changes to your resources, you can generate a change set, which is a summary of your proposed changes. Change sets allow you to see how your changes might impact your running resources, especially critical resources, before implementing them."
      },
      {
        "answer": "",
        "explanation": "Stack set: A stack set lets you create stacks in AWS accounts across regions by using a single CloudFormation template. All the resources included in each stack are defined by the stack set's CloudFormation template. As you create the stack set, you specify the template to use, in addition to any parameters and capabilities that the template requires."
      },
      {
        "answer": "",
        "explanation": "After you've defined a stack set, you can create, update, or delete stacks in the target accounts and AWS Regions you specify. When you create, update, or delete stacks, you can also specify operation preferences, such as the order of Regions in which you want the operation to be performed, the failure tolerance beyond which stack operations stop, and the number of accounts in which operations are performed on stacks concurrently. A stack set is a regional resource. If you create a stack set in one AWS Region, you can't see it or change it in other Regions."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you need to create a stack set that includes the CloudFormation template that you want to use to create stacks and then identify the AWS Regions in which you want to deploy stacks in your target accounts. A stack set ensures consistent deployment of the same stack resources, with the same settings, to all specified target accounts within the Regions you choose."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Create a CloudFormation stack from the template by using the AWS CLI, specify multiple regions using the --regions parameter to deploy the application</strong> - You can create a stack using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. The <code>aws cloudformation create-stack</code> command supports only a <code>region</code> parameter for a single region and not <code>regions</code> parameter for multiple regions."
      },
      {
        "link": "https://aws.amazon.com/blogs/infrastructure-and-automation/multiple-account-multiple-region-aws-cloudformation/"
      },
      {
        "answer": "",
        "explanation": "<strong>Create a shell script that uses the AWS CLI to query the current state in one region and output an AWS CloudFormation template. Create a CloudFormation stack from the template by using the AWS CLI, specifying the --region parameter to deploy the application to other regions</strong> - Creating a script is not an elegant solution to provision the same infrastructure in multiple AWS regions since this functionality is directly offered by AWS in the form of stack sets."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation change set from an administrator account to launch stack instances that deploy the application to various other regions</strong> - When you need to update a stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. You cannot use change sets to deploy an application to multiple AWS regions."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html",
      "https://aws.amazon.com/blogs/infrastructure-and-automation/multiple-account-multiple-region-aws-cloudformation/",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html"
    ]
  },
  {
    "id": 9,
    "question": "<p>A DevOps engineer is currently involved in a data archival project where the task is to migrate on-premises data to an Amazon S3 bucket. The engineer has created a script that handles the incremental archiving of on-premises data, specifically transferring data older than 6 months to Amazon S3. As part of the process, the data is removed from the on-premises location after being successfully transferred using the S3 PutObject operation.</p>\n\n<p>During a thorough code review, the DevOps engineer identified a crucial issue in the script. The script does not include any validation to confirm whether the data is copied to Amazon S3 without any corruption. To ensure data integrity throughout the transmission, the DevOps engineer needs to update the script accordingly. The new solution must use MD5 checksums to verify the data integrity before allowing the deletion of the on-premises data.</p>\n\n<p>Considering these requirements, what modifications or solutions should the DevOps engineer implement in the script to ensure successful data transfer and integrity validation? (Select two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Examine the returned response for the version ID. Compare the version ID value of the object with a calculated or previously stored Content-MD5 digest</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Examine the returned response for the ETag. Compare the ETag value of the object with a calculated or previously stored Content-MD5 digest</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command. Examine the Amazon S3's call return status to check for an error</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Provide the MD5 digest as a custom name-value pair in the metadata of the object. Examine the Amazon S3's call return status to check for an error</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Provide the MD5 digest as a trailing checksum of the object. Examine the Amazon S3's call return status to check for an error</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Resilient Cloud Solutions",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command. Examine the Amazon S3's call return status to check for an error</strong></p>\n\n<p>Amazon S3 uses checksum values to verify the integrity of data that you upload to or download from Amazon S3. In addition, you can request that another checksum value be calculated for any object that you store in Amazon S3. You can select from one of several checksum algorithms to use when uploading or copying your data. Amazon S3 uses this algorithm to compute an additional checksum value and store it as part of the object metadata.</p>\n\n<p>One way to verify the integrity of your object after uploading is to provide an MD5 digest of the object when you upload it. If you calculate the MD5 digest for your object, you can provide the digest with the PUT command by using the Content-MD5 header. After uploading the object, Amazon S3 calculates the MD5 digest of the object and compares it to the value that you provided. The request succeeds only if the two digests match.</p>\n\n<p><strong>Examine the returned response for the ETag. Compare the ETag value of the object with a calculated or previously stored Content-MD5 digest</strong></p>\n\n<p>The entity tag (ETag) for an object represents a specific version of that object. Keep in mind that the ETag reflects changes only to the content of an object, not to its metadata. If only the metadata of an object changes, the ETag remains the same. For objects where the ETag is the Content-MD5 digest of the object, you can compare the ETag value of the object with a calculated or previously stored Content-MD5 digest.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q37-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q37-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provide the MD5 digest as a trailing checksum of the object. Examine the Amazon S3's call return status to check for an error</strong> - When uploading objects to Amazon S3, you can either provide a precalculated checksum for the object or use an AWS SDK to automatically create trailing checksums on your behalf. You cannot provide a trailing checksum as it is calculated by AWS, so this option is incorrect.</p>\n\n<p><strong>Provide the MD5 digest as a custom name-value pair in the metadata of the object. Examine the Amazon S3's call return status to check for an error</strong> - You need to provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command, and NOT as a custom name-value pair in the metadata of the object.</p>\n\n<p><strong>Examine the returned response for the version ID. Compare the version ID value of the object with a calculated or previously stored Content-MD5 digest</strong> - Versioning in Amazon S3 is a means of keeping multiple variants of an object in the same bucket. When you enable versioning in a bucket, all new objects are versioned and given a unique version ID. You cannot use version ID to compare with the Content-MD5 digest.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html\">https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command. Examine the Amazon S3's call return status to check for an error</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 uses checksum values to verify the integrity of data that you upload to or download from Amazon S3. In addition, you can request that another checksum value be calculated for any object that you store in Amazon S3. You can select from one of several checksum algorithms to use when uploading or copying your data. Amazon S3 uses this algorithm to compute an additional checksum value and store it as part of the object metadata."
      },
      {
        "answer": "",
        "explanation": "One way to verify the integrity of your object after uploading is to provide an MD5 digest of the object when you upload it. If you calculate the MD5 digest for your object, you can provide the digest with the PUT command by using the Content-MD5 header. After uploading the object, Amazon S3 calculates the MD5 digest of the object and compares it to the value that you provided. The request succeeds only if the two digests match."
      },
      {
        "answer": "",
        "explanation": "<strong>Examine the returned response for the ETag. Compare the ETag value of the object with a calculated or previously stored Content-MD5 digest</strong>"
      },
      {
        "answer": "",
        "explanation": "The entity tag (ETag) for an object represents a specific version of that object. Keep in mind that the ETag reflects changes only to the content of an object, not to its metadata. If only the metadata of an object changes, the ETag remains the same. For objects where the ETag is the Content-MD5 digest of the object, you can compare the ETag value of the object with a calculated or previously stored Content-MD5 digest."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Provide the MD5 digest as a trailing checksum of the object. Examine the Amazon S3's call return status to check for an error</strong> - When uploading objects to Amazon S3, you can either provide a precalculated checksum for the object or use an AWS SDK to automatically create trailing checksums on your behalf. You cannot provide a trailing checksum as it is calculated by AWS, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Provide the MD5 digest as a custom name-value pair in the metadata of the object. Examine the Amazon S3's call return status to check for an error</strong> - You need to provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command, and NOT as a custom name-value pair in the metadata of the object."
      },
      {
        "answer": "",
        "explanation": "<strong>Examine the returned response for the version ID. Compare the version ID value of the object with a calculated or previously stored Content-MD5 digest</strong> - Versioning in Amazon S3 is a means of keeping multiple variants of an object in the same bucket. When you enable versioning in a bucket, all new objects are versioned and given a unique version ID. You cannot use version ID to compare with the Content-MD5 digest."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html"
    ]
  },
  {
    "id": 10,
    "question": "<p>For deployments across AWS accounts, an e-commerce company has decided to use AWS CodePipeline to deploy an AWS CloudFormation stack in an AWS account (account A) to a different AWS account (account B).</p>\n\n<p>What combination of steps will you take to configure this requirement? (Select three)</p>",
    "corrects": [
      1,
      2,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>In account B, create a cross-account IAM role. In account A, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account B</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. Also, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account B access to the bucket</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>In account B, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account A</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. In account B, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account A access to the bucket</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>In account A, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account B, update the CodePipeline configuration to include the resources associated with account A</p>",
        "correct": false
      },
      {
        "id": 6,
        "answer": "<p>In account B, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account A, update the CodePipeline configuration to include the resources associated with account B</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Configuration Management and IaC",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. Also, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account B access to the bucket</strong></p>\n\n<p><strong>In account B, create a cross-account IAM role. In account A, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account B</strong></p>\n\n<p><strong>In account B, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account A, update the CodePipeline configuration to include the resources associated with account B</strong></p>\n\n<p>Complete list of steps for configuring the requirement:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q11-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q11-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/\">https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. In account B, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account A access to the bucket</strong></p>\n\n<p><strong>In account A, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account B, update the CodePipeline configuration to include the resources associated with account A</strong></p>\n\n<p><strong>In account B, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account A</strong></p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/\">https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. Also, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account B access to the bucket</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>In account B, create a cross-account IAM role. In account A, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account B</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>In account B, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account A, update the CodePipeline configuration to include the resources associated with account B</strong>"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q11-i1.jpg",
        "answer": "",
        "explanation": "Complete list of steps for configuring the requirement:"
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. In account B, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account A access to the bucket</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>In account A, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account B, update the CodePipeline configuration to include the resources associated with account A</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>In account B, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account A</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/"
    ]
  },
  {
    "id": 11,
    "question": "<p>A production support team manages a web application running on a fleet of Amazon EC2 instances configured with an Application Load balancer (ALB). The instances run in an EC2 Auto Scaling group across multiple Availability Zones. A critical bug fix has to be deployed to the production application. The team needs a deployment strategy that can:</p>\n\n<p>a) Create another fleet of instances with the same capacity and configuration as the original one.\nb) Continue access to the original application without a downtime\nc) Transition the traffic to the new fleet when the deployment is fully done. The production test team has requested a two-hour window to complete thorough testing on the new fleet of instances.\nd) Terminate the original fleet automatically once the test window expires.</p>\n\n<p>As a DevOps engineer, which deployment solution will you choose to cater to all the given requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to 'Terminate the original instances in the deployment group' and choose a waiting period of two hours. Choose <code>OneAtATime</code> Deployment configuration setting to deregister the original fleet of instances one at a time to provide increased test time for the production team</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure AWS Elastic Beanstalk to use rolling deployment policy. Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Production traffic is served unaffected. Use rolling restarts to restart the proxy and application servers running on your environment's instances without downtime</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to <code>Terminate the original instances in the deployment group</code> and choose a waiting period of two hours</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure AWS Elastic Beanstalk to perform a Blue/Green deployment. This will create a new environment different from the original environment to continue serving the production traffic. Terminate the original environment after two hours and confirm the DNS changes of the new environment have propagated correctly</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: SDLC Automation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to <code>Terminate the original instances in the deployment group</code> and choose a waiting period of two hours</strong></p>\n\n<p>Traditional deployments with in-place upgrades make it difficult to validate your new application version in a production deployment while also continuing to run the earlier version of the application. Blue/Green deployments provide a level of isolation between your blue and green application environments. This helps ensure spinning up a parallel green environment does not affect the resources underpinning your blue environment. This isolation reduces your deployment risk.</p>\n\n<p>After you deploy the green environment, you have the opportunity to validate it. You might do that with test traffic before sending production traffic to the green environment, or by using a very small fraction of production traffic, to better reflect real user traffic. This is called canary analysis or canary testing. If you discover the green environment is not operating as expected, there is no impact on the blue environment. You can route traffic back to it, minimizing impaired operation or downtime and limiting the blast radius of impact.</p>\n\n<p>This ability to simply roll traffic back to the operational environment is a key benefit of Blue/Green deployments. You can roll back to the blue environment at any time during the deployment process. Impaired operation or downtime is minimized because the impact is limited to the window of time between green environment issue detection and the shift of traffic back to the blue environment. Additionally, the impact is limited to the portion of traffic going to the green environment, not all traffic. If the blast radius of deployment errors is reduced, so is the overall deployment risk.</p>\n\n<p>In AWS CodeDeploy Blue/Green deployment type, for deployment groups that contain more than one instance, the overall deployment succeeds if the application revision is deployed to all of the instances. The exception to this rule is that if deployment to the last instance fails, the overall deployment still succeeds. This is because CodeDeploy allows only one instance at a time to be taken offline with the CodeDeployDefault.OneAtATime configuration (If you don't specify a deployment configuration, CodeDeploy uses the CodeDeployDefault.OneAtATime deployment configuration).</p>\n\n<p>If you choose <code>Terminate the original instances in the deployment group</code>: After traffic is rerouted to the replacement environment, the instances that were deregistered from the load balancer are terminated following the wait period you specify.</p>\n\n<p>Blue/Green deployment example:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q5-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q5-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html\">https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Elastic Beanstalk to perform a Blue/Green deployment. This will create a new environment different from the original environment to continue serving the production traffic. Terminate the original environment after two hours and confirming the DNS changes of the new environment have propagated correctly</strong></p>\n\n<p><strong>Configure AWS Elastic Beanstalk to use rolling deployment policy. Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Production traffic is served unaffected. Use rolling restarts to restart the proxy and application servers running on your environment's instances without downtime</strong></p>\n\n<p>These two options use AWS Elastic Beanstalk, which creates a new environment and not a new fleet of EC2 instances, as needed in the use case. Hence, these are incorrect.</p>\n\n<p><strong>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to <code>Terminate the original instances in the deployment group</code> and choose a waiting period of two hours. Choose <code>OneAtATime</code> Deployment configuration setting to deregister the original fleet of instances one at a time to provide increased test time for the production team</strong> - Deregistering the original fleet of instances one at a time is not possible. After traffic is successfully routed to the replacement environment, instances in the original environment are deregistered all at once no matter which deployment configuration is selected.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html\">https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to <code>Terminate the original instances in the deployment group</code> and choose a waiting period of two hours</strong>"
      },
      {
        "answer": "",
        "explanation": "Traditional deployments with in-place upgrades make it difficult to validate your new application version in a production deployment while also continuing to run the earlier version of the application. Blue/Green deployments provide a level of isolation between your blue and green application environments. This helps ensure spinning up a parallel green environment does not affect the resources underpinning your blue environment. This isolation reduces your deployment risk."
      },
      {
        "answer": "",
        "explanation": "After you deploy the green environment, you have the opportunity to validate it. You might do that with test traffic before sending production traffic to the green environment, or by using a very small fraction of production traffic, to better reflect real user traffic. This is called canary analysis or canary testing. If you discover the green environment is not operating as expected, there is no impact on the blue environment. You can route traffic back to it, minimizing impaired operation or downtime and limiting the blast radius of impact."
      },
      {
        "answer": "",
        "explanation": "This ability to simply roll traffic back to the operational environment is a key benefit of Blue/Green deployments. You can roll back to the blue environment at any time during the deployment process. Impaired operation or downtime is minimized because the impact is limited to the window of time between green environment issue detection and the shift of traffic back to the blue environment. Additionally, the impact is limited to the portion of traffic going to the green environment, not all traffic. If the blast radius of deployment errors is reduced, so is the overall deployment risk."
      },
      {
        "answer": "",
        "explanation": "In AWS CodeDeploy Blue/Green deployment type, for deployment groups that contain more than one instance, the overall deployment succeeds if the application revision is deployed to all of the instances. The exception to this rule is that if deployment to the last instance fails, the overall deployment still succeeds. This is because CodeDeploy allows only one instance at a time to be taken offline with the CodeDeployDefault.OneAtATime configuration (If you don't specify a deployment configuration, CodeDeploy uses the CodeDeployDefault.OneAtATime deployment configuration)."
      },
      {
        "answer": "",
        "explanation": "If you choose <code>Terminate the original instances in the deployment group</code>: After traffic is rerouted to the replacement environment, the instances that were deregistered from the load balancer are terminated following the wait period you specify."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q5-i1.jpg",
        "answer": "",
        "explanation": "Blue/Green deployment example:"
      },
      {
        "link": "https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Elastic Beanstalk to perform a Blue/Green deployment. This will create a new environment different from the original environment to continue serving the production traffic. Terminate the original environment after two hours and confirming the DNS changes of the new environment have propagated correctly</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Elastic Beanstalk to use rolling deployment policy. Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Production traffic is served unaffected. Use rolling restarts to restart the proxy and application servers running on your environment's instances without downtime</strong>"
      },
      {
        "answer": "",
        "explanation": "These two options use AWS Elastic Beanstalk, which creates a new environment and not a new fleet of EC2 instances, as needed in the use case. Hence, these are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to <code>Terminate the original instances in the deployment group</code> and choose a waiting period of two hours. Choose <code>OneAtATime</code> Deployment configuration setting to deregister the original fleet of instances one at a time to provide increased test time for the production team</strong> - Deregistering the original fleet of instances one at a time is not possible. After traffic is successfully routed to the replacement environment, instances in the original environment are deregistered all at once no matter which deployment configuration is selected."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html",
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html",
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html"
    ]
  },
  {
    "id": 12,
    "question": "<p>A DevOps Engineer is working on multiple applications that need to be configured for Application Auto scaling, however, there are some application constraints to be addressed:</p>\n\n<p>a) A serverless application is built on AWS Lambda with the following traffic pattern - The traffic for the application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday.\nb) Another flagship application runs on Spot Fleet. The CPU utilization of the fleet has to stay at around 50 percent when the load on the application changes.</p>\n\n<p>Which of the following solutions can address these requirements? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon CloudWatch metric to track the utilization of the AWS Lambda function. Setup step scaling policy by configuring the upper bound, lower bound, and breach threshold values for the Lambda function</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with AWS Lambda function as a scalable target. Specify the minimum and maximum capacity based on the requirements. AWS CLI, SDKs, or CloudFormation can be used for configuring the schedule scaling</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon CloudWatch metric to track average CPU utilization of 50 percent for the Spot Fleet. Create a target tracking auto-scaling policy CloudWatch alarm against the CloudWatch metric created from the AWS console</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with Spot instances as a scalable target. Specify the  minimum and maximum capacity based on the requirements</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Create a target tracking auto-scaling policy that targets an average CPU utilization of 50 percent for an application that runs on Spot Fleet. You can create and manage target tracking using the AWS CLI, SDKs, or CloudFormation</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Resilient Cloud Solutions",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Create a target tracking auto-scaling policy that targets an average CPU utilization of 50 percent for an application that runs on Spot Fleet. You can create and manage target tracking using the AWS CLI, SDKs, or CloudFormation</strong></p>\n\n<p>To create a target tracking scaling policy, you specify an Amazon CloudWatch metric and a target value that represents the ideal average utilization or throughput level for your application. Application Auto Scaling can then scale out the scalable target (add capacity) to handle peak traffic, and scale in the scalable target (remove capacity) to reduce costs during periods of low utilization or throughput.</p>\n\n<p>For example, let's say that you currently have an application that runs on Spot Fleet, and you want the CPU utilization of the fleet to stay at around 50 percent when the load on the application changes. This gives you extra capacity to handle traffic spikes without maintaining an excessive number of idle resources.</p>\n\n<p>You can meet this need by creating a target tracking scaling policy that targets an average CPU utilization of 50 percent. Then, Application Auto Scaling scales the number of instances to keep the actual metric value at or near 50 percent.</p>\n\n<p>Target tracking scaling policies for Application Auto Scaling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q30-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q30-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html</a><p></p>\n\n<p><strong>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with AWS Lambda function as a scalable target. Specify the minimum and maximum capacity based on the requirements. AWS CLI, SDKs, or CloudFormation can be used for configuring the schedule scaling</strong></p>\n\n<p>Scaling based on a schedule allows you to set your own scaling schedule according to predictable load changes. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can configure a schedule for Application Auto Scaling to increase capacity on Wednesday and decrease capacity on Friday.</p>\n\n<p>To use scheduled scaling, create scheduled actions, which tell Application Auto Scaling to perform scaling activities at specific times. When you create a scheduled action, you specify the scalable target, when the scaling activity should occur, a minimum capacity, and a maximum capacity. You can create scheduled actions that scale one time only or that scale on a recurring schedule.</p>\n\n<p>Scheduled scaling for Application Auto Scaling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q30-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q30-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html\">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon CloudWatch metric to track average CPU utilization of 50 percent for the Spot Fleet. Create a target tracking auto scaling policy CloudWatch alarm against the CloudWatch metric created from the AWS console</strong> - With target tracking scaling policies you do not create, edit, or delete the CloudWatch alarms that are used with a target tracking scaling policy. Application Auto Scaling creates and manages the CloudWatch alarms that are associated with your target tracking scaling policies and deletes them when no longer needed.</p>\n\n<p><strong>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with Spot instances as a scalable target. Specify the minimum and maximum capacity based on the requirements</strong> - Scheduled scaling is not the right option when a parameter has to be tracked and forms the basis of the scale-in and scale-out activity of the ASG. Target tracking is better suited for these use cases.</p>\n\n<p><strong>Create an Amazon CloudWatch metric to track the utilization of the AWS Lambda function. Setup step scaling policy by configuring the upper bound, lower bound, and breach threshold values for the Lambda function</strong> - With step scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process as well as define how your scalable target should be scaled when a threshold is in breach for a specified number of evaluation periods. Step scaling policies are not supported for DynamoDB, Amazon Comprehend, Lambda, Amazon Keyspaces, Amazon MSK, ElastiCache, or Neptune.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html\">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a target tracking auto-scaling policy that targets an average CPU utilization of 50 percent for an application that runs on Spot Fleet. You can create and manage target tracking using the AWS CLI, SDKs, or CloudFormation</strong>"
      },
      {
        "answer": "",
        "explanation": "To create a target tracking scaling policy, you specify an Amazon CloudWatch metric and a target value that represents the ideal average utilization or throughput level for your application. Application Auto Scaling can then scale out the scalable target (add capacity) to handle peak traffic, and scale in the scalable target (remove capacity) to reduce costs during periods of low utilization or throughput."
      },
      {
        "answer": "",
        "explanation": "For example, let's say that you currently have an application that runs on Spot Fleet, and you want the CPU utilization of the fleet to stay at around 50 percent when the load on the application changes. This gives you extra capacity to handle traffic spikes without maintaining an excessive number of idle resources."
      },
      {
        "answer": "",
        "explanation": "You can meet this need by creating a target tracking scaling policy that targets an average CPU utilization of 50 percent. Then, Application Auto Scaling scales the number of instances to keep the actual metric value at or near 50 percent."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q30-i1.jpg",
        "answer": "",
        "explanation": "Target tracking scaling policies for Application Auto Scaling:"
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with AWS Lambda function as a scalable target. Specify the minimum and maximum capacity based on the requirements. AWS CLI, SDKs, or CloudFormation can be used for configuring the schedule scaling</strong>"
      },
      {
        "answer": "",
        "explanation": "Scaling based on a schedule allows you to set your own scaling schedule according to predictable load changes. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can configure a schedule for Application Auto Scaling to increase capacity on Wednesday and decrease capacity on Friday."
      },
      {
        "answer": "",
        "explanation": "To use scheduled scaling, create scheduled actions, which tell Application Auto Scaling to perform scaling activities at specific times. When you create a scheduled action, you specify the scalable target, when the scaling activity should occur, a minimum capacity, and a maximum capacity. You can create scheduled actions that scale one time only or that scale on a recurring schedule."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q30-i2.jpg",
        "answer": "",
        "explanation": "Scheduled scaling for Application Auto Scaling:"
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon CloudWatch metric to track average CPU utilization of 50 percent for the Spot Fleet. Create a target tracking auto scaling policy CloudWatch alarm against the CloudWatch metric created from the AWS console</strong> - With target tracking scaling policies you do not create, edit, or delete the CloudWatch alarms that are used with a target tracking scaling policy. Application Auto Scaling creates and manages the CloudWatch alarms that are associated with your target tracking scaling policies and deletes them when no longer needed."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with Spot instances as a scalable target. Specify the minimum and maximum capacity based on the requirements</strong> - Scheduled scaling is not the right option when a parameter has to be tracked and forms the basis of the scale-in and scale-out activity of the ASG. Target tracking is better suited for these use cases."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon CloudWatch metric to track the utilization of the AWS Lambda function. Setup step scaling policy by configuring the upper bound, lower bound, and breach threshold values for the Lambda function</strong> - With step scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process as well as define how your scalable target should be scaled when a threshold is in breach for a specified number of evaluation periods. Step scaling policies are not supported for DynamoDB, Amazon Comprehend, Lambda, Amazon Keyspaces, Amazon MSK, ElastiCache, or Neptune."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html",
      "https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html",
      "https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html"
    ]
  },
  {
    "id": 13,
    "question": "<p>Consider a multi-account setup within AWS Organizations where a company is running a data ingestion application on Amazon EC2 instances through several Auto Scaling groups. These instances lack internet access due to sensitive data handling, and VPC endpoints have been deployed accordingly. The application operates on a custom AMI designed specifically for its needs.</p>\n\n<p>To effectively manage and troubleshoot the application, system administrators require automated and centralized login access to the EC2 instances. Additionally, the company's security team needs to be notified whenever such instances are accessed.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you suggest to satisfy these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a NAT gateway and a bastion host with internet access. Configure a security group that permits incoming traffic from the bastion host to all EC2 instances. Set up AWS Systems Manager Agent on the EC2 instances for monitoring and troubleshooting. Leverage Auto Scaling group lifecycle hooks for monitoring access. Utilize Systems Manager Session Manager for centralized login. Direct EC2 instance logs to an Amazon CloudWatch Logs log group. For auditing purposes, export data to Amazon S3 and notify the security team through S3 event notifications</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage EC2 Instance Connect for centralized access and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize AWS Systems Manager Automation to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version.  Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager and grant access to centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage Systems Manager Session Manager for centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 6: Security and Compliance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage Systems Manager Session Manager for centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong></p>\n\n<p>This option suggests using EC2 Image Builder to create an updated custom AMI with AWS Systems Manager Agent included. The Auto Scaling group is configured to attach the AmazonSSMManagedInstanceCore role to the instances, thereby enabling centralized management through Systems Manager, as it grants the EC2 instances the permissions needed for core Systems Manager functionality. Session Manager can be used for secure logins, and session details can be logged to Amazon S3. Additionally, an S3 event notification can be set up to alert the security team about new file uploads using Amazon SNS. This option aligns well with the requirement for centralized access and monitoring.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q35-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q35-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/image-builder/\">https://aws.amazon.com/image-builder/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a NAT gateway and a bastion host with internet access. Configure a security group that permits incoming traffic from the bastion host to all EC2 instances. Set up AWS Systems Manager Agent on the EC2 instances for monitoring and troubleshooting. Leverage Auto Scaling group lifecycle hooks for monitoring access. Utilize Systems Manager Session Manager for centralized login. Direct EC2 instance logs to an Amazon CloudWatch Logs log group. For auditing purposes, export data to Amazon S3 and notify the security team through S3 event notifications</strong> - This option includes unnecessary internet access for sensitive data via the combination of NAT Gateway and bastion host, so it is not the best fit for the given use case.</p>\n\n<p><strong>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage EC2 Instance Connect for centralized access and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong> - EC2 Instance Connect is a way to distribute short-lived SSH keys and control access by IAM policies. The actual connection is created with SSH client and all normal requirements (network connectivity, user on host etc) apply. The host must also have ec2-instance-connect “agent” installed. EC2 Instance Connect still requires public IP and network connectivity, so it is not the best fit for the given use case.</p>\n\n<p><strong>Utilize AWS Systems Manager Automation to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version.  Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager and grant access to centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong> - SCPs are similar to AWS Identity and Access Management (IAM) permission policies and use almost the same syntax. However, an SCP never grants permissions. Instead, SCPs are JSON policies that specify the maximum permissions for the affected accounts. So, for the given use case, you cannot use SCP for granting EC2 instance access to centralized and automated login for the Systems Manager.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/image-builder/\">https://aws.amazon.com/image-builder/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage Systems Manager Session Manager for centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong>"
      },
      {
        "answer": "",
        "explanation": "This option suggests using EC2 Image Builder to create an updated custom AMI with AWS Systems Manager Agent included. The Auto Scaling group is configured to attach the AmazonSSMManagedInstanceCore role to the instances, thereby enabling centralized management through Systems Manager, as it grants the EC2 instances the permissions needed for core Systems Manager functionality. Session Manager can be used for secure logins, and session details can be logged to Amazon S3. Additionally, an S3 event notification can be set up to alert the security team about new file uploads using Amazon SNS. This option aligns well with the requirement for centralized access and monitoring."
      },
      {
        "link": "https://aws.amazon.com/image-builder/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up a NAT gateway and a bastion host with internet access. Configure a security group that permits incoming traffic from the bastion host to all EC2 instances. Set up AWS Systems Manager Agent on the EC2 instances for monitoring and troubleshooting. Leverage Auto Scaling group lifecycle hooks for monitoring access. Utilize Systems Manager Session Manager for centralized login. Direct EC2 instance logs to an Amazon CloudWatch Logs log group. For auditing purposes, export data to Amazon S3 and notify the security team through S3 event notifications</strong> - This option includes unnecessary internet access for sensitive data via the combination of NAT Gateway and bastion host, so it is not the best fit for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage EC2 Instance Connect for centralized access and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong> - EC2 Instance Connect is a way to distribute short-lived SSH keys and control access by IAM policies. The actual connection is created with SSH client and all normal requirements (network connectivity, user on host etc) apply. The host must also have ec2-instance-connect “agent” installed. EC2 Instance Connect still requires public IP and network connectivity, so it is not the best fit for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Utilize AWS Systems Manager Automation to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version.  Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager and grant access to centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong> - SCPs are similar to AWS Identity and Access Management (IAM) permission policies and use almost the same syntax. However, an SCP never grants permissions. Instead, SCPs are JSON policies that specify the maximum permissions for the affected accounts. So, for the given use case, you cannot use SCP for granting EC2 instance access to centralized and automated login for the Systems Manager."
      }
    ],
    "references": [
      "https://aws.amazon.com/image-builder/",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
    ]
  },
  {
    "id": 14,
    "question": "<p>A company hosts all its web applications on Amazon EC2 instances. The company is looking for a security solution that will proactively detect software vulnerabilities and unintended network exposure of the instances. The solution should also include an audit trail of all login activities on the instances.</p>\n\n<p>Which solution will meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs. Configure your trail to send log events to CloudWatch Logs</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure Amazon ECR image scanning to scan for vulnerabilities on the EC2 instances. Amazon ECR sends an event to Amazon EventBridge when an image scan is completed. Configure CloudTrail to send its trail data to Amazon EventBridge for further processing/notification</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure Amazon GuardDuty to detect vulnerabilities and threats on the EC2 instances. Integrate with a workflow system to review the findings and trigger an AWS Lambda function to automate the remediation process</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure AWS Systems Manager's Systems Manager (SSM) agent to collect software vulnerabilities of the Amazon EC2 instances. Configure a Systems Manager Automation runbook to automatically patch the vulnerabilities identified by the SSM agent</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 6: Security and Compliance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs. Configure your trail to send log events to CloudWatch Logs</strong></p>\n\n<p>Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. To successfully scan Amazon EC2 instances for software vulnerabilities, Amazon Inspector requires that these instances have the SSM agent installed. Amazon Inspector uses SSM Agent to collect application inventory, which can be set up as Amazon Virtual Private Cloud (VPC) endpoints to avoid sending information over the internet.</p>\n\n<p>You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources.</p>\n\n<p>CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can then easily view them, search them for specific error codes or patterns, filter them based on specific fields, or archive them securely for future analysis. CloudWatch Logs enables you to see all of your logs, regardless of their source, as a single and consistent flow of events ordered by time.</p>\n\n<p>Configure CloudTrail with CloudWatch Logs to monitor your trail logs and be notified when specific activity occurs.</p>\n\n<p>How Amazon Inspector Works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q13-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q13-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Systems Manager's Systems Manager (SSM) agent to collect software vulnerabilities of the Amazon EC2 instances. Configure a Systems Manager Automation runbook to automatically patch the vulnerabilities identified by the SSM agent</strong> - SSM agent does not scan or detect vulnerabilities on Amazon EC2 instances.</p>\n\n<p><strong>Configure Amazon ECR image scanning to scan for vulnerabilities on the EC2 instances. Amazon ECR sends an event to Amazon EventBridge when an image scan is completed. Configure CloudTrail to send its trail data to Amazon EventBridge for further processing/notification</strong> - Amazon ECR provides basic scanning type which uses the Common Vulnerabilities and Exposures (CVEs) database from the open-source Clair project. It is for container applications alone. Also, ECR image scanning identifies software vulnerabilities only in operating system packages.</p>\n\n<p><strong>Configure Amazon GuardDuty to detect vulnerabilities and threats on the EC2 instances. Integrate with a workflow system to review the findings and trigger an AWS Lambda function to automate the remediation process</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, Amazon Elastic Compute Cloud (EC2) workloads, container applications, Amazon Aurora databases, and data stored in Amazon Simple Storage Service (S3). It is not a vulnerability management service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/inspector/v1/userguide/inspector_introduction.html\">https://docs.aws.amazon.com/inspector/v1/userguide/inspector_introduction.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/inspector/faqs/\">https://aws.amazon.com/inspector/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs. Configure your trail to send log events to CloudWatch Logs</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. To successfully scan Amazon EC2 instances for software vulnerabilities, Amazon Inspector requires that these instances have the SSM agent installed. Amazon Inspector uses SSM Agent to collect application inventory, which can be set up as Amazon Virtual Private Cloud (VPC) endpoints to avoid sending information over the internet."
      },
      {
        "answer": "",
        "explanation": "You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources."
      },
      {
        "answer": "",
        "explanation": "CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can then easily view them, search them for specific error codes or patterns, filter them based on specific fields, or archive them securely for future analysis. CloudWatch Logs enables you to see all of your logs, regardless of their source, as a single and consistent flow of events ordered by time."
      },
      {
        "answer": "",
        "explanation": "Configure CloudTrail with CloudWatch Logs to monitor your trail logs and be notified when specific activity occurs."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q13-i1.jpg",
        "answer": "",
        "explanation": "How Amazon Inspector Works:"
      },
      {
        "link": "https://aws.amazon.com/inspector/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Systems Manager's Systems Manager (SSM) agent to collect software vulnerabilities of the Amazon EC2 instances. Configure a Systems Manager Automation runbook to automatically patch the vulnerabilities identified by the SSM agent</strong> - SSM agent does not scan or detect vulnerabilities on Amazon EC2 instances."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon ECR image scanning to scan for vulnerabilities on the EC2 instances. Amazon ECR sends an event to Amazon EventBridge when an image scan is completed. Configure CloudTrail to send its trail data to Amazon EventBridge for further processing/notification</strong> - Amazon ECR provides basic scanning type which uses the Common Vulnerabilities and Exposures (CVEs) database from the open-source Clair project. It is for container applications alone. Also, ECR image scanning identifies software vulnerabilities only in operating system packages."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon GuardDuty to detect vulnerabilities and threats on the EC2 instances. Integrate with a workflow system to review the findings and trigger an AWS Lambda function to automate the remediation process</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, Amazon Elastic Compute Cloud (EC2) workloads, container applications, Amazon Aurora databases, and data stored in Amazon Simple Storage Service (S3). It is not a vulnerability management service."
      }
    ],
    "references": [
      "https://aws.amazon.com/inspector/",
      "https://docs.aws.amazon.com/inspector/v1/userguide/inspector_introduction.html",
      "https://aws.amazon.com/inspector/faqs/"
    ]
  },
  {
    "id": 15,
    "question": "<p>A company wants to create an automated monitoring solution to generate real-time customized notifications regarding unrestricted security groups in the company's production AWS account. The notification must contain the name and ID of the noncompliant security group. The DevOps team at the company has already activated the restricted-ssh AWS Config managed rule. The team has also set up an Amazon Simple Notification Service (Amazon SNS) topic and subscribed relevant personnel to it.</p>\n\n<p>Which of the following options represents the BEST solution for the given scenario?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for all AWS Config managed rules. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of ERROR for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up an Amazon EventBridge rule that matches all AWS Config evaluation results for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 6: Security and Compliance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</strong></p>\n\n<p>You can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes do not comply with the conditions in your rules.</p>\n\n<p>The restricted-ssh rule checks if the incoming SSH traffic for the security groups is accessible. The rule is COMPLIANT when IP addresses of the incoming SSH traffic in the security groups are restricted (CIDR other than 0.0.0.0/0). This rule applies only to IPv4.</p>\n\n<p>For the given use case, you need to monitor for the NON_COMPLIANT evaluation result of the rule, which implies that the rule has failed the conditions of the compliance check. You can then create an Amazon EventBridge rule (with AWS Config configured as a source) that is put in action when it matches the NON_COMPLIANT evaluation result of the restricted-ssh rule. The EventBridge rule, in turn, publishes a notification to the SNS topic.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q40-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q40-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for all AWS Config managed rules. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</strong></p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches all AWS Config evaluation results for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</strong></p>\n\n<p>You should note that you can only set up a filter policy on an SNS subscription and NOT on the SNS topic itself. In addition, it is wasteful to set up Amazon EventBridge rule on all AWS Config managed rules, rather than only the restricted-ssh rule. Therefore, both these options are incorrect.</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of ERROR for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</strong> - You get an ERROR evaluation result when one of the required/optional parameters is not valid, or not of the correct type, or is formatted incorrectly. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html\">https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes do not comply with the conditions in your rules."
      },
      {
        "answer": "",
        "explanation": "The restricted-ssh rule checks if the incoming SSH traffic for the security groups is accessible. The rule is COMPLIANT when IP addresses of the incoming SSH traffic in the security groups are restricted (CIDR other than 0.0.0.0/0). This rule applies only to IPv4."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you need to monitor for the NON_COMPLIANT evaluation result of the rule, which implies that the rule has failed the conditions of the compliance check. You can then create an Amazon EventBridge rule (with AWS Config configured as a source) that is put in action when it matches the NON_COMPLIANT evaluation result of the restricted-ssh rule. The EventBridge rule, in turn, publishes a notification to the SNS topic."
      },
      {
        "link": "https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for all AWS Config managed rules. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon EventBridge rule that matches all AWS Config evaluation results for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</strong>"
      },
      {
        "answer": "",
        "explanation": "You should note that you can only set up a filter policy on an SNS subscription and NOT on the SNS topic itself. In addition, it is wasteful to set up Amazon EventBridge rule on all AWS Config managed rules, rather than only the restricted-ssh rule. Therefore, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of ERROR for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</strong> - You get an ERROR evaluation result when one of the required/optional parameters is not valid, or not of the correct type, or is formatted incorrectly. This option has been added as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html",
      "https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html",
      "https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html"
    ]
  },
  {
    "id": 16,
    "question": "<p>A DevOps Engineer has been asked to chalk out a disaster recovery (DR) plan for a workload in production. The workload runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances are configured with an Auto Scaling group across multiple Availability Zones. Amazon Route 53 is configured to point to the ALB using an alias record. Amazon RDS for PostgreSQL DB instance is the database service. The draft DR plan mandates an RTO of three hours and an RPO of around 15 minutes.</p>\n\n<p>Which Disaster Recovery (DR) strategy should the DevOps Engineer opt for a cost-effective solution?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Opt for a pilot light DR strategy. Provision a copy of your entire workload infrastructure to a different AWS Region. Copy the first backup that consists of a full instance backup to the new RDS instance. In case of disaster, apply the incremental backup to the RDS instance in the new AWS Region. Configure Amazon Route 53 health checks to automatically initiate DNS failover to new Region</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Opt for a pilot light DR strategy. Provision a copy of your core workload infrastructure to a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS PostgreSQL DB instance. Configure Amazon Route 53 health checks to automatically initiate DNS failover to a new Region. Promote the read replica to the primary DB instance in case of a disaster</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure your workload to simultaneously run in multiple AWS Regions as part of a multi-site active/active DR strategy. Replicate your entire workload to another AWS Region. With this strategy, asynchronous data replication between the regions enables near-zero RPO. Configure Amazon Route 53 with latency-based routing to choose between the active regional endpoint for directing user traffic</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Opt for a Warm standby approach by ensuring that there is a scaled-down, but fully functional, copy of your production environment in another AWS Region. Then, deploy enough resources to handle initial traffic, ensuring low RTO, and then rely on Auto Scaling to ramp up for subsequent traffic</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 5: Incident and Event Response",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Opt for a pilot light DR strategy. Provision a copy of your core workload infrastructure to a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS PostgreSQL DB instance. Configure Amazon Route 53 health checks to automatically initiate DNS failover to a new Region. Promote the read replica to the primary DB instance in case of a disaster</strong></p>\n\n<p>With the pilot light approach, you replicate your data from one Region to another and provision a copy of your core workload infrastructure. Resources required to support data replication and backup, such as databases and object storage, are always on. Other elements, such as application servers, are loaded with application code and configurations, but are \"switched off\" and are only used during testing or when disaster recovery failover is invoked. In the cloud, you have the flexibility to de-provision resources when you do not need them, and provision them when you do. A best practice for “switched off” is to not deploy the resource, and then create the configuration and capabilities to deploy it (“switch on”) when needed. Unlike the backup and restore approach, your core infrastructure is always available and you always have the option to quickly provision a full-scale production environment by switching on and scaling out your application servers.</p>\n\n<p>A pilot light approach minimizes the ongoing cost of disaster recovery by minimizing the active resources and simplifies recovery at the time of a disaster because the core infrastructure requirements are all in place.</p>\n\n<p>For pilot light, continuous data replication to live databases and data stores in the DR region is the best approach for low RPO. When failing over to run your read/write workload from the disaster recovery Region, you must promote an RDS read replica to become the primary instance.</p>\n\n<p>For an active/passive configuration such as the pilot light, all traffic initially goes to the primary Region and switches to the disaster recovery Region if the primary Region is no longer available. This failover operation can be initiated either automatically or manually.</p>\n\n<p>Using Amazon Route 53, you can associate multiple IP endpoints in one or more AWS Regions with a Route 53 domain name. Then, you can route traffic to the appropriate endpoint under that domain name. On failover you need to switch traffic to the recovery endpoint, and away from the primary endpoint. Amazon Route 53 health checks monitor these endpoints. Using these health checks, you can configure automatically initiated DNS failover to ensure traffic is sent only to healthy endpoints, which is a highly reliable operation done on the data plane.</p>\n\n<p>The pilot light strategy is the one best suited for the given requirements since it keeps the overall costs down when compared to the rest of the options. Also, the core infrastructure is ready to be used as and when required. Since RTO is given in hours, we can spin up these resources well before the given time. The database is update-to-date and only needs a switch from replica to primary. This is the right strategy when RPO is in minutes.</p>\n\n<p>Pilot light DR strategy:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q16-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q16-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html\">https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for a pilot light DR strategy. Provision a copy of your entire workload infrastructure to a different AWS Region. Copy the first backup that consists of a full instance backup to the new RDS instance. In case of disaster, apply the incremental backup to the RDS instance in the new AWS Region. Configure Amazon Route 53 health checks to automatically initiate DNS failover to new Region</strong> - With pilot light DR strategy, resources required to support data replication and backup, such as databases and object storage, are always on. When the expected RPO is 15 minutes, it is not possible to apply incremental backups to the database since this could be huge amounts of data which may take hours. Hence, this option is incorrect.</p>\n\n<p><strong>Configure your workload to simultaneously run in multiple AWS Regions as part of a multi-site active/active DR strategy. Replicate your entire workload to another AWS Region. With this strategy, asynchronous data replication between the regions enables near-zero RPO. Configure Amazon Route 53 with latency-based routing to choose between the active regional endpoint for directing user traffic</strong> - You can run your workload simultaneously in multiple Regions as part of a multi-site active/active strategy. Multi-site active/active serves traffic from all regions to which it is deployed. With a multi-site active/active approach, users can access your workload in any of the Regions in which it is deployed. This approach is the most complex and costly approach to disaster recovery, but it can reduce your recovery time to near zero for most disasters with the correct technology choices and implementation. Since RTO is given as three hours, opting for this DR strategy will prove to be cost-ineffective.</p>\n\n<p><strong>Opt for a Warm standby approach by ensuring that there is a scaled-down, but fully functional, copy of your production environment in another AWS Region. Then, deploy enough resources to handle initial traffic, ensuring low RTO, and then rely on Auto Scaling to ramp up for subsequent traffic</strong> - The warm standby approach involves ensuring that there is a scaled-down, but fully functional, copy of your production environment in another Region. This approach extends the pilot light concept and decreases the time to recovery because your workload is always-on in another Region. Since RTO is given as three hours, the warm standby approach will end up being a costly alternative to an otherwise cost-effective pilot light strategy.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/establishing-rpo-and-rto-targets-for-cloud-applications/\">https://aws.amazon.com/blogs/mt/establishing-rpo-and-rto-targets-for-cloud-applications/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Opt for a pilot light DR strategy. Provision a copy of your core workload infrastructure to a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS PostgreSQL DB instance. Configure Amazon Route 53 health checks to automatically initiate DNS failover to a new Region. Promote the read replica to the primary DB instance in case of a disaster</strong>"
      },
      {
        "answer": "",
        "explanation": "With the pilot light approach, you replicate your data from one Region to another and provision a copy of your core workload infrastructure. Resources required to support data replication and backup, such as databases and object storage, are always on. Other elements, such as application servers, are loaded with application code and configurations, but are \"switched off\" and are only used during testing or when disaster recovery failover is invoked. In the cloud, you have the flexibility to de-provision resources when you do not need them, and provision them when you do. A best practice for “switched off” is to not deploy the resource, and then create the configuration and capabilities to deploy it (“switch on”) when needed. Unlike the backup and restore approach, your core infrastructure is always available and you always have the option to quickly provision a full-scale production environment by switching on and scaling out your application servers."
      },
      {
        "answer": "",
        "explanation": "A pilot light approach minimizes the ongoing cost of disaster recovery by minimizing the active resources and simplifies recovery at the time of a disaster because the core infrastructure requirements are all in place."
      },
      {
        "answer": "",
        "explanation": "For pilot light, continuous data replication to live databases and data stores in the DR region is the best approach for low RPO. When failing over to run your read/write workload from the disaster recovery Region, you must promote an RDS read replica to become the primary instance."
      },
      {
        "answer": "",
        "explanation": "For an active/passive configuration such as the pilot light, all traffic initially goes to the primary Region and switches to the disaster recovery Region if the primary Region is no longer available. This failover operation can be initiated either automatically or manually."
      },
      {
        "answer": "",
        "explanation": "Using Amazon Route 53, you can associate multiple IP endpoints in one or more AWS Regions with a Route 53 domain name. Then, you can route traffic to the appropriate endpoint under that domain name. On failover you need to switch traffic to the recovery endpoint, and away from the primary endpoint. Amazon Route 53 health checks monitor these endpoints. Using these health checks, you can configure automatically initiated DNS failover to ensure traffic is sent only to healthy endpoints, which is a highly reliable operation done on the data plane."
      },
      {
        "answer": "",
        "explanation": "The pilot light strategy is the one best suited for the given requirements since it keeps the overall costs down when compared to the rest of the options. Also, the core infrastructure is ready to be used as and when required. Since RTO is given in hours, we can spin up these resources well before the given time. The database is update-to-date and only needs a switch from replica to primary. This is the right strategy when RPO is in minutes."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q16-i1.jpg",
        "answer": "",
        "explanation": "Pilot light DR strategy:"
      },
      {
        "link": "https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Opt for a pilot light DR strategy. Provision a copy of your entire workload infrastructure to a different AWS Region. Copy the first backup that consists of a full instance backup to the new RDS instance. In case of disaster, apply the incremental backup to the RDS instance in the new AWS Region. Configure Amazon Route 53 health checks to automatically initiate DNS failover to new Region</strong> - With pilot light DR strategy, resources required to support data replication and backup, such as databases and object storage, are always on. When the expected RPO is 15 minutes, it is not possible to apply incremental backups to the database since this could be huge amounts of data which may take hours. Hence, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure your workload to simultaneously run in multiple AWS Regions as part of a multi-site active/active DR strategy. Replicate your entire workload to another AWS Region. With this strategy, asynchronous data replication between the regions enables near-zero RPO. Configure Amazon Route 53 with latency-based routing to choose between the active regional endpoint for directing user traffic</strong> - You can run your workload simultaneously in multiple Regions as part of a multi-site active/active strategy. Multi-site active/active serves traffic from all regions to which it is deployed. With a multi-site active/active approach, users can access your workload in any of the Regions in which it is deployed. This approach is the most complex and costly approach to disaster recovery, but it can reduce your recovery time to near zero for most disasters with the correct technology choices and implementation. Since RTO is given as three hours, opting for this DR strategy will prove to be cost-ineffective."
      },
      {
        "answer": "",
        "explanation": "<strong>Opt for a Warm standby approach by ensuring that there is a scaled-down, but fully functional, copy of your production environment in another AWS Region. Then, deploy enough resources to handle initial traffic, ensuring low RTO, and then rely on Auto Scaling to ramp up for subsequent traffic</strong> - The warm standby approach involves ensuring that there is a scaled-down, but fully functional, copy of your production environment in another Region. This approach extends the pilot light concept and decreases the time to recovery because your workload is always-on in another Region. Since RTO is given as three hours, the warm standby approach will end up being a costly alternative to an otherwise cost-effective pilot light strategy."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html",
      "https://aws.amazon.com/blogs/mt/establishing-rpo-and-rto-targets-for-cloud-applications/",
      "https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/"
    ]
  },
  {
    "id": 17,
    "question": "<p>The flagship application at a company is deployed on Amazon EC2 instances running behind an Application Load Balancer (ALB) within an Auto Scaling group. A DevOps Engineer wants to configure a Blue/Green deployment for this application and has already created launch templates and Auto Scaling groups for both blue and green environments, each deploying to their respective target groups. The ALB can direct traffic to either environment's target group, and an Amazon Route 53 record points to the ALB. The goal is to enable an all-at-once transition of traffic from the software running on the blue environment's EC2 instances to the newly deployed software on the green environment's EC2 instances.</p>\n\n<p>What steps should the DevOps Engineer take to fulfill these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an all-at-once deployment to the blue environment's EC2 instances. Perform a Route 53 DNS update to point to the green environment's endpoint on the ALB</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Once the rolling restart is complete, leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Perform a Route 53 DNS update to point to the green environment's endpoint on the ALB</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group. Then initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: SDLC Automation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Once the rolling restart is complete, leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group</strong></p>\n\n<p>A Blue/Green deployment is a deployment strategy in which you create two separate, but identical environments. One environment (blue) is running the current application version and one environment (green) is running the new application version. Using a Blue/Green deployment strategy increases application availability and reduces deployment risk by simplifying the rollback process if a deployment fails. Once testing has been completed on the green environment, live application traffic is directed to the green environment and the blue environment is deprecated.</p>\n\n<p>Several AWS deployment services support Blue/Green deployment strategies including Elastic Beanstalk, OpsWorks, CloudFormation, CodeDeploy, and Amazon ECS.</p>\n\n<p>For the given use case, the blue group carries the production load while the green group is staged and deployed with the new code. When it’s time to deploy, you simply attach the green group to the existing load balancer to introduce traffic to the new environment. As you scale up the green Auto Scaling group, you can take the blue Auto Scaling group instances out of service by either terminating them or putting them in a Standby state.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q36-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q36-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\">https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an all-at-once deployment to the blue environment's EC2 instances. Perform a Route 53 DNS swap to the green environment's endpoint on the ALB</strong></p>\n\n<p><strong>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Perform a Route 53 DNS swap to the green environment's endpoint on the ALB</strong></p>\n\n<p>Both these options have been added as distractors. Since there is only a single ALB per the given use case, so there is no alternate endpoint available for a Route 53 DNS update.</p>\n\n<p><strong>Leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group. Then initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances</strong> - The order of execution for this option is incorrect as it points the ALB to the green environment's target group before deploying the new software on the green environment's EC2 instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\">https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Once the rolling restart is complete, leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group</strong>"
      },
      {
        "answer": "",
        "explanation": "A Blue/Green deployment is a deployment strategy in which you create two separate, but identical environments. One environment (blue) is running the current application version and one environment (green) is running the new application version. Using a Blue/Green deployment strategy increases application availability and reduces deployment risk by simplifying the rollback process if a deployment fails. Once testing has been completed on the green environment, live application traffic is directed to the green environment and the blue environment is deprecated."
      },
      {
        "answer": "",
        "explanation": "Several AWS deployment services support Blue/Green deployment strategies including Elastic Beanstalk, OpsWorks, CloudFormation, CodeDeploy, and Amazon ECS."
      },
      {
        "answer": "",
        "explanation": "For the given use case, the blue group carries the production load while the green group is staged and deployed with the new code. When it’s time to deploy, you simply attach the green group to the existing load balancer to introduce traffic to the new environment. As you scale up the green Auto Scaling group, you can take the blue Auto Scaling group instances out of service by either terminating them or putting them in a Standby state."
      },
      {
        "link": "https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an all-at-once deployment to the blue environment's EC2 instances. Perform a Route 53 DNS swap to the green environment's endpoint on the ALB</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Perform a Route 53 DNS swap to the green environment's endpoint on the ALB</strong>"
      },
      {
        "answer": "",
        "explanation": "Both these options have been added as distractors. Since there is only a single ALB per the given use case, so there is no alternate endpoint available for a Route 53 DNS update."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group. Then initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances</strong> - The order of execution for this option is incorrect as it points the ALB to the green environment's target group before deploying the new software on the green environment's EC2 instances."
      }
    ],
    "references": [
      "https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf",
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html"
    ]
  },
  {
    "id": 18,
    "question": "<p>A company uses AWS CodeDeploy to deploy an AWS Lambda function as the final step of a CI/CD pipeline. The company has developed the Lambda function to handle incoming orders through an order-processing API. The DevOps team has noticed intermittent failures in the API occurring for a brief period after deploying the Lambda function. Upon investigation, the team suspects that the failures are a result of incomplete propagation of database changes before the Lambda function gets invoked.</p>\n\n<p>What measures can help resolve this issue?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add a <code>BeforeAllowTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Add a <code>BeforeInstall</code> hook to the AppSpec file that tests and waits for any necessary database changes before deploying a new version of the Lambda function</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add a <code>ValidateService</code> hook to the AppSpec file that validates that any necessary database changes are propagated</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Add an <code>AfterAllowTestTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes after allowing a test traffic</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Configuration Management and IaC",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a <code>BeforeAllowTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function</strong></p>\n\n<p><code>BeforeAllowTraffic</code> hook is used to run tasks before traffic is shifted to the deployed Lambda function version. By using the \"BeforeAllowTraffic\" hook, the DevOps team can ensure that traffic to the new version of the Lambda function is allowed only after necessary database changes have fully propagated, thus avoiding intermittent failures after deployment.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q39-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q39-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add a <code>BeforeInstall</code> hook to the AppSpec file that tests and waits for any necessary database changes before deploying a new version of the Lambda function</strong></p>\n\n<p><strong>Add a <code>ValidateService</code> hook to the AppSpec file that validates that any necessary database changes are propagated</strong></p>\n\n<p><strong>Add an <code>AfterAllowTestTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes after allowing test traffic</strong></p>\n\n<p>None of these three lifecycle event hooks are available for an AWS Lambda deployment, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Add a <code>BeforeAllowTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function</strong>"
      },
      {
        "answer": "",
        "explanation": "<code>BeforeAllowTraffic</code> hook is used to run tasks before traffic is shifted to the deployed Lambda function version. By using the \"BeforeAllowTraffic\" hook, the DevOps team can ensure that traffic to the new version of the Lambda function is allowed only after necessary database changes have fully propagated, thus avoiding intermittent failures after deployment."
      },
      {
        "link": "https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Add a <code>BeforeInstall</code> hook to the AppSpec file that tests and waits for any necessary database changes before deploying a new version of the Lambda function</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Add a <code>ValidateService</code> hook to the AppSpec file that validates that any necessary database changes are propagated</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Add an <code>AfterAllowTestTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes after allowing test traffic</strong>"
      },
      {
        "answer": "",
        "explanation": "None of these three lifecycle event hooks are available for an AWS Lambda deployment, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html"
    ]
  },
  {
    "id": 19,
    "question": "<p>During an AWS CloudFormation stack update process, an error occurred in the updated template, causing AWS CloudFormation to initiate an automatic stack rollback. After the rollback attempt, a DevOps engineer noticed that the application remained unavailable, and the stack is now in the UPDATE_ROLLBACK_FAILED state.</p>\n\n<p>To ensure the successful completion of the stack rollback, which actions should the DevOps engineer take? (Select two)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Automatically fix the issue by using AWS CloudFormation stack sets</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Automatically fix the issue by using AWS CloudFormation drift detection</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Update the existing AWS CloudFormation stack by using the original template</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Execute a ContinueUpdateRollback command from the AWS CloudFormation</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Manually fix the resources to match the correct state of the stack</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Configuration Management and IaC",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Execute a ContinueUpdateRollback command from the AWS CloudFormation</strong></p>\n\n<p><strong>Manually fix the resources to match the correct state of the stack</strong></p>\n\n<p>When an update to a CloudFormation stack fails, AWS CloudFormation automatically initiates a rollback process to revert the stack to its previous known stable state. In certain cases, such as when there are dependencies on external resources, the rollback process might stall or encounter an error. To help recover from a failed stack update, you can use the ContinueUpdateRollback command. This command instructs CloudFormation to continue the stack rollback and can help resolve the UPDATE_ROLLBACK_FAILED state.</p>\n\n<p>A dependent resource can't return to its original state, causing the rollback to fail (UPDATE_ROLLBACK_FAILED state). For example, you might have a stack that's rolling back to an old database instance that was deleted outside of AWS CloudFormation. Because AWS CloudFormation doesn't know the database was deleted, it assumes that the database instance still exists and attempts to roll back to it, causing the update rollback to fail.</p>\n\n<p>Depending on the cause of the failure, you can manually fix the error and continue the rollback. By continuing the rollback, you can return your stack to a working state (the UPDATE_ROLLBACK_COMPLETE state), and then try to update the stack again.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q34-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q34-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Automatically fix the issue by using AWS CloudFormation drift detection</strong></p>\n\n<p>While AWS CloudFormation drift detection can help identify resources that have drifted from their expected template configurations, it is not directly related to resolving the UPDATE_ROLLBACK_FAILED state. Drift detection is used to detect and compare configuration changes made outside of CloudFormation, and it helps to bring the resources back in line with the template. However, it doesn't address the issue of a failed stack rollback.</p>\n\n<p><strong>Automatically fix the issue by using AWS CloudFormation stack sets</strong></p>\n\n<p>AWS CloudFormation StackSets extends the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified AWS Regions. It cannot fix the issue with a failed stack rollback.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/images/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/images/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a><p></p>\n\n<p><strong>Update the existing AWS CloudFormation stack by using the original template</strong> - Reapplying the original template will not address the issue that caused the stack update to fail initially. The same problem will likely persist, and the stack rollback will continue to fail. To complete the rollback, the cause of the update failure should be addressed, and the rollback should be explicitly continued using the ContinueUpdateRollback command.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed</a></p>\n\n<p><a href=\"#\">[https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html)</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Execute a ContinueUpdateRollback command from the AWS CloudFormation</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Manually fix the resources to match the correct state of the stack</strong>"
      },
      {
        "answer": "",
        "explanation": "When an update to a CloudFormation stack fails, AWS CloudFormation automatically initiates a rollback process to revert the stack to its previous known stable state. In certain cases, such as when there are dependencies on external resources, the rollback process might stall or encounter an error. To help recover from a failed stack update, you can use the ContinueUpdateRollback command. This command instructs CloudFormation to continue the stack rollback and can help resolve the UPDATE_ROLLBACK_FAILED state."
      },
      {
        "answer": "",
        "explanation": "A dependent resource can't return to its original state, causing the rollback to fail (UPDATE_ROLLBACK_FAILED state). For example, you might have a stack that's rolling back to an old database instance that was deleted outside of AWS CloudFormation. Because AWS CloudFormation doesn't know the database was deleted, it assumes that the database instance still exists and attempts to roll back to it, causing the update rollback to fail."
      },
      {
        "answer": "",
        "explanation": "Depending on the cause of the failure, you can manually fix the error and continue the rollback. By continuing the rollback, you can return your stack to a working state (the UPDATE_ROLLBACK_COMPLETE state), and then try to update the stack again."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Automatically fix the issue by using AWS CloudFormation drift detection</strong>"
      },
      {
        "answer": "",
        "explanation": "While AWS CloudFormation drift detection can help identify resources that have drifted from their expected template configurations, it is not directly related to resolving the UPDATE_ROLLBACK_FAILED state. Drift detection is used to detect and compare configuration changes made outside of CloudFormation, and it helps to bring the resources back in line with the template. However, it doesn't address the issue of a failed stack rollback."
      },
      {
        "answer": "",
        "explanation": "<strong>Automatically fix the issue by using AWS CloudFormation stack sets</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS CloudFormation StackSets extends the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified AWS Regions. It cannot fix the issue with a failed stack rollback."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Update the existing AWS CloudFormation stack by using the original template</strong> - Reapplying the original template will not address the issue that caused the stack update to fail initially. The same problem will likely persist, and the stack rollback will continue to fail. To complete the rollback, the cause of the update failure should be addressed, and the rollback should be explicitly continued using the ContinueUpdateRollback command."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html",
      "#"
    ]
  },
  {
    "id": 20,
    "question": "<p>A company uses serverless application architecture to process thousands of requests using AWS Lambda with Amazon DynamoDB as the database. The Amazon API Gateway REST API is used to invoke an AWS Lambda function that loads a large amount of data from the Amazon DynamoDB database. This results in cold start latencies of 7-10 seconds. The DynamoDB tables have already been configured with DynamoDB Accelerator (DAX) to reduce latency. Yet, customers report of application latency, especially during peak access hours. The application receives maximum traffic between 2 PM -5 PM every day and gradually reduces thereafter, reporting a minimum traffic post 8 PM.</p>\n\n<p>How should a DevOps engineer configure the AWS Lambda function to reduce its latency at all times?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the AWS Lambda function to use Reserved concurrency. Configure the application Auto Scaling on the Lambda function with reserved concurrency as half of the Lambda function instances invoked during peak traffic hours</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the Lambda function layers for parallel processing of Lambda runtime requests. Deploy the Lambda function as a .zip file archive to utilize Lambda layer functionality</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure the Lambda function execution environment to use up to 10,240 MB of ephemeral storage. This transient cache for data between invocations can be accessed from the Lambda function code via /tmp space</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the AWS Lambda function to use provisioned concurrency. Configure the application Auto Scaling on the Lambda function with provisioned concurrency values of 1 (minimum) and 100 (maximum) respectively</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Resilient Cloud Solutions",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the AWS Lambda function to use provisioned concurrency. Configure the application Auto Scaling on the Lambda function with provisioned concurrency values of 1 (minimum) and 100 (maximum) respectively</strong></p>\n\n<p>For a Lambda function, concurrency is the number of in-flight requests your function is handling at the same time. Provisioned concurrency is the number of pre-initialized execution environments you want to allocate to your function. These execution environments are prepared to respond immediately to incoming function requests. Configuring provisioned concurrency incurs charges to your AWS account.</p>\n\n<p>Estimating required provisioned concurrency- If your function is currently serving traffic, you can easily view its concurrency metrics using CloudWatch metrics. Specifically, the <code>ConcurrentExecutions</code> metric shows you the number of concurrent invocations for each function in your account.</p>\n\n<p>When working with provisioned concurrency, Lambda suggests including a 10% buffer on top of the amount of concurrency your function typically needs. For example, if your function usually peaks at 200 concurrent requests, set your provisioned concurrency at 220 instead (200 concurrent requests + 10% = 220 provisioned concurrency).</p>\n\n<p>You can alleviate cold start issues by configuring the optimum provisioned concurrency for the Lambda function.</p>\n\n<p>Optimizing latency with provisioned concurrency:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q1-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q1-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a><p></p>\n\n<p>Accurately estimating required provisioned concurrency:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q1-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q1-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the AWS Lambda function to use Reserved concurrency. Configure the application Auto Scaling on the Lambda function with reserved concurrency as half of the Lambda function instances invoked during peak traffic hours</strong> - Reserved concurrency is the maximum number of concurrent instances you want to allocate to your function. When a function has reserved concurrency, no other function can use that concurrency. Reserved concurrency is well suited for use cases where traffic is constant and predictable over the day.</p>\n\n<p><strong>Configure the Lambda function execution environment to use up to 10,240 MB of ephemeral storage. This transient cache for data between invocations can be accessed from the Lambda function code via /tmp space</strong> - For data-intensive applications such as machine learning inference, and financial computations, customers often need to read and write large amounts of data to ephemeral storage. The Lambda execution environment provides an ephemeral file system for customers’ code to access via /tmp space, which is preserved for the lifetime of the execution environment, and can provide a transient cache for data between invocations. This option acts as a distractor.</p>\n\n<p><strong>Use the Lambda function layers for parallel processing of Lambda runtime requests. Deploy the Lambda function as a .zip file archive to utilize Lambda layer functionality</strong> - A Lambda function layer is a .zip file archive that can contain additional code or data. A layer can contain libraries, a custom runtime, data, or configuration files. Layers promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic. This option is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#calculating-concurrency\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#calculating-concurrency</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2022/03/aws-lambda-configure-ephemeral-storage/\">https://aws.amazon.com/about-aws/whats-new/2022/03/aws-lambda-configure-ephemeral-storage/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure the AWS Lambda function to use provisioned concurrency. Configure the application Auto Scaling on the Lambda function with provisioned concurrency values of 1 (minimum) and 100 (maximum) respectively</strong>"
      },
      {
        "answer": "",
        "explanation": "For a Lambda function, concurrency is the number of in-flight requests your function is handling at the same time. Provisioned concurrency is the number of pre-initialized execution environments you want to allocate to your function. These execution environments are prepared to respond immediately to incoming function requests. Configuring provisioned concurrency incurs charges to your AWS account."
      },
      {
        "answer": "",
        "explanation": "Estimating required provisioned concurrency- If your function is currently serving traffic, you can easily view its concurrency metrics using CloudWatch metrics. Specifically, the <code>ConcurrentExecutions</code> metric shows you the number of concurrent invocations for each function in your account."
      },
      {
        "answer": "",
        "explanation": "When working with provisioned concurrency, Lambda suggests including a 10% buffer on top of the amount of concurrency your function typically needs. For example, if your function usually peaks at 200 concurrent requests, set your provisioned concurrency at 220 instead (200 concurrent requests + 10% = 220 provisioned concurrency)."
      },
      {
        "answer": "",
        "explanation": "You can alleviate cold start issues by configuring the optimum provisioned concurrency for the Lambda function."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q1-i1.jpg",
        "answer": "",
        "explanation": "Optimizing latency with provisioned concurrency:"
      },
      {
        "link": "https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q1-i2.jpg",
        "answer": "",
        "explanation": "Accurately estimating required provisioned concurrency:"
      },
      {
        "link": "https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure the AWS Lambda function to use Reserved concurrency. Configure the application Auto Scaling on the Lambda function with reserved concurrency as half of the Lambda function instances invoked during peak traffic hours</strong> - Reserved concurrency is the maximum number of concurrent instances you want to allocate to your function. When a function has reserved concurrency, no other function can use that concurrency. Reserved concurrency is well suited for use cases where traffic is constant and predictable over the day."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure the Lambda function execution environment to use up to 10,240 MB of ephemeral storage. This transient cache for data between invocations can be accessed from the Lambda function code via /tmp space</strong> - For data-intensive applications such as machine learning inference, and financial computations, customers often need to read and write large amounts of data to ephemeral storage. The Lambda execution environment provides an ephemeral file system for customers’ code to access via /tmp space, which is preserved for the lifetime of the execution environment, and can provide a transient cache for data between invocations. This option acts as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the Lambda function layers for parallel processing of Lambda runtime requests. Deploy the Lambda function as a .zip file archive to utilize Lambda layer functionality</strong> - A Lambda function layer is a .zip file archive that can contain additional code or data. A layer can contain libraries, a custom runtime, data, or configuration files. Layers promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic. This option is irrelevant to the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#calculating-concurrency",
      "https://aws.amazon.com/about-aws/whats-new/2022/03/aws-lambda-configure-ephemeral-storage/"
    ]
  },
  {
    "id": 21,
    "question": "<p>A developer configured an AWS CloudFormation template to create custom resource necessary for the project. The AWS Lambda function for the custom resource executed successfully as seen by the successful creation of the custom resource. But, the CloudFormation stack is not transitioning from in-progress status (CREATE_IN_PROGRESS) to completion status (CREATE_COMPLETE).</p>\n\n<p>Which step did the developer possibly miss for the successful completion of the CloudFormation stack?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the AWS Lambda function to send the response(SUCCESS or FAILED) of the custom resource creation to a pre-signed Amazon Simple Storage Service URL</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>The AWS CloudFormation resource <code>AWS::CloudFormation::CustomResource</code> should be used to specify a custom resource in the template</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>If the template developer and custom resource provider are configured to the same person or entity then CloudFormation stack completion fails</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>After executing the <code>send</code> method in the <code>cfn-response</code> module, the Lambda function terminates, so anything written after this method is ignored</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 5: Incident and Event Response",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the AWS Lambda function to send the response(SUCCESS or FAILED) of the custom resource creation to a pre-signed Amazon Simple Storage Service URL</strong></p>\n\n<p>Custom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. Use the <code>AWS::CloudFormation::CustomResource</code> or <code>Custom::MyCustomResourceTypeName</code> resource type to define custom resources in your templates. Custom resources require one property: the service token, which specifies where AWS CloudFormation sends requests to, such as an Amazon SNS topic.</p>\n\n<p>The custom resource provider processes the AWS CloudFormation request and returns a response of SUCCESS or FAILED to the pre-signed URL. The custom resource provider responds with a JSON-formatted file and uploads it to the pre-signed S3 URL. If this URL is not provided, the calling template will not get an update of the status of the Lambda function and will remain in an in-progress state.</p>\n\n<p>How custom resources work:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q15-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q15-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The AWS CloudFormation resource <code>AWS::CloudFormation::CustomResource</code> should be used to specify a custom resource in the template</strong> - You can use the <code>AWS::CloudFormation::CustomResource</code> or <code>Custom::MyCustomResourceTypeName</code> resource type to define custom resources in your templates. However, this has no bearing on the given use case.</p>\n\n<p><strong>If the template developer and custom resource provider are configured to the same person or entity then CloudFormation stack completion fails</strong> - This statement is incorrect. The template developer and custom resource provider can be the same person or entity.</p>\n\n<p><strong>After executing the <code>send</code> method in the <code>cfn-response</code> module, the Lambda function terminates, so anything written after this method is ignored</strong> - The <code>cfn-response</code> module is available only when you use the ZipFile property to write your source code. This is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure the AWS Lambda function to send the response(SUCCESS or FAILED) of the custom resource creation to a pre-signed Amazon Simple Storage Service URL</strong>"
      },
      {
        "answer": "",
        "explanation": "Custom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. Use the <code>AWS::CloudFormation::CustomResource</code> or <code>Custom::MyCustomResourceTypeName</code> resource type to define custom resources in your templates. Custom resources require one property: the service token, which specifies where AWS CloudFormation sends requests to, such as an Amazon SNS topic."
      },
      {
        "answer": "",
        "explanation": "The custom resource provider processes the AWS CloudFormation request and returns a response of SUCCESS or FAILED to the pre-signed URL. The custom resource provider responds with a JSON-formatted file and uploads it to the pre-signed S3 URL. If this URL is not provided, the calling template will not get an update of the status of the Lambda function and will remain in an in-progress state."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q15-i1.jpg",
        "answer": "",
        "explanation": "How custom resources work:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The AWS CloudFormation resource <code>AWS::CloudFormation::CustomResource</code> should be used to specify a custom resource in the template</strong> - You can use the <code>AWS::CloudFormation::CustomResource</code> or <code>Custom::MyCustomResourceTypeName</code> resource type to define custom resources in your templates. However, this has no bearing on the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>If the template developer and custom resource provider are configured to the same person or entity then CloudFormation stack completion fails</strong> - This statement is incorrect. The template developer and custom resource provider can be the same person or entity."
      },
      {
        "answer": "",
        "explanation": "<strong>After executing the <code>send</code> method in the <code>cfn-response</code> module, the Lambda function terminates, so anything written after this method is ignored</strong> - The <code>cfn-response</code> module is available only when you use the ZipFile property to write your source code. This is irrelevant to the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html"
    ]
  },
  {
    "id": 22,
    "question": "<p>An e-commerce company has a serverless application stack that consists of CloudFront, API Gateway and Lambda functions. The company has hired you to improve the current deployment process which creates a new version of the Lambda function and then runs an AWS CLI script for deployment. In case the new version errors out, then another CLI script is invoked to deploy the previous working version of the Lambda function. The company has mandated you to decrease the time to deploy new versions of the Lambda functions and also reduce the time to detect and roll back when errors are identified.</p>\n\n<p>Which of the following solutions would you suggest for the given use case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up and deploy a CloudFormation stack containing a new API Gateway endpoint that points to the new Lambda version. Test the updated CloudFront origin that points to this new API Gateway endpoint and in case errors are detected then revert the CloudFront origin to the previous working API Gateway endpoint</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Serverless Application Model (SAM) and leverage the built-in traffic-shifting feature of SAM to deploy the new Lambda version via CodeDeploy and use pre-traffic and post-traffic test functions to verify code. Rollback in case CloudWatch alarms are triggered</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. Use pre-traffic and post-traffic test functions of the change set to verify the deployment. Rollback in case CloudWatch alarms are triggered</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. In case the Lambda function errors out, rollback the CloudFormation change set to the previous version</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Configuration Management and IaC",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Serverless Application Model (SAM) and leverage the built-in traffic-shifting feature of SAM to deploy the new Lambda version via CodeDeploy and use pre-traffic and post-traffic test functions to verify code. Rollback in case CloudWatch alarms are triggered</strong></p>\n\n<p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax. Then, CloudFormation provisions your resources with reliable deployment capabilities.</p>\n\n<p>To address the given use case, you can use the traffic shifting feature of SAM to easily test the new version of the Lambda function without having to manually move 100% of the traffic to the new version in one shot.</p>\n\n<p>You can use CodeDeploy to create a deployment process that publishes the new Lambda version but does not send any traffic to it. Then it executes a PreTraffic test to ensure that your new function works as expected. After the test succeeds, CodeDeploy automatically shifts traffic gradually to the new version of the Lambda function. This workflow addresses one of the key requirements of reducing the time to detect errors. You can roll back to the previous version in case the new version errors out.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q42-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q42-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/\">https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. In case the Lambda function errors out, rollback the CloudFormation change set to the previous version</strong> - You can use CloudFormation change sets to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set.</p>\n\n<p>This option does not help in reducing the time to detect any potential deployment errors as you would not know about any potential failures until you actually deploy the stack.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q42-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q42-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a><p></p>\n\n<p>Instead, you should use SAM to create your serverless application as it comes built-in with CodeDeploy to provide gradual Lambda deployments. Also, you can define pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. You can roll back the deployment if CloudWatch alarms are triggered.</p>\n\n<p><strong>Set up and deploy a CloudFormation stack containing a new API Gateway endpoint that points to the new Lambda version. Test the updated CloudFront origin that points to this new API Gateway endpoint and in case errors are detected then revert the CloudFront origin to the previous working API Gateway endpoint</strong> - This option does not help in reducing the time to detect any potential deployment errors as you would not know about any potential failures until you actually deploy the stack and point to the new endpoint.</p>\n\n<p>Instead, you should use SAM to create your serverless application as it comes built-in with CodeDeploy to provide gradual Lambda deployments. Also, you can define pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. You can roll back the deployment if CloudWatch alarms are triggered.</p>\n\n<p><strong>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. Use pre-traffic and post-traffic test functions of the change set to verify the deployment. Rollback in case CloudWatch alarms are triggered</strong> - This option has been added as a distractor since CloudFormation change sets do not have pre-traffic and post-traffic test functions. Therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/\">https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Serverless Application Model (SAM) and leverage the built-in traffic-shifting feature of SAM to deploy the new Lambda version via CodeDeploy and use pre-traffic and post-traffic test functions to verify code. Rollback in case CloudWatch alarms are triggered</strong>"
      },
      {
        "answer": "",
        "explanation": "The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax. Then, CloudFormation provisions your resources with reliable deployment capabilities."
      },
      {
        "answer": "",
        "explanation": "To address the given use case, you can use the traffic shifting feature of SAM to easily test the new version of the Lambda function without having to manually move 100% of the traffic to the new version in one shot."
      },
      {
        "answer": "",
        "explanation": "You can use CodeDeploy to create a deployment process that publishes the new Lambda version but does not send any traffic to it. Then it executes a PreTraffic test to ensure that your new function works as expected. After the test succeeds, CodeDeploy automatically shifts traffic gradually to the new version of the Lambda function. This workflow addresses one of the key requirements of reducing the time to detect errors. You can roll back to the previous version in case the new version errors out."
      },
      {
        "link": "https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. In case the Lambda function errors out, rollback the CloudFormation change set to the previous version</strong> - You can use CloudFormation change sets to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set."
      },
      {
        "answer": "",
        "explanation": "This option does not help in reducing the time to detect any potential deployment errors as you would not know about any potential failures until you actually deploy the stack."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html"
      },
      {
        "answer": "",
        "explanation": "Instead, you should use SAM to create your serverless application as it comes built-in with CodeDeploy to provide gradual Lambda deployments. Also, you can define pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. You can roll back the deployment if CloudWatch alarms are triggered."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up and deploy a CloudFormation stack containing a new API Gateway endpoint that points to the new Lambda version. Test the updated CloudFront origin that points to this new API Gateway endpoint and in case errors are detected then revert the CloudFront origin to the previous working API Gateway endpoint</strong> - This option does not help in reducing the time to detect any potential deployment errors as you would not know about any potential failures until you actually deploy the stack and point to the new endpoint."
      },
      {
        "answer": "",
        "explanation": "Instead, you should use SAM to create your serverless application as it comes built-in with CodeDeploy to provide gradual Lambda deployments. Also, you can define pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. You can roll back the deployment if CloudWatch alarms are triggered."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. Use pre-traffic and post-traffic test functions of the change set to verify the deployment. Rollback in case CloudWatch alarms are triggered</strong> - This option has been added as a distractor since CloudFormation change sets do not have pre-traffic and post-traffic test functions. Therefore this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html",
      "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html"
    ]
  },
  {
    "id": 23,
    "question": "<p>A media company extensively uses Amazon S3 buckets for storing images files, documents, and other business-specific data. The company has mandated enabling logging for all Amazon S3 buckets. The audit team publishes the reports of all AWS resources failing company security standards. Until recently, the security team would pick the list of noncompliant Amazon S3 buckets from the audit list and execute remediation actions manually for each resource. This process is not only time-consuming but also leaves noncompliant resources vulnerable for a long duration.</p>\n\n<p>Which combination of steps should a DevOps Engineer take to meet these requirements using an automated solution? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose AWS Lambda to implement a custom function that will enable S3 logging for the S3 bucket ID passed</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose <code>AWS-ConfigureS3BucketLogging</code></p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>While setting up remediation action, pass the resource ID of non-compliant resources to the remediation action. This configuration is mandatory for auto-remediation to work</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-logging-enabled</code>. Create your own custom remediation action using AWS Systems Manager Automation documents to enable logging on the S3 bucket</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 6: Security and Compliance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose <code>AWS-ConfigureS3BucketLogging</code></strong></p>\n\n<p><strong>The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config</strong></p>\n\n<p>The AWS Config Auto Remediation feature automatically remediates non-compliant resources evaluated by AWS Config rules. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention. You can:</p>\n\n<ol>\n<li>Choose the remediation action you want to associate from a pre-populated list.</li>\n<li>Create your own custom remediation actions using AWS Systems Manager Automation documents.</li>\n</ol>\n\n<p>If a resource is still non-compliant after auto-remediation, you can set the rule to try auto-remediation again.</p>\n\n<p>For the above use case: You must have AWS Config enabled in your AWS account. The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config, and that role must have whatever permissions the SSM document requires.</p>\n\n<p>Steps to set up Auto Remediation for s3-bucket-logging-enabled:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q23-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q23-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-logging-enabled</code>. Create your own custom remediation action using AWS Systems Manager Automation documents to enable logging on the S3 bucket</strong> - While you can create a custom remediation action using SSM, for this particular use case, it is not required since auto-remediation for S3 server logging is already present in the remediation action list of AWS Config rules.</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose AWS Lambda to implement a custom function that will enable S3 logging for the S3 bucket ID passed</strong> - As discussed above, the AWS config remediation action pre-populated list already has an action defined for enabling S3 logging. Hence, custom code is unnecessary for this use case.</p>\n\n<p><strong>While setting up remediation action, pass the resource ID of non-compliant resources to the remediation action. This configuration is mandatory for auto-remediation to work</strong>- While setting up remediation action if you want to pass the resource ID of non-compliant resources to the remediation action, choose the Resource ID parameter. If selected at runtime, the parameter is substituted with the ID of the resource to be remediated. This is not mandatory though.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose <code>AWS-ConfigureS3BucketLogging</code></strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config</strong>"
      },
      {
        "answer": "",
        "explanation": "The AWS Config Auto Remediation feature automatically remediates non-compliant resources evaluated by AWS Config rules. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention. You can:"
      },
      {},
      {
        "answer": "",
        "explanation": "If a resource is still non-compliant after auto-remediation, you can set the rule to try auto-remediation again."
      },
      {
        "answer": "",
        "explanation": "For the above use case: You must have AWS Config enabled in your AWS account. The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config, and that role must have whatever permissions the SSM document requires."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q23-i1.jpg",
        "answer": "",
        "explanation": "Steps to set up Auto Remediation for s3-bucket-logging-enabled:"
      },
      {
        "link": "https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-logging-enabled</code>. Create your own custom remediation action using AWS Systems Manager Automation documents to enable logging on the S3 bucket</strong> - While you can create a custom remediation action using SSM, for this particular use case, it is not required since auto-remediation for S3 server logging is already present in the remediation action list of AWS Config rules."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose AWS Lambda to implement a custom function that will enable S3 logging for the S3 bucket ID passed</strong> - As discussed above, the AWS config remediation action pre-populated list already has an action defined for enabling S3 logging. Hence, custom code is unnecessary for this use case."
      },
      {
        "answer": "",
        "explanation": "<strong>While setting up remediation action, pass the resource ID of non-compliant resources to the remediation action. This configuration is mandatory for auto-remediation to work</strong>- While setting up remediation action if you want to pass the resource ID of non-compliant resources to the remediation action, choose the Resource ID parameter. If selected at runtime, the parameter is substituted with the ID of the resource to be remediated. This is not mandatory though."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/"
    ]
  },
  {
    "id": 24,
    "question": "<p>The DevOps team for an e-commerce company wants to implement a patching plan on AWS Cloud for a large mixed fleet of Windows and Linux servers. The patching plan has to be auditable and must be implemented securely to ensure compliance with the company's business requirements.</p>\n\n<p>Which of the following options would you recommend to address these requirements with MINIMAL effort? (Select two)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an OS-native patching service to manage the update frequency and release approval for all instances. Set up AWS Config to provide audit and compliance reporting</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure CloudFormation automatic patching support for all applications which will keep the OS up-to-date following the initial installation. Set up AWS Config to provide audit and compliance reporting</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up Systems Manager Agent on all instances to manage patching. Test patches in pre-production and then deploy as a maintenance window task with the appropriate approval</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Apply patch baselines using the AWS-ApplyPatchBaseline SSM document</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Apply patch baselines using the AWS-RunPatchBaseline SSM document</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 6: Security and Compliance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Set up Systems Manager Agent on all instances to manage patching. Test patches in pre-production and then deploy as a maintenance window task with the appropriate approval</strong></p>\n\n<p><strong>Apply patch baselines using the AWS-RunPatchBaseline SSM document</strong></p>\n\n<p>Systems Manager is an AWS service that you can use to view and control your infrastructure on AWS. Using the Systems Manager console, you can view operational data from multiple AWS services and automate operational tasks across your AWS resources. Systems Manager helps you maintain security and compliance by scanning your managed instances and reporting on (or taking corrective action on) any policy violations it detects. AWS Systems Manager Agent (SSM Agent) is Amazon software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for the Systems Manager to update, manage, and configure these resources.</p>\n\n<p><img src=\"https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a><p></p>\n\n<p>You can use Patch Manager to apply patches for both operating systems and applications. (On Windows Server, application support is limited to updates for Microsoft applications). Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches individually or to large groups of instances by using Amazon EC2 tags.</p>\n\n<p>For the given use case, you can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task.</p>\n\n<p>Systems Manager supports an SSM document for Patch Manager, AWS-RunPatchBaseline, which performs patching operations on instances for both security-related and other types of updates. When the document is run, it uses the patch baseline currently specified as the \"default\" for an operating system type.</p>\n\n<p>The AWS-ApplyPatchBaseline SSM document supports patching on Windows instances only and doesn't support Linux instances. For applying patch baselines to both Windows Server and Linux instances, the recommended SSM document is AWS-RunPatchBaseline.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q44-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q44-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q44-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q44-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure CloudFormation automatic patching support for all applications which will keep the OS up-to-date following the initial installation. Set up AWS Config to provide audit and compliance reporting</strong> - AWS CloudFormation is designed to allow resource lifecycles to be managed repeatably, predictable, and safely, while allowing for automatic rollbacks, automated state management, and management of resources across accounts and regions.</p>\n\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”.</p>\n\n<p>There is no such thing as CloudFormation automatic patching support, as you need to use Systems Manager for patch management. So, this option acts as a distractor.</p>\n\n<p><strong>Set up an OS-native patching service to manage the update frequency and release approval for all instances. Set up AWS Config to provide audit and compliance reporting</strong> - You could use an OS-native patching service to manage the update frequency and release approval for all instances but it would take considerable effort to set up and configure this solution. This violates the minimal effort requirement of the given use case.</p>\n\n<p><strong>Apply patch baselines using the AWS-ApplyPatchBaseline SSM document</strong> - As mentioned in the explanation above, the AWS-ApplyPatchBaseline SSM document supports patching on Windows instances only and doesn't support Linux instances. For applying patch baselines to both Windows Server and Linux instances, the recommended SSM document is AWS-RunPatchBaseline.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up Systems Manager Agent on all instances to manage patching. Test patches in pre-production and then deploy as a maintenance window task with the appropriate approval</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Apply patch baselines using the AWS-RunPatchBaseline SSM document</strong>"
      },
      {
        "answer": "",
        "explanation": "Systems Manager is an AWS service that you can use to view and control your infrastructure on AWS. Using the Systems Manager console, you can view operational data from multiple AWS services and automate operational tasks across your AWS resources. Systems Manager helps you maintain security and compliance by scanning your managed instances and reporting on (or taking corrective action on) any policy violations it detects. AWS Systems Manager Agent (SSM Agent) is Amazon software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for the Systems Manager to update, manage, and configure these resources."
      },
      {
        "link": "https://aws.amazon.com/systems-manager/"
      },
      {
        "answer": "",
        "explanation": "You can use Patch Manager to apply patches for both operating systems and applications. (On Windows Server, application support is limited to updates for Microsoft applications). Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches individually or to large groups of instances by using Amazon EC2 tags."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task."
      },
      {
        "answer": "",
        "explanation": "Systems Manager supports an SSM document for Patch Manager, AWS-RunPatchBaseline, which performs patching operations on instances for both security-related and other types of updates. When the document is run, it uses the patch baseline currently specified as the \"default\" for an operating system type."
      },
      {
        "answer": "",
        "explanation": "The AWS-ApplyPatchBaseline SSM document supports patching on Windows instances only and doesn't support Linux instances. For applying patch baselines to both Windows Server and Linux instances, the recommended SSM document is AWS-RunPatchBaseline."
      },
      {
        "link": "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html"
      },
      {
        "link": "https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure CloudFormation automatic patching support for all applications which will keep the OS up-to-date following the initial installation. Set up AWS Config to provide audit and compliance reporting</strong> - AWS CloudFormation is designed to allow resource lifecycles to be managed repeatably, predictable, and safely, while allowing for automatic rollbacks, automated state management, and management of resources across accounts and regions."
      },
      {
        "answer": "",
        "explanation": "AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”."
      },
      {
        "answer": "",
        "explanation": "There is no such thing as CloudFormation automatic patching support, as you need to use Systems Manager for patch management. So, this option acts as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an OS-native patching service to manage the update frequency and release approval for all instances. Set up AWS Config to provide audit and compliance reporting</strong> - You could use an OS-native patching service to manage the update frequency and release approval for all instances but it would take considerable effort to set up and configure this solution. This violates the minimal effort requirement of the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Apply patch baselines using the AWS-ApplyPatchBaseline SSM document</strong> - As mentioned in the explanation above, the AWS-ApplyPatchBaseline SSM document supports patching on Windows instances only and doesn't support Linux instances. For applying patch baselines to both Windows Server and Linux instances, the recommended SSM document is AWS-RunPatchBaseline."
      }
    ],
    "references": [
      "https://aws.amazon.com/systems-manager/",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html"
    ]
  },
  {
    "id": 25,
    "question": "<p>A company has hired you as an AWS Certified DevOps Engineer - Professional to provide recommendations for a failed security audit of its flagship project. You have been tasked to review the company's buildspec.yaml file for its AWS CodeBuild project. Upon investigation, you have noticed that the file has hard-coded values for the environment variables that reference the AWS <code>Access Key ID</code>, <code>Secret Access Key</code>, and the database password. In addition, to perform one-time configuration changes during the build phase, the file has commands to <code>ssh</code> and <code>scp</code> into an EC2 instance using an SSH private key stored on Amazon S3.</p>\n\n<p>What changes would you recommend to comply with AWS security best practices? (Select three)</p>",
    "corrects": [
      2,
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage the AWS Systems Manager automation document to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure the CodeBuild project role with the necessary permissions policy and then delete the environment variables referencing the AWS credentials from the buildspec.yaml file</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Store the database password as a SecureString value in AWS Systems Manager Parameter Store which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Leverage the AWS Systems Manager <code>run</code> command to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Store the database password as a SecureString value in AWS Secrets Manager which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</p>",
        "correct": false
      },
      {
        "id": 6,
        "answer": "<p>Store the environment variables that reference the AWS <code>Access Key ID</code>, <code>Secret Access Key</code>, and the database password in an encrypted format on Amazon S3. Leverage a Python script to dynamically import the values from S3 during the pre-build phase</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 6: Security and Compliance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure the CodeBuild project role with the necessary permissions policy and then delete the environment variables referencing the AWS credentials from the buildspec.yaml file</strong></p>\n\n<p>It is considered a bad practice to store hard-coded AWS credentials in code, configuration, or build files such as the buildspec.yaml file. AWS recommends using an appropriate permissions policy that is attached to the CodeBuild project role to grant the necessary access to the required resources.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q31-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q31-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html</a><p></p>\n\n<p><strong>Store the database password as a SecureString value in AWS Systems Manager Parameter Store which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</strong></p>\n\n<p>It is considered a bad practice to store hard-coded database credentials in code, configuration, or build files such as the buildspec.yaml file. As a security best practice, AWS recommends using the database password as a SecureString value in AWS Systems Manager Parameter Store which can then be referenced in the buildspec file like so:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q31-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q31-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a><p></p>\n\n<p>The configuration above is required when you want to retrieve custom environment variables stored in Amazon EC2 Systems Manager Parameter Store. This contains a mapping of key/value scalars, where each mapping represents a single custom environment variable stored in the Amazon EC2 Systems Manager Parameter Store. key is the name you use later in your build commands to refer to this custom environment variable, and value is the name of the custom environment variable stored in Amazon EC2 Systems Manager Parameter Store. To allow CodeBuild to retrieve custom environment variables stored in Amazon EC2 Systems Manager Parameter Store, you must add the ssm:GetParameters action to your CodeBuild service role.</p>\n\n<p><strong>Leverage the AWS Systems Manager <code>run</code> command to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</strong></p>\n\n<p>Using Run Command, a capability of AWS Systems Manager, you can remotely and securely manage the configuration of your managed nodes. A managed node is any Amazon Elastic Compute Cloud (Amazon EC2) instance or non-EC2 machine in your hybrid and multi-cloud environment that has been configured for Systems Manager. Run Command allows you to automate common administrative tasks and perform one-time configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface (AWS CLI), AWS Tools for Windows PowerShell, or the AWS SDKs. Run Command is offered at no additional cost.</p>\n\n<p>It is considered a security bad practice to store the SSH private key on Amazon S3. Using the AWS Systems Manager <code>run</code> command for the given use case meets the security best practices.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the database password as a SecureString value in AWS Secrets Manager which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</strong> - In Secrets Manager, a secret consists of secret information, the secret value, plus metadata about the secret. A secret value can be a string or binary. Secrets Manager uses IAM permission policies to make sure that only authorized users can access or modify a secret. There is no such thing as a SecureString value in AWS Secrets Manager. This option has been added as a distractor.</p>\n\n<p><strong>Store the environment variables that reference the AWS <code>Access Key ID</code>, <code>Secret Access Key</code>, and the database password in an encrypted format on Amazon S3. Leverage a Python script to dynamically import the values from S3 during the pre-build phase</strong> - It is considered a security bad practice to store the AWS access credentials on Amazon S3, so this option is incorrect.</p>\n\n<p><strong>Leverage the AWS Systems Manager automation document to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</strong> - Automation feature of the AWS Systems Manager helps you to build automated solutions to deploy, configure, and manage AWS resources at scale. With Automation, you have granular control over the concurrency of your automation. This means you can specify how many resources to target concurrently, and how many errors can occur before an automation is stopped. Since the given use case involves one-time configuration changes, it is better to use the AWS Systems Manager <code>run</code> command.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/concepts.html#concepts-how-it-works\">https://docs.aws.amazon.com/codebuild/latest/userguide/concepts.html#concepts-how-it-works</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/getting-started.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/getting-started.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.env.parameter-store\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.env.parameter-store</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-sessions-start.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-sessions-start.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/run-command.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/run-command.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure the CodeBuild project role with the necessary permissions policy and then delete the environment variables referencing the AWS credentials from the buildspec.yaml file</strong>"
      },
      {
        "answer": "",
        "explanation": "It is considered a bad practice to store hard-coded AWS credentials in code, configuration, or build files such as the buildspec.yaml file. AWS recommends using an appropriate permissions policy that is attached to the CodeBuild project role to grant the necessary access to the required resources."
      },
      {
        "link": "https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Store the database password as a SecureString value in AWS Systems Manager Parameter Store which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</strong>"
      },
      {
        "answer": "",
        "explanation": "It is considered a bad practice to store hard-coded database credentials in code, configuration, or build files such as the buildspec.yaml file. As a security best practice, AWS recommends using the database password as a SecureString value in AWS Systems Manager Parameter Store which can then be referenced in the buildspec file like so:"
      },
      {
        "link": "https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html"
      },
      {
        "answer": "",
        "explanation": "The configuration above is required when you want to retrieve custom environment variables stored in Amazon EC2 Systems Manager Parameter Store. This contains a mapping of key/value scalars, where each mapping represents a single custom environment variable stored in the Amazon EC2 Systems Manager Parameter Store. key is the name you use later in your build commands to refer to this custom environment variable, and value is the name of the custom environment variable stored in Amazon EC2 Systems Manager Parameter Store. To allow CodeBuild to retrieve custom environment variables stored in Amazon EC2 Systems Manager Parameter Store, you must add the ssm:GetParameters action to your CodeBuild service role."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage the AWS Systems Manager <code>run</code> command to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "Using Run Command, a capability of AWS Systems Manager, you can remotely and securely manage the configuration of your managed nodes. A managed node is any Amazon Elastic Compute Cloud (Amazon EC2) instance or non-EC2 machine in your hybrid and multi-cloud environment that has been configured for Systems Manager. Run Command allows you to automate common administrative tasks and perform one-time configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface (AWS CLI), AWS Tools for Windows PowerShell, or the AWS SDKs. Run Command is offered at no additional cost."
      },
      {
        "answer": "",
        "explanation": "It is considered a security bad practice to store the SSH private key on Amazon S3. Using the AWS Systems Manager <code>run</code> command for the given use case meets the security best practices."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the database password as a SecureString value in AWS Secrets Manager which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</strong> - In Secrets Manager, a secret consists of secret information, the secret value, plus metadata about the secret. A secret value can be a string or binary. Secrets Manager uses IAM permission policies to make sure that only authorized users can access or modify a secret. There is no such thing as a SecureString value in AWS Secrets Manager. This option has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Store the environment variables that reference the AWS <code>Access Key ID</code>, <code>Secret Access Key</code>, and the database password in an encrypted format on Amazon S3. Leverage a Python script to dynamically import the values from S3 during the pre-build phase</strong> - It is considered a security bad practice to store the AWS access credentials on Amazon S3, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage the AWS Systems Manager automation document to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</strong> - Automation feature of the AWS Systems Manager helps you to build automated solutions to deploy, configure, and manage AWS resources at scale. With Automation, you have granular control over the concurrency of your automation. This means you can specify how many resources to target concurrently, and how many errors can occur before an automation is stopped. Since the given use case involves one-time configuration changes, it is better to use the AWS Systems Manager <code>run</code> command."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html",
      "https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html",
      "https://docs.aws.amazon.com/codebuild/latest/userguide/concepts.html#concepts-how-it-works",
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/getting-started.html",
      "https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.env.parameter-store",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-sessions-start.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/run-command.html"
    ]
  },
  {
    "id": 26,
    "question": "<p>A CloudFormation stack consists of the following AWS resources - Amazon Simple Storage Service (Amazon S3) bucket, Amazon Amazon Elastic Compute Cloud (Amazon EC2) instance, and an Amazon EBS Volume. Due to a high-impact security issue, the DevOps team has been asked to rename the AWS CloudFormation stack. However, the resources created by the stack cannot be deleted for business purposes.</p>\n\n<p>What steps will you take to rename the CloudFormation stack without deleting the resources created?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of each of these resources. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> attribute from the stack to revert to the original template</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Launch a new CloudFormation stack that deploys all the resources of the stack. Launch another CloudFormation stack that deploys all the resources of the stack with a new name. In the second CloudFormation stack use, the <code>DependsOn</code> attribute to create the resource dependency on the first stack. Delete both the stacks while retaining the resources</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of the S3 bucket and EC2 instance. Add a 'Snapshot' attribute to the deletion policy of Amazon EBS Volume. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> and 'Snapshot' attribute from the stack to revert to the original template</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use CloudFormation registry to create custom hooks for all the resources of the CloudFormation stack. Then, delete the original CloudFormation stack and recreate a new one with the updated name. Using the hooks created earlier, import the resources back into the newly created stack</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Configuration Management and IaC",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of each of these resources. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> attribute from the stack to revert to the original template</strong></p>\n\n<p>With the DeletionPolicy attribute, you can preserve, and in some cases, backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default. To keep a resource when its stack is deleted, specify <code>Retain</code> for that resource. You can use <code>Retain</code> for any resource. For resources that support snapshots, such as <code>AWS::EC2::Volume</code>, specify Snapshot to have CloudFormation create a snapshot before deleting the resource.</p>\n\n<p>These steps need to be followed:\n1. Launch a CloudFormation stack that deploys the necessary resources.\n2. Add a <code>Retain</code> attribute to the deletion policy of all the resources deployed by the stack.\n3. Delete the stack and verify that the resources are retained.\n4. Create a new stack and import the resources that were retained from the original stack. This stack is created with a new name.\n5. Remove the <code>Retain</code> attribute from the stack to revert to the original template.</p>\n\n<p>Refer to the example that walks through the process of retaining a single resource—a VPC—when changing the name of a CloudFormation stack:</p>\n\n<p>Process overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q24-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q24-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/\">https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of S3 bucket and EC2 instance. Add a 'Snapshot' attribute to the deletion policy of Amazon EBS Volume. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> and 'Snapshot' attribute from the stack to revert to the original template</strong> - 'Snapshot' attribute will still delete the resource after taking its snapshot. So this option is incorrect.</p>\n\n<p><strong>Use the CloudFormation registry to create custom hooks for all the resources of the CloudFormation stack. Then, delete the original CloudFormation stack and recreate a new one with the updated name. Using the hooks created earlier, import the resources back into the newly created stack</strong> - A hook is an executable custom logic that automatically inspects resources before they're provisioned. Hooks can inspect the resources that CloudFormation is about to provision. If a hook finds any resource that doesn't comply with your organizational guidelines, it can prevent CloudFormation from continuing the provisioning process. Any hook based logic is not useful for the given use case.</p>\n\n<p><strong>Launch a new CloudFormation stack that deploys all the resources of the stack. Launch another CloudFormation stack that deploys all the resources of the stack with a new name. In the second CloudFormation stack use, <code>DependsOn</code> attribute to create the resource dependency on the first stack. Delete both stacks while retaining the resources</strong> - This option just acts as a distractor, as it does not solve the given issue.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/\">https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cloudformation-cli/latest/userguide/hooks.html\">https://docs.aws.amazon.com/cloudformation-cli/latest/userguide/hooks.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of each of these resources. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> attribute from the stack to revert to the original template</strong>"
      },
      {
        "answer": "",
        "explanation": "With the DeletionPolicy attribute, you can preserve, and in some cases, backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default. To keep a resource when its stack is deleted, specify <code>Retain</code> for that resource. You can use <code>Retain</code> for any resource. For resources that support snapshots, such as <code>AWS::EC2::Volume</code>, specify Snapshot to have CloudFormation create a snapshot before deleting the resource."
      },
      {
        "answer": "",
        "explanation": "These steps need to be followed:\n1. Launch a CloudFormation stack that deploys the necessary resources.\n2. Add a <code>Retain</code> attribute to the deletion policy of all the resources deployed by the stack.\n3. Delete the stack and verify that the resources are retained.\n4. Create a new stack and import the resources that were retained from the original stack. This stack is created with a new name.\n5. Remove the <code>Retain</code> attribute from the stack to revert to the original template."
      },
      {
        "answer": "",
        "explanation": "Refer to the example that walks through the process of retaining a single resource—a VPC—when changing the name of a CloudFormation stack:"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q24-i1.jpg",
        "answer": "",
        "explanation": "Process overview:"
      },
      {
        "link": "https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of S3 bucket and EC2 instance. Add a 'Snapshot' attribute to the deletion policy of Amazon EBS Volume. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> and 'Snapshot' attribute from the stack to revert to the original template</strong> - 'Snapshot' attribute will still delete the resource after taking its snapshot. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the CloudFormation registry to create custom hooks for all the resources of the CloudFormation stack. Then, delete the original CloudFormation stack and recreate a new one with the updated name. Using the hooks created earlier, import the resources back into the newly created stack</strong> - A hook is an executable custom logic that automatically inspects resources before they're provisioned. Hooks can inspect the resources that CloudFormation is about to provision. If a hook finds any resource that doesn't comply with your organizational guidelines, it can prevent CloudFormation from continuing the provisioning process. Any hook based logic is not useful for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Launch a new CloudFormation stack that deploys all the resources of the stack. Launch another CloudFormation stack that deploys all the resources of the stack with a new name. In the second CloudFormation stack use, <code>DependsOn</code> attribute to create the resource dependency on the first stack. Delete both stacks while retaining the resources</strong> - This option just acts as a distractor, as it does not solve the given issue."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html",
      "https://docs.aws.amazon.com/cloudformation-cli/latest/userguide/hooks.html"
    ]
  },
  {
    "id": 27,
    "question": "<p>A company implements access control by creating different policies for different job functions. These policies are attached to IAM roles/groups with minimum permissions necessary for the job function. Up until this point, the solution has been functioning effectively. However, with business expansion, the administrator has to frequently update the existing policies to allow access to new resources.</p>\n\n<p>Which of the following solutions can make the access control applicable to all new resources without the need to update the policies?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a Resource-based policy on the newly created resources to add to the existing permissions of the IAM roles accessing the created resources</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure Attribute-based access control by use of tags. Create tags for IAM roles based on their function. Whenever a new resource is created, add these tag(s) to the resource for immediate access to the resource created</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure Access control lists (ACLs) to allow access to certain principals in the account. Add the ACLs to a session policy to dynamically add permissions to the IAM role that the user already has</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an AWS Organization and enable all the features. Attach identity-based IAM roles for newly created resources and share them using Service control policies (SCPs) with all member accounts of the organization</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 6: Security and Compliance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Attribute-based access control by use of tags. Create tags for IAM roles based on their function. Whenever a new resource is created, add these tag(s) to the resource for immediate access to the resource created</strong></p>\n\n<p>The traditional authorization model used in IAM is called role-based access control (RBAC). RBAC defines permissions based on a person's job function, known outside of AWS as a role. Within AWS a role usually refers to an IAM role, which is an identity in IAM that you can assume.</p>\n\n<p>Attribute-based access control (ABAC) is an authorization strategy that defines permissions based on attributes. In AWS, these attributes are called tags. You can attach tags to IAM resources, including IAM entities (users or roles), and to AWS resources. You can create a single ABAC policy or a small set of policies for your IAM principals. These ABAC policies can be designed to allow operations when the principal's tag matches the resource tag. ABAC is helpful in environments that are growing rapidly and help with situations where policy management becomes cumbersome.</p>\n\n<p>Comparing ABAC to the traditional RBAC model:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q26-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q26-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Organization and enable all the features. Attach identity-based IAM roles for newly created resources and share them using Service control policies (SCPs) with all member accounts of the organization</strong> - SCP is a policy that specifies the services and actions that users and roles can use in the accounts that the SCP affects. SCPs are similar to IAM permissions policies except that they don't grant any permissions. Instead, SCPs specify the maximum permissions for an organization, organizational unit (OU), or account.</p>\n\n<p><strong>Create a Resource-based policy on the newly created resources to add to the existing permissions of the IAM roles accessing the created resources</strong> - If you have to create a Resource-based policy for every new resource that is added to the account, then we are pretty much at the same place without offering any solution. Hence, this option is not a valid choice for the given use case.</p>\n\n<p><strong>Configure Access control lists (ACLs) to allow access to certain principals in the account. Add the ACLs to a session policy to dynamically add permissions to the IAM role that the user already has</strong> - Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account.</p>\n\n<p>Session policies are advanced policies that you pass as a parameter when you programmatically create a temporary session for a role or federated user. The permissions for a session are the intersection of the identity-based policies for the IAM entity (user or role) used to create the session and the session policies.</p>\n\n<p>Neither ACL nor a session policy is the right choice for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_session\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_session</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Attribute-based access control by use of tags. Create tags for IAM roles based on their function. Whenever a new resource is created, add these tag(s) to the resource for immediate access to the resource created</strong>"
      },
      {
        "answer": "",
        "explanation": "The traditional authorization model used in IAM is called role-based access control (RBAC). RBAC defines permissions based on a person's job function, known outside of AWS as a role. Within AWS a role usually refers to an IAM role, which is an identity in IAM that you can assume."
      },
      {
        "answer": "",
        "explanation": "Attribute-based access control (ABAC) is an authorization strategy that defines permissions based on attributes. In AWS, these attributes are called tags. You can attach tags to IAM resources, including IAM entities (users or roles), and to AWS resources. You can create a single ABAC policy or a small set of policies for your IAM principals. These ABAC policies can be designed to allow operations when the principal's tag matches the resource tag. ABAC is helpful in environments that are growing rapidly and help with situations where policy management becomes cumbersome."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q26-i1.jpg",
        "answer": "",
        "explanation": "Comparing ABAC to the traditional RBAC model:"
      },
      {
        "link": "https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an AWS Organization and enable all the features. Attach identity-based IAM roles for newly created resources and share them using Service control policies (SCPs) with all member accounts of the organization</strong> - SCP is a policy that specifies the services and actions that users and roles can use in the accounts that the SCP affects. SCPs are similar to IAM permissions policies except that they don't grant any permissions. Instead, SCPs specify the maximum permissions for an organization, organizational unit (OU), or account."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a Resource-based policy on the newly created resources to add to the existing permissions of the IAM roles accessing the created resources</strong> - If you have to create a Resource-based policy for every new resource that is added to the account, then we are pretty much at the same place without offering any solution. Hence, this option is not a valid choice for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Access control lists (ACLs) to allow access to certain principals in the account. Add the ACLs to a session policy to dynamically add permissions to the IAM role that the user already has</strong> - Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account."
      },
      {
        "answer": "",
        "explanation": "Session policies are advanced policies that you pass as a parameter when you programmatically create a temporary session for a role or federated user. The permissions for a session are the intersection of the identity-based policies for the IAM entity (user or role) used to create the session and the session policies."
      },
      {
        "answer": "",
        "explanation": "Neither ACL nor a session policy is the right choice for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_session"
    ]
  },
  {
    "id": 28,
    "question": "<p>A support team wants to be notified via an Amazon Simple Notification Service (Amazon SNS) notification when an AWS Glue job fails a retry.</p>\n\n<p>As a DevOps Engineer, how will you implement this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Amazon EventBridge events for AWS Glue. Configure an Amazon Simple Notification Service (Amazon SNS) notification when the Glue job fails a retry</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Check the AWS Personal Health Dashboard for failed AWS Glue jobs. Schedule an AWS Lambda function to pick the failed event from the service health dashboard and trigger an Amazon Simple Notification Service (Amazon SNS) notification when a retry fails</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Simple Notification Service (Amazon SNS) cannot retry failures, leverage Amazon Simple Queue Service (Amazon SQS) dead-letter queues to retry the failed Glue jobs</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure Amazon EventBridge events for AWS Glue. Define the AWS Lambda function as a target to the EventBridge. The Lambda function will have the logic to process the events and filter the AWS Glue job retry failure event. Publish a message to Amazon Simple Notification Service (Amazon SNS) notification if such an event is found</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 5: Incident and Event Response",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon EventBridge events for AWS Glue. Define the AWS Lambda function as a target to the EventBridge. The Lambda function will have the logic to process the events and filter the AWS Glue job retry failure event. Publish a message to Amazon Simple Notification Service (Amazon SNS) notification if such an event is found</strong></p>\n\n<p>Amazon EventBridge events for AWS Glue can be used to create Amazon SNS alerts, but the alerts might not be specific enough for certain situations. To receive SNS notifications for certain AWS Glue Events, such as an AWS Glue job failing on retry, you can use AWS Lambda. You can create a Lambda function to do the following:</p>\n\n<ol>\n<li>Check the incoming event for a specific string.</li>\n<li>Publish a message to Amazon SNS if the string in the event matches the string in the Lambda function.</li>\n</ol>\n\n<p>To use an AWS Lambda function to receive an email from SNS when any of your AWS Glue jobs fail a retry, do the following:\n1. Create an Amazon SNS topic.\n2. Create an AWS Lambda function.\n3. Create an Amazon EventBridge event that uses the Lambda function to initiate email notifications.</p>\n\n<p>AWS Lambda function logic:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q28-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q28-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts\">https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon EventBridge events for AWS Glue. Configure an Amazon Simple Notification Service (Amazon SNS) notification when the Glue job fails a retry</strong> - Amazon EventBridge cannot be directly used without a Lambda function since the use case needs notifications only for Glue job retry failure. So this logic has to be included in a Lambda function.</p>\n\n<p><strong>Amazon Simple Notification Service (Amazon SNS) cannot retry failures, leverage Amazon Simple Queue Service (Amazon SQS) dead-letter queues to retry the failed Glue jobs</strong> - The use case is not about retrying the Glue job, but about sending an SNS notification when the retry of the job fails.</p>\n\n<p><strong>Check the AWS Personal Health Dashboard for failed AWS Glue jobs. Schedule an AWS Lambda function to pick the failed event from the service health dashboard and trigger an Amazon Simple Notification Service (Amazon SNS) notification when a retry fails</strong> - AWS Personal Health Dashboard provides proactive notifications of scheduled activities, such as any changes to the infrastructure powering your resources, enabling you to better plan for events that may affect you. This option is not relevant to the given requirements.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts\">https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon EventBridge events for AWS Glue. Define the AWS Lambda function as a target to the EventBridge. The Lambda function will have the logic to process the events and filter the AWS Glue job retry failure event. Publish a message to Amazon Simple Notification Service (Amazon SNS) notification if such an event is found</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon EventBridge events for AWS Glue can be used to create Amazon SNS alerts, but the alerts might not be specific enough for certain situations. To receive SNS notifications for certain AWS Glue Events, such as an AWS Glue job failing on retry, you can use AWS Lambda. You can create a Lambda function to do the following:"
      },
      {},
      {
        "answer": "",
        "explanation": "To use an AWS Lambda function to receive an email from SNS when any of your AWS Glue jobs fail a retry, do the following:\n1. Create an Amazon SNS topic.\n2. Create an AWS Lambda function.\n3. Create an Amazon EventBridge event that uses the Lambda function to initiate email notifications."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q28-i1.jpg",
        "answer": "",
        "explanation": "AWS Lambda function logic:"
      },
      {
        "link": "https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon EventBridge events for AWS Glue. Configure an Amazon Simple Notification Service (Amazon SNS) notification when the Glue job fails a retry</strong> - Amazon EventBridge cannot be directly used without a Lambda function since the use case needs notifications only for Glue job retry failure. So this logic has to be included in a Lambda function."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Notification Service (Amazon SNS) cannot retry failures, leverage Amazon Simple Queue Service (Amazon SQS) dead-letter queues to retry the failed Glue jobs</strong> - The use case is not about retrying the Glue job, but about sending an SNS notification when the retry of the job fails."
      },
      {
        "answer": "",
        "explanation": "<strong>Check the AWS Personal Health Dashboard for failed AWS Glue jobs. Schedule an AWS Lambda function to pick the failed event from the service health dashboard and trigger an Amazon Simple Notification Service (Amazon SNS) notification when a retry fails</strong> - AWS Personal Health Dashboard provides proactive notifications of scheduled activities, such as any changes to the infrastructure powering your resources, enabling you to better plan for events that may affect you. This option is not relevant to the given requirements."
      }
    ],
    "references": [
      "https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts"
    ]
  },
  {
    "id": 29,
    "question": "<p>A web application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). While using CodeDeploy Blue/Green deployment to deploy a new version, the deployment failed during the <code>AllowTraffic</code> lifecycle event. The DevOps team has found no errors in the deployment logs.</p>\n\n<p>Which of the following would you identify as the root cause behind the failure of the deployment?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The cause of the failure could be a script from the last successful deployment that never runs successfully. Create a new deployment and specify that the ApplicationStop, BeforeBlockTraffic, and AfterBlockTraffic failures should be ignored</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Incorrectly configured health checks on Application Load Balancer (ALB) are responsible for this issue</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>If an instance is terminated between lifecycle events or before the first lifecycle event step starts, then <code>AllowTraffic</code> lifecycle event fails without generating logs</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A scale-in event or any other termination event, during an in-progress deployment, causes the instance to detach from the Amazon EC2 Auto Scaling group and the instance fails the <code>AllowTraffic</code> lifecycle event</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: SDLC Automation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Incorrectly configured health checks on Application Load Balancer (ALB) are responsible for this issue</strong></p>\n\n<p>In some cases, a Blue/Green deployment fails during the AllowTraffic lifecycle event, but the deployment logs do not indicate the cause for the failure.</p>\n\n<p>This failure is typically due to incorrectly configured health checks in Elastic Load Balancing for the Classic Load Balancer, Application Load Balancer, or Network Load Balancer used to manage traffic for the deployment group.</p>\n\n<p>To resolve the issue, review and correct any errors in the health check configuration for the load balancer.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The cause of the failure could be a script from the last successful deployment that never runs successfully. Create a new deployment and specify that the ApplicationStop, BeforeBlockTraffic, and AfterBlockTraffic failures should be ignored</strong> - During a deployment, the CodeDeploy agent runs the scripts specified for ApplicationStop, BeforeBlockTraffic, and AfterBlockTraffic in the AppSpec file from the previous successful deployment. (All other scripts are run from the AppSpec file in the current deployment.) If one of these scripts contains an error and does not run successfully, the deployment can fail. This kind of failure generates logs and will not fail at <code>AllowTraffic</code> lifecycle event.</p>\n\n<p><strong>If an instance is terminated between lifecycle events or before the first lifecycle event step starts, then <code>AllowTraffic</code> lifecycle event fails without generating logs</strong> - This statement is incorrect. Deployments do not fail for up to an hour when an instance is terminated during a deployment.</p>\n\n<p><strong>A scale-in event or any other termination event, during an in-progress deployment, causes the instance to detach from the Amazon EC2 Auto Scaling group and the instance fails the <code>AllowTraffic</code> lifecycle event</strong> - During an in-progress deployment, a scale-in event or any other termination event causes the instance to detach from the Amazon EC2 Auto Scaling group and then terminate. Because the deployment cannot be completed, it fails. This error is specific to Auto Scaling Groups (ASG) and hence does not relate to the given issue.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-ec2-instances.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-ec2-instances.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Incorrectly configured health checks on Application Load Balancer (ALB) are responsible for this issue</strong>"
      },
      {
        "answer": "",
        "explanation": "In some cases, a Blue/Green deployment fails during the AllowTraffic lifecycle event, but the deployment logs do not indicate the cause for the failure."
      },
      {
        "answer": "",
        "explanation": "This failure is typically due to incorrectly configured health checks in Elastic Load Balancing for the Classic Load Balancer, Application Load Balancer, or Network Load Balancer used to manage traffic for the deployment group."
      },
      {
        "answer": "",
        "explanation": "To resolve the issue, review and correct any errors in the health check configuration for the load balancer."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The cause of the failure could be a script from the last successful deployment that never runs successfully. Create a new deployment and specify that the ApplicationStop, BeforeBlockTraffic, and AfterBlockTraffic failures should be ignored</strong> - During a deployment, the CodeDeploy agent runs the scripts specified for ApplicationStop, BeforeBlockTraffic, and AfterBlockTraffic in the AppSpec file from the previous successful deployment. (All other scripts are run from the AppSpec file in the current deployment.) If one of these scripts contains an error and does not run successfully, the deployment can fail. This kind of failure generates logs and will not fail at <code>AllowTraffic</code> lifecycle event."
      },
      {
        "answer": "",
        "explanation": "<strong>If an instance is terminated between lifecycle events or before the first lifecycle event step starts, then <code>AllowTraffic</code> lifecycle event fails without generating logs</strong> - This statement is incorrect. Deployments do not fail for up to an hour when an instance is terminated during a deployment."
      },
      {
        "answer": "",
        "explanation": "<strong>A scale-in event or any other termination event, during an in-progress deployment, causes the instance to detach from the Amazon EC2 Auto Scaling group and the instance fails the <code>AllowTraffic</code> lifecycle event</strong> - During an in-progress deployment, a scale-in event or any other termination event causes the instance to detach from the Amazon EC2 Auto Scaling group and then terminate. Because the deployment cannot be completed, it fails. This error is specific to Auto Scaling Groups (ASG) and hence does not relate to the given issue."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html",
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-ec2-instances.html"
    ]
  },
  {
    "id": 30,
    "question": "<p>A data analytics company wants to move all its clients belonging to the regulated and security-sensitive industries such as financial services and healthcare to the AWS Cloud as it wants to leverage the out-of-box security-specific capabilities offered by AWS. The DevOps team at the company is developing a framework to validate the adoption of AWS best practices and industry-recognized compliance standards. The AWS Management Console is the preferred method for the in-house teams wanting to provision resources.</p>\n\n<p>Which of the following strategies would you adopt to address these business requirements for continuously assessing, auditing and monitoring the configurations of AWS resources? (Select two)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage AWS Config rules to audit changes to AWS resources and monitor the compliance of the configuration by running the evaluations for the rule at a frequency that you choose. Develop AWS Config custom rules to establish a test-driven development approach by triggering the evaluation when any resource that matches the rule's scope changes in configuration</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Leverage CloudTrail integration with SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services. Use Lambda functions to automatically revert non-authorized changes in AWS resources</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable trails and set up CloudTrail events to review and monitor management activities of all AWS accounts by logging these activities into CloudWatch Logs using a KMS key. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Leverage EventBridge events near-real-time capabilities to monitor system events patterns to trigger Lambda functions to automatically revert non-authorized changes in AWS resources. Send notifications via SNS topics to improve the incidence response time</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Leverage CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that match mutating API calls. Use CloudWatch alarms to send notifications via SNS when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and analyze via Athena</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 6: Security and Compliance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Leverage AWS Config rules to audit changes to AWS resources and monitor the compliance of the configuration by running the evaluations for the rule at a frequency that you choose. Develop AWS Config custom rules to establish a test-driven development approach by triggering the evaluation when any resource that matches the rule's scope changes in configuration</strong></p>\n\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”.</p>\n\n<p>How AWS Config Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a><p></p>\n\n<p>For the given use case, you can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. You can also create your own custom rules. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes violate any of the conditions in your rules. If a resource violates a rule, AWS Config flags the resource and marks the rule as non-compliant.</p>\n\n<p>There are two types of evaluation trigger types for Config rules:</p>\n\n<p>Configuration changes – AWS Config triggers the evaluation when any resource that matches the rule's scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification.</p>\n\n<p>Periodic – AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p>\n\n<p><strong>Enable trails and set up CloudTrail events to review and monitor management activities of all AWS accounts by logging these activities into CloudWatch Logs using a KMS key. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services</strong></p>\n\n<p>CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or AWS service are recorded as events in CloudTrail. An event in CloudTrail is the record of activity in an AWS account. This activity can be an action taken by a user, role, or service that is monitorable by CloudTrail. CloudTrail events provide a history of both API and non-API account activity made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.</p>\n\n<p>CloudTrail data events are disabled by default. You can enable logging at an additional cost. Data events are also known as data plane operations and are often high-volume activities. Data events aren't viewable in CloudTrail event history and are charged for all copies at a reduced rate compared to management events.</p>\n\n<p>CloudTrail records management events for the last 90 days free of charge, and are viewable in the Event History with the CloudTrail console. For Amazon S3 delivery of CloudTrail events, the first copy delivered is free. Additional copies of management events are charged.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q43-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q43-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage the CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that match mutating API calls. Use CloudWatch alarms to send notifications via SNS when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and analyze via Athena</strong> - One of the key constraints for the given scenario is that the AWS Management Console is the preferred method for the in-house teams wanting to provision resources. Although this option is technically feasible, it focuses on using the CloudWatch Logs agent to collect all the AWS SDK logs. The given use case has no specific requirements for AWS SDKs or AWS APIs because AWS Management Console is the preferred method to provision resources. So this option is not the best-fit solution.</p>\n\n<p><strong>Leverage CloudTrail integration with SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services. Use Lambda functions to automatically revert non-authorized changes in AWS resources</strong> - One of the key constraints for the given scenario is that the AWS Management Console is the preferred method for the in-house teams wanting to provision resources. Although this option is technically feasible, it focuses on capturing unauthorized API activities. The given use case has no specific requirements for AWS SDKs or AWS APIs because AWS Management Console is the preferred method to provision resources. In addition, the use case just talks about assessing, auditing, and monitoring the configurations of AWS resources. Reverting non-authorized changes in AWS resources is not part of the mandate. So this option is not correct.</p>\n\n<p><strong>Leverage EventBridge events near-real-time capabilities to monitor system event patterns to trigger Lambda functions to automatically revert non-authorized changes in AWS resources. Send notifications via SNS topics to improve the incidence response time</strong> - The use-case just talks about assessing, auditing and monitoring the configurations of AWS resources. Reverting non-authorized changes in AWS resources is not part of the mandate. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_manage-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_manage-rules.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Config rules to audit changes to AWS resources and monitor the compliance of the configuration by running the evaluations for the rule at a frequency that you choose. Develop AWS Config custom rules to establish a test-driven development approach by triggering the evaluation when any resource that matches the rule's scope changes in configuration</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png",
        "answer": "",
        "explanation": "How AWS Config Works:"
      },
      {
        "link": "https://aws.amazon.com/config/"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. You can also create your own custom rules. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes violate any of the conditions in your rules. If a resource violates a rule, AWS Config flags the resource and marks the rule as non-compliant."
      },
      {
        "answer": "",
        "explanation": "There are two types of evaluation trigger types for Config rules:"
      },
      {
        "answer": "",
        "explanation": "Configuration changes – AWS Config triggers the evaluation when any resource that matches the rule's scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification."
      },
      {
        "answer": "",
        "explanation": "Periodic – AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours)."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable trails and set up CloudTrail events to review and monitor management activities of all AWS accounts by logging these activities into CloudWatch Logs using a KMS key. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services</strong>"
      },
      {
        "answer": "",
        "explanation": "CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or AWS service are recorded as events in CloudTrail. An event in CloudTrail is the record of activity in an AWS account. This activity can be an action taken by a user, role, or service that is monitorable by CloudTrail. CloudTrail events provide a history of both API and non-API account activity made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services."
      },
      {
        "answer": "",
        "explanation": "CloudTrail data events are disabled by default. You can enable logging at an additional cost. Data events are also known as data plane operations and are often high-volume activities. Data events aren't viewable in CloudTrail event history and are charged for all copies at a reduced rate compared to management events."
      },
      {
        "answer": "",
        "explanation": "CloudTrail records management events for the last 90 days free of charge, and are viewable in the Event History with the CloudTrail console. For Amazon S3 delivery of CloudTrail events, the first copy delivered is free. Additional copies of management events are charged."
      },
      {
        "link": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage the CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that match mutating API calls. Use CloudWatch alarms to send notifications via SNS when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and analyze via Athena</strong> - One of the key constraints for the given scenario is that the AWS Management Console is the preferred method for the in-house teams wanting to provision resources. Although this option is technically feasible, it focuses on using the CloudWatch Logs agent to collect all the AWS SDK logs. The given use case has no specific requirements for AWS SDKs or AWS APIs because AWS Management Console is the preferred method to provision resources. So this option is not the best-fit solution."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage CloudTrail integration with SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services. Use Lambda functions to automatically revert non-authorized changes in AWS resources</strong> - One of the key constraints for the given scenario is that the AWS Management Console is the preferred method for the in-house teams wanting to provision resources. Although this option is technically feasible, it focuses on capturing unauthorized API activities. The given use case has no specific requirements for AWS SDKs or AWS APIs because AWS Management Console is the preferred method to provision resources. In addition, the use case just talks about assessing, auditing, and monitoring the configurations of AWS resources. Reverting non-authorized changes in AWS resources is not part of the mandate. So this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage EventBridge events near-real-time capabilities to monitor system event patterns to trigger Lambda functions to automatically revert non-authorized changes in AWS resources. Send notifications via SNS topics to improve the incidence response time</strong> - The use-case just talks about assessing, auditing and monitoring the configurations of AWS resources. Reverting non-authorized changes in AWS resources is not part of the mandate. So this option is not correct."
      }
    ],
    "references": [
      "https://aws.amazon.com/config/",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events",
      "https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_manage-rules.html"
    ]
  },
  {
    "id": 31,
    "question": "<p>An application runs on a fleet of Amazon EC2 instances that are configured with an Auto Scaling group (ASG). Both Spot and On-Demand instances are utilized as per the ASG configuration. For the most part, the ASG seems to be working fine as expected. There are a few issues that the DevOps team has flagged:</p>\n\n<p>a) During a scale-in activity, ASG has terminated instance in the Availability Zone (AZ) that already had fewer instances than the other\nb) For some duration, ASG exceeded the specified maximum capacity of the group</p>\n\n<p>What reasons can you identify for this behavior? (Select two)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The instance that was launched from the oldest launch template or launch configuration is terminated first. This can temporarily lead to an unbalanced group</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling will first identify which of the two types (Spot or On-Demand) should be terminated. This can temporarily cause a misbalance between the AZs</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>With Default termination policy, instances that are closest to the next billing hour are terminated first. This can temporarily cause instance distribution imbalance among the AZs</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon EC2 Auto Scaling can temporarily exceed the specified maximum capacity of a group by a 10 percent margin (or by a margin of one instance, whichever is greater) during a rebalancing activity</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>You can only specify one Lambda function in the termination policies for an Auto Scaling group. If more than one Lambda functions are configured, the ASG behavior is ambiguous</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 5: Incident and Event Response",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Amazon EC2 Auto Scaling can temporarily exceed the specified maximum capacity of a group by a 10 percent margin (or by a margin of one instance, whichever is greater) during a rebalancing activity</strong></p>\n\n<p>When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application.</p>\n\n<p>Because Amazon EC2 Auto Scaling attempts to launch new instances before terminating the old ones, being at or near the specified maximum capacity could impede or completely stop rebalancing activities. To avoid this problem, the system can temporarily exceed the specified maximum capacity of a group by a 10 percent margin (or by a margin of one instance, whichever is greater) during a rebalancing activity. The margin is extended only if the group is at or near maximum capacity and needs rebalancing, either because of user-requested rezoning or to compensate for zone availability issues. The extension lasts only as long as needed to rebalance the group.</p>\n\n<p><strong>When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling will first identify which of the two types (Spot or On-Demand) should be terminated. This can temporarily cause a misbalance between the AZs</strong></p>\n\n<p>When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling still uses termination policies to prioritize which instances to terminate, but first it identifies which of the two types (Spot or On-Demand) should be terminated. It then applies the termination policies in each Availability Zone individually. It also identifies which instances (within the identified purchase option) in which Availability Zones to terminate that will result in the Availability Zones being most balanced.</p>\n\n<p>Understanding ASG termination policy for mixed instances groups:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q25-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q25-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The instance that was launched from the oldest launch template or launch configuration is terminated first. This can temporarily lead to an unbalanced group</strong> - Amazon EC2 Auto Scaling always balances instances across Availability Zones first, regardless of which termination policy is used. As a result, you might encounter situations in which some newer instances are terminated before older instances. So this option is incorrect.</p>\n\n<p><strong>You can only specify one Lambda function in the termination policies for an Auto Scaling group. If more than one Lambda functions are configured, the ASG behavior is ambiguous</strong> - It is not possible to configure more than one AWS Lambda function in the termination policies for an Auto Scaling group. Also, the AWS Lambda function is used to configure custom termination policy and hence is not related to the given use case.</p>\n\n<p><strong>With the Default termination policy, instances that are closest to the next billing hour are terminated first. This can temporarily cause instance distribution imbalance among the AZs</strong> - This statement is incorrect. ASG uses multiple termination criteria before selecting an instance for termination, and not just the next billing hour as the only criterion.</p>\n\n<p>Understanding the default termination policy of ASG:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q25-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q25-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups</a><p></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 Auto Scaling can temporarily exceed the specified maximum capacity of a group by a 10 percent margin (or by a margin of one instance, whichever is greater) during a rebalancing activity</strong>"
      },
      {
        "answer": "",
        "explanation": "When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application."
      },
      {
        "answer": "",
        "explanation": "Because Amazon EC2 Auto Scaling attempts to launch new instances before terminating the old ones, being at or near the specified maximum capacity could impede or completely stop rebalancing activities. To avoid this problem, the system can temporarily exceed the specified maximum capacity of a group by a 10 percent margin (or by a margin of one instance, whichever is greater) during a rebalancing activity. The margin is extended only if the group is at or near maximum capacity and needs rebalancing, either because of user-requested rezoning or to compensate for zone availability issues. The extension lasts only as long as needed to rebalance the group."
      },
      {
        "answer": "",
        "explanation": "<strong>When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling will first identify which of the two types (Spot or On-Demand) should be terminated. This can temporarily cause a misbalance between the AZs</strong>"
      },
      {
        "answer": "",
        "explanation": "When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling still uses termination policies to prioritize which instances to terminate, but first it identifies which of the two types (Spot or On-Demand) should be terminated. It then applies the termination policies in each Availability Zone individually. It also identifies which instances (within the identified purchase option) in which Availability Zones to terminate that will result in the Availability Zones being most balanced."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q25-i1.jpg",
        "answer": "",
        "explanation": "Understanding ASG termination policy for mixed instances groups:"
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The instance that was launched from the oldest launch template or launch configuration is terminated first. This can temporarily lead to an unbalanced group</strong> - Amazon EC2 Auto Scaling always balances instances across Availability Zones first, regardless of which termination policy is used. As a result, you might encounter situations in which some newer instances are terminated before older instances. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>You can only specify one Lambda function in the termination policies for an Auto Scaling group. If more than one Lambda functions are configured, the ASG behavior is ambiguous</strong> - It is not possible to configure more than one AWS Lambda function in the termination policies for an Auto Scaling group. Also, the AWS Lambda function is used to configure custom termination policy and hence is not related to the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>With the Default termination policy, instances that are closest to the next billing hour are terminated first. This can temporarily cause instance distribution imbalance among the AZs</strong> - This statement is incorrect. ASG uses multiple termination criteria before selecting an instance for termination, and not just the next billing hour as the only criterion."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q25-i2.jpg",
        "answer": "",
        "explanation": "Understanding the default termination policy of ASG:"
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html"
    ]
  },
  {
    "id": 32,
    "question": "<p>A company has configured AWS Organizations to manage its multiple AWS accounts. The company uses Amazon Elastic File System (Amazon EFS) as a shared storage service, configured in AWS Account A of the company. To implement a serverless architecture, the company has decided to move its applications to AWS Lambda. The Lambda functions will be managed through another AWS account (Account B). All the Lambda functions will be deployed in a VPC. A DevOps team needs help to continue using Amazon EFS in Account A with the Lambda function in Account B.</p>\n\n<p>How will you reconfigure the existing EFS file system for use with AWS Lambda function? (Select two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a VPC peering connection to connect Account A to Account B. Create Service control policies (SCPs) to set permission guardrails for access to Amazon EFS from AWS Lambda function execution role</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Update the Lambda execution roles with permission to access the VPC and the EFS file system</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure the Lambda functions in Account B to assume an existing IAM role in Account A for the cross-region, or cross-AZ connectivity between EFS and Lambda</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Lambda charges for data transfer between VPCs. To manage costs, create a new EFS file system in Account B. Configure AWS DataSync to transfer data from EFS in Account B to EFS in Account B</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Resilient Cloud Solutions",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A</strong></p>\n\n<p><strong>Update the Lambda execution roles with permission to access the VPC and the EFS file system</strong></p>\n\n<p>You can configure a function to mount an Amazon Elastic File System (Amazon EFS) file system to a local directory. With Amazon EFS, your function code can access and modify shared resources safely and at high concurrency.</p>\n\n<p>Execution role and user permissions: If the file system doesn't have a user-configured AWS Identity and Access Management (IAM) policy, EFS uses a default policy that grants full access to any client that can connect to the file system using a file system mount target. If the file system has a user-configured IAM policy, your function's execution role must have the correct <code>elasticfilesystem</code> permissions.</p>\n\n<p>Configuring a file system and access point: To connect an EFS file system with a Lambda function, you use an EFS access point, an application-specific entry point into an EFS file system that includes the operating system user and group to use when accessing the file system, file system permissions, and can limit access to a specific path in the file system. This helps keep file system configuration decoupled from the application code.</p>\n\n<p>You can access the same EFS file system from multiple functions, using the same or different access points. For example, using different EFS access points, each Lambda function can access different paths in a file system, or use different file system permissions.</p>\n\n<p>Connecting to a file system: A function connects to a file system over the local network in a VPC. The subnets that your function connects to can be the same subnets that contain mount points for your file system, or subnets in the same Availability Zone that can route NFS traffic (port 2049) to the file system. To mount an EFS file system, your Lambda functions must be connected to an Amazon Virtual Private Cloud (Amazon VPC) that can reach the EFS mount targets.</p>\n\n<p>A Lambda function in one account can mount a file system in a different account. For this scenario, you configure VPC peering between the function VPC and the file system VPC.</p>\n\n<p>An example showcasing the use of EFS with AWS Lambda:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q8-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q8-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/\">https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a VPC peering connection to connect Account A to Account B. Create Service control policies (SCPs) to set permission guardrails for access to Amazon EFS from AWS Lambda function execution role</strong> - A VPC peering connection is needed since EFS and AWS Lambda are in different AWS accounts. SCPs alone are not sufficient to grant permissions to the accounts in your organization. You should note that no permissions are granted by an SCP. The Lambda execution role will need explicit permissions for access to EFS.</p>\n\n<p><strong>Configure the Lambda functions in Account B to assume an existing IAM role in Account A for the cross-region or cross-AZ connectivity between EFS and Lambda</strong> - This statement is incorrect. AWS does not support cross-region, or cross AZ connectivity between EFS and Lambda.</p>\n\n<p><strong>Lambda charges for data transfer between VPCs. To manage costs, create a new EFS file system in Account B. Configure AWS DataSync to transfer data from EFS in Account B to EFS in Account B</strong> - It is mentioned in the use case that EFS and AWS Lambda will remain in their respective accounts. Hence, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Update the Lambda execution roles with permission to access the VPC and the EFS file system</strong>"
      },
      {
        "answer": "",
        "explanation": "You can configure a function to mount an Amazon Elastic File System (Amazon EFS) file system to a local directory. With Amazon EFS, your function code can access and modify shared resources safely and at high concurrency."
      },
      {
        "answer": "",
        "explanation": "Execution role and user permissions: If the file system doesn't have a user-configured AWS Identity and Access Management (IAM) policy, EFS uses a default policy that grants full access to any client that can connect to the file system using a file system mount target. If the file system has a user-configured IAM policy, your function's execution role must have the correct <code>elasticfilesystem</code> permissions."
      },
      {
        "answer": "",
        "explanation": "Configuring a file system and access point: To connect an EFS file system with a Lambda function, you use an EFS access point, an application-specific entry point into an EFS file system that includes the operating system user and group to use when accessing the file system, file system permissions, and can limit access to a specific path in the file system. This helps keep file system configuration decoupled from the application code."
      },
      {
        "answer": "",
        "explanation": "You can access the same EFS file system from multiple functions, using the same or different access points. For example, using different EFS access points, each Lambda function can access different paths in a file system, or use different file system permissions."
      },
      {
        "answer": "",
        "explanation": "Connecting to a file system: A function connects to a file system over the local network in a VPC. The subnets that your function connects to can be the same subnets that contain mount points for your file system, or subnets in the same Availability Zone that can route NFS traffic (port 2049) to the file system. To mount an EFS file system, your Lambda functions must be connected to an Amazon Virtual Private Cloud (Amazon VPC) that can reach the EFS mount targets."
      },
      {
        "answer": "",
        "explanation": "A Lambda function in one account can mount a file system in a different account. For this scenario, you configure VPC peering between the function VPC and the file system VPC."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q8-i1.jpg",
        "answer": "",
        "explanation": "An example showcasing the use of EFS with AWS Lambda:"
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a VPC peering connection to connect Account A to Account B. Create Service control policies (SCPs) to set permission guardrails for access to Amazon EFS from AWS Lambda function execution role</strong> - A VPC peering connection is needed since EFS and AWS Lambda are in different AWS accounts. SCPs alone are not sufficient to grant permissions to the accounts in your organization. You should note that no permissions are granted by an SCP. The Lambda execution role will need explicit permissions for access to EFS."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure the Lambda functions in Account B to assume an existing IAM role in Account A for the cross-region or cross-AZ connectivity between EFS and Lambda</strong> - This statement is incorrect. AWS does not support cross-region, or cross AZ connectivity between EFS and Lambda."
      },
      {
        "answer": "",
        "explanation": "<strong>Lambda charges for data transfer between VPCs. To manage costs, create a new EFS file system in Account B. Configure AWS DataSync to transfer data from EFS in Account B to EFS in Account B</strong> - It is mentioned in the use case that EFS and AWS Lambda will remain in their respective accounts. Hence, this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/",
      "https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html"
    ]
  },
  {
    "id": 33,
    "question": "<p>A media application runs on a host of Amazon EC2 instances fronted with an Application Load Balancer (ALB) and Amazon S3 buckets as storage service. For enhanced security, an AWS Web Application Firewall (AWS WAF) has been set up to monitor the requests coming to the ALB. The DevOps team needs to submit a quarterly report on the web requests received by AWS WAF, having detailed information about each web request as well as the details about rules that the request matched. The team has reached out to you for implementing the changes needed for collecting the security data for the coming months.</p>\n\n<p>As DevOps Engineer, how will you implement this requirement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data firehose name should start with the prefix <code>aws-waf-logs-</code></p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Monitoring and Logging",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</strong></p>\n\n<p>You can enable logging AWS WAF web ACL traffic, to get detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched. You can send your logs to an Amazon CloudWatch Logs log group, an Amazon Simple Storage Service (Amazon S3) bucket, or an Amazon Kinesis Data Firehose.</p>\n\n<p>To send your web ACL traffic logs to Amazon S3, you need to set up an Amazon S3 bucket for the logs. When you enable logging for AWS WAF, you provide the bucket ARN. Your web ACLs publish their log files to the Amazon S3 bucket at 5-minute intervals. Each log file contains log records for the traffic recorded in the previous 5 minutes.</p>\n\n<p>The maximum file size for a log file is 75 MB. If the log file reaches the file size limit within the 5-minute period, the log stops adding records to it, publishes it to the Amazon S3 bucket, and then creates a new log file.</p>\n\n<p>Your bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For example, <code>aws-waf-logs-DOC-EXAMPLE-BUCKET-SUFFIX</code>.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</strong> - AWS WAF supports encryption with Amazon S3 buckets for key type Amazon S3 key (SSE-S3) and AWS Key Management Service (SSE-KMS) AWS KMS keys. AWS WAF doesn't support encryption for AWS Key Management Service keys that are managed by AWS.</p>\n\n<p><strong>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as a WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data Firehose name should start with the prefix <code>aws-waf-logs-</code></strong> - AWS recommends that the Kinesis stream should not be used as a source when configuring Data Firehose as a WAF logging destination.</p>\n\n<p><strong>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</strong>"
      },
      {
        "answer": "",
        "explanation": "You can enable logging AWS WAF web ACL traffic, to get detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched. You can send your logs to an Amazon CloudWatch Logs log group, an Amazon Simple Storage Service (Amazon S3) bucket, or an Amazon Kinesis Data Firehose."
      },
      {
        "answer": "",
        "explanation": "To send your web ACL traffic logs to Amazon S3, you need to set up an Amazon S3 bucket for the logs. When you enable logging for AWS WAF, you provide the bucket ARN. Your web ACLs publish their log files to the Amazon S3 bucket at 5-minute intervals. Each log file contains log records for the traffic recorded in the previous 5 minutes."
      },
      {
        "answer": "",
        "explanation": "The maximum file size for a log file is 75 MB. If the log file reaches the file size limit within the 5-minute period, the log stops adding records to it, publishes it to the Amazon S3 bucket, and then creates a new log file."
      },
      {
        "answer": "",
        "explanation": "Your bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For example, <code>aws-waf-logs-DOC-EXAMPLE-BUCKET-SUFFIX</code>."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</strong> - AWS WAF supports encryption with Amazon S3 buckets for key type Amazon S3 key (SSE-S3) and AWS Key Management Service (SSE-KMS) AWS KMS keys. AWS WAF doesn't support encryption for AWS Key Management Service keys that are managed by AWS."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as a WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data Firehose name should start with the prefix <code>aws-waf-logs-</code></strong> - AWS recommends that the Kinesis stream should not be used as a source when configuring Data Firehose as a WAF logging destination."
      },
      {
        "answer": "",
        "explanation": "<strong>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</strong> - This is a made-up option, given only as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html",
      "https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html"
    ]
  },
  {
    "id": 34,
    "question": "<p>An e-commerce company is deploying its flagship application on Amazon EC2 instances. The DevOps team at the company needs a solution to query both the application logs as well as the AWS account API activity.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you recommend to meet these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize Amazon Athena to query both sets of logs</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up AWS CloudTrail to deliver the API logs to CloudWatch Logs. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize the CloudWatch Logs Insights to query both sets of logs</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. Utilize Amazon Athena to query both sets of logs</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up AWS CloudTrail to deliver the API logs to Kinesis Data Streams. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Kinesis Data Streams. Direct both the Kinesis Data Streams to direct the stream output to Kinesis Data Analytics for running near-real-time queries on both sets of logs</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Monitoring and Logging",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to CloudWatch Logs. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize the CloudWatch Logs Insights to query both sets of logs</strong></p>\n\n<p>CloudTrail is enabled by default for your AWS account. You can use Event history in the CloudTrail console to view, search, download, archive, analyze, and respond to account activity across your AWS infrastructure. This includes activity made through the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs. For an ongoing record of events in your AWS account, you can create a trail. A trail enables CloudTrail to deliver log files to an Amazon S3 bucket. By default, when you create a trail in the console, the trail applies to all AWS Regions. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs.</p>\n\n<p>You can also configure CloudTrail with CloudWatch Logs to monitor your trail API logs and be notified when specific activity occurs. When you configure your trail to send events to CloudWatch Logs, CloudTrail sends only the events that match your trail settings. For example, if you configure your trail to log data events only, your trail sends data events only to your CloudWatch Logs log group. CloudTrail supports sending data, Insights, and management events to CloudWatch Logs.</p>\n\n<p>You can collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch agent. You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is CWAgent, although you can specify a different namespace when you configure the agent. The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs</p>\n\n<p>For the given use case, you can have both AWS CloudTrail as well as the Amazon CloudWatch Agent deliver the respective logs to CloudWatch Logs. You can then use the CloudWatch Logs Insights to query both sets of logs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. Utilize Amazon Athena to query both sets of logs</strong> - You cannot use the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. So, this option is incorrect.</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Kinesis Data Streams. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Kinesis Data Streams. Direct both the Kinesis Data Streams to direct the stream output to Kinesis Data Analytics for running near-real-time queries on both sets of logs</strong> - You cannot deliver the API logs from AWS CloudTrail to Kinesis Data Streams. Similarly, you cannot have the Amazon CloudWatch Agent deliver logs from the EC2 instances to Kinesis Data Streams. So, both these options are incorrect.</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize Amazon Athena to query both sets of logs</strong> - You cannot use Amazon Athena to query logs published to Amazon CloudWatch Logs, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-getting-started.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-getting-started.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up AWS CloudTrail to deliver the API logs to CloudWatch Logs. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize the CloudWatch Logs Insights to query both sets of logs</strong>"
      },
      {
        "answer": "",
        "explanation": "CloudTrail is enabled by default for your AWS account. You can use Event history in the CloudTrail console to view, search, download, archive, analyze, and respond to account activity across your AWS infrastructure. This includes activity made through the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs. For an ongoing record of events in your AWS account, you can create a trail. A trail enables CloudTrail to deliver log files to an Amazon S3 bucket. By default, when you create a trail in the console, the trail applies to all AWS Regions. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs."
      },
      {
        "answer": "",
        "explanation": "You can also configure CloudTrail with CloudWatch Logs to monitor your trail API logs and be notified when specific activity occurs. When you configure your trail to send events to CloudWatch Logs, CloudTrail sends only the events that match your trail settings. For example, if you configure your trail to log data events only, your trail sends data events only to your CloudWatch Logs log group. CloudTrail supports sending data, Insights, and management events to CloudWatch Logs."
      },
      {
        "answer": "",
        "explanation": "You can collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch agent. You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is CWAgent, although you can specify a different namespace when you configure the agent. The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can have both AWS CloudTrail as well as the Amazon CloudWatch Agent deliver the respective logs to CloudWatch Logs. You can then use the CloudWatch Logs Insights to query both sets of logs."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. Utilize Amazon Athena to query both sets of logs</strong> - You cannot use the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up AWS CloudTrail to deliver the API logs to Kinesis Data Streams. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Kinesis Data Streams. Direct both the Kinesis Data Streams to direct the stream output to Kinesis Data Analytics for running near-real-time queries on both sets of logs</strong> - You cannot deliver the API logs from AWS CloudTrail to Kinesis Data Streams. Similarly, you cannot have the Amazon CloudWatch Agent deliver logs from the EC2 instances to Kinesis Data Streams. So, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize Amazon Athena to query both sets of logs</strong> - You cannot use Amazon Athena to query logs published to Amazon CloudWatch Logs, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-getting-started.html",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html"
    ]
  },
  {
    "id": 35,
    "question": "<p>A DevOps Engineer needs to use the AWS CloudFormation stack to deploy an application. But the DevOps Engineer does not have the required permissions to provision the resources specified in the AWS CloudFormation template.</p>\n\n<p>Which solution will allow the DevOps Engineer to deploy the stack while providing the least privileges possible?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an AWS CloudFormation service role with full permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions. Limit the permissions to pass a role based on tags attached to the role using the <code>ResourceTag/key-name</code> condition key</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Use this newly created service role during stack deployments</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an AWS CloudFormation service role with necessary permissions and use <code>aws:SourceIp</code> AWS-wide condition to specify the IP addresses of the developers. Associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Configuration Management and IaC",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</strong></p>\n\n<p>A service role is an AWS Identity and Access Management (IAM) role that allows AWS CloudFormation to make calls to resources in a stack on your behalf. You can specify an IAM role that allows AWS CloudFormation to create, update, or delete your stack resources. By default, AWS CloudFormation uses a temporary session that it generates from your user credentials for stack operations. If you specify a service role, AWS CloudFormation uses that role's credentials.</p>\n\n<p>Use a service role to explicitly specify the actions that AWS CloudFormation can perform, which might not always be the same actions that you or other users can do. For example, you might have administrative privileges, but you can limit AWS CloudFormation access to only Amazon EC2 actions.</p>\n\n<p>When you specify a service role, AWS CloudFormation always uses that role for all operations that are performed on that stack. It is not possible to remove a service role attached to a stack after the stack is created. Other users that have permission to perform operations on this stack will be able to use this role, but they must have the <code>iam:PassRole</code> permission.</p>\n\n<p>To pass a role (and its permissions) to an AWS service, a user must have permission to pass the role to the service. This helps administrators ensure that only approved users can configure a service with a role that grants permissions. To allow a user to pass a role to an AWS service, you must grant PassRole permission to the user's IAM user, role, or group.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q3-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q3-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS CloudFormation service role with full permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions. Limit the permissions to pass a role based on tags attached to the role using the <code>ResourceTag/key-name</code> condition key</strong> - Giving full permissions is against the security best practice that says a role should have the least possible privileges to complete the action needed. AWS suggests not using the ResourceTag condition key in a policy with the <code>iam:PassRole</code> action. You cannot use the tag on an IAM role to control access to who can pass that role.</p>\n\n<p><strong>Create an AWS CloudFormation service role with necessary permissions and use <code>aws:SourceIp</code> AWS-wide condition to specify the IP addresses of the developers. Associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</strong> - This statement is incorrect. AWS suggests not using the <code>aws:SourceIp</code> AWS-wide condition. AWS CloudFormation provisions resources by using its own IP address, not the IP address of the originating request. For example, when you create a stack, AWS CloudFormation makes requests from its IP address to launch an Amazon EC2 instance or to create an Amazon S3 bucket, not from the IP address from the CreateStack call or the AWS CloudFormation create-stack command.</p>\n\n<p><strong>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Use this newly created service role during stack deployments</strong> - To pass a role (and its permissions) to an AWS service, a user must have permission to pass the role to the service. To allow a user to pass a role to an AWS service, you must grant the PassRole permission to the user's IAM user, role, or group. Granting the developer <code>iam:PassRole</code> permissions to pass the role to the service is a necessary step if the developers have to deploy the CloudFormation stacks.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</strong>"
      },
      {
        "answer": "",
        "explanation": "A service role is an AWS Identity and Access Management (IAM) role that allows AWS CloudFormation to make calls to resources in a stack on your behalf. You can specify an IAM role that allows AWS CloudFormation to create, update, or delete your stack resources. By default, AWS CloudFormation uses a temporary session that it generates from your user credentials for stack operations. If you specify a service role, AWS CloudFormation uses that role's credentials."
      },
      {
        "answer": "",
        "explanation": "Use a service role to explicitly specify the actions that AWS CloudFormation can perform, which might not always be the same actions that you or other users can do. For example, you might have administrative privileges, but you can limit AWS CloudFormation access to only Amazon EC2 actions."
      },
      {
        "answer": "",
        "explanation": "When you specify a service role, AWS CloudFormation always uses that role for all operations that are performed on that stack. It is not possible to remove a service role attached to a stack after the stack is created. Other users that have permission to perform operations on this stack will be able to use this role, but they must have the <code>iam:PassRole</code> permission."
      },
      {
        "answer": "",
        "explanation": "To pass a role (and its permissions) to an AWS service, a user must have permission to pass the role to the service. This helps administrators ensure that only approved users can configure a service with a role that grants permissions. To allow a user to pass a role to an AWS service, you must grant PassRole permission to the user's IAM user, role, or group."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an AWS CloudFormation service role with full permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions. Limit the permissions to pass a role based on tags attached to the role using the <code>ResourceTag/key-name</code> condition key</strong> - Giving full permissions is against the security best practice that says a role should have the least possible privileges to complete the action needed. AWS suggests not using the ResourceTag condition key in a policy with the <code>iam:PassRole</code> action. You cannot use the tag on an IAM role to control access to who can pass that role."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an AWS CloudFormation service role with necessary permissions and use <code>aws:SourceIp</code> AWS-wide condition to specify the IP addresses of the developers. Associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</strong> - This statement is incorrect. AWS suggests not using the <code>aws:SourceIp</code> AWS-wide condition. AWS CloudFormation provisions resources by using its own IP address, not the IP address of the originating request. For example, when you create a stack, AWS CloudFormation makes requests from its IP address to launch an Amazon EC2 instance or to create an Amazon S3 bucket, not from the IP address from the CreateStack call or the AWS CloudFormation create-stack command."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Use this newly created service role during stack deployments</strong> - To pass a role (and its permissions) to an AWS service, a user must have permission to pass the role to the service. To allow a user to pass a role to an AWS service, you must grant the PassRole permission to the user's IAM user, role, or group. Granting the developer <code>iam:PassRole</code> permissions to pass the role to the service is a necessary step if the developers have to deploy the CloudFormation stacks."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html"
    ]
  },
  {
    "id": 36,
    "question": "<p>A company uses multiple AWS accounts to help isolate and manage business applications. This multi-account environment consists of an AWS Transit Gateway to route all outbound traffic through a common network account. A firewall appliance inspects all traffic before it is forwarded to an internet gateway. The firewall appliance is configured to send logs to Amazon CloudWatch Logs for all events generated.</p>\n\n<p>Recently, the security team has advised about probable illegal access of resources. As DevOps Engineer, you have been advised to configure an alert to the security team if the firewall appliance generates an event of Critical severity.</p>\n\n<p>How should a DevOps engineer configure this requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send notification to an Amazon Simple Notification Service (Amazon SNS) topic if Critical event is detected. Subscribe the email address of the security team to the SNS topic</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 5: Incident and Event Response",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong></p>\n\n<p>You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p>\n\n<p>A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Amazon EC2 Auto Scaling action, or creating an OpsItem or incident in Systems Manager.</p>\n\n<p>Amazon CloudWatch metrics and alarms:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q14-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q14-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send a notification to an Amazon Simple Notification Service (Amazon SNS) topic if a Critical event is detected. Subscribe the email address of the security team to the SNS topic</strong> - You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. Supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations. Kinesis Data Firehose is not supported as a destination for the metric streams.</p>\n\n<p><strong>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - Transit Gateway Flow Logs is a feature that enables you to capture information about the IP traffic going to and from your transit gateways. This does not meet our objective of analyzing firewall log data.</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - CloudWatch Lambda Insights is a monitoring and troubleshooting solution for serverless applications running on AWS Lambda. The solution collects, aggregates, and summarizes system-level metrics including CPU time, memory, disk, and network. This option acts as a distractor for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong>"
      },
      {
        "answer": "",
        "explanation": "You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on."
      },
      {
        "answer": "",
        "explanation": "A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Amazon EC2 Auto Scaling action, or creating an OpsItem or incident in Systems Manager."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q14-i1.jpg",
        "answer": "",
        "explanation": "Amazon CloudWatch metrics and alarms:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send a notification to an Amazon Simple Notification Service (Amazon SNS) topic if a Critical event is detected. Subscribe the email address of the security team to the SNS topic</strong> - You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. Supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations. Kinesis Data Firehose is not supported as a destination for the metric streams."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - Transit Gateway Flow Logs is a feature that enables you to capture information about the IP traffic going to and from your transit gateways. This does not meet our objective of analyzing firewall log data."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - CloudWatch Lambda Insights is a monitoring and troubleshooting solution for serverless applications running on AWS Lambda. The solution collects, aggregates, and summarizes system-level metrics including CPU time, memory, disk, and network. This option acts as a distractor for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html",
      "https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html"
    ]
  },
  {
    "id": 37,
    "question": "<p>As a security best practice, a company has decided to back up all of its Amazon Elastic Block Store (Amazon EBS) volumes every week. To implement this change, developers are mandated to tag all Amazon EBS volumes with a custom tag. The company runs an automated solution that reads the custom tag having the value of the desired backup frequency as weekly for each EBS volume and then the solution schedules the backup. However, a recent audit report has highlighted the fact that a few EBS volumes were not backed up as expected because of the missing custom tag.</p>\n\n<p>As a DevOps engineer which solution will you choose to enforce backup for all EBS volumes used by an AWS account?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>AWS::EC2::Volume</code> that returns a compliance failure if the custom tag is not applied on the EBS volume. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Monitoring and Logging",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Volume</code> that returns a compliance failure if the custom tag is not applied to the EBS volume. Configure a remediation action that uses custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong></p>\n\n<p>Set up AWS Config in the AWS account that needs the security best practice implemented. You can use the managed rule <code>required-tags</code> to check if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instances have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time. This managed rule is applicable for the <code>EC2::Volume</code> Resource Type.</p>\n\n<p>AWS Config managed rule that checks required-tags:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a><p></p>\n\n<p>This managed rule checks for the existence of the custom tag on the EBS volume. When an EBS volume is found to be non-compliant, you can specify a remediation action through a custom AWS Systems Manager Automation document to apply the custom tag with a predefined backup frequency to all non-compliant EBS volumes.</p>\n\n<p>AWS Config rules remediation with automation runbooks architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong> - In this option, the Resource type is mentioned as <code>AWS::EC2::Instance</code>. This is incorrect since the compliance check has to be done only for the EBS volume and not for the EC2 instance.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - This option is incorrect, as it's possible to remove the custom tag post the creation of the EBS volume and this action would still stay undetected. The right solution is to leverage AWS Config to ensure ongoing backup compliance for EBS volumes for the given use case.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - AWS Trusted Advisor scans your AWS infrastructure, compares with AWS best practices, and provides recommended actions. Remedial actions are not possible with AWS Trusted Advisor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Volume</code> that returns a compliance failure if the custom tag is not applied to the EBS volume. Configure a remediation action that uses custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong>"
      },
      {
        "answer": "",
        "explanation": "Set up AWS Config in the AWS account that needs the security best practice implemented. You can use the managed rule <code>required-tags</code> to check if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instances have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time. This managed rule is applicable for the <code>EC2::Volume</code> Resource Type."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i1.jpg",
        "answer": "",
        "explanation": "AWS Config managed rule that checks required-tags:"
      },
      {
        "link": "https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html"
      },
      {
        "answer": "",
        "explanation": "This managed rule checks for the existence of the custom tag on the EBS volume. When an EBS volume is found to be non-compliant, you can specify a remediation action through a custom AWS Systems Manager Automation document to apply the custom tag with a predefined backup frequency to all non-compliant EBS volumes."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i2.jpg",
        "answer": "",
        "explanation": "AWS Config rules remediation with automation runbooks architecture:"
      },
      {
        "link": "https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong> - In this option, the Resource type is mentioned as <code>AWS::EC2::Instance</code>. This is incorrect since the compliance check has to be done only for the EBS volume and not for the EC2 instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - This option is incorrect, as it's possible to remove the custom tag post the creation of the EBS volume and this action would still stay undetected. The right solution is to leverage AWS Config to ensure ongoing backup compliance for EBS volumes for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - AWS Trusted Advisor scans your AWS infrastructure, compares with AWS best practices, and provides recommended actions. Remedial actions are not possible with AWS Trusted Advisor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html",
      "https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/"
    ]
  },
  {
    "id": 38,
    "question": "<p>A developer has uploaded an object of size 100 MB to an Amazon S3 bucket as a single-part direct upload using the REST API that has checksum enabled. The checksum of the object uploaded via the REST API was the checksum of the entire object. Later that day, the developer used the AWS Management Console to rename the object, copy it and edit its metadata. Later, when the developer checked for the checksum of the object updated via the AWS Management Console, the checksum was not the checksum of the entire object. Confused by the behavior, the developer has reached out to you for a possible solution.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, which of the following options would you identify as the reason for this behavior?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>When you change metadata of an object in S3, the checksum algorithm of the objects changes by default. This is an expected behavior</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>If an object is greater than 16 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer's initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>If an object is greater than 50 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer's initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A new checksum value for the object, that is calculated based on the checksum values of the individual parts, has been created. This behavior is expected</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 5: Incident and Event Response",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>A new checksum value for the object that is calculated based on the checksum values of the individual parts has been created. This behavior is expected</strong></p>\n\n<p>When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB in size. In this case, the checksum is not a direct checksum of the full object, but rather a calculation based on the checksum values of each individual part.</p>\n\n<p>For example, consider an object 100 MB in size that you uploaded as a single-part direct upload using the REST API. The checksum in this case is a checksum of the entire object. If you later use the console to rename that object, copy it, change the storage class, or edit the metadata, Amazon S3 uses the multipart upload functionality to update the object. As a result, Amazon S3 creates a new checksum value for the object that is calculated based on the checksum values of the individual parts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If an object is greater than 50 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer's initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</strong> - This option is incorrect. When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB (NOT 50 MB) in size.</p>\n\n<p><strong>When you change metadata of an object in S3, the checksum algorithm of the objects changes by default. This is an expected behavior</strong> - The checksum algorithm does not change when you change the metadata of the S3 object, so this option is incorrect.</p>\n\n<p><strong>If an object is greater than 16 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer's initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</strong> - This option is incorrect. When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB in size. In this case, the checksum is not a direct checksum of the full object, but rather a calculation based on the checksum values of each individual part. On the other hand, when you upload an object as a single-part direct upload using the REST API, the checksum in this case is a checksum of the entire object. So you can say that the developer's initial calculation for the REST API based checksum was indeed correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>A new checksum value for the object that is calculated based on the checksum values of the individual parts has been created. This behavior is expected</strong>"
      },
      {
        "answer": "",
        "explanation": "When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB in size. In this case, the checksum is not a direct checksum of the full object, but rather a calculation based on the checksum values of each individual part."
      },
      {
        "answer": "",
        "explanation": "For example, consider an object 100 MB in size that you uploaded as a single-part direct upload using the REST API. The checksum in this case is a checksum of the entire object. If you later use the console to rename that object, copy it, change the storage class, or edit the metadata, Amazon S3 uses the multipart upload functionality to update the object. As a result, Amazon S3 creates a new checksum value for the object that is calculated based on the checksum values of the individual parts."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>If an object is greater than 50 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer's initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</strong> - This option is incorrect. When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB (NOT 50 MB) in size."
      },
      {
        "answer": "",
        "explanation": "<strong>When you change metadata of an object in S3, the checksum algorithm of the objects changes by default. This is an expected behavior</strong> - The checksum algorithm does not change when you change the metadata of the S3 object, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>If an object is greater than 16 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer's initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</strong> - This option is incorrect. When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB in size. In this case, the checksum is not a direct checksum of the full object, but rather a calculation based on the checksum values of each individual part. On the other hand, when you upload an object as a single-part direct upload using the REST API, the checksum in this case is a checksum of the entire object. So you can say that the developer's initial calculation for the REST API based checksum was indeed correct."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html"
    ]
  },
  {
    "id": 39,
    "question": "<p>A company is implementing AWS serverless architecture with Amazon API Gateway, AWS Lambda, and Amazon DynamoDB services. The company's existing users are primarily located in Europe and Asia-Pacific regions. The company is now looking for a quick-start solution that offers high reliability and low latency for a global user base across regions as its offerings are getting popular worldwide.</p>\n\n<p>How will you implement this requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use geolocation routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use failover routing and Application Recovery Controller health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use latency-based routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure Amazon Route 53 to point to AWS Global Accelerator. Configure Global Accelerator to point to API Gateway API endpoint(s) of both regions via an Application Load Balancer(ALB). Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Resilient Cloud Solutions",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use latency-based routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong></p>\n\n<p>Route 53: If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server. Amazon Route 53 latency-based routing will reduce latency by serving the user from the fastest available route between the two AWS regions.</p>\n\n<p>You can use Amazon Route 53 health checks to control DNS failover from an API Gateway API in a primary AWS Region to one in a secondary Region. This can help mitigate impacts in the event of a Regional issue.</p>\n\n<p>AWS Lambda provides easy scaling and high availability to your serverless architecture without additional effort on your part.</p>\n\n<p>Amazon DynamoDB global tables is a fully managed, serverless, multi-Region, and multi-active database. Global tables provide you 99.999% availability, increased application resiliency, and improved business continuity. As global tables replicate your Amazon DynamoDB tables automatically across your choice of AWS Regions, you can achieve fast, local read and write performance.</p>\n\n<p>A DynamoDB global table is comprised of multiple replica tables. Each replica table exists in a different Region, but all replicas have the same name and primary key. When data is written to any replica table, DynamoDB automatically replicates that data to all other replica tables in the global table. Global tables enable the users of your application to have low-latency access to the data no matter where they are located. In the unlikely event that one AWS Region was to become temporarily unavailable, your customers can still access the replica tables in the other Regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use geolocation routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong> - Route 53 geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. While using geolocation routing can help you localize your content and present some or all of your website in the language of your users, it is not suitable to serve content with the lowest latency.</p>\n\n<p><strong>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use failover routing and Application Recovery Controller health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong> - While failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy, it is not suitable when the key requirement is to serve content with lowest possible latency.</p>\n\n<p><strong>Configure Amazon Route 53 to point to AWS Global Accelerator. Configure Global Accelerator to point to API Gateway API endpoint(s) of both regions via an Application Load Balancer(ALB). Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong> - AWS Global Accelerator can be used in conjunction with the Amazon API Gateway to present Internet-facing API via static IP addresses to end users. This design addresses the need for static IP safe listing, however, it is not useful for the given requirements.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/accessing-an-aws-api-gateway-via-static-ip-addresses-provided-by-aws-global-accelerator/\">https://aws.amazon.com/blogs/networking-and-content-delivery/accessing-an-aws-api-gateway-via-static-ip-addresses-provided-by-aws-global-accelerator/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use latency-based routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong>"
      },
      {
        "answer": "",
        "explanation": "Route 53: If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server. Amazon Route 53 latency-based routing will reduce latency by serving the user from the fastest available route between the two AWS regions."
      },
      {
        "answer": "",
        "explanation": "You can use Amazon Route 53 health checks to control DNS failover from an API Gateway API in a primary AWS Region to one in a secondary Region. This can help mitigate impacts in the event of a Regional issue."
      },
      {
        "answer": "",
        "explanation": "AWS Lambda provides easy scaling and high availability to your serverless architecture without additional effort on your part."
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB global tables is a fully managed, serverless, multi-Region, and multi-active database. Global tables provide you 99.999% availability, increased application resiliency, and improved business continuity. As global tables replicate your Amazon DynamoDB tables automatically across your choice of AWS Regions, you can achieve fast, local read and write performance."
      },
      {
        "answer": "",
        "explanation": "A DynamoDB global table is comprised of multiple replica tables. Each replica table exists in a different Region, but all replicas have the same name and primary key. When data is written to any replica table, DynamoDB automatically replicates that data to all other replica tables in the global table. Global tables enable the users of your application to have low-latency access to the data no matter where they are located. In the unlikely event that one AWS Region was to become temporarily unavailable, your customers can still access the replica tables in the other Regions."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use geolocation routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong> - Route 53 geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. While using geolocation routing can help you localize your content and present some or all of your website in the language of your users, it is not suitable to serve content with the lowest latency."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use failover routing and Application Recovery Controller health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong> - While failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy, it is not suitable when the key requirement is to serve content with lowest possible latency."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon Route 53 to point to AWS Global Accelerator. Configure Global Accelerator to point to API Gateway API endpoint(s) of both regions via an Application Load Balancer(ALB). Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong> - AWS Global Accelerator can be used in conjunction with the Amazon API Gateway to present Internet-facing API via static IP addresses to end users. This design addresses the need for static IP safe listing, however, it is not useful for the given requirements."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html",
      "https://aws.amazon.com/dynamodb/global-tables/",
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html",
      "https://aws.amazon.com/blogs/networking-and-content-delivery/accessing-an-aws-api-gateway-via-static-ip-addresses-provided-by-aws-global-accelerator/"
    ]
  },
  {
    "id": 40,
    "question": "<p>An application runs on a fleet of Amazon EC2 Windows instances configured with an Auto Scaling group (ASG). When scaling-in takes place in the ASG, the instances are terminated without notification. The application team wants to create an AMI and remove the Amazon EC2 Windows instance from its domain before terminating the scaled-in instances.</p>\n\n<p>As a DevOps Engineer, which combination of steps will you choose to implement this requirement? (Select two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add an AWS Systems Manager Patch Manager as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add an AWS Systems Manager automation document as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Add a lifecycle hook that puts the instance in <code>Terminating:Wait</code> status and setup an Amazon CloudWatch event to monitor the <code>Terminating:Wait</code> status</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Add a lifecycle hook that puts the instance in <code>Terminating:Pending</code> status and setup an Amazon CloudWatch event to monitor the <code>Terminating:Pending</code> status</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Configure an AWS Systems Manager Maintenance Window to schedule an action to run a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Monitoring and Logging",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Add a lifecycle hook that puts the instance in <code>Terminating:Wait</code> status and setup an Amazon CloudWatch event to monitor the <code>Terminating:Wait</code> status</strong></p>\n\n<p><strong>Add an AWS Systems Manager automation document as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the instance from the domain and create an AMI of the EC2 instance</strong></p>\n\n<p>Oftentimes, you may want to execute some code and actions before terminating an Amazon Elastic Compute Cloud (Amazon EC2) instance that is part of an Amazon EC2 Auto Scaling group.</p>\n\n<p>One way to execute code and actions before terminating an instance is to create a lifecycle hook that puts the instance in <code>Terminating:Wait</code> status. This allows you to perform any desired actions before immediately terminating the instance within the Auto Scaling group. The <code>Terminating:Wait</code> status can be monitored by an Amazon CloudWatch event, which triggers an AWS Systems Manager automation document to perform the action you want.</p>\n\n<p>Broadly, the steps needed for the above configuration:\n1. Add a lifecycle hook.\n2. Create a Systems Manager automation document.\n3. Create AWS Identity and Access Management (IAM) policies and a role to delegate permissions to the Systems Manager automation document.\n4. Create IAM policies and a role to delegate permissions to CloudWatch Events, which invokes the Systems Manager automation document.\n5. Create a CloudWatch Events rule.\n6. Add a Systems Manager automation document as a CloudWatch Event target.</p>\n\n<p>Using Lifecycle hooks to run code before terminating an EC2 Auto Scaling instance:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q20-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q20-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add a lifecycle hook that puts the instance in <code>Terminating:Pending</code> status and set up an Amazon CloudWatch event to monitor the <code>Terminating:Pending</code> status</strong> - <code>Terminating:Pending</code> is not a valid state. The following are the transitions between instance states in the Amazon EC2 Auto Scaling lifecycle.</p>\n\n<p>Amazon EC2 Auto Scaling instance lifecycle:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q20-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q20-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html</a><p></p>\n\n<p><strong>Add an AWS Systems Manager Patch Manager as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</strong> - Patch Manager is a capability of AWS Systems Manager that automates the process of patching managed nodes with both security-related updates and other types of updates. It cannot be used for the processing of custom logic/code.</p>\n\n<p><strong>Configure an AWS Systems Manager Maintenance Window to schedule an action to run a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</strong> - Maintenance Windows is a capability of AWS Systems Manager that helps you define a schedule for when to perform potentially disruptive actions on your nodes such as patching an operating system, updating drivers, or installing software or patches. Maintenance Windows is not relevant for the given use case, since we want the custom logic to run immediately and return to the instance termination triggered by Auto Scaling Group.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-availability\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-availability</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-maintenance.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-maintenance.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Add a lifecycle hook that puts the instance in <code>Terminating:Wait</code> status and setup an Amazon CloudWatch event to monitor the <code>Terminating:Wait</code> status</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Add an AWS Systems Manager automation document as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the instance from the domain and create an AMI of the EC2 instance</strong>"
      },
      {
        "answer": "",
        "explanation": "Oftentimes, you may want to execute some code and actions before terminating an Amazon Elastic Compute Cloud (Amazon EC2) instance that is part of an Amazon EC2 Auto Scaling group."
      },
      {
        "answer": "",
        "explanation": "One way to execute code and actions before terminating an instance is to create a lifecycle hook that puts the instance in <code>Terminating:Wait</code> status. This allows you to perform any desired actions before immediately terminating the instance within the Auto Scaling group. The <code>Terminating:Wait</code> status can be monitored by an Amazon CloudWatch event, which triggers an AWS Systems Manager automation document to perform the action you want."
      },
      {
        "answer": "",
        "explanation": "Broadly, the steps needed for the above configuration:\n1. Add a lifecycle hook.\n2. Create a Systems Manager automation document.\n3. Create AWS Identity and Access Management (IAM) policies and a role to delegate permissions to the Systems Manager automation document.\n4. Create IAM policies and a role to delegate permissions to CloudWatch Events, which invokes the Systems Manager automation document.\n5. Create a CloudWatch Events rule.\n6. Add a Systems Manager automation document as a CloudWatch Event target."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q20-i1.jpg",
        "answer": "",
        "explanation": "Using Lifecycle hooks to run code before terminating an EC2 Auto Scaling instance:"
      },
      {
        "link": "https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Add a lifecycle hook that puts the instance in <code>Terminating:Pending</code> status and set up an Amazon CloudWatch event to monitor the <code>Terminating:Pending</code> status</strong> - <code>Terminating:Pending</code> is not a valid state. The following are the transitions between instance states in the Amazon EC2 Auto Scaling lifecycle."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q20-i2.jpg",
        "answer": "",
        "explanation": "Amazon EC2 Auto Scaling instance lifecycle:"
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Add an AWS Systems Manager Patch Manager as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</strong> - Patch Manager is a capability of AWS Systems Manager that automates the process of patching managed nodes with both security-related updates and other types of updates. It cannot be used for the processing of custom logic/code."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure an AWS Systems Manager Maintenance Window to schedule an action to run a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</strong> - Maintenance Windows is a capability of AWS Systems Manager that helps you define a schedule for when to perform potentially disruptive actions on your nodes such as patching an operating system, updating drivers, or installing software or patches. Maintenance Windows is not relevant for the given use case, since we want the custom logic to run immediately and return to the instance termination triggered by Auto Scaling Group."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-availability",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-maintenance.html"
    ]
  },
  {
    "id": 41,
    "question": "<p>A multi-national company with hundreds of AWS accounts has slowly adopted AWS Organizations with all features enabled. The company has also configured a few Organization Units (OUs) to serve its business objectives. The company has some AWS Identity and Access Management (IAM) roles that need to be configured for every new AWS account created for the company. Also, the security policy mandates enabling AWS CloudTrail for all AWS accounts. The company is looking for an automated solution that can add the mandatory IAM Roles and CloudTrail configurations to all newly created accounts and also delete the resources/configurations when an account leaves the organization without manual intervention.</p>\n\n<p>What should a DevOps engineer do to meet these requirements with the minimal overhead?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: SDLC Automation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</strong></p>\n\n<p>You can centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions. For example, you can deploy your centralized AWS Identity and Access Management (IAM) roles, provision Amazon Elastic Compute Cloud (Amazon EC2) instances or AWS Lambda functions across AWS Regions and accounts in your organization. CloudFormation StackSets simplify the configuration of cross-account permissions and allow for the automatic creation and deletion of resources when accounts are joined or removed from your Organization.</p>\n\n<p>You can get started by enabling data sharing between CloudFormation and Organizations from the StackSets console. Once done, you will be able to use StackSets in the AWS Organizations master account to deploy stacks to all accounts in your organization or specific organizational units (OUs). A new service-managed permission model is available with these StackSets. Choosing <code>Service managed permissions</code> allows StackSets to automatically configure the necessary IAM permissions required to deploy your stack to the accounts in your organization.</p>\n\n<p>In addition to setting permissions, CloudFormation StackSets offer the option for automatically creating or removing your CloudFormation stacks when a new AWS account joins or quits your Organization. You do not need to remember to manually connect to the new account to deploy your common infrastructure or to delete infrastructure when an account is removed from your Organization. When an account leaves the organization, the stack will be removed from the management of StackSets. However, you can choose to either delete or retain the resources managed by the stack.</p>\n\n<p>Lastly, you choose whether to deploy a stack to your entire organization or just to one or more Organization Units (OU). You also choose a couple of deployment options: how many accounts will be prepared in parallel, and how many failures you tolerate before stopping the entire deployment.</p>\n\n<p>Use AWS CloudFormation StackSets for Multiple Accounts in an AWS Organization:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by an AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</strong> - This option involves using too many services which unnecessarily adds to the complexity and cost of the overall solution. So, this option is incorrect.</p>\n\n<p><strong>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</strong> - While you can use AWS Systems Manager to automate tasks across multiple accounts in AWS Organization, the other details in this option are irrelevant to the given use case.</p>\n\n<p>When you run automation across multiple Regions and accounts, you target resources by using tags or the name of an AWS resource group. The resource group must exist in each target account and Region. The resource group name must be the same in each target account and Region. The automation fails to run on those resources that don't have the specified tag or that aren't included in the specified resource group.</p>\n\n<p><strong>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</strong> - It is possible to enable CloudTrail logging from the management account of AWS Organizations, and is referred to as the organization trail. The use case is to be able to log all Trail events to a commonplace. Also, creating an IAM role in the management account and sharing it across all member accounts is not straightforward and requires manual work. Hence, this option is incorrect for the given use case.</p>\n\n<p>IAM policies usage in AWS Organizations:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html</a><p></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</strong>"
      },
      {
        "answer": "",
        "explanation": "You can centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions. For example, you can deploy your centralized AWS Identity and Access Management (IAM) roles, provision Amazon Elastic Compute Cloud (Amazon EC2) instances or AWS Lambda functions across AWS Regions and accounts in your organization. CloudFormation StackSets simplify the configuration of cross-account permissions and allow for the automatic creation and deletion of resources when accounts are joined or removed from your Organization."
      },
      {
        "answer": "",
        "explanation": "You can get started by enabling data sharing between CloudFormation and Organizations from the StackSets console. Once done, you will be able to use StackSets in the AWS Organizations master account to deploy stacks to all accounts in your organization or specific organizational units (OUs). A new service-managed permission model is available with these StackSets. Choosing <code>Service managed permissions</code> allows StackSets to automatically configure the necessary IAM permissions required to deploy your stack to the accounts in your organization."
      },
      {
        "answer": "",
        "explanation": "In addition to setting permissions, CloudFormation StackSets offer the option for automatically creating or removing your CloudFormation stacks when a new AWS account joins or quits your Organization. You do not need to remember to manually connect to the new account to deploy your common infrastructure or to delete infrastructure when an account is removed from your Organization. When an account leaves the organization, the stack will be removed from the management of StackSets. However, you can choose to either delete or retain the resources managed by the stack."
      },
      {
        "answer": "",
        "explanation": "Lastly, you choose whether to deploy a stack to your entire organization or just to one or more Organization Units (OU). You also choose a couple of deployment options: how many accounts will be prepared in parallel, and how many failures you tolerate before stopping the entire deployment."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i1.jpg",
        "answer": "",
        "explanation": "Use AWS CloudFormation StackSets for Multiple Accounts in an AWS Organization:"
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by an AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</strong> - This option involves using too many services which unnecessarily adds to the complexity and cost of the overall solution. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</strong> - While you can use AWS Systems Manager to automate tasks across multiple accounts in AWS Organization, the other details in this option are irrelevant to the given use case."
      },
      {
        "answer": "",
        "explanation": "When you run automation across multiple Regions and accounts, you target resources by using tags or the name of an AWS resource group. The resource group must exist in each target account and Region. The resource group name must be the same in each target account and Region. The automation fails to run on those resources that don't have the specified tag or that aren't included in the specified resource group."
      },
      {
        "answer": "",
        "explanation": "<strong>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</strong> - It is possible to enable CloudTrail logging from the management account of AWS Organizations, and is referred to as the organization trail. The use case is to be able to log all Trail events to a commonplace. Also, creating an IAM role in the management account and sharing it across all member accounts is not straightforward and requires manual work. Hence, this option is incorrect for the given use case."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i2.jpg",
        "answer": "",
        "explanation": "IAM policies usage in AWS Organizations:"
      },
      {
        "link": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html"
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html"
    ]
  },
  {
    "id": 42,
    "question": "<p>A production environment has Amazon EC2 instances configured to log all application/system logs via the CloudWatch Logs agent that has been configured on all instances. The company has recently introduced a security policy that mandates terminating any Amazon EC2 instance accessed manually by a user other than the administrators within an hour. All the production instances are configured with Auto Scaling groups.</p>\n\n<p>As a DevOps Engineer, how will you automate this process?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a CloudWatch Logs subscription to an AWS Step Function that coordinates with AWS Lambda function and Amazon EventBridge to create a serverless automated solution. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function every hour, which will terminate all instances with this tag</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an Amazon CloudWatch alarm that is triggered by login event data to call an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke the Lambda function every hour, to terminate all EC2 instances with the decommission tag</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a CloudWatch Logs subscription to deliver the login event data of Amazon EC2 instances to an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke another Lambda function every hour, to terminate all EC2 instances with the decommission tag</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon CloudWatch alarm that is triggered by the login event data. Create alarm action that notifies system administrators through a message using the Amazon Simple Notification Service topic. The system administrators will then manually terminate the instance</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Monitoring and Logging",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Logs subscription to deliver the login event data of Amazon EC2 instances to an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke another Lambda function every hour, to terminate all EC2 instances with the decommission tag</strong></p>\n\n<p>You can use CloudWatch Log subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. When log events are sent to the receiving service, they are base64 encoded and compressed with the gzip format.</p>\n\n<p>For the given use case, you can invoke a Lambda function via the CloudWatch Logs subscription. The Lambda then adds a decommission tag to the EC2 instance that produced the login event. Lastly, you can set up an Amazon EventBridge rule to invoke another Lambda function on an hourly schedule, to terminate all EC2 instances that are marked with the decommission tag.</p>\n\n<p>An Amazon EventBridge rule matches incoming events and sends them to targets for processing. A rule can run in response to an event, or at certain time intervals. For example, to periodically run an AWS Lambda function, you can create a rule to run on a schedule.</p>\n\n<p>In EventBridge, you can create two types of scheduled rules:</p>\n\n<ol>\n<li>Rules that run at a regular rate</li>\n</ol>\n\n<p>EventBridge runs these rules at regular intervals; for example, every 20 minutes.</p>\n\n<p>To specify the rate for a scheduled rule, you define a rate expression.</p>\n\n<ol>\n<li>Rules that run at specific times</li>\n</ol>\n\n<p>EventBridge runs these rules at specific times and dates; for example, 8:00 a.m. PST on the first Monday of every month.</p>\n\n<p>To specify the time and dates a scheduled rule runs, you define a cron expression.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon CloudWatch alarm that is triggered by the login event data. Create alarm action that notifies system administrators through a message using the Amazon Simple Notification Service topic. The system administrators will then manually terminate the instance</strong> - This option does not provide a solution for complete automation since system administrators are expected to terminate the systems manually.</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that is triggered by login event data to call an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke the Lambda function every hour, to terminate all EC2 instances with the decommission tag</strong> - Amazon CloudWatch alarm does not support having AWS Lambda function as an alarm action. Amazon Simple Notification Service can be used as an intermediatory service if you wish to use AWS Lambda with CloudWatch alarms.</p>\n\n<p><strong>Create a CloudWatch Logs subscription to an AWS Step Function that coordinates with the AWS Lambda function and Amazon EventBridge to create a serverless automated solution. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function every hour, which will terminate all instances with this tag</strong> - AWS Step Functions is a serverless orchestration service that lets you integrate with AWS Lambda functions and other AWS services to build business-critical applications. Step Function is overkill for the simple automation via the AWS Lambda function that the given use case needs.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\">https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a CloudWatch Logs subscription to deliver the login event data of Amazon EC2 instances to an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke another Lambda function every hour, to terminate all EC2 instances with the decommission tag</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use CloudWatch Log subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. When log events are sent to the receiving service, they are base64 encoded and compressed with the gzip format."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can invoke a Lambda function via the CloudWatch Logs subscription. The Lambda then adds a decommission tag to the EC2 instance that produced the login event. Lastly, you can set up an Amazon EventBridge rule to invoke another Lambda function on an hourly schedule, to terminate all EC2 instances that are marked with the decommission tag."
      },
      {
        "answer": "",
        "explanation": "An Amazon EventBridge rule matches incoming events and sends them to targets for processing. A rule can run in response to an event, or at certain time intervals. For example, to periodically run an AWS Lambda function, you can create a rule to run on a schedule."
      },
      {
        "answer": "",
        "explanation": "In EventBridge, you can create two types of scheduled rules:"
      },
      {},
      {
        "answer": "",
        "explanation": "EventBridge runs these rules at regular intervals; for example, every 20 minutes."
      },
      {
        "answer": "",
        "explanation": "To specify the rate for a scheduled rule, you define a rate expression."
      },
      {},
      {
        "answer": "",
        "explanation": "EventBridge runs these rules at specific times and dates; for example, 8:00 a.m. PST on the first Monday of every month."
      },
      {
        "answer": "",
        "explanation": "To specify the time and dates a scheduled rule runs, you define a cron expression."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon CloudWatch alarm that is triggered by the login event data. Create alarm action that notifies system administrators through a message using the Amazon Simple Notification Service topic. The system administrators will then manually terminate the instance</strong> - This option does not provide a solution for complete automation since system administrators are expected to terminate the systems manually."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon CloudWatch alarm that is triggered by login event data to call an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke the Lambda function every hour, to terminate all EC2 instances with the decommission tag</strong> - Amazon CloudWatch alarm does not support having AWS Lambda function as an alarm action. Amazon Simple Notification Service can be used as an intermediatory service if you wish to use AWS Lambda with CloudWatch alarms."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a CloudWatch Logs subscription to an AWS Step Function that coordinates with the AWS Lambda function and Amazon EventBridge to create a serverless automated solution. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function every hour, which will terminate all instances with this tag</strong> - AWS Step Functions is a serverless orchestration service that lets you integrate with AWS Lambda functions and other AWS services to build business-critical applications. Step Function is overkill for the simple automation via the AWS Lambda function that the given use case needs."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html",
      "https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html",
      "https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html"
    ]
  },
  {
    "id": 43,
    "question": "<p>A project has two AWS accounts, a development account and a production account, in the us-east-1 Region. A DevOps engineer has to deploy artifacts from the development account's S3 bucket to the production account's S3 bucket using AWS CodePipeline with Amazon S3 deploy action.</p>\n\n<p>What configurations are mandatory for this cross-account deployment? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure a cross-account role in the development account. Attach a policy to the S3 bucket in the production account that allows access to the cross-account role that you created</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an AWS KMS key to use with CodePipeline in the development account. Also, the input bucket from the development account must have versioning activated to work with CodePipeline</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>You need to use an AWS KMS multi-Region key with multiple replicas</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create Secrets Manager key to use with CodePipeline in the development account</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Configure a cross-account role in the production account. Attach a policy to your CodePipeline service role in the development account that allows it to assume the cross-account role that you created</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Configuration Management and IaC",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an AWS KMS key to use with CodePipeline in the development account. Also, the input bucket from the development account must have versioning activated to work with CodePipeline</strong></p>\n\n<p>You must use the AWS Key Management Service (AWS KMS) customer-managed key for cross-account deployments. If the key isn't configured, then CodePipeline encrypts the objects with default encryption, which can't be decrypted by the role in the destination account.</p>\n\n<p>The input bucket must have versioning activated to work with CodePipeline.</p>\n\n<p><strong>Configure a cross-account role in the production account. Attach a policy to your CodePipeline service role in the development account that allows it to assume the cross-account role that you created</strong></p>\n\n<p>In the deploy action, the CodePipeline service role assumes the cross-account role in the production account. CodePipeline uses the cross-account role to access the KMS key and artifact bucket in the development account. Then, CodePipeline deploys the extracted files to the production output S3 bucket in the production account.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create Secrets Manager key to use with CodePipeline in the development account</strong> - This option is not relevant for the given use case.</p>\n\n<p><strong>Configure a cross-account role in the development account. Attach a policy to the S3 bucket in the production account that allows access to the cross-account role that you created</strong> - This statement is incorrect and given only as a distractor.</p>\n\n<p><strong>You need to use an AWS KMS multi-Region key with multiple replicas</strong> - This option has been added as a distractor. Only when the production account's Region is different than your pipeline's Region, then you must also use an AWS KMS multi-Region key with multiple replicas.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/codepipeline-artifacts-s3\">https://repost.aws/knowledge-center/codepipeline-artifacts-s3</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/action-reference-S3Deploy.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/action-reference-S3Deploy.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an AWS KMS key to use with CodePipeline in the development account. Also, the input bucket from the development account must have versioning activated to work with CodePipeline</strong>"
      },
      {
        "answer": "",
        "explanation": "You must use the AWS Key Management Service (AWS KMS) customer-managed key for cross-account deployments. If the key isn't configured, then CodePipeline encrypts the objects with default encryption, which can't be decrypted by the role in the destination account."
      },
      {
        "answer": "",
        "explanation": "The input bucket must have versioning activated to work with CodePipeline."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a cross-account role in the production account. Attach a policy to your CodePipeline service role in the development account that allows it to assume the cross-account role that you created</strong>"
      },
      {
        "answer": "",
        "explanation": "In the deploy action, the CodePipeline service role assumes the cross-account role in the production account. CodePipeline uses the cross-account role to access the KMS key and artifact bucket in the development account. Then, CodePipeline deploys the extracted files to the production output S3 bucket in the production account."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create Secrets Manager key to use with CodePipeline in the development account</strong> - This option is not relevant for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a cross-account role in the development account. Attach a policy to the S3 bucket in the production account that allows access to the cross-account role that you created</strong> - This statement is incorrect and given only as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>You need to use an AWS KMS multi-Region key with multiple replicas</strong> - This option has been added as a distractor. Only when the production account's Region is different than your pipeline's Region, then you must also use an AWS KMS multi-Region key with multiple replicas."
      }
    ],
    "references": [
      "https://repost.aws/knowledge-center/codepipeline-artifacts-s3",
      "https://docs.aws.amazon.com/codepipeline/latest/userguide/action-reference-S3Deploy.html"
    ]
  },
  {
    "id": 44,
    "question": "<p>A company has hundreds of AWS accounts and has also created an organization in AWS Organizations to manage the accounts. The company wants a dashboard to seamlessly search, visualize, and analyze CloudWatch metrics data, logs data, and traces (from AWS X-Ray) from all the linked accounts into a single security and operations account. The solution should automatically onboard any new AWS accounts created later in the organization.</p>\n\n<p>As a DevOps Engineer, what solution do you suggest to address the given requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon CloudWatch cross-account observability to set up security and operations account as the monitoring account and link it with rest of the member accounts of the organization using AWS Organizations</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a CloudWatch alarm for the CloudWatch metrics and trigger an event on Amazon EventBridge. Write the metrics data to the Amazon S3 bucket. Use Amazon Athena to create visualizations and dashboards from CloudWatch metrics data, logs data, and traces</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the Amazon CloudWatch cross-account observability feature from the CloudWatch console to create the monitoring account and connect the individual AWS accounts to the monitoring account</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure CloudWatch Metric Streams to stream real-time metrics data to Kinesis Data Firehose. Firehose will push the metrics data to Amazon Simple Storage Service (Amazon S3) bucket. configure Amazon Athena to create a dashboard with the metrics data</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Monitoring and Logging",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon CloudWatch cross-account observability to set up a security and operations account as the monitoring account and link it with the rest of the member accounts of the organization using AWS Organizations</strong></p>\n\n<p>With Amazon CloudWatch cross-account observability, you can monitor and troubleshoot applications that span multiple accounts within a Region. Seamlessly search, visualize, and analyze your metrics, logs, and traces in any of the linked accounts without account boundaries.</p>\n\n<p>Set up one or more AWS accounts as monitoring accounts and link them with multiple source accounts. A monitoring account is a central AWS account that can view and interact with observability data generated from source accounts. A source account is an individual AWS account that generates observability data for the resources that reside in it. Source accounts share their observability data with the monitoring account.</p>\n\n<p>The shared observability data can include the following types of telemetry:\n1. Metrics in Amazon CloudWatch\n2. Log groups in Amazon CloudWatch Logs\n3. Traces in AWS X-Ray</p>\n\n<p>There are two options for linking source accounts to your monitoring account. You can use one or both options.\n1. Use AWS Organizations to link accounts in an organization or organizational unit to the monitoring account.\n2. Connect individual AWS accounts to the monitoring account.</p>\n\n<p>AWS recommends that you use Organizations so that new AWS accounts created later in the organization are automatically onboarded to cross-account observability as source accounts. This is our use case requirement and hence choosing Organizations to implement the requirements.</p>\n\n<p>Demonstration of setting up CloudWatch cross-account observability:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q29-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q29-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the Amazon CloudWatch cross-account observability feature from the CloudWatch console to create the monitoring account and connect the individual AWS accounts to the monitoring account</strong> - While this option is correct, it cannot automatically onboard any new AWS accounts created later in the organization.</p>\n\n<p><strong>Create a CloudWatch alarm for the CloudWatch metrics and trigger an event on Amazon EventBridge. Write the metrics data to the Amazon S3 bucket. Use Amazon Athena to create visualizations and dashboards from CloudWatch metrics data, logs data, and traces</strong> - Amazon EventBridge does not support Amazon S3 bucket as a target. Hence, this option is incorrect.</p>\n\n<p><strong>Configure CloudWatch Metric Streams to stream real-time metrics data to Kinesis Data Firehose. Firehose will push the metrics data to Amazon Simple Storage Service (Amazon S3) bucket. configure Amazon Athena to create a dashboard with the metrics data</strong> - You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. The use case does not talk about real-time data, so configuring CloudWatch Metric Streams with Firehose is not the best fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/cloudwatch-metric-streams-send-aws-metrics-to-partners-and-to-your-apps-in-real-time/\">https://aws.amazon.com/blogs/aws/cloudwatch-metric-streams-send-aws-metrics-to-partners-and-to-your-apps-in-real-time/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon CloudWatch cross-account observability to set up a security and operations account as the monitoring account and link it with the rest of the member accounts of the organization using AWS Organizations</strong>"
      },
      {
        "answer": "",
        "explanation": "With Amazon CloudWatch cross-account observability, you can monitor and troubleshoot applications that span multiple accounts within a Region. Seamlessly search, visualize, and analyze your metrics, logs, and traces in any of the linked accounts without account boundaries."
      },
      {
        "answer": "",
        "explanation": "Set up one or more AWS accounts as monitoring accounts and link them with multiple source accounts. A monitoring account is a central AWS account that can view and interact with observability data generated from source accounts. A source account is an individual AWS account that generates observability data for the resources that reside in it. Source accounts share their observability data with the monitoring account."
      },
      {
        "answer": "",
        "explanation": "The shared observability data can include the following types of telemetry:\n1. Metrics in Amazon CloudWatch\n2. Log groups in Amazon CloudWatch Logs\n3. Traces in AWS X-Ray"
      },
      {
        "answer": "",
        "explanation": "There are two options for linking source accounts to your monitoring account. You can use one or both options.\n1. Use AWS Organizations to link accounts in an organization or organizational unit to the monitoring account.\n2. Connect individual AWS accounts to the monitoring account."
      },
      {
        "answer": "",
        "explanation": "AWS recommends that you use Organizations so that new AWS accounts created later in the organization are automatically onboarded to cross-account observability as source accounts. This is our use case requirement and hence choosing Organizations to implement the requirements."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q29-i1.jpg",
        "answer": "",
        "explanation": "Demonstration of setting up CloudWatch cross-account observability:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the Amazon CloudWatch cross-account observability feature from the CloudWatch console to create the monitoring account and connect the individual AWS accounts to the monitoring account</strong> - While this option is correct, it cannot automatically onboard any new AWS accounts created later in the organization."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a CloudWatch alarm for the CloudWatch metrics and trigger an event on Amazon EventBridge. Write the metrics data to the Amazon S3 bucket. Use Amazon Athena to create visualizations and dashboards from CloudWatch metrics data, logs data, and traces</strong> - Amazon EventBridge does not support Amazon S3 bucket as a target. Hence, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure CloudWatch Metric Streams to stream real-time metrics data to Kinesis Data Firehose. Firehose will push the metrics data to Amazon Simple Storage Service (Amazon S3) bucket. configure Amazon Athena to create a dashboard with the metrics data</strong> - You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. The use case does not talk about real-time data, so configuring CloudWatch Metric Streams with Firehose is not the best fit."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html",
      "https://aws.amazon.com/blogs/aws/cloudwatch-metric-streams-send-aws-metrics-to-partners-and-to-your-apps-in-real-time/"
    ]
  },
  {
    "id": 45,
    "question": "<p>The security policy of a company mandates encrypting all AMIs that the company shares across its AWS accounts. An AWS account (Account A) has a custom AMI that is not encrypted. This AMI needs to be shared with another AWS Account B. Account B has Amazon EC2 instances configured with an Auto Scaling group that will use the AMI. Account A already has an AWS Key Management Service (AWS KMS) key.</p>\n\n<p>As a DevOps Engineer, which combination of steps will you take to share the AMI with Account B while adhering to the company's security policy? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify permissions on the AMI to be accessible from Account B</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>In Account A, create an encrypted AMI from the unencrypted version and encrypt the associated EBS snapshots with it. Specify the KMS key in the copy action. Share the encrypted AMI with Account B</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>In Account A, create an encrypted AMI from the unencrypted version with AWS managed key. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>In Account B, create a KMS grant that delegates permissions to the service-linked role attached to the Auto Scaling group</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 6: Security and Compliance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</strong></p>\n\n<p><strong>In Account B, create a KMS grant that delegates permissions to the service-linked role attached to the Auto Scaling group</strong></p>\n\n<p>As per the security policy of the company, encrypt the unencrypted AMI using the KMS key. The encrypted snapshots must be encrypted with a KMS key. You can’t share AMIs that are backed by snapshots that are encrypted with the default AWS-managed key.</p>\n\n<p>Amazon EC2 Auto Scaling uses service-linked roles to delegate permissions to other AWS services. Amazon EC2 Auto Scaling service-linked roles are predefined and include permissions that Amazon EC2 Auto Scaling requires to call other AWS services on your behalf. The predefined permissions also include access to your AWS-managed keys. However, they do not include access to your customer-managed keys, allowing you to maintain full control over these keys.</p>\n\n<p>If you create a customer-managed key in a different account than the Auto Scaling group, you must use a grant in combination with the key policy to allow cross-account access to the key. This is a two-step process (refer to the image attached)</p>\n\n<ol>\n<li><p>The first policy allows Account A to give an IAM user or role in the specified Account B permission to create a grant for the key. However, this does not by itself give any users access to the key.</p></li>\n<li><p>Then, from Account B which contains the Auto Scaling group, create a grant that delegates the relevant permissions to the appropriate service-linked role. The <code>Grantee Principal</code> element of the grant is the ARN of the appropriate service-linked role. The <code>key-id</code> is the ARN of the key.</p></li>\n</ol>\n\n<p>Key policy sections that allow cross-account access to the customer-managed key:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q18-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q18-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version with AWS managed key. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</strong></p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify permissions on the AMI to be accessible from Account B</strong></p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version and encrypt the associated EBS snapshots with it. Specify the KMS key in the copy action. Share the encrypted AMI with Account B</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>In Account B, create a KMS grant that delegates permissions to the service-linked role attached to the Auto Scaling group</strong>"
      },
      {
        "answer": "",
        "explanation": "As per the security policy of the company, encrypt the unencrypted AMI using the KMS key. The encrypted snapshots must be encrypted with a KMS key. You can’t share AMIs that are backed by snapshots that are encrypted with the default AWS-managed key."
      },
      {
        "answer": "",
        "explanation": "Amazon EC2 Auto Scaling uses service-linked roles to delegate permissions to other AWS services. Amazon EC2 Auto Scaling service-linked roles are predefined and include permissions that Amazon EC2 Auto Scaling requires to call other AWS services on your behalf. The predefined permissions also include access to your AWS-managed keys. However, they do not include access to your customer-managed keys, allowing you to maintain full control over these keys."
      },
      {
        "answer": "",
        "explanation": "If you create a customer-managed key in a different account than the Auto Scaling group, you must use a grant in combination with the key policy to allow cross-account access to the key. This is a two-step process (refer to the image attached)"
      },
      {},
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q18-i1.jpg",
        "answer": "",
        "explanation": "Key policy sections that allow cross-account access to the customer-managed key:"
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>In Account A, create an encrypted AMI from the unencrypted version with AWS managed key. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify permissions on the AMI to be accessible from Account B</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>In Account A, create an encrypted AMI from the unencrypted version and encrypt the associated EBS snapshots with it. Specify the KMS key in the copy action. Share the encrypted AMI with Account B</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html"
    ]
  }
]