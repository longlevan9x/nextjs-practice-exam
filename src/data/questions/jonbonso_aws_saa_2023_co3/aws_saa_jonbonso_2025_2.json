[
  {
    "id": 1,
    "question": "<p>A company has an on-premises MySQL database that needs to be replicated in Amazon S3 as CSV files. The database will eventually be launched to an Amazon Aurora Serverless cluster and be integrated with an RDS Proxy to allow the web applications to pool and share database connections. Once data has been fully copied, the ongoing changes to the on-premises database should be continually streamed into the S3 bucket. The company wants a solution that can be implemented with little management overhead yet still highly secure.</p><p>Which ingestion pattern should a solutions architect take?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>AWS Database Migration Service (AWS DMS)</strong> is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. You can use AWS DMS to migrate your data into the AWS Cloud, between on-premises instances (through an AWS Cloud setup) or between combinations of cloud and on-premises setups. With AWS DMS, you can perform one-time migrations, and you can replicate ongoing changes to keep sources and targets in sync.</p><p><img src=\"https://media.tutorialsdojo.com/aws-dms-tutorialsdojo-SSL-endpoint-saa-c03.png\"></p><p>You can migrate data to Amazon S3 using AWS DMS from any of the supported database sources. When using Amazon S3 as a target in an AWS DMS task, both full load and change data capture (CDC) data is written to comma-separated value (.csv) format by default.</p><p>The comma-separated value (.csv) format is the default storage format for Amazon S3 target objects. For more compact storage and faster queries, you can instead use Apache Parquet (.parquet) as the storage format.</p><p>You can encrypt connections for source and target endpoints by using Secure Sockets Layer (SSL). To do so, you can use the AWS DMS Management Console or AWS DMS API to assign a certificate to an endpoint. You can also use the AWS DMS console to manage your certificates.</p><p>Not all databases use SSL in the same way. Amazon Aurora MySQL-Compatible Edition uses the server name, the endpoint of the primary instance in the cluster, as the endpoint for SSL. An Amazon Redshift endpoint already uses an SSL connection and does not require an SSL connection set up by AWS DMS.</p><p>Hence, the correct answer is: <strong>Create a full load and change data capture (CDC) replication task using AWS Database Migration Service (AWS DMS). Add a new Certificate Authority (CA) certificate and create an AWS DMS endpoint with SSL.</strong></p><p>The option that says: <strong>Set up a full load replication task using AWS Database Migration Service (AWS DMS). Launch an AWS DMS endpoint with SSL using the AWS Network Firewall service </strong>is incorrect because a full load replication task alone won't capture ongoing changes to the database. You still need to implement a change data capture (CDC) replication to copy the recent changes after the migration. Moreover, the AWS Network Firewall service is not capable of creating an AWS DMS endpoint with SSL. The Certificate Authority (CA) certificate can be directly uploaded to the AWS DMS console without the AWS Network Firewall at all.</p><p>The option that says: <strong>Use an AWS Snowball Edge cluster to migrate data to Amazon S3 and AWS DataSync to capture ongoing changes </strong>is incorrect. While this is doable, it's more suited to the migration of large databases which require the use of two or more Snowball Edge appliances. Also, the usage of AWS DataSync for replicating ongoing changes to Amazon S3 requires extra steps that can be simplified with AWS DMS.</p><p>The option that says:<strong> Use AWS Schema Conversion Tool (AWS SCT) to convert MySQL data to CSV files. Set up the AWS Application Migration Service (AWS MGN) to capture ongoing changes from the on-premises MySQL database and send them to Amazon S3 </strong>is incorrect. AWS SCT is not used for data replication; it just eases up the conversion of source databases to a format compatible with the target database when migrating. In addition, using the AWS Application Migration Service (AWS MGN) for this scenario is inappropriate. This service is primarily used for lift-and-shift migrations of applications from physical infrastructure, VMware vSphere, Microsoft Hyper-V, Amazon Elastic Compute Cloud (AmazonEC2), Amazon Virtual Private Cloud (Amazon VPC), and other clouds to AWS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/big-data/loading-ongoing-data-lake-changes-with-aws-dms-and-aws-glue/\">https://aws.amazon.com/blogs/big-data/loading-ongoing-data-lake-changes-with-aws-dms-and-aws-glue/</a></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html\">https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#CHAP_Security.SSL.Limitations\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#CHAP_Security.SSL.Limitations</a></p><p><br></p><p><strong>Check out this AWS Database Migration Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-database-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-database-migration-service/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a full load replication task using AWS Database Migration Service (AWS DMS). Launch an AWS DMS endpoint with SSL using the AWS Network Firewall service.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a full load and change data capture (CDC) replication task using AWS Database Migration Service (AWS DMS). Add a new Certificate Authority (CA) certificate and create an AWS DMS endpoint with SSL.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use an AWS Snowball Edge cluster to migrate data to Amazon S3 and AWS DataSync to capture ongoing changes. Create your own custom AWS KMS envelope encryption key for the associated AWS Snowball Edge job.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Schema Conversion Tool (AWS SCT) to convert MySQL data to CSV files. Set up the AWS Application Migration Service (AWS MGN) to capture ongoing changes from the on-premises MySQL database and send them to Amazon S3.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 2,
    "question": "<p>A company has a serverless application made up of AWS Amplify, Amazon API Gateway and a Lambda function. The application is connected to an Amazon RDS MySQL database instance inside a private subnet. A Lambda Function URL is also implemented as the dedicated HTTPS endpoint for the function, which has the following value:</p><p><code>https://12june1898pil1pinas.lambda-url.us-west-2.on.aws/</code> </p><p>There are times during peak loads when the database throws a “too many connections” error preventing the users from accessing the application.</p><p>Which solution could the company take to resolve the issue?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>If a \"Too Many Connections\" error happens to a client connecting to a MySQL database, it means all available connections are in use by other clients. Opening a connection consumes resources on the database server. Since Lambda functions can scale to tens of thousands of concurrent connections, your database needs more resources to open and maintain connections instead of executing queries. The maximum number of connections a database can support is largely determined by the amount of memory allocated to it. Upgrading to a database instance with higher memory is a straightforward way of solving the problem. Another approach would be to maintain a connection pool that clients can reuse. This is where RDS Proxy comes in.</p><p><img src=\"https://media.tutorialsdojo.com/amazon-rds-proxy.jpg\"></p><p>RDS Proxy helps you manage a large number of connections from Lambda to an RDS database by establishing a warm connection pool to the database. Your Lambda functions interact with RDS Proxy instead of your database instance. It handles the connection pooling necessary for scaling many simultaneous connections created by concurrent Lambda functions. This allows your Lambda applications to reuse existing connections, rather than creating new connections for every function invocation.</p><p>Thus, the correct answer is: <strong>Provision an RDS Proxy between the Lambda function and RDS database instance format</strong></p><p>The option that says: <strong>Increase the concurrency limit of the Lambda function</strong> is incorrect. The concurrency limit refers to the maximum requests AWS Lambda can handle at the same time. Increasing the limit will allow for more requests to open a database connection, which could potentially worsen the problem.</p><p>The option that says: <strong>Increase the rate limit of API Gateway</strong> is incorrect. This won't fix the issue at all as all it does is increase the number of API requests a client can make.</p><p>The option that says: <strong>Increase the memory allocation of the Lambda function </strong>is incorrect. Increasing the Lambda function's memory would only make it run processes faster. It can help but it won't likely do any significant effect to get rid of the error. The \"too many connections\" error is a database-related issue. Solutions that have to do with databases, like upgrading to a larger database instance or, in this case, creating a database connection pool using RDS Proxy have better chance of resolving the problem.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/proxy/\">https://aws.amazon.com/rds/proxy/</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/\">https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Increase the concurrency limit of the Lambda function</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Provision an RDS Proxy between the Lambda function and RDS database instance</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Increase the rate limit of API Gateway</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Increase the memory allocation of the Lambda function</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 3,
    "question": "<p>A music publishing company is building a multitier web application that requires a key-value store which will save the document models. Each model is composed of band ID, album ID, song ID, composer ID, lyrics, and other data. The web tier will be hosted in an Amazon ECS cluster with AWS Fargate launch type. </p><p>Which of the following is the MOST suitable setup for the database-tier?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>Amazon DynamoDB</strong> is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed cloud database and supports both document and key-value store models. Its flexible data model, reliable performance, and automatic scaling of throughput capacity makes it a great fit for mobile, web, gaming, ad tech, IoT, and many other applications.</p><p><img src=\"https://media.tutorialsdojo.com/2018-10-23_05-24-29-74b3e6dadc8ce683ccd2a5bd00f99889.png\"></p><p>Hence, the correct answer is: <strong>Launch a DynamoDB table.</strong></p><p>The option that says: <strong>Launch an Amazon RDS database with Read Replicas</strong> is incorrect because this is a relational database. This is not suitable to be used as a key-value store. A better option is to use DynamoDB as it supports both document and key-value store models.</p><p>The option that says: <strong>Use Amazon WorkDocs to store the document models</strong> is incorrect because Amazon WorkDocs simply enables you to share content, provide rich feedback, and collaboratively edit documents. It is not a key-value store like DynamoDB.</p><p>The option that says: <strong>Launch an Amazon Aurora Serverless database</strong> is incorrect because this type of database is not suitable to be used as a key-value store. Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora where the database will automatically start-up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads and not as a key-value store.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p><p><a href=\"https://aws.amazon.com/nosql/key-value/\">https://aws.amazon.com/nosql/key-value/</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a><br></p><p><strong>Amazon DynamoDB Overview:</strong></p><p><a href=\"https://youtu.be/3ZOyUNIeorU\">https://youtu.be/3ZOyUNIeorU</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch an Amazon RDS database with Read Replicas.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Launch a DynamoDB table.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon WorkDocs to store the document models.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Launch an Amazon Aurora Serverless database.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 4,
    "question": "<p>A company plans to migrate its suite of containerized applications running on-premises to a container service in AWS. The solution must be cloud-agnostic and use an open-source platform that can automatically manage containerized workloads and services. It should also use the same configuration and tools across various production environments.</p><p>What should the Solution Architect do to properly migrate and satisfy the given requirement?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon EKS</strong> provisions and scales the Kubernetes control plane, including the API servers and backend persistence layer, across multiple AWS availability zones for high availability and fault tolerance. Amazon EKS automatically detects and replaces unhealthy control plane nodes and provides patching for the control plane. Amazon EKS is integrated with many AWS services to provide scalability and security for your applications. These services include Elastic Load Balancing for load distribution, IAM for authentication, Amazon VPC for isolation, and AWS CloudTrail for logging.</p><p><img src=\"https://media.tutorialsdojo.com/Landing-Page-Diagram-EKS@2x.e7f23e44e4b460fc9f46d5f77a3a8d60807dc111.png\"></p><p>To migrate the application to a container service, you can use Amazon ECS or Amazon EKS. But the key point in this scenario is a cloud-agnostic and open-source platforms. Take note that Amazon ECS is an AWS proprietary container service. This means that it is not an open-source platform. Amazon EKS is a portable, extensible, and open-source platform for managing containerized workloads and services. Kubernetes is considered cloud-agnostic because it allows you to move your containers to other cloud service providers.</p><p>Amazon EKS runs up-to-date versions of the open-source Kubernetes software, so you can use all of the existing plugins and tools from the Kubernetes community. Applications running on Amazon EKS are fully compatible with applications running on any standard Kubernetes environment, whether running in on-premises data centers or public clouds. This means that you can easily migrate any standard Kubernetes application to Amazon EKS without any code modification required.</p><p>Hence, the correct answer is:<strong> Migrate the application to Amazon Elastic Kubernetes Service with EKS worker nodes.</strong></p><p>The option that says:<strong> Migrate the application to Amazon Container Registry (ECR) with Amazon EC2 instance worker nodes </strong>is incorrect because Amazon ECR is just a fully-managed Docker container registry. Also, this option is not an open-source platform that can manage containerized workloads and services.</p><p>The option that says:<strong> Migrate the application to Amazon Elastic Container Service with ECS tasks that use the AWS Fargate launch type </strong>is incorrect because it is stated in the scenario that you have to migrate the application suite to an open-source platform. AWS Fargate is just a serverless compute engine for containers. It is not cloud-agnostic since you cannot use the same configuration and tools if you move it to another cloud service provider such as Microsoft Azure or Google Cloud Platform (GCP).</p><p>The option that says:<strong> Migrate the application to Amazon Elastic Container Service with ECS tasks that use the Amazon EC2 launch type</strong> is incorrect because Amazon ECS is an AWS proprietary managed container orchestration service. You should use Amazon EKS since Kubernetes is an open-source platform and is considered cloud-agnostic. With Kubernetes, you can use the same configuration and tools that you're currently using in AWS even if you move your containers to another cloud service provider.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\">https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html</a></p><p><a href=\"https://aws.amazon.com/eks/faqs/\">https://aws.amazon.com/eks/faqs/</a></p><p><br></p><p><strong>Check out our library of AWS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/links-to-all-aws-cheat-sheets/?src=udemy\">https://tutorialsdojo.com/links-to-all-aws-cheat-sheets/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Migrate the application to Amazon Container Registry (ECR) with Amazon EC2 instance worker nodes.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Migrate the application to Amazon Elastic Kubernetes Service with EKS worker nodes.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Migrate the application to Amazon Elastic Container Service with ECS tasks that use the AWS Fargate launch type.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Migrate the application to Amazon Elastic Container Service with ECS tasks that use the Amazon EC2 launch type.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 5,
    "question": "<p>A media company has two VPCs: VPC-1 and VPC-2 with peering connection between each other. VPC-1 only contains private subnets while VPC-2 only contains public subnets. The company uses a single AWS Direct Connect connection and a virtual interface to connect their on-premises network with VPC-1. <br><br>Which of the following options increase the fault tolerance of the connection to VPC-1? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "<p>In this scenario, you have two VPCs which have peering connections with each other. Note that a VPC peering connection does not support edge to edge routing. This means that if either VPC in a peering relationship has one of the following connections, you cannot extend the peering relationship to that connection:</p><p>- A VPN connection or an AWS Direct Connect connection to a corporate network</p><p>- An Internet connection through an Internet gateway</p><p>- An Internet connection in a private subnet through a NAT device</p><p>- A gateway VPC endpoint to an AWS service; for example, an endpoint to Amazon S3.</p><p>- (IPv6) A ClassicLink connection. You can enable IPv4 communication between a linked EC2-Classic instance and instances in a VPC on the other side of a VPC peering connection. However, IPv6 is not supported in EC2-Classic, so you cannot extend this connection for IPv6 communication.</p><p><img src=\"https://media.tutorialsdojo.com/edge-to-edge-vpn-diagram.png\"> For example, if VPC A and VPC B are peered, and VPC A has any of these connections, then instances in VPC B cannot use the connection to access resources on the other side of the connection. Similarly, resources on the other side of a connection cannot use the connection to access VPC B.</p><p>Hence, this means that you cannot use VPC-2 to extend the peering relationship that exists between VPC-1 and the on-premises network. For example, traffic from the corporate network can't directly access VPC-1 by using the VPN connection or the AWS Direct Connect connection to VPC-2, which is why the following options are incorrect:</p><p><strong>- Use the AWS VPN CloudHub to create a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2.</strong></p><p><strong>- Establish a hardware VPN over the Internet between VPC-2 and the on-premises network.</strong></p><p><strong>- Establish a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2.</strong></p><p>You can do the following to provide a highly available, fault-tolerant network connection:</p><p><strong>- Establish a hardware VPN over the Internet between the VPC and the on-premises network.</strong></p><p><strong>- Establish another AWS Direct Connect connection and private virtual interface in the same AWS region.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/invalid-peering-configurations.html#edge-to-edge-vgw\">https://docs.aws.amazon.com/vpc/latest/peering/invalid-peering-configurations.html#edge-to-edge-vgw</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/\">https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/</a></p><p><a href=\"https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/\">https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AWS VPN CloudHub to create a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Establish a hardware VPN over the Internet between VPC-1 and the on-premises network.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Establish a hardware VPN over the Internet between VPC-2 and the on-premises network.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Establish a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Establish another AWS Direct Connect connection and private virtual interface in the same AWS region as VPC-1.",
        "correct": true
      }
    ],
    "corrects": [
      2,
      5
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 6,
    "question": "<p>An online events registration system is hosted in AWS and uses ECS to host its front-end tier and an RDS configured with Multi-AZ for its database tier. What are the events that will make Amazon RDS automatically perform a failover to the standby replica? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>Amazon RDS</strong> provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for Oracle, PostgreSQL, MySQL, and MariaDB DB instances use Amazon's failover technology. SQL Server DB instances use SQL Server Database Mirroring (DBM).</p><p>In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance and help protect your databases against DB instance failure and Availability Zone disruption.</p><p>Amazon RDS detects and automatically recovers from the most common failure scenarios for Multi-AZ deployments so that you can resume database operations as quickly as possible without administrative intervention.</p><p><img src=\"https://media.tutorialsdojo.com/con-multi-AZ.png\"></p><p>The high-availability feature is not a scaling solution for read-only scenarios; you cannot use a standby replica to serve read traffic. To service read-only traffic, you should use a Read Replica.</p><p>Amazon RDS automatically performs a failover in the event of any of the following:</p><ol><li><p>Loss of availability in primary Availability Zone.</p></li><li><p>Loss of network connectivity to primary.</p></li><li><p>Compute unit failure on primary.</p></li><li><p>Storage failure on primary.</p></li></ol><p>Hence, the correct answers are:</p><p><strong>- Loss of availability in primary Availability Zone</strong><br><strong>- Storage failure on primary</strong></p><p>The following options are incorrect because all these scenarios do not affect the primary database. Automatic failover only occurs if the primary database is the one that is affected.</p><p><strong>- Storage failure on secondary DB instance</strong></p><p><strong>- In the event of Read Replica failure</strong></p><p><strong>- Compute unit failure on secondary DB instance</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/details/multi-az/\">https://aws.amazon.com/rds/details/multi-az/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Loss of availability in primary Availability Zone",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Storage failure on primary</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Storage failure on secondary DB instance</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>In the event of Read Replica failure</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Compute unit failure on secondary DB instance</p>",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 7,
    "question": "<p>A company wants to streamline the process of creating multiple AWS accounts within an AWS Organization. Each organization unit (OU) must be able to launch new accounts with preapproved configurations from the security team which will standardize the baselines and network configurations for all accounts in the organization.</p><p>Which solution entails the least amount of effort to implement?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS Control Tower</strong> provides a single location to easily set up your new well-architected multi-account environment and govern your AWS workloads with rules for security, operations, and internal compliance. You can automate the setup of your AWS environment with best-practices blueprints for multi-account structure, identity, access management, and account provisioning workflow. For ongoing governance, you can select and apply pre-packaged policies enterprise-wide or to specific groups of accounts.</p><p><img src=\"https://media.tutorialsdojo.com/aws-control-tower-landing-zone.png\"></p><p>AWS Control Tower provides three methods for creating member accounts:</p><p>- Through the Account Factory console that is part of AWS Service Catalog.</p><p>- Through the Enroll account feature within AWS Control Tower.</p><p>- From your AWS Control Tower landing zone's management account, using Lambda code and appropriate IAM roles.</p><p>AWS Control Tower offers \"guardrails\" for ongoing governance of your AWS environment. Guardrails provide governance controls by preventing the deployment of resources that don’t conform to selected policies or detecting non-conformance of provisioned resources. AWS Control Tower automatically implements guardrails using multiple building blocks such as AWS CloudFormation to establish a baseline, AWS Organizations service control policies (SCPs) to prevent configuration changes, and AWS Config rules to continuously detect non-conformance.</p><p>In this scenario, the requirement is to simplify the creation of AWS accounts that have governance guardrails and a defined baseline in place. To save time and resources, you can use AWS Control Tower to automate account creation. With the appropriate user group permissions, you can specify standardized baselines and network configurations for all accounts in the organization.</p><p>Hence, the correct answer is:<strong> Set up an AWS Control Tower Landing Zone. Enable pre-packaged guardrails to enforce policies or detect violations.</strong></p><p>The option that says: <strong>Configure AWS Resource Access Manager (AWS RAM) to launch new AWS accounts as well as standardize the baselines and network configurations for each organization unit </strong>is incorrect. The AWS Resource Access Manager (RAM) service simply helps you to securely share your resources across AWS accounts or within your organization or organizational units (OUs) in AWS Organizations. It is not capable of launching new AWS accounts with preapproved configurations.</p><p>The option that says:<strong> Set up an AWS Config aggregator on the root account of the organization to enable multi-account, multi-region data aggregation. Deploy conformance packs to standardize the baselines and network configurations for all accounts</strong> is incorrect. AWS Config cannot provision accounts. A conformance pack is only a collection of AWS Config rules and remediation actions that can be easily deployed as a single entity in an account and a Region or across an organization in AWS Organizations.</p><p>The option that says: <strong>Centralized the creation of AWS accounts using AWS Systems Manager OpsCenter. Enforce policies and detect violations to all AWS accounts using AWS Security Hub </strong>is incorrect. AWS Systems Manager is just a collection of services used to manage applications and infrastructure running in AWS that is usually in a single AWS account. The AWS Systems Manager OpsCenter service is just one of the capabilities of AWS Systems Manager, provides a central location where operations engineers and IT professionals can view, investigate, and resolve operational work items (OpsItems) related to AWS resources.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/controltower/latest/userguide/account-factory.html\">https://docs.aws.amazon.com/controltower/latest/userguide/account-factory.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/how-to-automate-the-creation-of-multiple-accounts-in-aws-control-tower/\">https://aws.amazon.com/blogs/mt/how-to-automate-the-creation-of-multiple-accounts-in-aws-control-tower/</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/aws-control-tower-set-up-govern-a-multi-account-aws-environment/\">https://aws.amazon.com/blogs/aws/aws-control-tower-set-up-govern-a-multi-account-aws-environment/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an AWS Control Tower Landing Zone. Enable pre-packaged guardrails to enforce policies or detect violations.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure AWS Resource Access Manager (AWS RAM) to launch new AWS accounts as well as standardize the baselines and network configurations for each organization unit</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up an AWS Config aggregator on the root account of the organization to enable multi-account, multi-region data aggregation. Deploy conformance packs to standardize the baselines and network configurations for all accounts.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Centralized the creation of AWS accounts using AWS Systems Manager OpsCenter. Enforce policies and detect violations to all AWS accounts using AWS Security Hub.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 8,
    "question": "<p>A Solutions Architect created a new Standard-class S3 bucket to store financial reports that are not frequently accessed but should immediately be available when an auditor requests them. To save costs, the Architect changed the storage class of the S3 bucket from Standard to Infrequent Access storage class.</p><p>In Amazon S3 Standard - Infrequent Access storage class, which of the following statements are true? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon S3 Standard - Infrequent Access (Standard - IA)</strong> is an Amazon S3 storage class for data that is accessed less frequently, but requires rapid access when needed. Standard - IA offers the high durability, throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee.</p><p><img src=\"https://media.tutorialsdojo.com/S3-1024x585.jpg\"></p><p>This combination of low cost and high performance make Standard - IA ideal for long-term storage, backups, and as a data store for disaster recovery. The Standard - IA storage class is set at the object level and can exist in the same bucket as Standard, allowing you to use lifecycle policies to automatically transition objects between storage classes without any application changes.</p><p><strong>Key Features:</strong></p><p>- Same low latency and high throughput performance of Standard</p><p>- Designed for durability of 99.999999999% of objects</p><p>- Designed for 99.9% availability over a given year</p><p>- Backed with the Amazon S3 Service Level Agreement for availability</p><p>- Supports SSL encryption of data in transit and at rest</p><p>- Lifecycle management for automatic migration of objects</p><p>Hence, the correct answers are:</p><p>- <strong>It is designed for data that is accessed less frequently.</strong></p><p>- <strong>It is designed for data that requires rapid access when needed.</strong></p><p>The option that says: <strong>It automatically moves data to the most cost-effective access tier without any operational overhead</strong> is incorrect as it actually refers to Amazon S3 - Intelligent Tiering, which is the only cloud storage class that delivers automatic cost savings by moving objects between different access tiers when access patterns change.</p><p>The option that says: <strong>It provides high latency and low throughput performance</strong> is incorrect as it should be \"low latency\" and \"high throughput\" instead. S3 automatically scales performance to meet user demands.</p><p>The option that says: <strong>Ideal to use for data archiving</strong> is incorrect because this statement refers to Amazon S3 Glacier. Glacier is a secure, durable, and extremely low-cost cloud storage service for data archiving and long-term backup.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><a href=\"https://aws.amazon.com/s3/faqs\">https://aws.amazon.com/s3/faqs</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>It is designed for data that is accessed less frequently.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>It automatically moves data to the most cost-effective access tier without any operational overhead.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>It is designed for data that requires rapid access when needed.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>It provides high latency and low throughput performance</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Ideal to use for data archiving.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 9,
    "question": "<p>A solutions architect is designing a secure, cost-effective, and highly available storage solution for a company’s data. One of the requirements is to ensure that the previous version of a file is preserved and can be retrieved if a modified version is uploaded. Additionally, the company must comply with strict regulatory requirements that mandate data retention in an immutable state for at least 3 years before being moved to an archive. Once archived, the data will only be accessed once a year.</p><p>How should the solutions architect build the solution?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p>Amazon S3 has capabilities like versioning, Object Lock, and lifecycle policies to help with data preservation and archiving. Object Lock enables you to store objects using a write-once, read-many (WORM) model, which can assist prevent objects from being deleted or rewritten for a set period of time or indefinitely.</p><p><strong>- S3 Object Lock in Compliance Mode</strong> ensures that an object version cannot be overwritten or deleted by any user, including the root user of the AWS account. This mode is typically used for regulatory compliance purposes.</p><p><strong>- S3 Glacier Deep Archive</strong> is an Amazon S3 storage class that provides secure, durable, and extremely low-cost storage for long-term data archiving and digital preservation. Data can be retained in Glacier Deep Archive to meet the requirement of keeping it for 3 years and accessing it only once a year.</p><p><img src=\"https://media.tutorialsdojo.com/public/lifecycle-transitions-0542pm-05-29-23.png\">Hence, the correct answer is:<strong> Create an S3 Standard bucket with S3 Object Lock in compliance mode enabled then configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years.</strong></p><p>The option that says: <strong>Create an S3 Standard bucket with object-level versioning enabled and configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years </strong>is incorrect because it doesn't meet regulatory compliance. Although it preserves versions and archives data, it simply lacks S3 Object Lock in compliance mode, which is essential to prevent deletion or modification during the retention period. Without this feature, the data is at risk, leading to potential non-compliance.</p><p>The option that says:<strong> Create an S3 Standard bucket and enable S3 Object Lock in governance mode </strong>is incorrect because it does not only provide the same level of protection as compliance mode. Governance mode allows certain users to remove the lock, which may not meet the strict regulatory compliance requirement.</p><p>The option that says: <strong>Create a One-Zone-IA bucket with object-level versioning enabled and configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years</strong> is incorrect because it does not typically offer the same durability and availability as S3 Standard, and it is not recommended for critical data that needs to be retained for compliance reasons. Moreover, like Option 1, it also lacks S3 Object Lock in compliance mode.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-amazon-s3-storage-class-glacier-deep-archive/\">https://aws.amazon.com/blogs/aws/new-amazon-s3-storage-class-glacier-deep-archive/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html#object-lock-overview\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html#object-lock-overview</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an S3 Standard bucket with object-level versioning enabled and configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an S3 Standard bucket and enable S3 Object Lock in governance mode.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an S3 Standard bucket with S3 Object Lock in compliance mode enabled then configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create a One-Zone-IA bucket with object-level versioning enabled and configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 10,
    "question": "<p>A Solutions Architect is building a cloud infrastructure where EC2 instances require access to various AWS services such as S3 and Redshift. The Architect will also need to provide access to system administrators so they can deploy and test their changes.</p><p>Which configuration should be used to ensure<strong> </strong>that the access to the resources is secured and not compromised? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "<p>In this scenario, the correct answers are:</p><p><strong>- Enable Multi-Factor Authentication</strong></p><p><strong>- Assign an IAM role to the Amazon EC2 instance</strong></p><p>Always remember that you should associate IAM roles to EC2 instances and not an IAM user, for the purpose of accessing other AWS services. IAM roles are designed so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.</p><p><img src=\"https://dmhnzl5mp9mj6.cloudfront.net/security_awsblog/images/MFAAPI4.png\"></p><p><strong>AWS Multi-Factor Authentication (MFA)</strong> is a simple best practice that adds an extra layer of protection on top of your user name and password. With MFA enabled, when a user signs in to an AWS website, they will be prompted for their user name and password (the first factor—what they know), as well as for an authentication code from their AWS MFA device (the second factor—what they have). Taken together, these multiple factors provide increased security for your AWS account settings and resources. You can enable MFA for your AWS account and for individual IAM users you have created under your account. MFA can also be used to control access to AWS service APIs.</p><p><strong>Storing the AWS Access Keys in the EC2 instance</strong> is incorrect. This is not recommended by AWS as it can be compromised. Instead of storing access keys on an EC2 instance for use by applications that run on the instance and make AWS API requests, you can use an IAM role to provide temporary access keys for these applications.</p><p><strong>Assigning an IAM user for each Amazon EC2 Instance</strong> is incorrect because there is no need to create an IAM user for this scenario since IAM roles already provide greater flexibility and easier management.</p><p><strong>Storing the AWS Access Keys in ACM</strong> is incorrect because ACM is just a service that lets you easily provision, manage, and deploy public and private SSL/TLS certificates for use with AWS services and your internal connected resources. It is not used as a secure storage for your access keys.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/iam/details/mfa/\">https://aws.amazon.com/iam/details/mfa/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Enable Multi-Factor Authentication.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Assign an IAM role to the Amazon EC2 instance.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Store the AWS Access Keys in the EC2 instance.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Assign an IAM user for each Amazon EC2 Instance.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Store the AWS Access Keys in ACM.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 11,
    "question": "<p>A media company is setting up an ECS batch architecture for its image processing application. It will be hosted in an Amazon ECS Cluster with two ECS tasks that will handle image uploads from the users and image processing. The first ECS task will process the user requests, store the image in an S3 input bucket, and push a message to a queue. The second task reads from the queue, parses the message containing the object name, and then downloads the object. Once the image is processed and transformed, it will upload the objects to the S3 output bucket. To complete the architecture, the Solutions Architect must create a queue and the necessary IAM permissions for the ECS tasks.</p><p>Which of the following should the Architect do next?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>Docker containers are particularly suited for batch job workloads, which are often short-lived and embarrassingly parallel. You can package your batch processing application into a Docker image and deploy it anywhere, such as in an Amazon ECS task.</p><p>Amazon ECS supports batch jobs. You can use Amazon ECS <em>Run Task</em> action to run one or more tasks once. The Run Task action starts the ECS task on an instance that meets the task’s requirements including CPU, memory, and ports.</p><p><img src=\"https://media.tutorialsdojo.com/ECSBatchRefArch.png\"></p><p>For example, you can set up an ECS Batch architecture for an image processing application. You can set up an AWS CloudFormation template that creates an Amazon S3 bucket, an Amazon SQS queue, an Amazon CloudWatch alarm, an ECS cluster, and an ECS task definition. Objects uploaded to the input S3 bucket trigger an event that sends object details to the SQS queue. The ECS task deploys a Docker container that reads from that queue, parses the message containing the object name and then downloads the object. Once transformed it will upload the objects to the S3 output bucket.</p><p>By using the SQS queue as the location for all object details, you can take advantage of its scalability and reliability. The queue will automatically scale based on the incoming messages, and message retention can be configured. The ECS Cluster will then be able to scale services up or down based on the number of messages in the queue.</p><p>You have to create an IAM Role that the ECS task assumes in order to get access to the S3 buckets and SQS queue. Note that the permissions of the IAM role don't specify the S3 bucket ARN for the incoming bucket. This is to avoid a circular dependency issue in the CloudFormation template. You should always make sure to assign the least amount of privileges needed to an IAM role.</p><p>Hence, the correct answer is: <strong>Launch a new Amazon SQS queue and configure the second ECS task to read from it. Create an IAM role that the ECS tasks can assume in order to get access to the S3 buckets and SQS queue. Declare the IAM Role (</strong><code><strong>taskRoleArn</strong></code><strong>) in the task definition.</strong></p><p>The option that says: <strong>Launch a new Amazon AppStream 2.0 queue and configure the second ECS task to read from it. Create an IAM role that the ECS tasks can assume in order to get access to the S3 buckets and AppStream 2.0 queue. Declare the IAM Role (</strong><code><strong>taskRoleArn</strong></code><strong>) in the task definition</strong> is incorrect because Amazon AppStream 2.0 is a fully managed application streaming service and can't be used as a queue. You have to use Amazon SQS instead.</p><p>The option that says: <strong>Launch a new Amazon Data Firehose and configure the second ECS task to read from it. Create an IAM role that the ECS tasks can assume in order to get access to the S3 buckets and Data Firehose. Specify the ARN of the IAM Role in the (</strong><code><strong>taskDefinitionArn</strong></code><strong>) field of the task definition</strong> is incorrect because Amazon Data Firehose is a fully managed service for delivering real-time streaming data. Although it can stream data to an S3 bucket, it is not suitable to be used as a queue for a batch application in this scenario. In addition, the ARN of the IAM Role should be declared in the <code>taskRoleArn</code> and not in the <code>taskDefinitionArn</code> field.</p><p>The option that says: <strong>Launch a new Amazon MQ queue and configure the second ECS task to read from it. Create an IAM role that the ECS tasks can assume in order to get access to the S3 buckets and Amazon MQ queue. Set the (</strong><code><strong>EnableTaskIAMRole</strong></code><strong>) option to true in the task definition</strong> is incorrect because Amazon MQ is primarily used as a managed message broker service and not a queue. The <code>EnableTaskIAMRole</code> option is only applicable for Windows-based ECS Tasks that require extra configuration.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://github.com/aws-samples/ecs-refarch-batch-processing\">https://github.com/aws-samples/ecs-refarch-batch-processing</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/common_use_cases.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/common_use_cases.html</a></p><p><a href=\"https://aws.amazon.com/ecs/faqs/\">https://aws.amazon.com/ecs/faqs/</a></p><p><br></p><p><strong>Check out these Amazon Elastic Container Service and Amazon SQS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch a new Amazon SQS queue and configure the second ECS task to read from it. Create an IAM role that the ECS tasks can assume in order to get access to the S3 buckets and SQS queue. Declare the IAM Role (<code>taskRoleArn</code>) in the task definition.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Launch a new Amazon AppStream 2.0 queue and configure the second ECS task to read from it. Create an IAM role that the ECS tasks can assume in order to get access to the S3 buckets and AppStream 2.0 queue. Declare the IAM Role (<code>taskRoleArn</code>) in the task definition.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Launch a new Amazon Data Firehose and configure the second ECS task to read from it. Create an IAM role that the ECS tasks can assume in order to get access to the S3 buckets and Data Firehose. Specify the ARN of the IAM Role in the <code>taskDefinitionArn</code> field of the task definition.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Launch a new Amazon MQ queue and configure the second ECS task to read from it. Create an IAM role that the ECS tasks can assume in order to get access to the S3 buckets and Amazon MQ queue. Set the (<code>EnableTaskIAMRole</code>) option to true in the task definition.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 12,
    "question": "<p>A company is running a dashboard application on a Spot EC2 instance inside a private subnet. The dashboard is reachable via a domain name that maps to the private IPv4 address of the instance’s network interface. A solutions architect needs to increase network availability by allowing the traffic flow to resume in another instance if the primary instance is terminated.</p><p>Which solution accomplishes these requirements?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>If one of your instances serving a particular function fails, its network interface can be attached to a replacement or hot standby instance pre-configured for the same role in order to rapidly recover the service. For example, you can use a network interface as your primary or secondary network interface to a critical service such as a database instance or a NAT instance. If the instance fails, you (or more likely, the code running on your behalf) can attach the network interface to a hot standby instance.</p><p><img src=\"https://media.tutorialsdojo.com/reattach-secondary-network-interface.jpg\">Because the interface maintains its private IP addresses, Elastic IP addresses, and MAC address, network traffic begins flowing to the standby instance as soon as you attach the network interface to the replacement instance. Users experience a brief loss of connectivity between the time the instance fails and the time that the network interface is attached to the standby instance, but no changes to the route table or your DNS server are required.</p><p>Hence, the correct answer is <strong>Create a secondary elastic network interface and point its private IPv4 address to the application’s domain name. Attach the new network interface to the primary instance. If the instance goes down, move the secondary network interface to another instance.</strong></p><p>The option that says: <strong>Attach an elastic IP address to the instance’s primary network interface and point its IP address to the application’s domain name. Automatically move the EIP to a secondary instance if the primary instance becomes unavailable using the AWS Transit Gateway </strong>is incorrect. Elastic IPs are not needed in the solution since the application is private. Furthermore, an AWS Transit Gateway is primarily used to connect your Amazon Virtual Private Clouds (VPCs) and on-premises networks through a central hub. This particular networking service cannot be used to automatically move an Elastic IP address to another EC2 instance.</p><p>The option that says: <strong>Set up AWS Transfer for FTPS service in Implicit FTPS mode to automatically disable the </strong><code><strong>source/destination</strong></code><strong> checks on the instance’s primary elastic network interface and reassociate it to another instance </strong>is incorrect. First of all, the AWS Transfer for FTPS service is not capable of automatically disabling the source/destination checks and it only supports Explicit FTPS mode. Disabling the source/destination check only allows the instance to which the ENI is connected to act as a gateway (both a sender and a receiver). It is not possible to make the primary ENI of any EC2 instance detachable. A more appropriate solution would be to use an Elastic IP address which can be reassociated with your secondary instance.</p><p>The option that says: <strong>Use the AWS Network Firewall to detach the instance’s primary elastic network interface and move it to a new instance upon failure </strong>is incorrect. It's not possible to detach the primary network interface of an EC2 instance. In addition, the AWS Network Firewall is only used for filtering traffic at the perimeter of your VPC and not for detaching ENIs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/scenarios-enis.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/scenarios-enis.html</a></p><p><a href=\"https://aws.amazon.com/aws-transfer-family/faqs/\">https://aws.amazon.com/aws-transfer-family/faqs/</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a secondary elastic network interface and point its private IPv4 address to the application’s domain name. Attach the new network interface to the primary instance. If the instance goes down, move the secondary network interface to another instance.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Attach an elastic IP address to the instance’s primary network interface and point its IP address to the application’s domain name. Automatically move the EIP to a secondary instance if the primary instance becomes unavailable using the AWS Transit Gateway.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the AWS Network Firewall to detach the instance’s primary elastic network interface and move it to a new instance upon failure.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up AWS Transfer for FTPS service in Implicit FTPS mode to automatically disable the <code>source/destination</code> checks on the instance’s primary elastic network interface and reassociate it to another instance.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 13,
    "question": "<p>A company has an enterprise web application hosted on Amazon ECS Docker containers that use an Amazon FSx for Lustre filesystem for its high-performance computing workloads. A warm standby environment is running in another AWS region for disaster recovery. A Solutions Architect was assigned to design a system that will automatically route the live traffic to the disaster recovery (DR) environment only in the event that the primary application stack experiences an outage.</p><p>What should the Architect do to satisfy this requirement?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>Use an active-passive failover configuration when you want a primary resource or group of resources to be available majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes only the healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response to DNS queries.</p><p>To create an active-passive failover configuration with one primary record and one secondary record, you just create the records and specify <strong>Failover</strong> for the routing policy. When the primary resource is healthy, Route 53 responds to DNS queries using the primary record. When the primary resource is unhealthy, Route 53 responds to DNS queries using the secondary record.</p><p>You can configure a health check that monitors an endpoint that you specify either by IP address or by domain name. At regular intervals that you specify, Route 53 submits automated requests over the Internet to your application, server, or other resource to verify that it's reachable, available, and functional. Optionally, you can configure the health check to make requests similar to those that your users make, such as requesting a web page from a specific URL.</p><p><img src=\"https://media.tutorialsdojo.com/route_53_elb_config_1.png\"></p><p>When Route 53 checks the health of an endpoint, it sends an HTTP, HTTPS, or TCP request to the IP address and port that you specified when you created the health check. For a health check to succeed, your router and firewall rules must allow inbound traffic from the IP addresses that the Route 53 health checkers use.</p><p>Hence, the correct answer is: <strong>Set up a failover routing policy configuration in Route 53 by adding a health check on the primary service endpoint. Configure Route 53 to direct the DNS queries to the secondary record when the primary resource is unhealthy. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks. Enable the </strong><code><strong><em>Evaluate Target Health</em></strong></code><strong> option by setting it to </strong><code><strong><em>Yes.</em></strong></code></p><p>The option that says: <strong>Set up a Weighted routing policy configuration in Route 53 by adding health checks on both the primary stack and the DR environment. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks. Enable the </strong><code><strong>Evaluate Target Health</strong></code><strong> option by setting it to </strong><code><strong>Yes</strong></code> is incorrect because Weighted routing simply lets you associate multiple resources with a single domain name (tutorialsdojo.com) or subdomain name (blog.tutorialsdojo.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software, but not for a failover configuration. Remember that the scenario says that the solution should automatically route the live traffic to the disaster recovery (DR) environment only in the event that the primary application stack experiences an outage. This configuration is incorrectly distributing the traffic on both the primary and DR environment.</p><p>The option that says: <strong>Set up a CloudWatch Alarm to monitor the primary Route 53 DNS endpoint and create a custom Lambda function. Execute the </strong><code><strong>ChangeResourceRecordSets</strong></code><strong> API call using the function to initiate the failover to the secondary DNS record</strong> is incorrect because setting up a CloudWatch Alarm and using the Route 53 API is not applicable nor useful at all in this scenario. Remember that CloudWatch Alam is primarily used for monitoring CloudWatch metrics. You have to use a Failover routing policy instead.</p><p>The option that says: <strong>Set up a CloudWatch Events rule to monitor the primary Route 53 DNS endpoint and create a custom Lambda function. Execute the</strong><code><strong>ChangeResourceRecordSets</strong></code><strong> API call using the function to initiate the failover to the secondary DNS record</strong> is incorrect because the Amazon CloudWatch Events service is commonly used to deliver a near real-time stream of system events that describe changes in <strong>some</strong> Amazon Web Services (AWS) resources. There is no direct way for CloudWatch Events to monitor the status of your Route 53 endpoints. You have to configure a health check and a failover configuration in Route 53 instead to satisfy the requirement in this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-router-firewall-rules.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-router-firewall-rules.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a failover routing policy configuration in Route 53 by adding a health check on the primary service endpoint. Configure Route 53 to direct the DNS queries to the secondary record when the primary resource is unhealthy. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks. Enable the <code>Evaluate Target Health</code> option by setting it to <code>Yes</code>.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up a CloudWatch Events rule to monitor the primary Route 53 DNS endpoint and create a custom Lambda function. Execute the <code>ChangeResourceRecordSets</code> API call using the function to initiate the failover to the secondary DNS record.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up a Weighted routing policy configuration in Route 53 by adding health checks on both the primary stack and the DR environment. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks. Enable the <code>Evaluate Target Health</code> option by setting it to <code>Yes</code>.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a CloudWatch Alarm to monitor the primary Route 53 DNS endpoint and create a custom Lambda function. Execute the <code>ChangeResourceRecordSets</code> API call using the function to initiate the failover to the secondary DNS record.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 14,
    "question": "<p>A company developed a meal planning application that provides meal recommendations for the week as well as the food consumption of the users. The application resides on an EC2 instance which requires access to various AWS services for its day-to-day operations.</p><p>Which of the following is the best way to allow the EC2 instance to access the S3 bucket and other AWS services?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>The best practice in handling API Credentials is to create a new role in the Identity Access Management (IAM) service and then assign it to a specific EC2 instance. In this way, you have a secure and centralized way of storing and managing your credentials.</p><p><img src=\"https://media.tutorialsdojo.com/AWS-IAMRole-Trust.png\"></p><p><strong>Storing the API credentials in the EC2 instance</strong>, <strong>adding the API Credentials in the Security Group and assigning it to the EC2 instance</strong>, and <strong>storing the API credentials in a bastion host</strong> are incorrect because it is not secure to store nor use the API credentials from an EC2 instance. You should use IAM service instead.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Create a role in IAM and assign it to the EC2 instance.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Store the API credentials in the EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Add the API Credentials in the Security Group and assign it to the EC2 instance.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Store the API credentials in a bastion host.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 15,
    "question": "<p>A company has an application that continually sends encrypted documents to Amazon S3. The company requires that the configuration for data access is in line with their strict compliance standards. They should also be alerted if there is any risk of unauthorized access or suspicious access patterns.</p><p>Which step is needed to meet the requirements?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>Amazon GuardDuty can generate findings based on suspicious activities such as requests coming from known malicious IP addresses, changing of bucket policies/ACLs to expose an S3 bucket publicly, or suspicious API call patterns that attempt to discover misconfigured bucket permissions.</p><p><img src=\"https://media.tutorialsdojo.com/aws-guardduty-s3-findings.jpg\"></p><p>To detect possibly malicious behavior, GuardDuty uses a combination of anomaly detection, machine learning, and continuously updated threat intelligence.</p><p>Hence, the correct answer is:<strong><em> </em>Use Amazon GuardDuty to monitor malicious activity on S3.</strong></p><p>The option that says: <strong>Use Amazon Rekognition to monitor and recognize patterns on S3</strong> is incorrect because Amazon Rekognition is simply a service that can identify the objects, people, text, scenes, and activities on your images or videos, as well as detect any inappropriate content.</p><p>The option that says: <strong>Use AWS CloudTrail to monitor and detect access patterns on S3</strong> is incorrect. While AWS CloudTrail can track API calls for your account, including calls made by the AWS Management Console, AWS SDKs, command line tools, and other AWS services, its primary function is not to monitor and detect access patterns on S3. It’s more about governance, compliance, operational auditing, and risk auditing.</p><p>The option that says: <strong>Use Amazon Inspector to alert whenever a security violation is detected on S3<em> </em></strong>is incorrect because Inspector is basically an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-using-amazon-guardduty-to-protect-your-s3-buckets/\">https://aws.amazon.com/blogs/aws/new-using-amazon-guardduty-to-protect-your-s3-buckets/</a></p><p><br></p><p><strong>Check out this Amazon GuardDuty Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-guardduty/?src=udemy\">https://tutorialsdojo.com/amazon-guardduty/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon GuardDuty to monitor malicious activity on S3.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS CloudTrail to monitor and detect access patterns on S3.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Rekognition to monitor and recognize patterns on S3.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Inspector to alert whenever a security violation is detected on S3.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 16,
    "question": "<p>A digital media company shares static content to its premium users around the world and also to their partners who syndicate their media files. The company is looking for ways to reduce its server costs and securely deliver their data to their customers globally with low latency. </p><p>Which combination of services should be used to provide the MOST suitable and cost-effective architecture? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p>Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.</p><p>CloudFront is integrated with AWS – both physical locations that are directly connected to the AWS global infrastructure, as well as other AWS services. CloudFront works seamlessly with services, including AWS Shield for DDoS mitigation, Amazon S3, Elastic Load Balancing or Amazon EC2 as origins for your applications, and Lambda@Edge to run custom code closer to customers’ users and to customize the user experience. Lastly, if you use AWS origins such as Amazon S3, Amazon EC2 or Elastic Load Balancing, you don’t pay for any data transferred between these services and CloudFront.</p><p><img src=\"https://media.tutorialsdojo.com/4-v-2.png\"></p><p>Amazon S3 is object storage built to store and retrieve any amount of data from anywhere on the Internet. It’s a simple storage service that offers an extremely durable, highly available, and infinitely scalable data storage infrastructure at very low costs.</p><p>AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.</p><p>Hence, the correct options are<strong> Amazon CloudFront</strong> and <strong>Amazon S3.</strong></p><p><strong>AWS Fargate</strong> is incorrect because this service is just a serverless compute engine for containers that work with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Although this service is more cost-effective than its server-based counterpart, Amazon S3 still costs way less than Fargate, especially for storing static content.</p><p><strong>AWS Lambda</strong> is incorrect because this simply lets you run your code serverless without provisioning or managing servers. Although this is also a cost-effective service since you have to pay only for the compute time you consume, you can't use this to store static content or as a Content Delivery Network (CDN). A better combination is Amazon CloudFront and Amazon S3.</p><p><strong>AWS Global Accelerator</strong> is incorrect because this service is more suitable for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Moreover, there is no direct way that you can integrate AWS Global Accelerator with Amazon S3. It's more suitable to use Amazon CloudFront instead in this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p><p><a href=\"https://aws.amazon.com/global-accelerator/faqs/\">https://aws.amazon.com/global-accelerator/faqs/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon CloudFront</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>AWS Fargate</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Lambda</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon S3</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>AWS Global Accelerator</p>",
        "correct": false
      }
    ],
    "corrects": [
      1,
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 17,
    "question": "<p>A company launched a website that accepts high-quality photos and turns them into a downloadable video montage. The website offers a free and a premium account that guarantees faster processing. All requests by both free and premium members go through a single SQS queue and then processed by a group of EC2 instances that generate the videos. The company needs to ensure that the premium users who paid for the service have higher priority than the free members.</p><p>How should the company re-design its architecture to address this requirement?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon Simple Queue Service (SQS)</strong> is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume without losing messages or requiring other services to be available.</p><p><img src=\"https://media.tutorialsdojo.com/OrderDispatcher-1024x534.png\"></p><p>In this scenario, it is best to create 2 separate SQS queues for each type of member. The SQS queues for the premium members can be polled first by the EC2 Instances and once completed, the messages from the free members can be processed next.</p><p>Hence, the correct answer is: <strong>Create an SQS queue for free members and another one for premium members. Configure your EC2 instances to consume messages from the premium queue first and if it is empty, poll from the free members' SQS queue.</strong></p><p>The option that says: <strong>For the requests made by premium members, set a higher priority in the SQS queue so it will be processed first compared to the requests made by free members</strong> is incorrect as you cannot set a priority to individual items in the SQS queue.</p><p>The option that says: <strong>Using Amazon Kinesis to process the photos and generate the video montage in real time</strong> is incorrect as Amazon Kinesis is used to process streaming data and it is not applicable in this scenario.</p><p>The option that says: <strong>Using Amazon S3 to store and process the photos and then generating the video montage afterward</strong> is incorrect as Amazon S3 is used for durable storage and not for processing data.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-best-practices.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-best-practices.html</a></p><p><br></p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "For the requests made by premium members, set a higher priority in the SQS queue so it will be processed first compared to the requests made by free members.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an SQS queue for free members and another one for premium members. Configure your EC2 instances to consume messages from the premium queue first and if it is empty, poll from the free members' SQS queue.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Kinesis to process the photos and generate the video montage in real-time.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon S3 to store and process the photos and then generate the video montage afterward.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 18,
    "question": "<p>A solutions architect is designing a cost-efficient, highly available storage solution for company data. One of the requirements is to ensure that the previous state of a file is preserved and retrievable if a modified version of it is uploaded. Also, to meet regulatory compliance, data over 3 years must be retained in an archive and will only be accessible once a year.</p><p>How should the solutions architect build the solution?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p>Amazon S3 has capabilities like versioning, Object Lock, and lifecycle policies to help with data preservation and archiving. Object Lock enables you to store objects using a write-once, read-many (WORM) model, which can assist prevent objects from being deleted or rewritten for a set period of time or indefinitely.</p><p><strong>- S3 Object Lock in Compliance Mode</strong> ensures that an object version cannot be overwritten or deleted by any user, including the root user of the AWS account. This mode is typically used for regulatory compliance purposes.</p><p><strong>- S3 Glacier Deep Archive</strong> is an Amazon S3 storage class that provides secure, durable, and extremely low-cost storage for long-term data archiving and digital preservation. Data can be retained in Glacier Deep Archive to meet the requirement of keeping it for 3 years and accessing it only once a year.</p><p><img src=\"https://media.tutorialsdojo.com/public/lifecycle-transitions-0542pm-05-29-23.png\">Hence, the correct answer is:<strong> Create an S3 Standard bucket with S3 Object Lock in compliance mode enabled then configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years.</strong></p><p>The option that says: <strong>Create an S3 Standard bucket with object-level versioning enabled and configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years </strong>is incorrect because it doesn't meet regulatory compliance. Although it preserves versions and archives data, it simply lacks S3 Object Lock in compliance mode, which is essential to prevent deletion or modification during the retention period. Without this feature, the data is at risk, leading to potential non-compliance.</p><p>The option that says:<strong> Create an S3 Standard bucket and enable S3 Object Lock in governance mode </strong>is incorrect because it does not only provide the same level of protection as compliance mode. Governance mode allows certain users to remove the lock, which may not meet the strict regulatory compliance requirement.</p><p>The option that says: <strong>Create a One-Zone-IA bucket with object-level versioning enabled and configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years</strong> is incorrect because it does not typically offer the same durability and availability as S3 Standard, and it is not recommended for critical data that needs to be retained for compliance reasons. Moreover, like Option 1, it also lacks S3 Object Lock in compliance mode.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-amazon-s3-storage-class-glacier-deep-archive/\">https://aws.amazon.com/blogs/aws/new-amazon-s3-storage-class-glacier-deep-archive/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html#object-lock-overview\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html#object-lock-overview</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an S3 Standard bucket with object-level versioning enabled and configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an S3 Standard bucket and enable S3 Object Lock in governance mode.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an S3 Standard bucket with S3 Object Lock in compliance mode enabled then configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create a One-Zone-IA bucket with object-level versioning enabled and configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 19,
    "question": "<p>A GraphQL API hosted is hosted in an Amazon EKS cluster with Fargate launch type and deployed using AWS SAM. The API is connected to an Amazon DynamoDB table with an Amazon DynamoDB Accelerator (DAX) as its data store. Both resources are hosted in the us-east-1 region.</p><p>The AWS IAM authenticator for Kubernetes is integrated into the EKS cluster for role-based access control (RBAC) and cluster authentication. A solutions architect must improve network security by preventing database calls from traversing the public internet. An automated cross-account backup for the DynamoDB table is also required for long-term retention.</p><p>Which of the following should the solutions architect implement to meet the requirement?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>Since DynamoDB tables are public resources, applications within a VPC rely on an Internet Gateway to route traffic to/from Amazon DynamoDB. You can use a Gateway endpoint if you want to keep the traffic between your VPC and Amazon DynamoDB within the Amazon network. This way, resources residing in your VPC can use their private IP addresses to access DynamoDB with no exposure to the public internet.</p><p>When you create a DynamoDB Gateway endpoint, you specify the VPC where it will be deployed as well as the route table that will be associated with the endpoint. The route table will be updated with an Amazon DynamoDB prefix list (list of CIDR blocks) as the destination and the endpoint's ID as the target.</p><p><img src=\"https://media.tutorialsdojo.com/amazon-dynamodb-gateway-endpoint.jpg\"></p><p>DynamoDB on-demand backups are available at no additional cost beyond the normal pricing that's associated with backup storage size. DynamoDB on-demand backups cannot be copied to a different account or Region. To create backup copies across AWS accounts and Regions and for other advanced features, you should use AWS Backup.</p><p>With AWS Backup, you can configure backup policies and monitor activity for your AWS resources and on-premises workloads in one place. Using DynamoDB with AWS Backup, you can copy your on-demand backups across AWS accounts and Regions, add cost allocation tags to on-demand backups, and transition on-demand backups to cold storage for lower costs. To use these advanced features, you must opt into AWS Backup. Opt-in choices apply to the specific account and AWS Region, so you might have to opt into multiple Regions using the same account.</p><p>Hence, the correct answer is: <strong>Create a DynamoDB gateway endpoint. Associate the endpoint to the appropriate route table. Use AWS Backup to automatically copy the on-demand DynamoDB backups to another AWS account for disaster recovery.</strong></p><p>The option that says: <strong>Create a DynamoDB interface endpoint. Associate the endpoint to the appropriate route table. Enable Point-in-Time Recovery (PITR) to restore the DynamoDB table to a particular point in time on the same or a different AWS account</strong> is incorrect. While this option addresses the network security requirement, Point-in-Time Recovery (PITR) is only used for restoring a DynamoDB table to a specific point in time within the same AWS account and region. It does not support cross-account backups or long-term retention. If this functionality is needed, you have to use the AWS Backup service instead.</p><p>The option that says: <strong>Create a DynamoDB gateway endpoint. Set up a Network Access Control List (NACL) rule that allows outbound traffic to the </strong><code><strong>dynamodb.us-east-1.amazonaws.com</strong></code><strong> gateway endpoint. Use the built-in on-demand DynamoDB backups for cross-account backup and recovery</strong> is incorrect because using a Network Access Control List alone is not enough to prevent traffic traversing to the public Internet. Moreover, you cannot copy DynamoDB on-demand backups to a different account or Region.</p><p>The option that says: <strong>Create a DynamoDB interface endpoint. Set up a stateless rule using AWS Network Firewall to control all outbound traffic to only use the </strong><code><strong>dynamodb.us-east-1.amazonaws.com</strong></code><strong> endpoint. Integrate the DynamoDB table with Amazon Timestream to allow point-in-time recovery from a different AWS account</strong> is incorrect. Keep in mind that the <code><strong>dynamodb.us-east-1.amazonaws.com</strong></code> is a public service endpoint for Amazon DynamoDB. Since the application is able to communicate with Amazon DynamoDB prior to the required architectural change, it's implied that no firewalls (security group, NACL, etc.) are blocking traffic to/from Amazon DynamoDB, hence, adding an NACL rule to allow outbound traffic to DynamoDB is unnecessary. Furthermore, the use of the AWS Network Firewall in this solution is simply incorrect as you have to integrate this with your Amazon VPC. The use of Amazon Timestream is also wrong since this is a time series database service in AWS for IoT and operational applications. You cannot directly integrate DynamoDB and Amazon Timestream for the purpose of point-in-time data recovery.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html</a></p><p><a href=\"https://aws.amazon.com/blogs/database/how-to-configure-a-private-network-environment-for-amazon-dynamodb-using-vpc-endpoints/\">https://aws.amazon.com/blogs/database/how-to-configure-a-private-network-environment-for-amazon-dynamodb-using-vpc-endpoints/</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a DynamoDB gateway endpoint. Associate the endpoint to the appropriate route table. Use AWS Backup to automatically copy the on-demand DynamoDB backups to another AWS account for disaster recovery.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a DynamoDB interface endpoint. Associate the endpoint to the appropriate route table. Enable Point-in-Time Recovery (PITR) to restore the DynamoDB table to a particular point in time on the same or a different AWS account.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a DynamoDB gateway endpoint. Set up a Network Access Control List (NACL) rule that allows outbound traffic to the <code>dynamodb.us-east-1.amazonaws.com</code> gateway endpoint. Use the built-in on-demand DynamoDB backups for cross-account backup and recovery.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a DynamoDB interface endpoint. Set up a stateless rule using AWS Network Firewall to control all outbound traffic to only use the&nbsp; <code>dynamodb.us-east-1.amazonaws.com</code> endpoint. Integrate the DynamoDB table with Amazon Timestream to allow point-in-time recovery from a different AWS account.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 20,
    "question": "<p>A company has two On-Demand EC2 instances inside the Virtual Private Cloud in the same Availability Zone but are deployed to different subnets. One EC2 instance is running a database and the other EC2 instance a web application that connects with the database. You need to ensure that these two instances can communicate with each other for the system to work properly.</p><p>What are the things you have to check so that these EC2 instances can communicate inside the VPC? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "<p>First, the Network ACL should be properly set to allow communication between the two subnets. The security group should also be properly configured so that your web server can communicate with the database server.</p><p><img src=\"https://media.tutorialsdojo.com/SGNCL-latest.jpg\"></p><p>Hence, these are the correct answers:</p><ol><li><p><strong>Check if all security groups are set to allow the application host to communicate to the database on the right port and protocol.</strong></p></li><li><p><strong>Check the Network ACL if it allows communication between the two subnets.</strong></p></li></ol><p>The option that says: <strong>Check if both instances are the same instance class</strong> is incorrect because the EC2 instances do not need to be of the same class in order to communicate with each other.</p><p>The option that says: <strong>Check if the default route is set to a NAT instance or Internet Gateway (IGW) for them to communicate</strong> is incorrect because an Internet gateway is primarily used to communicate to the Internet.</p><p>The option that says: <strong>Ensure that the EC2 instances are in the same Placement Group</strong> is incorrect because Placement Group is mainly used to provide low-latency network performance necessary for tightly-coupled node-to-node communication.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html\">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Check the Network ACL if it allows communication between the two subnets.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Check if both instances are the same instance class.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Check if the default route is set to a NAT instance or Internet Gateway (IGW) for them to communicate.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Check if all security groups are set to allow the application host to communicate to the database on the right port and protocol.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Ensure that the EC2 instances are in the same Placement Group.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1,
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 21,
    "question": "<p>A global company has deployed numerous AWS Outposts servers in various remote locations worldwide. These servers frequently need to download software updates consisting of multiple files from an S3 bucket in the <code>us-west-2</code> region. The company is experiencing significant delays in distributing these updates across all servers.<br>What solution would most effectively reduce the deployment latency while minimizing operational overhead?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>AWS Outposts</strong> is a fully managed service that brings AWS infrastructure, services, APIs, and tools directly to customer locations. It's tailored for workloads that must remain on-premises due to low latency or the need for local data processing.</p><p><strong>Amazon S3 </strong>is an object storage service offering industry-leading scalability, data availability, security, and performance. It's commonly used to store large amounts of unstructured data, including datasets for machine learning.</p><p><strong>Amazon CloudFront</strong> is a fast CDN service that securely delivers data, videos, applications, and APIs to customers worldwide, ensuring low latency and high transfer speeds. It seamlessly integrates with other AWS services and provides easy-to-use APIs for developers to customize the service to meet their needs. CloudFront utilizes a global network of edge locations to cache content closer to end users, enhancing performance and reducing the load on origin servers.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-Amazon-Cloudfront-distribution-creation-in-us-west-2-12-11-24.png\"></p><p>To address the company's challenge of distributing large software updates to AWS Outposts servers globally with reduced latency and minimal operational overhead, use Amazon CloudFront with the S3 bucket in <code>us-west-2</code> as the origin. This solution provides global content delivery through edge locations, reducing latency for all Outposts servers. It also offers caching capabilities, which can significantly speed up access to frequently downloaded files. Next, the Signed URLs add an extra layer of security for software distribution. Lastly, it can potentially reduce overall data transfer costs compared to direct S3 access, and once set up, CloudFront requires minimal ongoing management.</p><p>Hence, the correct answer is: <strong>Create an Amazon CloudFront distribution with the </strong><code><strong>us-west-2</strong></code><strong> S3 bucket as the origin. Use signed URLs for software downloads.</strong></p><p>The option that says: <strong>Set up an Amazon CloudFront distribution with the </strong><code><strong>us-west-2</strong></code><strong> S3 bucket as the primary origin and create a secondary origin in another region, implementing a </strong><code><strong>CachingDisabled</strong></code><strong> cache policy. Use signed URLs for downloads </strong>is incorrect. While CloudFront can help distribute content globally, setting up a secondary origin in another region doesn't add significant value. The CachingDisabled policy would only negate the benefits of CloudFront's caching, which is crucial for reducing latency for large files.</p><p>The option that says: <strong>Set up AWS Global Accelerator to route traffic from Outposts servers to the nearest AWS edge location, then use private VIF connections to access the S3 bucket in </strong><code><strong>us-west-2</strong></code><strong> </strong>is incorrect. Global Accelerator is designed to improve applications' availability and performance, not optimize S3 downloads. It doesn't integrate directly with S3 for file downloads. Additionally, Private VIF (Virtual Interface) is typically used with Direct Connect, not S3 buckets.</p><p>The option that says: <strong>Use Amazon S3 Transfer Acceleration on the existing S3 bucket and have the Outposts servers use the Transfer Acceleration endpoint for downloads</strong> incorrect. While this option can improve transfer speeds over long distances by leveraging Amazon's global network infrastructure, it may not provide significant benefits for all locations, especially those closer to <code>us-west-2</code>. It incurs additional costs for data transfer and does not provide caching capabilities, which could benefit frequently accessed files.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.SimpleDistribution.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.SimpleDistribution.html</a></p><p><a href=\"https://docs.aws.amazon.com/outposts/latest/server-userguide/region-connectivity.html\">https://docs.aws.amazon.com/outposts/latest/server-userguide/region-connectivity.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html</a></p><p><br></p><p><strong>Check out the following Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/aws-outposts/?src=udemy\">https://tutorialsdojo.com/aws-outposts/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an Amazon CloudFront distribution with the <code>us-west-2</code> S3 bucket as the primary origin and create a secondary origin in another region, implementing a <code>CachingDisabled</code> cache policy. Use signed URLs for downloads.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up AWS Global Accelerator to route traffic from Outposts servers to the nearest AWS edge location, then use private VIF connections to access the S3 bucket in <code>us-west-2</code>.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 Transfer Acceleration on the existing S3 bucket and have the Outposts servers use the Transfer Acceleration endpoint for downloads.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon CloudFront distribution with the <code>us-west-2</code> S3 bucket as the origin. Use signed URLs for software downloads.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 22,
    "question": "<p>A media company hosts large volumes of archive data that are about 250 TB in size on their internal servers. They have decided to move these data to S3 because of its durability and redundancy. The company currently has a 100 Mbps dedicated line connecting their head office to the Internet. </p><p>Which of the following is the FASTEST and the MOST cost-effective way to import all these data to Amazon S3?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p>AWS Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of the AWS cloud. Using Snowball addresses common challenges with large-scale data transfers, including high network costs, long transfer times, and security concerns. Transferring data with Snowball is simple, fast, secure, and can be as little as one-fifth the cost of high-speed Internet.</p><p><img src=\"https://media.tutorialsdojo.com/Snowball-opening-600w.png\"></p><p>Snowball is a strong choice for data transfer if you need to more securely and quickly transfer terabytes to many petabytes of data to AWS. Snowball can also be the right choice if you don’t want to make expensive upgrades to your network infrastructure, if you frequently experience large backlogs of data, if you're located in a physically isolated environment, or if you're in an area where high-speed Internet connections are not available or cost-prohibitive.</p><p>As a rule of thumb, if it takes more than one week to upload your data to AWS using the spare capacity of your existing Internet connection, then you should consider using Snowball. For example, if you have a 100 Mb connection that you can solely dedicate to transferring your data and need to transfer 100 TB of data, it takes more than 100 days to complete data transfer over that connection. You can make the same transfer by using multiple Snowballs in about a week.</p><p><img src=\"https://media.tutorialsdojo.com/2020-02-16_11-15-22-2595be662f78560da845fae54e4f2aca.png\"></p><p>Hence, <strong>ordering multiple AWS Snowball devices to upload the files to Amazon S3 </strong>is the correct answer.</p><p><strong>Uploading it directly to S3</strong> is incorrect since this would take too long to finish due to the slow Internet connection of the company.</p><p><strong>Establishing an AWS Direct Connect connection then transferring the data over to S3</strong> is incorrect since provisioning a line for Direct Connect would take too much time and might not give you the fastest data transfer solution. In addition, the scenario didn't warrant an establishment of a dedicated connection from your on-premises data center to AWS. The primary goal is to just do a one-time migration of data to AWS which can be accomplished by using AWS Snowball devices.</p><p><strong>Using AWS Snowmobile to transfer the data over to S3 </strong>is incorrect because Snowmobile is more suitable if you need to move extremely large amounts of data to AWS or need to transfer up to 100PB of data. This will be transported on a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. Take note that you only need to migrate 250 TB of data, hence, this is not the most suitable and cost-effective solution.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p><p><a href=\"https://aws.amazon.com/snowball/faqs/\">https://aws.amazon.com/snowball/faqs/</a></p><p><br></p><p><strong>S3 Transfer Acceleration vs Direct Connect vs VPN vs Snowball vs Snowmobile:</strong></p><p><a href=\"https://tutorialsdojo.com/s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/?src=udemy\">https://tutorialsdojo.com/s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Upload it directly to S3",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Establish an AWS Direct Connect connection then transfer the data over to S3.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Snowmobile to transfer the data over to S3.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Order multiple AWS Snowball devices to upload the files to Amazon S3.</p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 23,
    "question": "<p>A company plans to migrate all of their applications to AWS. The Solutions Architect suggested to store all the data to EBS volumes. The Chief Technical Officer is worried that EBS volumes are not appropriate for the existing workloads due to compliance requirements, downtime scenarios, and IOPS performance.</p><p>Which of the following are valid points in proving that EBS is the best service to use for migration? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>An Amazon EBS volume is a durable, block-level storage device that you can attach to a single EC2 instance. You can use EBS volumes as primary storage for data that requires frequent updates, such as the system drive for an instance or storage for a database application. You can also use them for throughput-intensive applications that perform continuous disk scans. EBS volumes persist independently from the running life of an EC2 instance.</p><p>Here is a list of important information about EBS Volumes:</p><p>- When you create an EBS volume in an Availability Zone, it is automatically replicated within that zone to prevent data loss due to a failure of any single hardware component.</p><p>- An EBS volume can only be attached to one EC2 instance at a time.</p><p>- After you create a volume, you can attach it to any EC2 instance in the same Availability Zone</p><p>- An EBS volume is off-instance storage that can persist independently from the life of an instance. You can specify not to terminate the EBS volume when you terminate the EC2 instance during instance creation.</p><p>- EBS volumes support live configuration changes while in production which means that you can modify the volume type, volume size, and IOPS capacity without service interruptions.</p><p>- Amazon EBS encryption uses 256-bit Advanced Encryption Standard algorithms (AES-256)</p><p>- EBS Volumes offer 99.999% SLA.</p><p><br></p><p>The option that says: <strong>When you create an EBS volume in an Availability Zone, it is automatically replicated on a separate AWS region to prevent data loss due to a failure of any single hardware component</strong> is incorrect because when you create an EBS volume in an Availability Zone, it is automatically replicated within that zone only, and not on a separate AWS region, to prevent data loss due to a failure of any single hardware component.</p><p>The option that says: <strong>EBS volumes can be attached to any EC2 Instance in any Availability Zone</strong> is incorrect as EBS volumes can only be attached to an EC2 instance in the same Availability Zone.</p><p>The option that says: <strong>Amazon EBS provides the ability to create snapshots (backups) of any EBS volume and write a copy of the data in the volume to Amazon RDS, where it is stored redundantly in multiple Availability Zones</strong> is almost correct. But instead of storing the volume to Amazon RDS, the EBS Volume snapshots are actually sent to Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html\">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html</a></p><p><a href=\"https://aws.amazon.com/ebs/features/\">https://aws.amazon.com/ebs/features/</a></p><p><br></p><p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-ebs/ ?src=udemy\">https://tutorialsdojo.com/amazon-ebs/</a></p><p><br></p><p><strong>Here is a short video tutorial on EBS:</strong></p><p><a href=\"https://youtu.be/ljYH5lHQdxo\">https://youtu.be/ljYH5lHQdxo</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>When you create an EBS volume in an Availability Zone, it is automatically replicated on a separate AWS region to prevent data loss due to a failure of any single hardware component.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "EBS volumes can be attached to any EC2 Instance in any Availability Zone.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "An EBS volume is off-instance storage that can persist independently from the life of an instance.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "EBS volumes support live configuration changes while in production which means that you can modify the volume type, volume size, and IOPS capacity without service interruptions.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Amazon EBS provides the ability to create snapshots (backups) of any EBS volume and write a copy of the data in the volume to Amazon RDS, where it is stored redundantly in multiple Availability Zones </p>",
        "correct": false
      }
    ],
    "corrects": [
      3,
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 24,
    "question": "<p>A company has a top priority requirement to monitor certain database metrics and send email notifications to the Operations team if any issues occur.</p><p>Which combination of AWS services can accomplish this requirement? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>Amazon CloudWatch</strong> and <strong>Amazon Simple Notification Service (SNS)</strong> are correct. In this requirement, you can use Amazon CloudWatch to monitor the database and then Amazon SNS to send the emails to the Operations team. Take note that you should use SNS instead of SES (Simple Email Service) when you want to monitor your EC2 instances.</p><p><img src=\"https://media.tutorialsdojo.com/product-page-diagram_Cloudwatch_v4.55c15d1cc086395cbd5ad279a2f1fc37e8452e77.png\"></p><p>CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, providing you with a unified view of AWS resources, applications, and services that run on AWS, and on-premises servers.</p><p>SNS is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.</p><p><strong>Amazon Simple Email Service</strong> is incorrect. SES is primarily designed for sending bulk emails, transactional emails, and marketing communications, rather than system notifications.</p><p><strong>Amazon Simple Queue Service (SQS)</strong> is incorrect. SQS is a fully-managed message queuing service. It does not monitor applications nor send email notifications, unlike SES.</p><p><strong>Amazon EC2 Instance with a running Berkeley Internet Name Domain (BIND) Server</strong> is incorrect because BIND is primarily used as a Domain Name System (DNS) web service. This is only applicable if you have a private hosted zone in your AWS account. It does not monitor applications nor send email notifications.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudwatch/\">https://aws.amazon.com/cloudwatch/</a></p><p><a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Amazon Simple Email Service",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon CloudWatch",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Amazon Simple Queue Service (SQS)",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon EC2 Instance with a running Berkeley Internet Name Domain (BIND) Server.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Amazon Simple Notification Service (SNS)",
        "correct": true
      }
    ],
    "corrects": [
      2,
      5
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 25,
    "question": "<p>A Solutions Architect of a multinational gaming company develops video games for PS4, Xbox One, and Nintendo Switch consoles, plus a number of mobile games for Android and iOS. Due to the wide range of their products and services, the architect proposed that they use API Gateway.</p><p>What are the key features of API Gateway that the architect can tell to the client? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>Amazon API Gateway</strong> is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. With a few clicks in the AWS Management Console, you can create an API that acts as a “front door” for applications to access data, business logic, or functionality from your back-end services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, or any web application. Since it can use AWS Lambda, you can run your APIs without servers.</p><p><img src=\"https://media.tutorialsdojo.com/Serverlesswebapp.45052e1feb8f1748d96a678311d73434599095b1.png\"></p><p>Amazon API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, authorization and access control, monitoring, and API version management. Amazon API Gateway has no minimum fees or startup costs. You pay only for the API calls you receive and the amount of data transferred out.</p><p>Hence, the correct answers are:</p><p><strong>- Enables you to build RESTful APIs and WebSocket APIs that are optimized for serverless workloads</strong></p><p><strong>- You pay only for the API calls you receive and the amount of data transferred out.</strong></p><p>The option that says: <strong>It automatically provides a query language for your APIs similar to GraphQL</strong> is incorrect because this is not provided by API Gateway.</p><p>The option that says: <strong>Provides you with static anycast IP addresses that serve as a fixed entry point to your applications hosted in one or more AWS Regions</strong> is incorrect because this is a capability of AWS Global Accelerator and not API Gateway.</p><p>The option that says: <strong>Enables you to run applications requiring high levels of inter-node communications at scale on AWS through its custom-built operating system (OS) bypass hardware interface</strong> is incorrect because this is a capability of Elastic Fabric Adapter and not API Gateway.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p><p><a href=\"https://aws.amazon.com/api-gateway/features/\">https://aws.amazon.com/api-gateway/features/</a></p><p><br></p><p><strong>Check out this Amazon API Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-api-gateway/?src=udemy\">https://tutorialsdojo.com/amazon-api-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>It automatically provides a query language for your APIs similar to GraphQL.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Provides you with static anycast IP addresses that serve as a fixed entry point to your applications hosted in one or more AWS Regions.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enables you to build RESTful APIs and WebSocket APIs that are optimized for serverless workloads.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Enables you to run applications requiring high levels of inter-node communications at scale on AWS through its custom-built operating system (OS) bypass hardware interface.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>You pay only for the API calls you receive and the amount of data transferred out.</p>",
        "correct": true
      }
    ],
    "corrects": [
      3,
      5
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 26,
    "question": "<p>A media company wants to ensure that the images it delivers through Amazon CloudFront are compatible across various user devices. The company plans to serve images in WebP format to user agents that support it and return to JPEG format for those that don't. Additionally, they want to add a custom header to the response for tracking purposes.</p><p>As a solution architect, what approach would you recommend to meet these requirements while minimizing operational overhead?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon CloudFront</strong> is a content delivery network (CDN) service that enables the efficient distribution of web content to users across the globe. It reduces latency by caching static and dynamic content in multiple edge locations worldwide and improves the overall user experience.</p><p><strong>Lambda@Edge </strong>allows you to run Lambda functions at the edge locations of the CloudFront CDN. With this, you can perform various tasks, such as modifying HTTP headers, generating dynamic responses, implementing security measures, and customizing content based on user preferences, device type, location, or other criteria.</p><p><img src=\" https://media.tutorialsdojo.com/public/Amazon-CloudFont-Lambda-26-06-2023.png\"></p><p>When a request is made to a CloudFront distribution, Lambda@Edge enables you to intercept the request and execute custom code before CloudFront processes it. Similarly, you can intercept the response generated by CloudFront and modify it before it's returned to the viewer. In the given scenario, Lambda@Edge can be used to dynamically serve different image formats based on the User-agent header received by CloudFront. Additionally, you can inject custom response headers before CloudFront returns the response to the viewer.</p><p>Hence the correct answer is: <strong>Configure CloudFront behaviors to handle different image formats based on the User-Agent header. Use Lambda@Edge functions to modify the response headers and serve the appropriate format.</strong></p><p>The option that says:<strong> Create multiple CloudFront distributions, each serving a specific image format (WebP or JPEG). Route incoming requests based on the User-Agent header to the respective distribution using Amazon Route 53 </strong>is incorrect because creating multiple CloudFront distributions for each image format is unnecessary and just increases operational overhead.</p><p>The option that says:<strong> Generate a CloudFront response headers policy. Utilize the policy to deliver the suitable image format according to the User-Agent HTTP header in the incoming request </strong>is incorrect. CloudFront response headers policies simply tell which HTTP headers should be included or excluded in the responses sent by CloudFront. You cannot use them to dynamically select and serve image formats based on the User-agent.</p><p>The option that says: <strong>Implement an image conversion service on EC2 instances and integrate it with CloudFront. Use Lambda functions to modify the response headers and serve the appropriate format based on the User-Agent header </strong>is incorrect. Building an image conversion service using EC2 instances requires additional operational management. You can instead use Lambda@Edge functions to modify response headers and serve the correct image format based on the User-agent header.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html</a></p><p><br></p><p><strong>Check out these AWS Lambda and Amazon CloudFront Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-lambda/?src=udemy\">https://tutorialsdojo.com/aws-lambda/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure CloudFront behaviors to handle different image formats based on the User-Agent header. Use Lambda@Edge functions to modify the response headers and serve the appropriate format.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create multiple CloudFront distributions, each serving a specific image format (WebP or JPEG). Route incoming requests based on the User-Agent header to the respective distribution using Amazon Route 53.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Generate a CloudFront response headers policy. Utilize the policy to deliver the suitable image format according to the User-Agent HTTP header in the incoming request.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Implement an image conversion service on EC2 instances and integrate it with CloudFront. Use Lambda functions to modify the response headers and serve the appropriate format based on the User-Agent header.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 27,
    "question": "<p>A company has a static corporate website hosted in a standard S3 bucket and a new web domain name that was registered using Route 53. You are instructed by your manager to integrate these two services in order to successfully launch their corporate website.</p><p>What are the prerequisites when routing traffic using Amazon Route 53 to a website that is hosted in an Amazon S3 Bucket? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>Here are the prerequisites for routing traffic to a website that is hosted in an Amazon S3 Bucket:</p><p><strong>- An S3 bucket that is configured to host a static website. The bucket must have the same name as your domain or subdomain. For example, if you want to use the subdomain portal.tutorialsdojo.com, the name of the bucket must be portal.tutorialsdojo.com.</strong></p><p><strong>- A registered domain name. You can use Route 53 as your domain registrar, or you can use a different registrar.</strong></p><p>- Route 53 as the DNS service for the domain. If you register your domain name by using Route 53, we automatically configure Route 53 as the DNS service for the domain.</p><p><img src=\"https://media.tutorialsdojo.com/2019-02-13_01-03-54-a79dccacb816c6b4a8da3bd3ac9c2ce6.png\"></p><p>The option that says: <strong>The record set must be of type \"MX\"</strong> is incorrect since an MX record specifies the mail server responsible for accepting email messages on behalf of a domain name. This is not what is being asked by the question.</p><p>The option that says: <strong>The S3 bucket must be in the same region as the hosted zone</strong> is incorrect. There is no constraint that the S3 bucket must be in the same region as the hosted zone in order for the Route 53 service to route traffic into it.</p><p>The option that says: <strong>The Cross-Origin Resource Sharing (CORS) option should be enabled in the S3 bucket</strong> is incorrect because you only need to enable Cross-Origin Resource Sharing (CORS) when your client web application on one domain interacts with the resources in a different domain.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "The S3 bucket name must be the same as the domain name",
        "correct": true
      },
      {
        "id": 2,
        "answer": "A registered domain name",
        "correct": true
      },
      {
        "id": 3,
        "answer": "The record set must be of type \"MX\"",
        "correct": false
      },
      {
        "id": 4,
        "answer": "The S3 bucket must be in the same region as the hosted zone",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>The Cross-Origin Resource Sharing (CORS) option should be enabled in the S3 bucket</p>",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 28,
    "question": "<p>A software company has resources hosted in AWS and on-premises servers. You have been requested to create a decoupled architecture for applications which make use of both resources. <br><br>Which of the following options are valid? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon Simple Queue Service (SQS)</strong> and&nbsp;<strong>Amazon Simple Workflow Service (SWF)</strong> are the services that you can use for creating a decoupled architecture in AWS.&nbsp;Decoupled architecture is a type of computing architecture that enables computing components or layers to execute independently while still interfacing with each other.</p><p>Amazon SQS offers reliable, highly-scalable hosted queues for storing messages while they travel between applications or microservices. Amazon SQS lets you move data between distributed application components and helps you decouple these components. Amazon SWF is a web service that makes it easy to coordinate work across distributed application components.</p><p><strong>Using RDS to utilize both on-premises servers and EC2 instances for your decoupled application</strong> and <strong>using DynamoDB to utilize both on-premises servers and EC2 instances for your decoupled application</strong> are incorrect as RDS and DynamoDB are database services.</p><p><strong>Using VPC peering to connect both on-premises servers and EC2 instances for your decoupled application</strong>&nbsp;is incorrect because you can't create a VPC peering for your on-premises network and AWS VPC.</p><p>&nbsp;</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p><p><a href=\"http://docs.aws.amazon.com/amazonswf/latest/developerguide/swf-welcome.html\">http://docs.aws.amazon.com/amazonswf/latest/developerguide/swf-welcome.html</a></p><p>&nbsp;</p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p><p>&nbsp;</p><p><strong>Amazon Simple Workflow (SWF) vs AWS Step Functions vs Amazon SQS:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-simple-workflow-swf-vs-aws-step-functions-vs-amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-simple-workflow-swf-vs-aws-step-functions-vs-amazon-sqs/</a></p><p>&nbsp;</p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Use SWF to utilize both on-premises servers and EC2 instances for your decoupled application",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use RDS to utilize both on-premises servers and EC2 instances for your decoupled application",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use SQS to utilize both on-premises servers and EC2 instances for your decoupled application",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use VPC peering to connect both on-premises servers and EC2 instances for your decoupled application</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Use DynamoDB to utilize both on-premises servers and EC2 instances for your decoupled application",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 29,
    "question": "<p>A company owns a photo-sharing app that stores user uploads on Amazon S3. There has been an increase in the number of explicit and offensive images being reported. The company currently relies on human efforts to moderate content, and they want to streamline this process by using Artificial Intelligence to only flag images for review. For added security, any communication with your resources on your Amazon VPC must not traverse the public Internet.</p><p>How can this task be accomplished with the LEAST amount of effort?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon Rekognition</strong> can help you streamline or automate image and video moderation workflows using machine learning. Using fully managed image and video moderation APIs, you can proactively detect inappropriate, unwanted, or offensive content containing nudity, suggestiveness, violence, and other such categories.</p><p>Amazon Rekognition returns a hierarchical taxonomy of moderation-related labels that make it easy for you to define granular business rules as per your own Standards and Practices (S&amp;P), User Safety, or compliance guidelines - without requiring any machine learning experience.</p><p><img src=\"https://media.tutorialsdojo.com/amazon-rekognition-sample.jpg\"></p><p>If you use Amazon Virtual Private Cloud (Amazon VPC) to host your AWS resources, you can establish a private connection between your VPC and Amazon Rekognition. You can use this connection to enable Amazon Rekognition to communicate with your resources on your VPC without going through the public internet.</p><p>To connect your VPC to Amazon Rekognition, you define an interface VPC endpoint for Amazon Rekognition. An interface endpoint is an elastic network interface with a private IP address that serves as an entry point for traffic destined to a supported AWS service. The endpoint provides reliable, scalable connectivity to Amazon Rekognition—and it doesn't require an internet gateway, a network address translation (NAT) instance, or a VPN connection.</p><p>In this scenario, it is best to use Amazon Rekognition to automatically analyze images for you instead of manually scanning them and tagging those that you find offensive. Of course, this is not a holy grail solution, as you'd still have to go over those flagged images for further review, but it would definitely help speed up the process of content moderation.</p><p>Hence, the correct answer is:<strong> Use Amazon Rekognition to detect images with graphic nudity or violence in Amazon S3. Create an Interface VPC endpoint for Amazon Rekognition with the necessary policies to prevent any traffic from traversing the public Internet.</strong></p><p>The option that says: <strong>Use an image classification model in Amazon SageMaker. Set up Amazon GuardDuty and connect it with Amazon SageMaker to ensure that all communications do not traverse the public Internet</strong> is incorrect. Using Amazon SageMaker will require you to actually train a machine learning model; it does not come off the shelf, unlike Amazon Rekognition. Take note that the scenario explicitly mentioned that the task must be accomplished with the least amount of effort. In addition, the Amazon GuardDuty service is not capable of ensuring that all traffic in Amazon SageMaker is private. Amazon GuardDuty is primarily used as an intelligent threat detection solution and not a networking service.</p><p>The option that says: <strong>Use Amazon Detective to detect images with graphic nudity or violence in Amazon S3. Ensure that all communications made by your AWS resources do not traverse the public Internet via the AWS Audit Manager service </strong>is incorrect. Amazon Detective is commonly used to analyze, investigate, and quickly identify the root cause of potential security issues in your AWS workloads, as well as for detecting suspicious activities. This service can't detect any graphic images. Moreover, the AWS Audit Manager just continuously audits your AWS usage to simplify how you assess risk and compliance with regulations and industry standards. The AWS Audit Manager, by itself, cannot halt any outbound traffic traversing the public Internet from your VPC.</p><p>The option that says <strong>Use Amazon Monitron to monitor each user upload in S3. Use the AWS Transit Gateway Network Manager to block any outbound requests to the public Internet </strong>is incorrect. Amazon Monitron is simply a service that detects abnormal conditions in industrial equipment such as fans, compressors, motors, etc. In addition, the AWS Transit Gateway Network Manager is simply a feature of AWS Transit Gateway that centralizes the management and monitoring of networking resources and connections to remote branch locations.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rekognition/content-moderation/\">https://aws.amazon.com/rekognition/content-moderation/</a></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/amazon-rekognition-adds-support-for-six-new-content-moderation-categories/\">https://aws.amazon.com/blogs/machine-learning/amazon-rekognition-adds-support-for-six-new-content-moderation-categories/</a></p><p><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/vpc.html\">https://docs.aws.amazon.com/rekognition/latest/dg/vpc.html</a></p><p><br></p><p><strong>Check out this Amazon Rekognition Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sagemaker/?src=udemy\">https://tutorialsdojo.com/amazon-rekognition/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Rekognition to detect images with graphic nudity or violence in Amazon S3. Create an Interface VPC endpoint for Amazon Rekognition with the necessary policies to prevent any traffic from traversing the public Internet.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use an image classification model in Amazon SageMaker. Set up Amazon GuardDuty and connect it with Amazon SageMaker to ensure that all communications do not traverse the public Internet.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Detective to detect images with graphic nudity or violence in Amazon S3. Ensure that all communications made by your AWS resources do not traverse the public Internet via the AWS Audit Manager service.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Monitron to monitor each user upload in S3. Use the AWS Transit Gateway Network Manager to block any outbound requests to the public Internet.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 30,
    "question": "<p>A startup has multiple AWS accounts that are assigned to its development teams. Since the company is projected to grow rapidly, the management wants to consolidate all of its AWS accounts into a multi-account setup. To simplify the login process on the AWS accounts, the management wants to utilize its existing directory service for user authentication</p><p>Which combination of actions should a solutions architect recommend to meet these requirements? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS Organizations</strong> is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business. As an administrator of an organization, you can create accounts in your organization and invite existing accounts to join the organization.</p><p><strong>AWS IAM Identity Center (successor to AWS Single Sign-On)</strong> provides single sign-on access for all of your AWS accounts and cloud applications. It connects with Microsoft Active Directory through AWS Directory Service to allow users in that directory to sign in to a personalized AWS access portal using their existing Active Directory user names and passwords. From the AWS access portal, users have access to all the AWS accounts and cloud applications that they have permission for.</p><p>Users in your self-managed directory in Active Directory (AD) can also have single sign-on access to AWS accounts and cloud applications in the AWS access portal.</p><p><img src=\"https://media.tutorialsdojo.com/saa_aws_organization_sso.png\"></p><p>Therefore, the correct answers are:</p><p><strong>-On the master account, use AWS Organizations to create a new organization with all features turned on. Invite the child accounts to this new organization.</strong></p><p><strong>-Configure AWS IAM Identity Center (AWS Single Sign-On) for the organization and integrate it with the company’s directory service using the Active Directory Connector</strong></p><p>The option that says: <strong>On the master account, use AWS Organizations to create a new organization with all features turned on. Enable the organization’s external authentication and point it to use the company’s directory service </strong>is incorrect. There is no option to use an external authentication on AWS Organizations. You will need to configure AWS SSO if you want to use an existing Directory Service.</p><p>The option that says: <strong>Create an identity pool on Amazon Cognito and configure it to use the company’s directory service. Configure AWS IAM Identity Center (AWS Single Sign-On) to accept Cognito authentication</strong> is incorrect. Amazon Cognito is used for single sign-on in mobile and web applications. You don't have to use it if you already have an existing Directory Service to be used for authentication.</p><p>The option that says: <strong>Create Service Control Policies (SCP) in the organization to manage the child accounts. Configure AWS IAM Identity Center (AWS Single Sign-On) to use AWS Directory Service</strong> is incorrect. SCPs are not necessarily needed for logging in on this scenario. You can use SCP if you want to restrict or implement a policy across several accounts in the organization.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html</a></p><p><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-sso.html\">https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-sso.html</a></p><p><br></p><p><strong>Check out AWS Organizations Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>On the master account, use AWS Organizations to create a new organization with all features turned on. Enable the organization’s external authentication and point it to use the company’s directory service.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an identity pool on Amazon Cognito and configure it to use the company’s directory service. Configure AWS IAM Identity Center (AWS Single Sign-On) to accept Cognito authentication.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create Service Control Policies (SCP) in the organization to manage the child accounts. Configure AWS IAM Identity Center (AWS Single Sign-On) to use AWS Directory Service.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>On the master account, use AWS Organizations to create a new organization with all features turned on. Invite the child accounts to this new organization.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Configure AWS IAM Identity Center (AWS Single Sign-On) for the organization and integrate it with the company’s directory service using the Active Directory Connector</p>",
        "correct": true
      }
    ],
    "corrects": [
      4,
      5
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 31,
    "question": "<p>A company has a dynamic web app written in MEAN stack that is going to be launched in the next month. There is a probability that the traffic will be quite high in the first couple of weeks. In the event of a load failure, how can you set up DNS failover to a static website?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>For this scenario, <strong>using Route 53 with the failover option to a static S3 website bucket or CloudFront distribution</strong> is correct. You can create a new Route 53 with the failover option to a static S3 website bucket or CloudFront distribution as an alternative.</p><p><img src=\"https://media.tutorialsdojo.com/2019-02-13_01-05-29-0096a577e991922c675f02801d48a8a4.png\"></p><p><strong>Duplicating the exact application architecture in another region and configuring DNS weight-based routing</strong> is incorrect because running a duplicate system is not a cost-effective solution. Remember that you are trying to build a failover mechanism for your web app, not a distributed setup.</p><p><strong>Enabling failover to an application hosted in an on-premises data center</strong> is incorrect. Although you can set up failover to your on-premises data center, you are not maximizing the AWS environment such as using Route 53 failover.</p><p><strong>Adding more servers in case the application fails</strong> is incorrect because this is not the best way to handle a failover event. If you add more servers only in case the application fails, then there would be a period of downtime in which your application is unavailable. Since there are no running servers on that period, your application will be unavailable for a certain period of time until your new server is up and running.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/fail-over-s3-r53/\">https://aws.amazon.com/premiumsupport/knowledge-center/fail-over-s3-r53/</a></p><p><a href=\"http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html</a></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Duplicate the exact application architecture in another region and configure DNS weight-based routing.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable failover to an application hosted in an on-premises data center.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Route 53 with the failover option to a static S3 website bucket or CloudFront distribution.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Add more servers in case the application fails.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 32,
    "question": "<p>A company has a requirement to move 80 TB data warehouse to the cloud. It would take 2 months to transfer the data given their current bandwidth allocation. </p><p>Which is the most cost-effective service that would allow you to quickly upload their data into AWS?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p><strong>AWS Snowball Edge</strong> is a type of Snowball device with on-board storage and compute power for select AWS capabilities. Snowball Edge can undertake local processing and edge-computing workloads in addition to transferring data between your local environment and the AWS Cloud.</p><p>Each Snowball Edge device can transport data at speeds faster than the internet. This transport is done by shipping the data in the appliances through a regional carrier. The appliances are rugged shipping containers, complete with E Ink shipping labels. The AWS Snowball Edge device differs from the standard Snowball because it can bring the power of the AWS Cloud to your on-premises location, with local storage and compute functionality.</p><p>Snowball Edge devices have three options for device configurations – storage optimized, compute optimized, and with GPU.</p><p>Hence, the correct answer is: <strong>AWS Snowball Edge.</strong></p><p><strong>AWS Snowmobile</strong> is incorrect because this is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. It is not suitable for transferring a small amount of data, like 80 TB in this scenario. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. A more cost-effective solution here is to order a Snowball Edge device instead.</p><p><strong>AWS Direct Connect</strong> is incorrect because it is primarily used to establish a dedicated network connection from your premises network to AWS. This is not suitable for one-time data transfer tasks, like what is depicted in the scenario.</p><p><strong>Amazon S3 Multipart Upload</strong> is incorrect because this feature simply enables you to upload large objects in multiple parts. It still uses the same Internet connection of the company, which means that the transfer will still take time due to its current bandwidth allocation.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html&nbsp;\">https://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html</a></p><p><a href=\"https://docs.aws.amazon.com/snowball/latest/ug/device-differences.html\">https://docs.aws.amazon.com/snowball/latest/ug/device-differences.html</a></p><p><br></p><p><strong>Check out this AWS Snowball Edge Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-snowball-edge/?src=udemy\">https://tutorialsdojo.com/aws-snowball-edge/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Snowmobile</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Snowball Edge</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS Direct Connect</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon S3 Multipart Upload</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 33,
    "question": "<p>An Intelligence Agency developed a missile tracking application that is hosted on both development and production AWS accounts. The Intelligence agency’s junior developer only has access to the development account. The developer has received security clearance to access the agency’s production account but the access is only temporary and only write access to Amazon EC2 and Amazon S3 is allowed.</p><p>Which of the following allows the developer to issue short-lived access tokens that act as temporary security credentials to allow access to AWS resources?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS Security Token Service (STS)</strong> is the service that you can use to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use.</p><p>In this diagram, IAM user Alice in the Dev account (the role-assuming account) needs to access the Prod account (the role-owning account). Here’s how it works:</p><ol><li><p>Alice in the Dev account assumes an IAM role (WriteAccess) in the Prod account by calling AssumeRole.</p></li><li><p>STS returns a set of temporary security credentials.</p></li><li><p>Alice uses the temporary security credentials to access services and resources in the Prod account. Alice could, for example, make calls to Amazon S3 and Amazon EC2, which are granted by the WriteAccess role.</p></li></ol><p><img src=\"https://media.tutorialsdojo.com/2018-10-23_06-52-31-201df4af92968773479c7a09268baf1e.png\"></p><p>Hence, the correct answer is: <strong>Use AWS Security Token Service (STS).</strong></p><p><strong>Using Amazon Cognito to issue JSON Web Tokens (JWT)</strong> is incorrect because the Amazon Cognito service is primarily used for user authentication and not for providing access to your AWS resources. A JSON Web Token (JWT) is meant to be used for user authentication and session management.</p><p><strong>Using AWS AWS IAM Identity Center</strong> is incorrect because this simply helps you securely create or connect your workforce identities and manage their access centrally across AWS accounts and applications. IAM Identity Center is the recommended approach for workforce authentication and authorization on AWS for organizations of any size and type, but not for generating tokens.</p><p>The option that says <strong>All of the above</strong> is incorrect as only STS has the ability to provide temporary security credentials.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Cognito to issue JSON Web Tokens (JWT)</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Security Token Service (STS)</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS SSO</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>All of the given options are correct.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 34,
    "question": "<p>As part of the Business Continuity Plan of your company, your IT Director instructed you to set up an automated backup of all of the EBS Volumes for your EC2 instances as soon as possible.  </p><p>What is the fastest and most cost-effective solution to automatically back up all of your EBS Volumes?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>You can use Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation, retention, and deletion of snapshots taken to back up your Amazon EBS volumes. Automating snapshot management helps you to:</p><p>- Protect valuable data by enforcing a regular backup schedule.</p><p>- Retain backups as required by auditors or internal compliance.</p><p>- Reduce storage costs by deleting outdated backups.</p><p><br></p><p>Combined with the monitoring features of Amazon CloudWatch Events and AWS CloudTrail, Amazon DLM provides a complete backup solution for EBS volumes at no additional cost.</p><p>Hence, <strong>using Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation of EBS snapshots</strong> is the correct answer as it is the fastest and most cost-effective solution that provides an automated way of backing up your EBS volumes.</p><p>The option that says: <strong>For an automated solution, create a scheduled job that calls the \"create-snapshot\" command via the AWS CLI to take a snapshot of production EBS volumes periodically</strong> is incorrect because even though this is a valid solution, you would still need additional time to create a scheduled job that calls the \"create-snapshot\" command. It would be better to use Amazon Data Lifecycle Manager (Amazon DLM) instead as this provides you the fastest solution which enables you to automate the creation, retention, and deletion of the EBS snapshots without having to write custom shell scripts or creating scheduled jobs.</p><p><strong>Setting your Amazon Storage Gateway with EBS volumes as the data source and storing the backups in your on-premises servers through the storage gateway</strong> is incorrect as the Amazon Storage Gateway is used only for creating a backup of data from your on-premises server and not from the Amazon Virtual Private Cloud.</p><p><strong>Using an EBS-cycle policy in Amazon S3 to automatically back up the EBS volumes</strong> is incorrect as there is no such thing as EBS-cycle policy in Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html</a></p><p><a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html\">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</a></p><p><br></p><p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-ebs/?src=udemy\">https://tutorialsdojo.com/amazon-ebs/</a></p><p><br></p><p><strong>Amazon EBS Overview - SSD vs HDD:</strong></p><p><a href=\"https://www.youtube.com/watch?v=LW7x8wyLFvw&amp;t=8s\">https://www.youtube.com/watch?v=LW7x8wyLFvw&amp;t=8s</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>For an automated solution, create a scheduled job that calls the \"create-snapshot\" command via the AWS CLI to take a snapshot of production EBS volumes periodically.  </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set your Amazon Storage Gateway with EBS volumes as the data source and store the backups in your on-premises servers through the storage gateway.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an EBS-cycle policy in Amazon S3 to automatically back up the EBS volumes.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation of EBS snapshots.</p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 35,
    "question": "<p>A commercial bank has a forex trading application. They created an Auto Scaling group of EC2 instances that allow the bank to cope with the current traffic and achieve cost-efficiency. They want the Auto Scaling group to behave in such a way that it will follow a predefined set of parameters before it scales down the number of EC2 instances, which protects the system from unintended slowdown or unavailability.</p><p>Which of the following statements are true regarding the cooldown period? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>In Auto Scaling, the following statements are correct regarding the cooldown period:</p><p>It ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect.</p><p>Its default value is 300 seconds.</p><p>It is a configurable setting for your Auto Scaling group.</p><p>The following options are incorrect:</p><p><strong>- It ensures that before the Auto Scaling group scales out, the EC2 instances have ample time to cooldown.</strong></p><p><strong>- It ensures that the Auto Scaling group launches or terminates additional EC2 instances without any downtime.</strong></p><p><strong>- Its default value is 600 seconds.</strong></p><p>These statements are inaccurate and don't depict what the word \"cooldown\" actually means for Auto Scaling. The cooldown period is a configurable setting for your Auto Scaling group that helps to ensure that it doesn't launch or terminate additional instances before the previous scaling activity takes effect. After the Auto Scaling group dynamically scales using a simple scaling policy, it waits for the cooldown period to complete before resuming scaling activities.</p><p>The figure below demonstrates the scaling cooldown:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2018-10-23_05-13-47-8ff2ec72179862c346ba76ede3994182.png\"></p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"http://docs.aws.amazon.com/autoscaling/latest/userguide/as-instance-termination.html\">http://docs.aws.amazon.com/autoscaling/latest/userguide/as-instance-termination.html</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>It ensures that before the Auto Scaling group scales out, the EC2 instances have ample time to cooldown.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "It ensures that the Auto Scaling group launches or terminates additional EC2 instances without any downtime.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "It ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Its default value is 300 seconds.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Its default value is 600 seconds.",
        "correct": false
      }
    ],
    "corrects": [
      3,
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 36,
    "question": "<p>An application is hosted in AWS Fargate and uses RDS database in Multi-AZ Deployments configuration with several Read Replicas. A Solutions Architect was instructed to ensure that all of their database credentials, API keys, and other secrets are encrypted and rotated on a regular basis to improve data security. The application should also use the latest version of the encrypted credentials when connecting to the RDS database. </p><p>Which of the following is the MOST appropriate solution to secure the credentials?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS Secrets Manager</strong> is an AWS service that makes it easier for you to manage secrets. <em>Secrets</em> can be database credentials, passwords, third-party API keys, and even arbitrary text. You can store and control access to these secrets centrally by using the Secrets Manager console, the Secrets Manager command line interface (CLI), or the Secrets Manager API and SDKs.</p><p>In the past, when you created a custom application that retrieves information from a database, you typically had to embed the credentials (the secret) for accessing the database directly in the application. When it came time to rotate the credentials, you had to do much more than just create new credentials. You had to invest time in updating the application to use the new credentials. Then you had to distribute the updated application. If you had multiple applications that shared credentials and you missed updating one of them, the application would break. Because of this risk, many customers have chosen not to regularly rotate their credentials, which effectively substitutes one risk for another.</p><p><img src=\"https://media.tutorialsdojo.com/ASM-Basic-Scenario.png\"></p><p><strong>Secrets Manager</strong> enables you to replace hardcoded credentials in your code (including passwords), with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure that the secret can't be compromised by someone examining your code because the secret simply isn't there. Also, you can configure Secrets Manager to automatically rotate the secret for you according to the schedule that you specify. This enables you to replace long-term secrets with short-term ones, which helps to significantly reduce the risk of compromise.</p><p>Hence, the most appropriate solution for this scenario is: <strong>Use AWS Secrets Manager to store and encrypt the database credentials, API keys, and other secrets. Enable automatic rotation for all of the credentials.</strong></p><p>The option that says: <strong>Store the database credentials, API keys, and other secrets to Systems Manager Parameter Store each with a </strong><code><strong>SecureString</strong></code><strong> data type. The credentials are automatically rotated by default </strong>is incorrect because the Systems Manager Parameter Store doesn't rotate its parameters by default.</p><p>The option that says: <strong>Store the database credentials, API keys, and other secrets to AWS ACM</strong> is incorrect because it is just a managed private CA service that helps you easily and securely manage the lifecycle of your private certificates to allow SSL communication to your application. This is not a suitable service for storing databases or any other confidential credentials.</p><p>The option that says: <strong>Store the database credentials, API keys, and other secrets in AWS KMS</strong> is incorrect because this only makes it easy for you to create and manage encryption keys and control the use of encryption across a wide range of AWS services. This is primarily used for encryption and not for hosting your credentials.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-securely-provide-database-credentials-to-lambda-functions-by-using-aws-secrets-manager/\">https://aws.amazon.com/blogs/security/how-to-securely-provide-database-credentials-to-lambda-functions-by-using-aws-secrets-manager/</a></p><p><br></p><p><strong>Check out these AWS Secrets Manager and Systems Manager Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-secrets-manager/?src=udemy\">https://tutorialsdojo.com/aws-secrets-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the database credentials, API keys, and other secrets to Systems Manager Parameter Store each with a <code>SecureString</code> data type. The credentials are automatically rotated by default.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Secrets Manager to store and encrypt the database credentials, API keys, and other secrets. Enable automatic rotation for all of the credentials.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Store the database credentials, API keys, and other secrets to AWS ACM.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the database credentials, API keys, and other secrets in AWS KMS.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 37,
    "question": "<p>An insurance company utilizes SAP HANA for its day-to-day ERP operations. Since they can’t migrate this database due to customer preferences, they need to integrate it with the current AWS workload in the VPC in which they are required to establish a site-to-site VPN connection.</p><p>What needs to be configured outside of the VPC for them to have a successful site-to-site VPN connection?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>By default, instances that you launch into a virtual private cloud (VPC) can't communicate with your own network. You can enable access to your network from your VPC by attaching a virtual private gateway to the VPC, creating a custom route table, updating your security group rules, and creating an AWS managed VPN connection.</p><p>Although the term <strong>VPN connection</strong> is a general term, in the Amazon VPC documentation, a VPN connection refers to the connection between your VPC and your own network. AWS supports Internet Protocol security (IPsec) VPN connections.</p><p>A <strong>customer gateway</strong> is a physical device or software application on your side of the VPN connection.</p><p>To create a VPN connection, you must create a customer gateway resource in AWS, which provides information to AWS about your customer gateway device. Next, you have to set up an Internet-routable IP address (static) of the customer gateway's external interface.</p><p>The following diagram illustrates single VPN connections. The VPC has an attached virtual private gateway, and your remote network includes a customer gateway, which you must configure to enable the VPN connection. You set up the routing so that any traffic from the VPC bound for your network is routed to the virtual private gateway.</p><p><img src=\"https://media.tutorialsdojo.com/2018-10-27_22-45-01-dbcb3de60063eaba73e8d2d12c61d6dc.png\"></p><p>The options that say: <strong>A dedicated NAT instance in a public subnet</strong> and <strong>the main route table in your VPC to route traffic through a NAT instance</strong> are incorrect since you don't need a NAT instance for you to be able to create a VPN connection.</p><p><strong>An EIP to the Virtual Private Gateway</strong> is incorrect since you do not attach an EIP to a VPG.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/SetUpVPNConnections.html\">https://docs.aws.amazon.com/vpc/latest/userguide/SetUpVPNConnections.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "A dedicated NAT instance in a public subnet",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>An Internet-routable IP address (static) of the customer gateway's external interface for the on-premises network</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "The main route table in your VPC to route traffic through a NAT instance",
        "correct": false
      },
      {
        "id": 4,
        "answer": "An EIP to the Virtual Private Gateway",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 38,
    "question": "<p>A company is running a multi-tier web application farm in a virtual private cloud (VPC) that is not connected to their corporate network. They are connecting to the VPC over the Internet to manage the fleet of Amazon EC2 instances running in both the public and private subnets. The Solutions Architect has added a bastion host with Microsoft Remote Desktop Protocol (RDP) access to the application instance security groups, but the company wants to further limit administrative access to all of the instances in the VPC.</p><p>Which of the following bastion host deployment options will meet this requirement?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>The correct answer is to deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow RDP access to bastion only from the corporate IP addresses.</p><p>A bastion host is a special purpose computer on a network specifically designed and configured to withstand attacks. If you have a bastion host in AWS, it is basically just an EC2 instance. It should be in a public subnet with either a public or Elastic IP address with sufficient RDP or SSH access defined in the security group. Users log on to the bastion host via SSH or RDP and then use that session to manage other hosts in the private subnets.</p><p><img src=\"https://media.tutorialsdojo.com/2019-02-13_01-07-49-c56d7bdad437af14a0a3b8a17c9dcfd2.png\"></p><p><strong>Deploying a Windows Bastion host on the corporate network that has RDP access to all EC2 instances in the VPC</strong> is incorrect since you do not deploy the Bastion host to your corporate network. It should be in the public subnet of a VPC.</p><p><strong>Deploying a Windows Bastion host with an Elastic IP address in the private subnet and restricting RDP access to the bastion from only the corporate public IP addresses</strong> is incorrect since it should be deployed in a public subnet, not a private subnet.</p><p><strong>Deploying a Windows Bastion host with an Elastic IP address in the public subnet and allowing SSH access to the bastion from anywhere</strong> is incorrect. Since it is a Windows bastion, you should allow RDP access and not SSH as this is mainly used for Linux-based systems.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html\">https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy a Windows Bastion host on the corporate network that has RDP access to all EC2 instances in the VPC.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy a Windows Bastion host with an Elastic IP address in the private subnet, and restrict RDP access to the bastion from only the corporate public IP addresses.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow SSH access to the bastion from anywhere.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow RDP access to bastion only from the corporate IP addresses.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 39,
    "question": "<p>For data privacy, a healthcare company has been asked to comply with the Health Insurance Portability and Accountability Act (HIPAA). The company stores all its backups on an Amazon S3 bucket. It is required that data stored on the S3 bucket must be encrypted.</p><p>What is the best option to do this? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "<p>Server-side encryption is about data encryption at rest—that is, Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects. For example, if you share your objects using a pre-signed URL, that URL works the same way for both encrypted and unencrypted objects.</p><p><img src=\"https://media.tutorialsdojo.com/bucket_policies_defense_s3.43e6c93a095f2f55b33b30276f4782ab9ec79f47.png\"></p><p>You have three mutually exclusive options depending on how you choose to manage the encryption keys:</p><ol><li><p>Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</p></li><li><p>Use Server-Side Encryption with AWS KMS Keys (SSE-KMS)</p></li><li><p>Use Server-Side Encryption with Customer-Provided Keys (SSE-C)</p></li></ol><p>The options that says: <strong>Before sending the data to Amazon S3 over HTTPS, encrypt the data locally first using your own encryption keys</strong> and <strong>Enable Server-Side Encryption on an S3 bucket to make use of AES-256 encryption</strong> are correct because these options are using client-side encryption and Amazon S3-Managed Keys (SSE-S3) respectively. <em>Client-side encryption</em> is the act of encrypting data before sending it to Amazon S3 while SSE-S3 uses AES-256 encryption.</p><p><strong>Storing the data on EBS volumes with encryption enabled instead of using Amazon S3</strong> and <strong>storing the data in encrypted EBS snapshots</strong> are incorrect because both options use EBS encryption and not S3.</p><p><strong>Enabling Server-Side Encryption on an S3 bucket to make use of AES-128 encryption</strong> is incorrect as S3 doesn't provide AES-128 encryption, only AES-256.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Before sending the data to Amazon S3 over HTTPS, encrypt the data locally first using your own encryption keys.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Store the data on EBS volumes with encryption enabled instead of using Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Store the data in encrypted EBS snapshots.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Enable Server-Side Encryption on an S3 bucket to make use of AES-256 encryption.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Enable Server-Side Encryption on an S3 bucket to make use of AES-128 encryption.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 40,
    "question": "<p>An e-commerce company is receiving a large volume of sales data files in .csv format from its external partners on a daily basis. These data files are then stored in an Amazon S3 Bucket for processing and reporting purposes.</p><p>The company wants to create an automated solution to convert these .csv files into Apache Parquet format and store the output of the processed files in a new S3 bucket called “<code>tutorialsdojo-data-transformed</code>”. This new solution is meant to enhance the company’s data processing and analytics workloads while keeping its operating costs low.</p><p>Which of the following options must be implemented to meet these requirements with the LEAST operational overhead?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>AWS Glue</strong> is a fully managed extract, transform, and load (ETL) service. AWS Glue makes it cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams. This pattern provides different job types in AWS Glue and uses three different scripts to demonstrate authoring ETL jobs.</p><p>Apache Parquet is built to support efficient compression and encoding schemes. It can speed up your analytics workloads because it stores data in a columnar fashion. Converting data to Parquet can save you storage space, cost, and time in the longer run.<br></p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-08-06_02-29-43-6685e3751aa17c46a805615ca0565b86.png\"><p><br></p><p>AWS Glue retrieves data from sources and writes data to targets stored and transported in various data formats. AWS Glue supports using the Parquet format. This format is a performance-oriented, column-based data format You can use AWS Glue to read Parquet files from Amazon S3 and from streaming sources as well as write Parquet files to Amazon S3. You can read and write <code>bzip</code> and <code>gzip</code> archives containing Parquet files from S3.</p><p>When a crawler runs, it takes the following actions to interrogate a data store:</p><p><strong>Classifies data to determine the format, schema, and associated properties of the raw data</strong> – You can configure the results of classification by creating a custom classifier.</p><p><strong>Groups data into tables or partitions </strong>– Data is grouped based on crawler heuristics.</p><p><strong>Writes metadata to the Data Catalog </strong>– You can configure how the crawler adds, updates, and deletes tables and partitions.</p><p>Hence, the correct answer is the option that says: <strong>Use AWS Glue crawler to automatically discover the raw data file in S3 as well as check its corresponding schema. Create a scheduled ETL job in AWS Glue that will convert .csv files to Apache Parquet format and store the output of the processed files in the “</strong><code><strong>tutorialsdojo-data-transformed</strong></code><strong>\" bucket.</strong></p><p>The option that says: <strong>Integrate Amazon EMR File System (EMRFS) with the source S3 bucket to automatically discover the new data files. Use an Amazon EMR Serverless with Apache Spark to convert the .csv files to the Apache Parquet format and then store the output in the \"</strong><code><strong>tutorialsdojo-data-transformed</strong></code><strong>\" bucket</strong> is incorrect. Although Amazon EMR Serverless is a cost-effective managed cluster platform that simplifies running big data frameworks, using EMRFS in detecting new data files from an Amazon S3 bucket is not a suitable choice. EMRFS is simply an implementation of HDFS that all Amazon EMR clusters use for reading and writing regular files from Amazon EMR directly to Amazon S3. You should either use the S3 Event Notification or AWS Glue to discover the new data files in your source bucket.</p><p>The option that says:<strong> Utilize an AWS Batch job definition with Bash syntax to convert the .csv files to the Apache Parquet format. Configure the job definition to run automatically whenever a new .csv file is uploaded to the source bucket</strong> is incorrect because AWS Batch is mainly intended for managing batch processing tasks in Docker containers, which can make things complicated due to containerization and Bash script execution. The setup and maintenance of AWS Batch resources, such as compute environments and job queues, can be more challenging than using serverless or fully managed services. Furthermore, AWS Batch still requires manual scaling configuration and incurs costs based on resource usage, which can make cost management and optimization more difficult. Although AWS Batch can trigger jobs automatically when new files are uploaded, the overall setup and maintenance of the Batch environment require more manual effort.</p><p>The option that says:<strong> Use Amazon S3 event notifications to trigger an AWS Lambda function that converts .csv files to Apache Parquet format using Apache Spark on an Amazon EMR cluster. Save the processed files to the “</strong><code><strong>tutorialsdojo-data-transformed</strong></code><strong>\" bucket </strong>is incorrect because<strong> </strong>setting up and managing an Amazon EMR cluster can be complex and require additional configuration, maintenance, and monitoring efforts. This can result in higher costs associated with running and maintaining the cluster, which may not be cost-effective for solutions requiring minimal operational management. Additionally, the complexities involved in provisioning and scaling resources with EMR could cause delays in real-time data processing.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-data-to-apache-parquet.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-data-to-apache-parquet.html</a></p><p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-parquet-home.html\">https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-parquet-home.html</a></p><p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/crawler-running.html\">https://docs.aws.amazon.com/glue/latest/dg/crawler-running.html</a></p><p><br></p><p><strong>Check out this AWS Glue Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-glue/\">https://tutorialsdojo.com/aws-glue/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Integrate Amazon EMR File System (EMRFS) with the source S3 bucket to automatically discover the new data files. Use an Amazon EMR Serverless with Apache Spark to convert the .csv files to the Apache Parquet format and then store the output in the \"<code>tutorialsdojo-data-transformed</code>\" bucket.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Utilize an AWS Batch job definition with Bash syntax to convert the .csv files to the Apache Parquet format. Configure the job definition to run automatically whenever a new .csv file is uploaded to the source bucket.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 event notifications to trigger an AWS Lambda function that converts .csv files to Apache Parquet format using Apache Spark on an Amazon EMR cluster. Save the processed files to the “<code>tutorialsdojo-data-transformed</code>\" bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Glue crawler to automatically discover the raw data file in S3 as well as check its corresponding schema. Create a scheduled ETL job in AWS Glue that will convert .csv files to Apache Parquet format and store the output of the processed files in the “<code>tutorialsdojo-data-transformed</code>\" bucket.</p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 41,
    "question": "<p>A start-up company has an EC2 instance that is hosting a web application. The volume of users is expected to grow in the coming months, and hence, you need to add more elasticity and scalability in your AWS architecture to cope with the demand.</p><p>Which of the following options can satisfy the above requirement for the given scenario? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>Using an Elastic Load Balancer is an ideal solution for adding elasticity to your application. Alternatively, you can also create a policy in Route 53, such as a Weighted routing policy, to evenly distribute the traffic to 2 or more EC2 instances. Hence, <strong>setting up two EC2 instances and then put them behind an Elastic Load balancer (ELB)</strong> and <strong>setting up two EC2 instances and using Route 53 to route traffic based on a Weighted Routing Policy</strong> are the correct answers.</p><p><img src=\"https://media.tutorialsdojo.com/r53-cf-elb.png\"></p><p><strong>Setting up an S3 Cache in front of the EC2 instance</strong> is incorrect because doing so does not provide elasticity and scalability to your EC2 instances.</p><p><strong>Setting up an AWS WAF behind your EC2 Instance</strong> is incorrect because AWS WAF is a web application firewall that helps protect your web applications from common web exploits. This service is more about providing security to your applications.</p><p><strong>Setting up two EC2 instances deployed using Launch Templates and integrated with AWS Glue</strong> is incorrect because AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. It does not provide scalability or elasticity to your instances.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/elasticloadbalancing\">https://aws.amazon.com/elasticloadbalancing</a></p><p><a href=\"http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html\">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Load Balancing Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up two EC2 instances and then put them behind an Elastic Load balancer (ELB).</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up an S3 Cache in front of the EC2 instance.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up two EC2 instances and use Route 53 to route traffic based on a Weighted Routing Policy.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up an AWS WAF behind your EC2 Instance.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Set up two EC2 instances deployed using Launch Templates and integrated with AWS Glue. </p>",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 42,
    "question": "<p>An accounting application uses an RDS database configured with Multi-AZ deployments to improve availability. What would happen to RDS if the primary database instance fails?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>In <strong>Amazon RDS</strong>, failover is automatically handled so that you can resume database operations as quickly as possible without administrative intervention in the event that your primary database instance goes down. When failing over, Amazon RDS simply flips the canonical name record (CNAME) for your DB instance to point at the standby, which is in turn promoted to become the new primary.</p><p><img src=\"https://media.tutorialsdojo.com/rds_ha_5.png\"></p><p>Hence, the correct answer is: <strong>The canonical name record (CNAME) is switched from the primary to standby instance.</strong></p><p>The option that says: <strong>The IP address of the primary DB instance is switched to the standby DB instance</strong> is incorrect since IP addresses are per subnet, and subnets cannot span multiple AZs.</p><p>The option that says: <strong>The primary database instance will reboot</strong> is incorrect since in the event of a failure, there is no database to reboot with.</p><p>The option that says: <strong>A new database instance is created in the standby Availability Zone</strong> is incorrect since with multi-AZ enabled, you already have a standby database in another AZ.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/details/multi-az/\">https://aws.amazon.com/rds/details/multi-az/</a></p><p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "The IP address of the primary DB instance is switched to the standby DB instance.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "The primary database instance will reboot.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "A new database instance is created in the standby Availability Zone.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "The canonical name record (CNAME) is switched from the primary to standby instance.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 43,
    "question": "<p>A company plans to conduct a network security audit. The web application is hosted on an Auto Scaling group of EC2 Instances with an Application Load Balancer in front to evenly distribute the incoming traffic. A Solutions Architect has been tasked to enhance the security posture of the company’s cloud infrastructure and minimize the impact of DDoS attacks on its resources.</p><p>Which of the following is the most effective solution that should be implemented?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define. You can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts your web servers or origin servers running on EC2, or Amazon API Gateway for your APIs.</p><p>To detect and mitigate DDoS attacks, you can use <strong>AWS WAF</strong> in addition to AWS Shield. AWS WAF is a web application firewall that helps detect and mitigate web application layer DDoS attacks by inspecting traffic inline. Application layer DDoS attacks use well-formed but malicious requests to evade mitigation and consume application resources. You can define custom security rules that contain a set of conditions, rules, and actions to block attacking traffic. After you define web ACLs, you can apply them to CloudFront distributions, and web ACLs are evaluated in the priority order you specified when you configured them.</p><p><img src=\"https://media.tutorialsdojo.com/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\"></p><p>By using AWS WAF, you can configure web access control lists (Web ACLs) on your CloudFront distributions or Application Load Balancers to filter and block requests based on request signatures. Each Web ACL consists of rules that you can configure to string match or regex match one or more request attributes, such as the URI, query-string, HTTP method, or header key. In addition, by using AWS WAF's rate-based rules, you can automatically block the IP addresses of bad actors when requests matching a rule exceed a threshold that you define. Requests from offending client IP addresses will receive 403 Forbidden error responses and will remain blocked until request rates drop below the threshold. This is useful for mitigating HTTP flood attacks that are disguised as regular web traffic.</p><p>It is recommended that you add web ACLs with rate-based rules as part of your AWS Shield Advanced protection. These rules can alert you to sudden spikes in traffic that might indicate a potential DDoS event. A rate-based rule counts the requests that arrive from any individual address in any five-minute period. If the number of requests exceeds the limit that you define, the rule can trigger an action such as sending you a notification.</p><p>Hence, the correct answer is:<strong> Configure Amazon CloudFront distribution and set Application Load Balancer as the origin. Create a rate-based web ACL rule using AWS WAF and associate it with Amazon CloudFront.</strong></p><p>The option that says:<strong> Configure Amazon CloudFront distribution and set a Network Load Balancer as the origin. Use VPC Flow Logs to monitor abnormal traffic patterns. Set up a custom AWS Lambda function that processes the flow logs and invokes Amazon SNS for notification </strong>is incorrect because this option only allows you to monitor the traffic that is reaching your instance. You can't use VPC Flow Logs to mitigate DDoS attacks.</p><p>The option that says:<strong> Configure Amazon CloudFront distribution and set an Application Load Balancer as the origin. Create a security group rule and deny all the suspicious addresses. Use Amazon SNS for notification</strong> is incorrect. To deny suspicious addresses, you must manually insert the IP addresses of these hosts. This is a manual task which is not a sustainable solution. Take note that attackers generate large volumes of packets or requests to overwhelm the target system. Using a security group in this scenario won't help you mitigate DDoS attacks.</p><p>The option that says:<strong> Configure Amazon CloudFront distribution and set a Network Load Balancer as the origin. Use Amazon GuardDuty to block suspicious hosts based on its security findings. Set up a custom AWS Lambda function that processes the security logs and invokes Amazon SNS for notification</strong> is incorrect because Amazon GuardDuty is just a threat detection service. You should use AWS WAF and create your own AWS WAF rate-based rules for mitigating HTTP flood attacks that are disguised as regular web traffic.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/ddos-overview.html\">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-overview.html</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/ddos-get-started-rate-based-rules.html\">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-get-started-rate-based-rules.html</a></p><p><a href=\"https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf\">https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a></p><p><br></p><p><strong>Check out this AWS WAF Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Amazon CloudFront distribution and set Application Load Balancer as the origin. Create a rate-based web ACL rule using AWS WAF and associate it with Amazon CloudFront.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure Amazon CloudFront distribution and set a Network Load Balancer as the origin. Use VPC Flow Logs to monitor abnormal traffic patterns. Set up a custom AWS Lambda function that processes the flow logs and invokes Amazon SNS for notification.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure Amazon CloudFront distribution and set an Application Load Balancer as the origin. Create a security group rule and deny all the suspicious addresses. Use Amazon SNS for notification.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure Amazon CloudFront distribution and set a Network Load Balancer as the origin. Use Amazon GuardDuty to block suspicious hosts based on its security findings. Set up a custom AWS Lambda function that processes the security logs and invokes Amazon SNS for notification.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 44,
    "question": "<p>A DevOps Engineer is required to design a cloud architecture in AWS. The Engineer is planning to develop a highly available and fault-tolerant architecture consisting of an Elastic Load Balancer and an Auto Scaling group of EC2 instances deployed across multiple Availability Zones. This will be used by an online accounting application that requires path-based routing, host-based routing, and bi-directional streaming using Remote Procedure Call (gRPC).</p><p>Which configuration will satisfy the given requirement?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>Application Load Balancer</strong> operates at the request level (layer 7), routing traffic to targets (EC2 instances, containers, IP addresses, and Lambda functions) based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications. Application Load Balancer simplifies and improves the security of your application, by ensuring that the latest SSL/TLS ciphers and protocols are used at all times.</p><p><img src=\"https://media.tutorialsdojo.com/ALB-features.png\"></p><p>If your application is composed of several individual services, an Application Load Balancer can route a request to a service based on the content of the request such as Host field, Path URL, HTTP header, HTTP method, Query string, or Source IP address.</p><p><img src=\"https://media.tutorialsdojo.com/alb-grpc-support.jpg\"></p><p>ALBs can also route and load balance gRPC traffic between microservices or between gRPC-enabled clients and services. This will allow customers to seamlessly introduce gRPC traffic management in their architectures without changing any of the underlying infrastructure on their clients or services.</p><p>Therefore, the correct answer is: <strong>Configure an Application Load Balancer in front of the auto-scaling group. Select gRPC as the protocol version.</strong></p><p>The option that says: <strong>Configure a Network Load Balancer in front of the auto-scaling group. Use a UDP listener for routing </strong>is incorrect. Network Load Balancers do not support gRPC.</p><p>The option that says: <strong>Configure a Gateway Load Balancer in front of the auto-scaling group. Ensure that the IP Listener Routing uses the GENEVE protocol on port 6081 to allow gRPC response traffic</strong> is incorrect. A Gateway Load Balancer operates as a Layer 3 Gateway and a Layer 4 Load Balancing service. Do take note that the gRPC protocol is at Layer 7 of the OSI Model so this service is not appropriate for this scenario.</p><p>The option that says: <strong>Configure a Network Load Balancer in front of the auto-scaling group. Create an AWS Global Accelerator accelerator and set the load balancer as an endpoint</strong> is incorrect. AWS Global Accelerator simply optimizes application performance by routing user traffic to the congestion-free, redundant AWS global network instead of the public internet.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticloadbalancing/features\">https://aws.amazon.com/elasticloadbalancing/features</a></p><p><a href=\"https://aws.amazon.com/elasticloadbalancing/faqs/\">https://aws.amazon.com/elasticloadbalancing/faqs/</a></p><p><br></p><p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p><p><br></p><p><strong>Application Load Balancer vs Network Load Balancer vs Gateway Load Balancer:</strong></p><p><a href=\"https://tutorialsdojo.com/application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/?src=udemy\">https://tutorialsdojo.com/application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure an Application Load Balancer in front of the auto-scaling group. Select gRPC as the protocol version.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure a Network Load Balancer in front of the auto-scaling group. Use a UDP listener for routing.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure a Gateway Load Balancer in front of the auto-scaling group. Ensure that the IP Listener Routing uses the GENEVE protocol on port 6081 to allow gRPC response traffic.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a Network Load Balancer in front of the auto-scaling group. Create an AWS Global Accelerator accelerator and set the load balancer as an endpoint.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 45,
    "question": "<p>A start-up company that offers an intuitive financial data analytics service has consulted you about their AWS architecture. They have a fleet of Amazon EC2 worker instances that process financial data and then outputs reports which are used by their clients. You must store the generated report files in a durable storage. The number of files to be stored can grow over time as the start-up company is expanding rapidly overseas and hence, they also need a way to distribute the reports faster to clients located across the globe.  </p><p>Which of the following is a cost-efficient and scalable storage option that you should use for this scenario?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p>A Content Delivery Network (CDN) is a critical component of nearly any modern web application. It used to be that CDN merely improved the delivery of content by replicating commonly requested files (static content) across a globally distributed set of caching servers. However, CDNs have become much more useful over time.</p><p>For caching, a CDN will reduce the load on an application origin and improve the experience of the requestor by delivering a local copy of the content from a nearby cache edge, or Point of Presence (PoP). The application origin is off the hook for opening the connection and delivering the content directly as the CDN takes care of the heavy lifting. The end result is that the application origins don’t need to scale to meet demands for static content.</p><p><br></p><p><img src=\"https://d1.awsstatic.com/product-marketing/Cloudfront-cdn-diagram-v2.1a4a693fc371d29d794d318a01dfe1aecc4c658e.PNG\"></p><p><br></p><p>Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront is integrated with AWS – both physical locations that are directly connected to the AWS global infrastructure, as well as other AWS services.</p><p><strong><em>Amazon S3</em></strong> offers a highly durable, scalable, and secure destination for backing up and archiving your critical data. This is the correct option as the start-up company is looking for a durable storage to store the audio and text files. In addition, ElastiCache is only used for caching and not specifically as a Global Content Delivery Network (CDN).</p><p><strong><em>Using Amazon Redshift as the data storage and CloudFront as the CDN</em></strong> is incorrect as Amazon Redshift is usually used as a Data Warehouse.</p><p><strong><em>Using Amazon S3 Glacier as the data storage and ElastiCache as the CDN</em></strong> is incorrect as Amazon S3 Glacier is usually used for data archives.</p><p><strong><em>Using multiple EC2 instance stores for data storage and ElastiCache as the CDN</em></strong> is incorrect as data stored in an instance store is not durable.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/\">https://aws.amazon.com/s3/</a></p><p><a href=\"https://aws.amazon.com/caching/cdn/\">https://aws.amazon.com/caching/cdn/</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Redshift as the data storage and CloudFront as the CDN.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Glacier as the data storage and ElastiCache as the CDN.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 as the data storage and CloudFront as the CDN.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use multiple EC2 instance stores for data storage and ElastiCache as the CDN.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 46,
    "question": "<p>A multinational company currently operates multiple AWS accounts to support its operations across various branches and business units. The company needs a more efficient and secure approach in managing its vast AWS infrastructure to avoid costly operational overhead.</p><p>To address this, they plan to transition to a consolidated, multi-account architecture while integrating a centralized corporate directory service for authentication purposes.</p><p>Which combination of options can be used to meet the above requirements? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS Organization </strong>is a service that allows you to manage multiple AWS accounts easily. With this service, you can effectively consolidate billing and manage your resources across multiple accounts. AWS IAM Identity Center can be integrated with your corporate directory service for centralized authentication. This means you can sign in to multiple AWS accounts with just one set of credentials. This integration helps to streamline the authentication process and makes it easier for companies to switch between accounts.</p><p><img src=\"https://media.tutorialsdojo.com/aws_iam_identity_center_aws_organization.png\"></p><p>In addition to this, you can also configure a service control policy (SCP) to manage your AWS accounts. SCPs help you enforce policies across your organization and control the services and features accessible to your other account. This way, you can ensure that your organization's resources are used only as intended and prevent unauthorized access. You can provide secure and centralized management of your AWS accounts by setting up AWS Organization, integrating AWS IAM Identity Center with your corporate directory service, and configuring SCPs. This simplifies your management process and helps you maintain better control over your resources.</p><p>Hence the correct answers are:</p><p><strong>- Integrate AWS IAM Identity Center with the corporate directory service for centralized authentication. Configure a service control policy (SCP) to manage the AWS accounts.</strong></p><p><strong>- Implement AWS Organizations to create a multi-account architecture that provides a consolidated view and centralized management of AWS accounts.</strong></p><p>The option that says: <strong>Set up a new entity in AWS Organizations and configure its authentication system to utilize AWS Directory Service directly</strong> is incorrect. The primary function of the AWS Directory Service is to manage user directories such as Microsoft Active Directory, and it's not intended to be used directly for multi-account authentication purposes. Moreover, this option does not address the need for a centralized corporate directory service for authentication across all accounts in company branches.</p><p>The option that says: <strong>Establish an identity pool through Amazon Cognito and adjust the AWS IAM Identity Center settings to allow Amazon Cognito authentication</strong> is incorrect. While Amazon Cognito provides a service for managing user identities and access to applications, it is not designed to integrate with corporate directory services for centralized authentication directly. It only provides identity solutions for applications and websites, especially with external users, social logins, and federated identities.</p><p>The option that says: <strong>Utilize AWS CloudTrail to enable centralized logging and monitoring across all AWS accounts</strong> is incorrect. AWS CloudTrail is designed to record API calls and capture event history. This feature helps ensure compliance, conduct security analysis and track resources. However, it is not intended for implementing a consolidated, multi-account architecture or integrating with a corporate directory service for authentication.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p><p><a href=\"https://docs.aws.amazon.com/controltower/latest/userguide/organizations.html\">https://docs.aws.amazon.com/controltower/latest/userguide/organizations.html</a></p><p><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-accounts.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-accounts.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a new entity in AWS Organizations and configure its authentication system to utilize AWS Directory Service directly.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Establish an identity pool through Amazon Cognito and adjust the AWS IAM Identity Center settings to allow Amazon Cognito authentication.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize AWS CloudTrail to enable centralized logging and monitoring across all AWS accounts.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Integrate AWS IAM Identity Center with the corporate directory service for centralized authentication. Configure a service control policy (SCP) to manage the AWS accounts.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Implement AWS Organizations to create a multi-account architecture that provides a consolidated view and centralized management of AWS accounts.</p>",
        "correct": true
      }
    ],
    "corrects": [
      4,
      5
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 47,
    "question": "<p>A company is building an internal application that serves as a repository for images uploaded by a couple of users. Whenever a user uploads an image, it would be sent to Kinesis Data Streams for processing before it is stored in an S3 bucket. If the upload was successful, the application will return a prompt informing the user that the operation was successful. The entire processing typically takes about 5 minutes to finish.</p><p>Which of the following options will allow you to asynchronously process the request to the application from upload request to Kinesis, S3, and return a reply in the most cost-effective manner?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p><strong>AWS Lambda</strong> supports the synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke a Lambda function. When you use an AWS service as a trigger, the invocation type is predetermined for each service. You have no control over the invocation type that these event sources use when they invoke your Lambda function. Since processing only takes 5 minutes, Lambda is also a cost-effective choice.</p><p><img src=\"https://media.tutorialsdojo.com/product-page-diagram_Lambda-RealTimeStreamProcessing.d79d55b5f3a5d6b58142a6c0fc8a29eadc81c02b.png\"></p><p>You can use an AWS Lambda function to process messages in an Amazon Simple Queue Service (Amazon SQS) queue. Lambda event source mappings support standard queues and first-in, first-out (FIFO) queues. With Amazon SQS, you can offload tasks from one component of your application by sending them to a queue and processing them asynchronously.</p><p>Kinesis Data Streams is a real-time data streaming service that requires the provisioning of shards. Amazon SQS is a cheaper option because you only pay for what you use. Since there is no requirement for real-time processing in the scenario given, replacing Kinesis Data Streams with Amazon SQS would save more costs.</p><p>Hence, the correct answer is: <strong>Replace the Kinesis stream with an Amazon SQS queue. Create a Lambda function that will asynchronously process the requests.</strong></p><p><strong>Using a combination of Lambda and Step Functions to orchestrate service components and asynchronously process the requests</strong> is incorrect. The AWS Step Functions service lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Although this can be a valid solution, it is not cost-effective since the application does not have a lot of components to orchestrate. Lambda functions can effectively meet the requirements in this scenario without using Step Functions. This service is not as cost-effective as Lambda.</p><p><strong>Using a combination of SQS to queue the requests and then asynchronously processing them using On-Demand EC2 Instances</strong> and <strong>Using a combination of SNS to buffer the requests and then asynchronously processing them using On-Demand EC2 Instances</strong> are both incorrect as using On-Demand EC2 instances is not cost-effective. It is better to use a Lambda function instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/welcome.html\">https://docs.aws.amazon.com/lambda/latest/dg/welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-invocation.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-invocation.html</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/new-aws-lambda-controls-for-stream-processing-and-asynchronous-invocations/\">https://aws.amazon.com/blogs/compute/new-aws-lambda-controls-for-stream-processing-and-asynchronous-invocations/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a combination of Lambda and Step Functions to orchestrate service components and asynchronously process the requests.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use a combination of SQS to queue the requests and then asynchronously process them using On-Demand EC2 Instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Replace the Kinesis Data Streams with an Amazon SQS queue. Create a Lambda function that will asynchronously process the requests.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use a combination of SNS to buffer the requests and then asynchronously process them using On-Demand EC2 Instances.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 48,
    "question": "<p>A company has a cryptocurrency exchange portal that is hosted in an Auto Scaling group of EC2 instances behind an Application Load Balancer and is deployed across multiple AWS regions. The users can be found all around the globe, but the majority are from Japan and Sweden. Because of the compliance requirements in these two locations, you want the Japanese users to connect to the servers in the <code>ap-northeast-1</code> Asia Pacific (Tokyo) region, while the Swedish users should be connected to the servers in the <code>eu-west-1</code> EU (Ireland) region.</p><p>Which of the following services would allow you to easily fulfill this requirement?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Geolocation routing</strong> lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region.</p><p>When you use geolocation routing, you can localize your content and present some or all of your website in the language of your users. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights. Another possible use is for balancing load across endpoints in a predictable, easy-to-manage way so that each user location is consistently routed to the same endpoint.</p><p><img src=\"https://media.tutorialsdojo.com/how-route-53-routes-traffic.png\"></p><p><strong>Setting up an Application Load Balancers that will automatically route the traffic to the proper AWS region</strong> is incorrect because Elastic Load Balancers distribute traffic among EC2 instances across multiple Availability Zones but not across AWS regions.</p><p><strong>Setting up a new CloudFront web distribution with the geo-restriction feature enabled</strong> is incorrect because the CloudFront geo-restriction feature is primarily used to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront web distribution. It does not let you choose the resources that serve your traffic based on the geographic location of your users, unlike the Geolocation routing policy in Route 53.</p><p><strong>Using Route 53 Weighted Routing policy</strong> is incorrect because this is not a suitable solution to meet the requirements of this scenario. It just lets you associate multiple resources with a single domain name (tutorialsdojo.com) or subdomain name (forums.tutorialsdojo.com) and choose how much traffic is routed to each resource. You have to use a Geolocation routing policy instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/geolocation-routing-policy\">https://aws.amazon.com/premiumsupport/knowledge-center/geolocation-routing-policy</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p><p><br></p><p><strong>Latency Routing vs. Geoproximity Routing vs. Geolocation Routing:</strong></p><p><a href=\"https://tutorialsdojo.com/latency-routing-vs-geoproximity-routing-vs-geolocation-routing/?src=udemy\">https://tutorialsdojo.com/latency-routing-vs-geoproximity-routing-vs-geolocation-routing/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Route 53 Geolocation Routing policy.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up an Application Load Balancers that will automatically route the traffic to the proper AWS region.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up a new CloudFront web distribution with the geo-restriction feature enabled.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Route 53 Weighted Routing policy.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 49,
    "question": "<p>A company is hosting its web application in an Auto Scaling group of EC2 instances behind an Application Load Balancer. Recently, the Solutions Architect identified a series of SQL injection attempts and cross-site scripting attacks to the application, which had adversely affected their production data. </p><p>Which of the following should the Architect implement to mitigate this kind of attack?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS WAF</strong> is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon API Gateway API, Amazon CloudFront or an Application Load Balancer. AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, API Gateway, CloudFront or an Application Load Balancer responds to requests either with the requested content or with an HTTP 403 status code (Forbidden). You also can configure CloudFront to return a custom error page when a request is blocked.</p><p><img src=\"https://media.tutorialsdojo.com/waf-archs2-1024x354.png\"></p><p>At the simplest level, AWS WAF lets you choose one of the following behaviors:</p><p><strong>Allow all requests except the ones that you specify</strong> – This is useful when you want CloudFront or an Application Load Balancer to serve content for a public website, but you also want to block requests from attackers.</p><p><strong>Block all requests except the ones that you specify</strong> – This is useful when you want to serve content for a restricted website whose users are readily identifiable by properties in web requests, such as the IP addresses that they use to browse to the website.</p><p><strong>Count the requests that match the properties that you specify</strong> – When you want to allow or block requests based on new properties in web requests, you first can configure AWS WAF to count the requests that match those properties without allowing or blocking those requests. This lets you confirm that you didn't accidentally configure AWS WAF to block all the traffic to your website. When you're confident that you specified the correct properties, you can change the behavior to allow or block requests.</p><p>Hence, the correct answer in this scenario is:<strong><em> </em>Set up security rules that block SQL injection and cross-site scripting attacks in AWS Web Application Firewall (WAF). Associate the rules to the Application Load Balancer.</strong></p><p><strong>Using Amazon GuardDuty to prevent any further SQL injection and cross-site scripting attacks in your application</strong> is incorrect because Amazon GuardDuty is just a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads.</p><p><strong>Using AWS Firewall Manager to set up security rules that block SQL injection and cross-site scripting attacks, then associating the rules to the Application Load Balancer</strong><em> </em>is incorrect because AWS Firewall Manager just simplifies your AWS WAF and AWS Shield Advanced administration and maintenance tasks across multiple accounts and resources.</p><p><strong>Blocking all the IP addresses where the SQL injection and cross-site scripting attacks originated using the Network Access Control List</strong> is incorrect because this is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. NACLs are not effective in blocking SQL injection and cross-site scripting attacks</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p><p><br></p><p><strong>Check out this AWS WAF Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Guard​Duty to prevent any further SQL injection and cross-site scripting attacks in your application.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Using AWS Firewall Manager, set up security rules that block SQL injection and cross-site scripting attacks. Associate the rules to the Application Load Balancer.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Block all the IP addresses where the SQL injection and cross-site scripting attacks originated using the Network Access Control List.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up security rules that block SQL injection and cross-site scripting attacks in AWS Web Application Firewall (WAF). Associate the rules to the Application Load Balancer.</p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 50,
    "question": "<p>A solutions architect is instructed to host a website that consists of HTML, CSS, and some Javascript files. The web pages will display several high-resolution images. The website should have optimal loading times and be able to respond to high request rates.</p><p>Which of the following architectures can provide the most cost-effective and fastest loading experience?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p><strong>Amazon S3 </strong>is an object storage service that offers industry-leading scalability, data availability, security, and performance. Additionally, You can use Amazon S3 to host a static website. On a static website, individual webpages include static content. Amazon S3 is <strong>highly scalable and you only pay for what you use</strong>, you can start small and grow your application as you wish, with no compromise on performance or reliability.</p><p><strong>Amazon CloudFront</strong> is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds. CloudFront can be integrated with Amazon S3 for fast delivery of data originating from an S3 bucket to your end-users. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p><p><img src=\"https://media.tutorialsdojo.com/amazon-s3-static-website-hosting.jpg\"></p><p>In the scenario, Since we are only dealing with static content, we can leverage the web hosting feature of S3. Then we can improve the architecture further by integrating it with CloudFront. This way, users will be able to load both the web pages and images faster than if we hosted them on a webserver that we built from scratch.</p><p>Hence, the correct answer is: <strong>Upload the HTML, CSS, Javascript, and the images in a single bucket. Then enable website hosting. Create a CloudFront distribution and point the domain on the S3 website endpoint.</strong></p><p>The option that says: <strong>Host the website using an Nginx server in an EC2 instance. Upload the images in an S3 bucket. Use CloudFront as a CDN to deliver the images closer to end-users</strong> is incorrect. Creating your own web server to host a static website in AWS is a costly solution. Web Servers on an EC2 instance are usually used for hosting applications that require server-side processing (connecting to a database, data validation, etc.). Since static websites contain web pages with fixed content, we should use S3 website hosting instead.</p><p>The option that says:<strong> Launch an Auto Scaling Group using an AMI that has a pre-configured Apache web server, then configure the scaling policy accordingly. Store the images in an Elastic Block Store. Then, point your instance’s endpoint to AWS Global Accelerator</strong> is incorrect. This is how we serve static websites in the old days. Now, with the help of S3 website hosting, we can host our static contents from a durable, high-availability, and highly scalable environment without managing any servers. Hosting static websites in S3 is cheaper than hosting it in an EC2 instance. In addition, Using ASG for scaling instances that host a static website is an over-engineered solution that carries unnecessary costs. S3 automatically scales to high requests and you only pay for what you use.</p><p>The option that says: <strong>Host the website in an AWS Elastic Beanstalk environment. Upload the images in an S3 bucket. Use CloudFront as a CDN to deliver the images closer to your end-users</strong> is incorrect. AWS Elastic Beanstalk simply sets up the infrastructure (EC2 instance, load balancer, auto-scaling group) for your application. It's a more expensive and a bit of an overkill solution for hosting a bunch of client-side files.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p><p><br></p><p><strong>Check out these Amazon S3 and CloudFront Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Upload the HTML, CSS, Javascript, and the images in a single bucket. Then enable website hosting. Create a CloudFront distribution and point the domain on the S3 website endpoint.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Host the website using an Nginx server in an EC2 instance. Upload the images in an S3 bucket. Use CloudFront as a CDN to deliver the images closer to end-users.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Launch an Auto Scaling Group using an AMI that has a pre-configured Apache web server, then configure the scaling policy accordingly. Store the images in an Elastic Block Store. Then, point your instance’s endpoint to AWS Global Accelerator.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Host the website in an AWS Elastic Beanstalk environment. Upload the images in an S3 bucket. Use CloudFront as a CDN to deliver the images closer to your end-users.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 51,
    "question": "<p>Both historical records and frequently accessed data are stored on an on-premises storage system. The amount of current data is growing at an exponential rate. As the storage’s capacity is nearing its limit, the company’s Solutions Architect has decided to move the historical records to AWS to free up space for the active data.</p><p>Which of the following architectures deliver the best solution in terms of cost and operational management?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p><strong>AWS DataSync</strong> makes it simple and fast to move large amounts of data online between on-premises storage and Amazon S3, Amazon Elastic File System (Amazon EFS), or Amazon FSx for Windows File Server. Manual tasks related to data transfers can slow down migrations and burden IT operations. DataSync eliminates or automatically handles many of these tasks, including scripting copy jobs, scheduling, and monitoring transfers, validating data, and optimizing network utilization. The DataSync software agent connects to your Network File System (NFS), Server Message Block (SMB) storage, and your self-managed object storage, so you don’t have to modify your applications.</p><p>DataSync can transfer hundreds of terabytes and millions of files at speeds up to 10 times faster than open-source tools, over the Internet or AWS Direct Connect links. You can use DataSync to migrate active data sets or archives to AWS, transfer data to the cloud for timely analysis and processing, or replicate data to AWS for business continuity. Getting started with DataSync is easy: deploy the DataSync agent, connect it to your file system, select your AWS storage resources, and start moving data between them. You pay only for the data you move.</p><p><img src=\"https://media.tutorialsdojo.com/AWS-DataSync-How-it-Works-Diagram.d27ec6f14be8ca9c2d434f94dd4a98c3fdf5c88c.png\"></p><p>Since the problem is mainly about moving historical records from on-premises to AWS, using AWS DataSync is a more suitable solution. You can use DataSync to move cold data from expensive on-premises storage systems directly to durable and secure long-term storage, such as Amazon S3 Glacier or Amazon S3 Glacier Deep Archive.</p><p>Hence, the correct answer is the option that says: <strong>Use AWS DataSync to move the historical records from on-premises to AWS. Choose Amazon S3 Glacier Deep Archive to be the destination for the data.</strong></p><p>The following options are both incorrect:</p><p><strong>- Use AWS Storage Gateway to move the historical records from on-premises to AWS. Choose Amazon S3 Glacier Deep Archive to be the destination for the data.</strong></p><p><strong>- Use AWS Storage Gateway to move the historical records from on-premises to AWS. Choose Amazon S3 Glacier to be the destination for the data. Modify the S3 lifecycle configuration to move the data from the Standard tier to Amazon S3 Glacier Deep Archive after 30 days.</strong></p><p>Although you can copy data from on-premises to AWS with Storage Gateway, it is not suitable for transferring large sets of data to AWS. Storage Gateway is mainly used in providing low-latency access to data by caching frequently accessed data on-premises while storing archive data securely and durably in Amazon cloud storage services. Storage Gateway optimizes data transfer to AWS by sending only changed data and compressing data.</p><p>The option that says: <strong>Use AWS DataSync to move the historical records from on-premises to AWS. Choose Amazon S3 Standard to be the destination for the data. Modify the S3 lifecycle configuration to move the data from the Standard tier to Amazon S3 Glacier Deep Archive after 30 days</strong> is incorrect because, with AWS DataSync, you can transfer data from on-premises directly to Amazon S3 Glacier Deep Archive. You don’t have to configure the S3 lifecycle policy and wait for 30 days to move the data to Glacier Deep Archive.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/datasync/faqs/\">https://aws.amazon.com/datasync/faqs/</a></p><p><a href=\"https://aws.amazon.com/storagegateway/faqs/\">https://aws.amazon.com/storagegateway/faqs/</a></p><p><br></p><p><strong>Check out these AWS DataSync and Storage Gateway Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-datasync/?src=udemy\">https://tutorialsdojo.com/aws-datasync/</a></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS DataSync to move the historical records from on-premises to AWS. Choose Amazon S3 Glacier Deep Archive to be the destination for the data.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Storage Gateway to move the historical records from on-premises to AWS. Choose Amazon S3 Glacier Deep Archive to be the destination for the data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS DataSync to move the historical records from on-premises to AWS. Choose Amazon S3 Standard to be the destination for the data. Modify the S3 lifecycle configuration to move the data from the Standard tier to Amazon S3 Glacier Deep Archive after 30 days.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Storage Gateway to move the historical records from on-premises to AWS. Choose Amazon S3 Glacier to be the destination for the data. Modify the S3 lifecycle configuration to move the data from the Standard tier to Amazon S3 Glacier Deep Archive after 30 days.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 52,
    "question": "<p>All objects uploaded to an Amazon S3 bucket must be encrypted for security compliance. The bucket will use server-side encryption with Amazon S3-Managed encryption keys (SSE-S3) to encrypt data using 256-bit Advanced Encryption Standard (AES-256) block cipher.</p><p>Which of the following request headers must be used?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>Server-side encryption</strong> protects data at rest. If you use Server-Side Encryption with Amazon S3-Managed Encryption Keys (SSE-S3), Amazon S3 will encrypt each object with a unique key and as an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p><p><img src=\"https://media.tutorialsdojo.com/s3_sse_customer_key_2.png\"></p><p>If you need server-side encryption for all of the objects that are stored in a bucket, use a bucket policy. For example, the following bucket policy denies permissions to upload an object unless the request includes the <strong>x-amz-server-side-encryption</strong> header to request server-side encryption:</p><p>However, if you choose to use server-side encryption with customer-provided encryption keys (SSE-C), you must provide encryption key information using the following request headers:</p><p><code>x-amz-server-side-encryption-customer-algorithm</code></p><p><code>x-amz-server-side-encryption-customer-key</code></p><p><code>x-amz-server-side-encryption-customer-key-MD5</code></p><p>Hence, using the <strong>x-amz-server-side-encryption</strong> header is correct as this is the one being used for Amazon S3-Managed Encryption Keys (SSE-S3).</p><p>All other options are incorrect since they are used for SSE-C.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p><code>x-amz-server-side-encryption-customer-key</code></p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p><code>x-amz-server-side-encryption-customer-algorithm</code></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p><code>x-amz-server-side-encryption-customer-key-MD5</code></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p><code>x-amz-server-side-encryption</code></p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 53,
    "question": "<p>A company runs a messaging application in the <code>ap-northeast-1</code> and <code>ap-southeast-2</code> region. A Solutions Architect needs to create a routing policy wherein a larger portion of traffic from the Philippines and North India will be routed to the resource in the <code>ap-northeast-1</code> region.</p><p>Which Route 53 routing policy should the Solutions Architect use?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>Amazon Route 53</strong> is a highly available and scalable Domain Name System (DNS) web service. You can use Route 53 to perform three main functions in any combination: domain registration, DNS routing, and health checking. After you create a hosted zone for your domain, such as example.com, you create records to tell the Domain Name System (DNS) how you want traffic to be routed for that domain.</p><p>For example, you might create records that cause DNS to do the following:</p><p>- Route Internet traffic for example.com to the IP address of a host in your data center.</p><p>- Route email for that domain (jose.rizal@tutorialsdojo.com) to a mail server (mail.tutorialsdojo.com).</p><p>- Route traffic for a subdomain called operations.manila.tutorialsdojo.com to the IP address of a different host.</p><p>Each record includes the name of a domain or a subdomain, a record type (for example, a record with a type of MX routes email), and other information applicable to the record type (for MX records, the hostname of one or more mail servers and a priority for each server).</p><p><img src=\"https://media.tutorialsdojo.com/traffic-flow-geoproximity-map-example-no-bias.png\"></p><p>Route 53 has different routing policies that you can choose from. Below are some of the policies:</p><p><strong>Latency Routing</strong> lets Amazon Route 53 serve user requests from the AWS Region that provides the lowest latency. It does not, however, guarantee that users in the same geographic region will be served from the same location.</p><p><strong>Geoproximity Routing</strong> lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources. You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a <em>bias</em>. A <strong>bias </strong>expands or shrinks the size of the geographic region from which traffic is routed to a resource.</p><p><strong>Geolocation Routing</strong> lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from.</p><p><strong>Weighted Routing</strong> lets you associate multiple resources with a single domain name (tutorialsdojo.com) or subdomain name (subdomain.tutorialsdojo.com) and choose how much traffic is routed to each resource.</p><p>In this scenario, the problem requires a routing policy that will let Route 53 route traffic to the resource in the Tokyo region from a larger portion of the Philippines and North India.</p><p><img src=\"https://media.tutorialsdojo.com/geoproximity-map.PNG\"></p><p>You need to use Geoproximity Routing and specify a bias to control the size of the geographic region from which traffic is routed to your resource. The sample image above uses a bias of -40 in the Tokyo region and a bias of 1 in the Sydney Region. Setting up the bias configuration in this manner would cause Route 53 to route traffic coming from the middle and northern part of the Philippines, as well as the northern part of India to the resource in the Tokyo Region.</p><p>Hence, the correct answer is <strong>Geoproximity Routing.</strong></p><p><strong>Geolocation Routing </strong>is incorrect because you cannot control the coverage size from which traffic is routed to your instance in Geolocation Routing. It just lets you choose the instances that will serve traffic based on the location of your users.</p><p><strong>Latency Routing </strong>is incorrect because it is mainly used for improving performance by letting Route 53 serve user requests from the AWS Region that provides the lowest latency.</p><p><strong>Weighted Routing </strong>is incorrect because it is used for routing traffic to multiple resources in proportions that you specify. This can be useful for load balancing and testing new versions of software.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geoproximity\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geoproximity</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/rrsets-working-with.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/rrsets-working-with.html</a></p><p><br></p><p><strong>Latency Routing vs. Geoproximity Routing vs. Geolocation Routing:</strong></p><p><a href=\"https://tutorialsdojo.com/latency-routing-vs-geoproximity-routing-vs-geolocation-routing/?src=udemy\">https://tutorialsdojo.com/latency-routing-vs-geoproximity-routing-vs-geolocation-routing/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Geoproximity Routing</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Geolocation Routing</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Latency Routing</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Weighted Routing</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 54,
    "question": "<p>An organization stores and manages financial records of various companies in its on-premises data center, which is almost out of space. The management decided to move all of their existing records to a cloud storage service. All future financial records will also be stored in the cloud. For additional security, all records must be prevented from being deleted or overwritten.</p><p>Which of the following should you do to meet the above requirement?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS DataSync</strong> allows you to copy large datasets with millions of files without having to build custom solutions with open-source tools or licenses and manage expensive commercial network acceleration software. You can use DataSync to migrate active data to AWS, transfer data to the cloud for analysis and processing, archive data to free up on-premises storage capacity or replicate data to AWS for business continuity.</p><p><img src=\"https://media.tutorialsdojo.com/AWS-DataSync-How-it-Works-Diagram.d27ec6f14be8ca9c2d434f94dd4a98c3fdf5c88c.png\"></p><p>AWS DataSync enables you to migrate your on-premises data to Amazon S3, Amazon EFS, and Amazon FSx for Windows File Server. You can configure DataSync to make an initial copy of your entire dataset and schedule subsequent incremental transfers of changing data toward Amazon S3. Enabling S3 Object Lock prevents your existing and future records from being deleted or overwritten. <br></p><p>AWS DataSync is primarily used to migrate existing data to Amazon S3. On the other hand, AWS Storage Gateway is more suitable if you still want to retain access to the migrated data and for ongoing updates from your on-premises file-based applications.</p><p>Hence, the correct answer in this scenario is: <strong>Use AWS DataSync to move the data. Store all of your data in Amazon S3 and enable object lock</strong>.</p><p>The option that says: <strong>Use AWS DataSync to move the data. Store all of your data in Amazon EFS and enable object lock</strong> is incorrect because Amazon EFS only supports file locking. Object lock is a feature of Amazon S3 and not Amazon EFS.</p><p>The options that says: <strong>Use AWS Storage Gateway to establish hybrid cloud storage. Store all of your data in Amazon S3 and enable object lock</strong> is incorrect because the scenario requires that all of the existing records must be migrated to AWS. The future records will also be stored in AWS and not in the on-premises network. This means that setting up hybrid cloud storage is not necessary since the on-premises storage will no longer be used.</p><p>The option that says: <strong>Use AWS Storage Gateway to establish hybrid cloud storage. Store all of your data in Amazon EBS, and enable object lock</strong> is incorrect because Amazon EBS does not support object lock. Amazon S3 is the only service capable of locking objects to prevent an object from being deleted or overwritten.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/datasync/faqs/\">https://aws.amazon.com/datasync/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html\">https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html</a></p><p><br></p><p><strong>Check out this AWS DataSync Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-datasync/?src=udemy\">https://tutorialsdojo.com/aws-datasync/</a></p><p><br></p><p><strong>Amazon S3 vs EBS vs EFS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/?src=udemy\">https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Storage Gateway to establish hybrid cloud storage. Store all of your data in Amazon S3 and enable object lock.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS DataSync to move the data. Store all of your data in Amazon S3 and enable object lock.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS DataSync to move the data. Store all of your data in Amazon EFS and enable object lock.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Storage Gateway to establish hybrid cloud storage. Store all of your data in Amazon EBS and enable object lock.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 55,
    "question": "<p>An organization is currently using a tape backup solution to store its application data on-premises. They plan to use a cloud storage service to preserve the backup data for up to 10 years that may be accessed about once or twice a year.</p><p>Which of the following is the most cost-effective option to implement this solution?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p><strong>Tape Gateway</strong> enables you to replace physical tapes on-premises with virtual tapes in AWS without changing existing backup workflows. Tape Gateway supports all leading backup applications and caches virtual tapes on-premises for low-latency data access. Tape Gateway encrypts data between the gateway and AWS for secure data transfer and compresses data and transitions virtual tapes between Amazon S3 and Amazon S3 Glacier, or Amazon S3 Glacier Deep Archive, to minimize storage costs.</p><p><img src=\"https://media.tutorialsdojo.com/tape-gateway-diagram.4b6ca2b4e3f97d4df7988544400ae91424503248.png\"></p><p>The scenario requires you to back up your application data to a cloud storage service for long-term retention of data that will be retained for 10 years. Given that the organization uses a tape backup solution, an option that uses AWS Storage Gateway must be the possible answer. Tape Gateway can transition virtual tapes archived in Amazon S3 Glacier or Amazon S3 Glacier Deep Archive storage class, enabling you to further reduce the monthly cost to store long-term data in the cloud by up to 75%.</p><p>Hence, the correct answer is: <strong>Use AWS Storage Gateway to backup the data and transition it to Amazon S3 Glacier Deep Archive.</strong></p><p>The option that says: <strong>Use AWS Storage Gateway to backup the data directly to Amazon S3 Glacier</strong> is incorrect. Although this is a valid solution, moving to S3 Glacier is more expensive than directly backing it up to Glacier Deep Archive.</p><p>The option that says: <strong>Order an AWS Snowball Edge appliance to import the backup directly to Amazon S3 Glacier</strong> is incorrect because Snowball Edge can't directly integrate backups to S3 Glacier. Moreover, you have to use the Amazon S3 Glacier Deep Archive storage class as it is more cost-effective than the regular Glacier class.</p><p>The option that says: <strong>Use Amazon S3 to store the backup data and add a lifecycle rule to transition the current version to Amazon S3 Glacier</strong> is incorrect. Although this is a possible solution, it is difficult to directly integrate a tape backup solution to S3 without using Storage Gateway.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/faqs/\">https://aws.amazon.com/storagegateway/faqs/</a></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Storage Gateway to backup the data directly to Amazon S3 Glacier.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Order an AWS Snowball Edge appliance to import the backup directly to Amazon S3 Glacier.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Storage Gateway to backup the data and transition it to Amazon S3 Glacier Deep Archive.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon S3 to store the backup data and add a lifecycle rule to transition the current version to Amazon S3 Glacier.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 56,
    "question": "<p>An insurance company plans to implement a message filtering feature in their web application. To implement this solution, they need to create separate Amazon SQS queues for each type of quote request. The entire message processing should not exceed 24 hours.</p><p>As the Solutions Architect of the company, which of the following should you do to meet the above requirement?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon SNS</strong> is a fully managed pub/sub messaging service. With Amazon SNS, you can use topics to simultaneously distribute messages to multiple subscribing endpoints such as Amazon SQS queues, AWS Lambda functions, HTTP endpoints, email addresses, and mobile devices (SMS, Push).</p><p><strong>Amazon SQS</strong> is a message queue service used by distributed applications to exchange messages through a polling model. It can be used to decouple sending and receiving components without requiring each component to be concurrently available.</p><p>A fanout scenario occurs when a message published to an SNS topic is replicated and pushed to multiple endpoints, such as Amazon SQS queues, HTTP(S) endpoints, and Lambda functions. This allows for parallel asynchronous processing.</p><p><img src=\"https://media.tutorialsdojo.com/sns-fanout.png\"></p><p>For example, you can develop an application that publishes a message to an SNS topic whenever an order is placed for a product. Then, two or more SQS queues that are subscribed to the SNS topic receive identical notifications for the new order. An Amazon Elastic Compute Cloud (Amazon EC2) server instance attached to one of the SQS queues can handle the processing or fulfillment of the order. And you can attach another Amazon EC2 server instance to a data warehouse for analysis of all orders received.</p><p>By default, an Amazon SNS topic subscriber receives every message published to the topic. You can use Amazon SNS message filtering to assign a filter policy to the topic subscription, and the subscriber will only receive a message that they are interested in. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event. This method is known as fanout to Amazon SQS queues.</p><p>Hence, the correct answer is:<strong> Create one Amazon SNS topic and configure the Amazon SQS queues to subscribe to the SNS topic. Set the filter policies in the SNS subscriptions to publish the message to the designated SQS queue based on its quote request type.</strong></p><p>The option that says:<strong> Create one Amazon SNS topic and configure the Amazon SQS queues to subscribe to the SNS topic. Publish the same messages to all SQS queues. Filter the messages in each queue based on the quote request type</strong> is incorrect because this option will distribute the same messages on all SQS queues instead of its designated queue. You need to fan-out the messages to multiple SQS queues using a filter policy in Amazon SNS subscriptions to allow parallel asynchronous processing. By doing so, the entire message processing will not exceed 24 hours.</p><p>The option that says: <strong>Create multiple Amazon SNS topics and configure the Amazon SQS queues to subscribe to the SNS topics. Publish the message to the designated SQS queue based on the quote request type</strong> is incorrect because to implement the solution asked in the scenario, you only need to use one Amazon SNS topic. To publish it to the designated SQS queue, you must set a filter policy that allows you to fanout the messages. If you didn't set a filter policy in Amazon SNS, the subscribers would receive all the messages published to the SNS topic. Thus, using multiple SNS topics is not an appropriate solution for this scenario.</p><p>The option that says:<strong> Create a data stream in Amazon Kinesis Data Streams. Use the Amazon Kinesis Client Library to deliver all the records to the designated SQS queues based on the quote request type </strong>is incorrect because Amazon KDS is not a message filtering service. You should use Amazon SNS and SQS to distribute the topic to the designated queue.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/getting-started/hands-on/filter-messages-published-to-topics/\">https://aws.amazon.com/getting-started/hands-on/filter-messages-published-to-topics/</a></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html</a></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html</a></p><p><br></p><p><strong>Check out these Amazon SNS and SQS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sns/?src=udemy\">https://tutorialsdojo.com/amazon-sns/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create one Amazon SNS topic and configure the Amazon SQS queues to subscribe to the SNS topic. Set the filter policies in the SNS subscriptions to publish the message to the designated SQS queue based on its quote request type.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create one Amazon SNS topic and configure the Amazon SQS queues to subscribe to the SNS topic. Publish the same messages to all SQS queues. Filter the messages in each queue based on the quote request type.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create multiple Amazon SNS topics and configure the Amazon SQS queues to subscribe to the SNS topics. Publish the message to the designated SQS queue based on the quote request type.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a data stream in Amazon Kinesis Data Streams. Use the Amazon Kinesis Client Library to deliver all the records to the designated SQS queues based on the quote request type.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 57,
    "question": "<p>An organization needs to control the access for several S3 buckets. They plan to use a gateway endpoint to allow access to trusted buckets.</p><p>Which of the following could help you achieve this requirement?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>A Gateway endpoint is a type of VPC endpoint that provides reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC. Instances in your VPC do not require public IP addresses to communicate with resources in the service.</p><p>When you create a Gateway endpoint, you can attach an endpoint policy that controls access to the service to which you are connecting. You can modify the endpoint policy attached to your endpoint and add or remove the route tables used by the endpoint. An endpoint policy does not override or replace IAM user policies or service-specific policies (such as S3 bucket policies). It is a separate policy for controlling access from the endpoint to the specified service.<br><img src=\"https://media.tutorialsdojo.com/public/s3-gateway-endpoint-policy-sample.jpg\"></p><p>We can use a bucket policy or an endpoint policy to allow the traffic to trusted S3 buckets. The options that have 'trusted S3 buckets' key phrases will be the possible answer in this scenario. It would take you a lot of time to configure a bucket policy for each S3 bucket instead of using a single endpoint policy. Therefore, you should use an endpoint policy to control the traffic to the trusted Amazon S3 buckets.</p><p>Hence, the correct answer is: <strong>Generate an endpoint policy for trusted S3 buckets.</strong></p><p>The option that says:<strong> Generate a bucket policy for trusted S3 buckets</strong> is incorrect. Although this is a valid solution, it takes a lot of time to set up a bucket policy for each and every S3 bucket. This can be simplified by whitelisting access to trusted S3 buckets in a single S3 endpoint policy.</p><p>The option that says:<strong> Generate a bucket policy for trusted VPCs</strong> is incorrect because you are generating a policy for trusted VPCs. Remember that the scenario only requires you to allow the traffic for trusted S3 buckets, not to the VPCs.</p><p>The option that says:<strong> Generate an endpoint policy for trusted VPCs</strong> is incorrect because it only allows access to trusted VPCs, and not to trusted Amazon S3 buckets.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/\">https://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Generate a bucket policy for trusted VPCs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Generate a bucket policy for trusted S3 buckets.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Generate an endpoint policy for trusted S3 buckets.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Generate an endpoint policy for trusted VPCs.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 58,
    "question": "<p>A company needs to assess and audit all the configurations in their AWS account. It must enforce strict compliance by tracking all configuration changes made to any of its Amazon S3 buckets. Publicly accessible S3 buckets should also be identified automatically to avoid data breaches.</p><p>Which of the following options will meet this requirement?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS Config</strong> is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting.</p><p><img src=\"https://media.tutorialsdojo.com/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\"></p><p>You can use AWS Config to evaluate the configuration settings of your AWS resources. By creating an AWS Config rule, you can enforce your ideal configuration in your AWS account. It also checks if the applied configuration in your resources violates any of the conditions in your rules. The AWS Config dashboard shows the compliance status of your rules and resources. You can verify if your resources comply with your desired configurations and learn which specific resources are noncompliant.</p><p>Hence, the correct answer is: <strong>Use AWS Config to set up a rule in your AWS account</strong>.</p><p>The option that says: <strong>Use AWS Trusted Advisor to analyze your AWS environment</strong> is incorrect because AWS Trusted Advisor only provides best practice recommendations. It cannot define rules for your AWS resources.</p><p>The option that says: <strong>Use AWS IAM to generate a credential report</strong> is incorrect because this report will not help you evaluate resources. The IAM credential report is just a list of all IAM users in your AWS account.</p><p>The option that says: <strong>Use AWS CloudTrail and review the event history of your AWS account </strong>is incorrect. Although it can track changes and store a history of what happened to your resources, this service still cannot enforce rules to comply with your organization's policies.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html</a></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c02/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c02/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Trusted Advisor to analyze your AWS environment.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS IAM to generate a credential report.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Config to set up a rule in your AWS account.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS CloudTrail and review the event history of your AWS account.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 59,
    "question": "<p>A company has developed public APIs hosted in Amazon EC2 instances behind an Elastic Load Balancer. The APIs will be used by various clients from their respective on-premises data centers. A Solutions Architect received a report that the web service clients can only access trusted IP addresses whitelisted on their firewalls.</p><p>What should you do to accomplish the above requirement?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>A <strong>Network Load Balancer</strong> functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the default rule's target group. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration.</p><p><img src=\"https://d2908q01vomqb2.cloudfront.net/5b384ce32d8cdef02bc3a139d4cac0a22bb029e8/2018/05/04/Picture1-updated.jpg\"></p><p>Based on the given scenario, web service clients can only access trusted IP addresses. To resolve this requirement, you can use the Bring Your Own IP (BYOIP) feature to use the trusted IPs as Elastic IP addresses (EIP) to a Network Load Balancer (NLB). This way, there's no need to re-establish the whitelists with new IP addresses.</p><p>Hence, the correct answer is:<strong> Associate an Elastic IP address to a Network Load Balancer.</strong></p><p>The option that says: <strong>Associate an Elastic IP address to an Application Load Balancer </strong>is incorrect because you can't assign an Elastic IP address to an Application Load Balancer. The alternative method you can do is assign an Elastic IP address to a Network Load Balancer in front of the Application Load Balancer.</p><p>The option that says: <strong>Create a CloudFront distribution whose origin points to the private IP addresses of your web servers</strong> is incorrect because web service clients can only access trusted IP addresses. The fastest way to resolve this requirement is to attach an Elastic IP address to a Network Load Balancer.</p><p>The option that says:<strong> Create an Alias Record in Route 53 which maps to the DNS name of the load balancer</strong> is incorrect. This approach won't still allow them to access the application because of trusted IP addresses on their firewalls.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elb-attach-elastic-ip-to-public-nlb/\">https://aws.amazon.com/premiumsupport/knowledge-center/elb-attach-elastic-ip-to-public-nlb/</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/using-static-ip-addresses-for-application-load-balancers/\">https://aws.amazon.com/blogs/networking-and-content-delivery/using-static-ip-addresses-for-application-load-balancers/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Load Balancing Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Associate an Elastic IP address to a Network Load Balancer.</p><p><br></p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a CloudFront distribution whose origin points to the private IP addresses of your web servers.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Associate an Elastic IP address to an Application Load Balancer.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Alias Record in Route 53 which maps to the DNS name of the load balancer.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 60,
    "question": "<p>A company is running a custom application in an Auto Scaling group of Amazon EC2 instances. Several instances are failing due to insufficient swap space. The Solutions Architect has been instructed to troubleshoot the issue and effectively monitor the available swap space of each EC2 instance.</p><p>Which of the following options fulfills this requirement?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>Amazon CloudWatch</strong> is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. Amazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB tables, and Amazon RDS DB instances, as well as custom metrics generated by your applications and services and any log files your applications generate. You can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and operational health.</p><p><img src=\"https://media.tutorialsdojo.com/product-page-diagram_Cloudwatch_v4.55c15d1cc086395cbd5ad279a2f1fc37e8452e77.png\"></p><p>The main requirement in the scenario is to monitor the <code>SwapUtilization</code> metric. Take note that you can't use the default metrics of CloudWatch to monitor the <code>SwapUtilization</code> metric. To monitor custom metrics, you must install the CloudWatch agent on the EC2 instance. After installing the CloudWatch agent, you can now collect system metrics and log files of an EC2 instance.</p><p>Hence, the correct answer is: <strong>Install the CloudWatch agent on each instance and monitor the </strong><code><strong>SwapUtilization</strong></code><strong> metric.</strong></p><p>The option that says: <strong>Enable detailed monitoring on each instance and monitor the </strong><code><strong>SwapUtilization</strong></code> <strong>metric</strong> is incorrect because you can't monitor the <code>SwapUtilization</code> metric by just enabling the detailed monitoring option. You must install the CloudWatch agent on the instance.</p><p>The option that says: <strong>Create a CloudWatch dashboard and monitor the </strong><code><strong>SwapUsed</strong></code><strong> metric</strong> is incorrect because you must install the CloudWatch agent first to add the custom metric in the dashboard.</p><p>The option that says: <strong>Create a new trail in AWS CloudTrail and configure Amazon CloudWatch Logs to monitor your trail logs</strong> is incorrect because CloudTrail won't help you monitor custom metrics. CloudTrail is specifically used for monitoring API activities in an AWS account.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html</a></p><p><a href=\"https://aws.amazon.com/cloudwatch/faqs/\">https://aws.amazon.com/cloudwatch/faqs/</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable detailed monitoring on each instance and monitor the <code>SwapUtilization</code> metric.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a new trail in AWS CloudTrail and configure Amazon CloudWatch Logs to monitor your trail logs.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Install the CloudWatch agent on each instance and monitor the <code>SwapUtilization</code> metric.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create a CloudWatch dashboard and monitor the <code>SwapUsed</code> metric.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 61,
    "question": "<p>A company has multiple VPCs with IPv6 enabled for its suite of web applications. The Solutions Architect attempted to deploy a new Amazon EC2 instance but encountered an error indicating that there were no available IP addresses on the subnet. The VPC has a combination of IPv4 and IPv6 CIDR blocks, but the IPv4 CIDR blocks are nearing exhaustion. The architect needs a solution that will resolve this issue while allowing future scalability.</p><p>How should the Solutions Architect resolve this problem?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>Amazon Virtual Private Cloud (VPC) </strong>is a service that lets you launch AWS resources in a logically isolated virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. You can use both IPv4 and IPv6 for most resources in your virtual private cloud, helping to ensure secure and easy access to resources and applications.</p><p>A subnet is a range of IP addresses in your VPC. You can launch AWS resources into a specified subnet. When you create a VPC, you must specify a range of IPv4 addresses for the VPC in the form of a CIDR block. Each subnet must reside entirely within one Availability Zone and cannot span zones. You can also optionally assign an IPv6 CIDR block to your VPC, and assign IPv6 CIDR blocks to your subnets.</p><p><img src=\"https://media.tutorialsdojo.com/Amazon_VPC_IPv6.png\"></p><p>If you have an existing VPC that supports IPv4 only and resources in your subnet that are configured to use IPv4 only, you can enable IPv6 support for your VPC and resources. Your VPC can operate in dual-stack mode — your resources can communicate over IPv4, or IPv6, or both. IPv4 and IPv6 communication are independent of each other. You cannot disable IPv4 support for your VPC and subnets since this is the default IP addressing system for Amazon VPC and Amazon EC2.</p><p>By default, a new EC2 instance uses an IPv4 addressing protocol. To fix the problem in the scenario, you need to create a new IPv4 subnet and deploy the EC2 instance in the new subnet.</p><p>Hence, the correct answer is:<strong> Set up a new IPv4 subnet with a larger CIDR range. Associate the new subnet with the VPC and then launch the instance.</strong></p><p>The option that says:<strong> Set up a new IPv6-only subnet with a large CIDR range. Associate the new subnet with the VPC then launch the instance </strong>is incorrect. While it is possible to create an IPv6-only subnet, this feature is only supported for nitro EC2 instance type. Additionally, the scenario does not specify the instance type. Therefore, this option is not applicable for non-Nitro instances, despite the VPC being IPv6-enabled.</p><p>The option that says:<strong> Ensure that the VPC has IPv6 CIDRs only. Remove any IPv4 CIDRs associated with the VPC </strong>is incorrect because you can't have a VPC with IPv6 CIDRs only. The default IP addressing system in VPC is IPv4. You can only change your VPC to dual-stack mode where your resources can communicate over IPv4, or IPv6, or both, but not exclusively with IPv6 only.</p><p>The option that says: <strong>Disable the IPv4 support in the VPC and use the available IPv6 addresses </strong>is incorrect because you cannot disable the IPv4 support for your VPC and subnets since this is the default IP addressing system.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-migrate-ipv6.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-migrate-ipv6.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html</a></p><p><a href=\"https://aws.amazon.com/vpc/faqs/\">https://aws.amazon.com/vpc/faqs/</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a new IPv4 subnet with a larger CIDR range. Associate the new subnet with the VPC and then launch the instance.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Ensure that the VPC has IPv6 CIDRs only. Remove any IPv4 CIDRs associated with the VPC.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up a new IPv6-only subnet with a large CIDR range. Associate the new subnet with the VPC then launch the instance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Disable the IPv4 support in the VPC and use the available IPv6 addresses.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 62,
    "question": "<p>A company receives semi-structured and structured data from different sources, which are eventually stored in their Amazon S3 data lake. The Solutions Architect plans to use big data processing frameworks to analyze these data and access it using various business intelligence tools and standard SQL queries.</p><p>Which of the following provides the MOST high-performing solution that fulfills this requirement?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon EMR</strong> is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data. By using these frameworks and related open-source projects, such as Apache Hive and Apache Pig, you can process data for analytics purposes and business intelligence workloads. Additionally, you can use Amazon EMR to transform and move large amounts of data into and out of other AWS data stores and databases.</p><p>Amazon Redshift is the most widely used cloud data warehouse. It makes it fast, simple, and cost-effective to analyze all your data using standard SQL and your existing Business Intelligence (BI) tools. It allows you to run complex analytic queries against terabytes to petabytes of structured and semi-structured data, using sophisticated query optimization, columnar storage on high-performance storage, and massively parallel query execution.</p><p><img src=\"https://media.tutorialsdojo.com/AWS-EMR-Redshift-Services.png\"></p><p>The key phrases in the scenario are \"big data processing frameworks\" and \"various business intelligence tools and standard SQL queries\" to analyze the data. To leverage big data processing frameworks, you need to use Amazon EMR. The cluster will perform data transformations (ETL) and load the processed data into Amazon Redshift for analytic and business intelligence applications.</p><p>Hence, the correct answer is: <strong>Create an Amazon EMR cluster and store the processed data in Amazon Redshift.</strong></p><p>The option that says:<strong> Use AWS Glue and store the processed data in Amazon S3 </strong>is incorrect. While Glue can transform and load data into S3, it lacks the speed and SQL querying power of Amazon Redshift, making it less ideal for large-scale analytical queries and BI integrations. Redshift, in contrast, is purpose-built for fast SQL analytics and complex queries, especially for data lakes.</p><p>The option that says:<strong> Use Amazon Managed Service for Apache Flink Studio and store the processed data in Amazon DynamoDB</strong> is incorrect because Amazon Managed Service for Apache Flink Studio is more suitable for processing streaming data. Additionally, Amazon DynamoDB doesn't fully support the use of standard SQL and Business Intelligence (BI) tools, unlike Amazon Redshift. It also doesn't allow you to run complex analytic queries against terabytes to petabytes of structured and semi-structured data.</p><p>The option that says:<strong> Create an Amazon EC2 instance and store the processed data in Amazon EBS </strong>is incorrect because a single EBS-backed EC2 instance is quite limited in its computing capability. Moreover, it also entails an administrative overhead since you have to manually install and maintain the big data frameworks for the EC2 instance yourself. The most suitable solution to leverage big data frameworks is to use EMR clusters.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html</a></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/loading-data-from-emr.html\">https://docs.aws.amazon.com/redshift/latest/dg/loading-data-from-emr.html</a></p><p><br></p><p><strong>Check out this Amazon EMR Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-emr/?src=udemy\">https://tutorialsdojo.com/amazon-emr/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Glue and store the processed data in Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon EMR cluster and store the processed data in Amazon Redshift.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Managed Service for Apache Flink Studio and store the processed data in Amazon DynamoDB.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon EC2 instance and store the processed data in Amazon EBS.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 63,
    "question": "<p>A company wants to organize the way it tracks its spending on AWS resources. A report that summarizes the total billing accrued by each department must be generated at the end of the month.</p><p>Which solution will meet the requirements?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p>A tag is a label that you or AWS assigns to an AWS resource. Each tag consists of a key and a value. For each resource, each tag key must be unique, and each tag key can have only one value. You can use tags to organize your resources and cost allocation tags to track your AWS costs on a detailed level.</p><p><img src=\"https://media.tutorialsdojo.com/cost-allocation-tags-activation.jpg\"></p><p>After you or AWS applies tags to your AWS resources (such as Amazon EC2 instances or Amazon S3 buckets) and you activate the tags in the Billing and Cost Management console, AWS generates a cost allocation report as a comma-separated value (CSV file) with your usage and costs grouped by your active tags. You can apply tags that represent business categories (such as cost centers, application names, or owners) to organize your costs across multiple services.</p><p>Hence, the correct answer is: <strong>Tag resources with the department name and enable cost allocation tags.</strong></p><p>The option that says: <strong>Tag resources with the department name and configure a budget action in AWS Budget </strong>is incorrect. AWS Budgets only allows you to be alerted and run custom actions if your budget thresholds are exceeded.</p><p>The option that says: <strong>Use AWS Cost Explorer to view spending and filter usage data by </strong><code><strong>Resource</strong></code> is incorrect. The <code>Resource </code>filter just lets you track costs on EC2 instances. This is quite limited compared with using the Cost Allocation Tags method.</p><p>The option that says: <strong>Create a Cost and Usage report for AWS services that each department is using </strong>is incorrect. The report must contain a breakdown of costs incurred by each department based on tags and not based on AWS services, which is what the Cost and Usage Report (CUR) contains.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws-cloud-financial-management/cost-allocation-blog-series-2-aws-generated-vs-user-defined-cost-allocation-tag/\">https://aws.amazon.com/blogs/aws-cloud-financial-management/cost-allocation-blog-series-2-aws-generated-vs-user-defined-cost-allocation-tag/</a></p><p><br></p><p><strong>Check out this AWS Billing and Cost Management Cheat sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy\">https://tutorialsdojo.com/aws-billing-and-cost-management/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Tag resources with the department name and enable cost allocation tags.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Tag resources with the department name and configure a budget action in AWS Budget.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Cost Explorer to view spending and filter usage data by <code>Resource</code>.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a Cost and Usage report for AWS services that each department is using.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 64,
    "question": "<p>A business has a network of surveillance cameras installed within the premises of its data center. Management wants to leverage Artificial Intelligence to monitor and detect unauthorized personnel entering restricted areas. Should an unauthorized person be detected, the security team must be alerted via SMS.</p><p>Which solution satisfies the requirement?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon Kinesis Video Streams</strong> makes it easy to securely stream video from connected devices to AWS for analytics, machine learning (ML), playback, and other processing. Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to ingest streaming video data from millions of devices.</p><p><strong>Amazon Rekognition Video</strong> can detect objects, scenes, faces, celebrities, text, and inappropriate content in videos. You can also search for faces appearing in a video using your own repository or collection of face images.</p><p><img src=\"https://media.tutorialsdojo.com/kinesis-video-stream-amazon-rekognition-sns.jpg\"></p><p>The image above illustrates how we can combine these two services to create an intruder alert system using face recognition.</p><p>Hence, the correct answer is: <strong>Use Amazon Kinesis Video to stream live feeds from the cameras. Use Amazon Rekognition to detect unauthorized personnel. Set the phone numbers of the security as subscribers to an SNS topic.</strong></p><p>The option that says: <strong>Configure Amazon Elastic Transcoder to stream live feeds from the cameras. Use Amazon Kendra to detect unauthorized personnel. Set the phone numbers of the security as subscribers to an SNS topic </strong>is incorrect. Amazon Elastic Transcoder just allows you to convert media files from one format to another. Also, Amazon Kendra can't be used for face detection as it's just an intelligent search service.</p><p>The option that says: <strong>Replace the existing cameras with AWS IoT. Upload a face detection model to the AWS IoT devices and send them over to AWS Control Tower for checking and notification</strong> is incorrect. AWS IoT simply provides the cloud services that connect your IoT devices to other devices and AWS cloud services. This is basically a device software that can help you integrate your IoT devices into AWS IoT-based solutions and is not used as a physical camera. AWS Control Tower is primarily used to set up and govern a secure multi-account AWS environment and not for receiving video feeds.</p><p>The option that says: <strong>Set up Amazon Managed Service for Prometheus to stream live feeds from the cameras. Use Amazon Fraud Detector to detect unauthorized personnel. Set the phone numbers of the security as subscribers to an SNS topic </strong>is incorrect. The Amazon Managed Service for Prometheus is a Prometheus-compatible monitoring and alerting service, which is not used to stream live video feeds. This service makes it easy for you to monitor containerized applications and infrastructure at scale but not stream live feeds. Amazon Fraud Detector is a fully managed service that identifies potentially fraudulent online activities such as online payment fraud and fake account creation. Take note that the Amazon Fraud Detector service is not capable of detecting unauthorized personnel through live streaming feeds alone.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/what-is-kinesis-video.html\">https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/what-is-kinesis-video.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/launch-welcoming-amazon-rekognition-video-service/\">https://aws.amazon.com/blogs/aws/launch-welcoming-amazon-rekognition-video-service/</a></p><p><br></p><p><strong>Check out these Amazon Kinesis Video Streams and Amazon Rekognition Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-rekognition/?src=udemy\">https://tutorialsdojo.com/amazon-rekognition/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Kinesis Video to stream live feeds from the cameras. Use Amazon Rekognition to detect authorized personnel. Set the phone numbers of the security as subscribers to an SNS topic.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure Amazon Elastic Transcoder to stream live feeds from the cameras. Use Amazon Kendra to detect authorized personnel. Set the phone numbers of the security as subscribers to an SNS topic.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Replace the existing cameras with AWS IoT. Upload a face detection model to the AWS IoT devices and send them over to AWS Control Tower for checking and notification</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up Amazon Managed Service for Prometheus to stream live feeds from the cameras. Use Amazon Fraud Detector to detect unauthorized personnel. Set the phone numbers of the security as subscribers to an SNS topic.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 65,
    "question": "<p>A company hosts its web application on a set of Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The application has an embedded NoSQL database. As the application receives more traffic, the application becomes overloaded mainly due to database requests. The management wants to ensure that the database is eventually consistent and highly available.</p><p>Which of the following options can meet the company requirements with the least operational overhead?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>AWS Database Migration Service (AWS DMS)</strong> is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. You can use AWS DMS to migrate your data into the AWS Cloud or between combinations of cloud and on-premises setups.</p><p>With AWS DMS, you can perform one-time migrations, and you can replicate ongoing changes to keep sources and targets in sync. If you want to migrate to a different database engine, you can use the AWS Schema Conversion Tool (AWS SCT) to translate your database schema to the new platform. You then use AWS DMS to migrate the data. Because AWS DMS is a part of the AWS Cloud, you get the cost efficiency, speed to market, security, and flexibility that AWS services offer.</p><p>You can use AWS DMS to migrate data to an Amazon DynamoDB table. Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. AWS DMS supports using a relational database or MongoDB as a source.</p><p><a href=\"https://media.tutorialsdojo.com/saa_aws_dms_replication.png\"><img src=\"https://media.tutorialsdojo.com/saa_aws_dms_replication.png\"></a></p><p>Therefore, the correct answer is: <strong>Configure the Auto Scaling group to spread the Amazon EC2 instances across three Availability Zones. Use the AWS Database Migration Service (DMS) with a replication server and an ongoing replication task to migrate the embedded NoSQL database to Amazon DynamoDB. </strong>Using an Auto Scaling group of EC2 instances and migrating the embedded database to Amazon DynamoDB will ensure that both the application and database are highly available with low operational overhead.</p><p>The option that says: <strong>Change the ALB with a Network Load Balancer (NLB) to handle more traffic and integrate AWS Global Accelerator to ensure high availability. Configure replication of the NoSQL database on the set of Amazon EC2 instances to spread the database load </strong>is incorrect. It is not recommended to run a production system with an embedded database on EC2 instances. A better option is to migrate the database to a managed AWS service such as Amazon DynamoDB, so you won't have to manually maintain, patch, provision and scale your database yourself. In addition, using an AWS Global Accelerator is not warranted since the architecture is only hosted in a single AWS region and not in multiple regions.</p><p>The option that says: <strong>Change the ALB with a Network Load Balancer (NLB) to handle more traffic. Use the AWS Migration Service (DMS) to migrate the embedded NoSQL database to Amazon DynamoDB</strong> is incorrect. The scenario did not require handling millions of requests per second or very low latency to justify the use of NLB. The ALB should be able to able to scale and handle scaling traffic.</p><p>The option that says: <strong>Configure the Auto Scaling group to spread the Amazon EC2 instances across three Availability Zones. Configure replication of the NoSQL database on the set of Amazon EC2 instances to spread the database load</strong> is incorrect. This may be possible, but it entails an operational overhead of manually configuring the embedded database to replicate and scale with the EC2 instances. It would be better to migrate the database to a managed AWS database service such as Amazon DynamoDB.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.DynamoDB.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.DynamoDB.html</a></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html\">https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><br></p><p><strong>Check out these Amazon DynamoDB and AWS DMS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><a href=\"https://tutorialsdojo.com/aws-database-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-database-migration-service/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Change the ALB with a Network Load Balancer (NLB) to handle more traffic and integrate AWS Global Accelerator to ensure high availability. Configure replication of the NoSQL database on the set of Amazon EC2 instances to spread the database load.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure the Auto Scaling group to spread the Amazon EC2 instances across three Availability Zones. Use the AWS Database Migration Service (DMS) with a replication server and an ongoing replication task to migrate the embedded NoSQL database to Amazon DynamoDB</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Change the ALB with a Network Load Balancer (NLB) to handle more traffic. Use the AWS Migration Service (DMS) to migrate the embedded NoSQL database to Amazon DynamoDB.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the Auto Scaling group to spread the Amazon EC2 instances across three Availability Zones. Configure replication of the NoSQL database on the set of Amazon EC2 instances to spread the database load.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  }
]