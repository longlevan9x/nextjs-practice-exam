[
  {
    "id": 301,
    "question": "A company is building an application that will run on an AWS Lambda function. Hundreds of customers will use the application. The company wants to give each customer a quota of requests for a specific time period. The quotas must match customer usage patterns. Some customers must receive a higher quota for a shorter time period.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon API Gateway REST API with a proxy integration to invoke the Lambda function. For each customer, configure an API Gateway usage plan that includes an appropriate request quota. Create an API key from the usage plan for each user that the customer needs.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an Amazon API Gateway HTTP API with a proxy integration to invoke the Lambda function. For each customer configure an API Gateway usage plan that includes an appropriate request quota Configure route-level throttling for each usage plan. Create an API Key from the usage plan for each user that the customer needs.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a Lambda function alias for each customer. Include a concurrency limit with an appropriate request quota. Create a Lambda function URL for each function alias. Share the Lambda function URL for each alias with the relevant customer.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Application Load Balancer (ALB) in a VPC. Configure the Lambda function as a target for the ALB. Configure an AWS WAF web ACL for the ALB. For each customer configure a rale-based rule that includes an appropriate request quota.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 302,
    "question": "A company is planning to migrate its on-premises VMware cluster of 120 VMs to AWS. The VMs have many different operating systems and many custom software packages installed. The company also has an on-premises NFS server that is 10 TB in size. The company has set up a 10 Gbps AWS Direct Connect connection to AWS for the migration.<br><br>Which solution will complete the migration to AWS in the LEAST amount of time?",
    "answers": [
      {
        "id": 1,
        "answer": "Export the on-premises VMs and copy them to an Amazon S3 bucket. Use VM Import/Export to create AMIs from the VM images that are stored in Amazon S3. Order an AWS Snowball Edge device. Copy the NFS server data to the device. Restore the NFS server data to an Amazon EC2 instance that has NFS configured.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure AWS Application Migration Service with a connection to the VMware cluster. Create a replication job for the VMS. Create an Amazon Elastic File System (Amazon EFS) file system. Configure AWS DataSync to copy the NFS server data to the EFS file system over the Direct Connect connection.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Recreate the VMs on AWS as Amazon EC2 instances. Install all the required software packages. Create an Amazon FSx for Lustre file system. Configure AWS DataSync to copy the NFS server data to the FSx for Lustre file system over the Direct Connect connection.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Order two AWS Snowball Edge devices. Copy the VMs and the NFS server data to the devices. Run VM Import/Export after the data from the devices is loaded to an Amazon S3 bucket. Create an Amazon Elastic File System (Amazon EFS) file system. Copy the NFS server data from Amazon S3 to the EFS file system.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 303,
    "question": "An online survey company runs its application in the AWS Cloud. The application is distributed and consists of microservices that run in an automatically scaled Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster is a target for an Application Load Balancer (ALB). The ALB is a custom origin for an Amazon CloudFront distribution.<br><br>The company has a survey that contains sensitive data. The sensitive data must be encrypted when it moves through the application. The application's data-handling microservice is the only microservice that should be able to decrypt the data<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a field-level encryption profile and a configuration. Associate the KMS key and the configuration with the CloudFront cache behavior.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an RSA key pair that is dedicated to the data-handing microservice. Upload the public key to the CloudFront distribution. Create a field-level encryption profile and a configuration. Add the configuration to the CloudFront cache behavior.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the KMS key to encrypt the sensitive data.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an RSA key pair that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the private key of the RSA key pair to encrypt the sensitive data.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 304,
    "question": "A solutions architect is determining the DNS strategy for an existing VPC. The VPC is provisioned to use the 10.24.34.0/24 CIDR block. The VPC also uses Amazon Route 53 Resolver for DNS. New requirements mandate that DNS queries must use private hosted zones. Additionally instances that have public IP addresses must receive corresponding public hostnames<br><br>Which solution will meet these requirements to ensure that the domain names are correctly resolved within the VPC?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a private hosted zone. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=10.24.34.2.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a private hosted zone Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Create a new VPC DHCP options set, and configure domain-name-servers=AmazonProvidedDNS. Associate the new DHCP options set with the VPC.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Deactivate the enableDnsSupport attribute for the VPActivate the enableDnsHostnames attribute for the VPCreate a new VPC DHCP options set, and configure doman-name-servers=10.24.34.2. Associate the new DHCP options set with the VPC.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a private hosted zone. Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute for the VPC. Deactivate the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=AmazonProvidedDNS.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 305,
    "question": "A data analytics company has an Amazon Redshift cluster that consists of several reserved nodes. The cluster is experiencing unexpected bursts of usage because a team of employees is compiling a deep audit analysis report. The queries to generate the report are complex read queries and are CPU intensive.<br><br>Business requirements dictate that the cluster must be able to service read and write queries at all times. A solutions architect must devise a solution that accommodates the bursts of usage.<br><br>Which solution meets these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Provision an Amazon EMR cluster Offload the complex data processing tasks.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using a classic resize operation when the cluster’s CPU metrics in Amazon CloudWatch reach 80%.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using an elastic resize operation when the cluster’s CPU metrics in Amazon CloudWatch reach 80%.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Turn on the Concurrency Scaling feature for the Amazon Redshift cluster.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 306,
    "question": "A research center is migrating to the AWS Cloud and has moved its on-premises 1 PB object storage to an Amazon S3 bucket. One hundred scientists are using this object storage to store their work-related documents. Each scientist has a personal folder on the object store. All the scientists are members of a single IAM user group.<br><br>The research center's compliance officer is worried that scientists will be able to access each other's work. The research center has a strict obligation to report on which scientist accesses which documents. The team that is responsible for these reports has little AWS experience and wants a ready-to-use solution that minimizes operational overhead.<br><br>Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Create an identity policy that grants the user read and write access. Add a condition that specifies that the S3 paths must be prefixed with $(aws:username). Apply the policy on the scientists’ IAM user group.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket. Store the trail output in another S3 bucket. Use Amazon Athena to query the logs and generate reports.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Enable S3 server access logging. Configure another S3 bucket as the target for log delivery. Use Amazon Athena to query the logs and generate reports.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an S3 bucket policy that grants read and write access to users in the scientists’ IAM user group.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket and write the events to Amazon CloudWatch. Use the Amazon Athena CloudWatch connector to query the logs and generate reports.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 307,
    "question": "A company uses AWS Organizations to manage a multi-account structure. The company has hundreds of AWS accounts and expects the number of accounts to increase. The company is building a new application that uses Docker images. The company will push the Docker images to Amazon Elastic Container Registry (Amazon ECR). Only accounts that are within the company’s organization should have access to the images.<br><br>The company has a CI/CD process that runs frequently. The company wants to retain all the tagged images. However, the company wants to retain only the five most recent untagged images.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a private repository in Amazon ECR. Create a permissions policy for the repository that allows only required ECR operations. Include a condition to allow the ECR operations if the value of the aws:PrincipalOrglD condition key is equal to the ID of the company’s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a public repository in Amazon ECR. Create an IAM role in the ECR account. Set permissions so that any account can assume the role if the value of the aws:PrincipalOrglD condition key is equal to the ID of the company’s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a private repository in Amazon ECR. Create a permissions policy for the repository that includes only required ECR operations. Include a condition to allow the ECR operations for all account IDs in the organization Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images over the count of five.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a public repository in Amazon ECR. Configure Amazon ECR to use an interface VPC endpoint with an endpoint policy that includes the required permissions for images that the company needs to pull. Include a condition to allow the ECR operations for all account IDs in the company’s organization. Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images over the count of five.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 308,
    "question": "A solutions architect is reviewing a company's process for taking snapshots of Amazon RDS DB instances. The company takes automatic snapshots every day and retains the snapshots for 7 days.<br><br>The solutions architect needs to recommend a solution that takes snapshots every 6 hours and retains the snapshots for 30 days. The company uses AWS Organizations to manage all of its AWS accounts. The company needs a consolidated view of the health of the RDS snapshots.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Turn on the cross-account management feature in AWS Backup. Create a backup plan that specifies the frequency and retention requirements. Add a tag to the DB instances. Apply the backup plan by using tags. Use AWS Backup to monitor the status of the backups.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Turn on the cross-account management feature in Amazon RDS. Create a snapshot global policy that specifies the frequency and retention requirements. Use the RDS console in the management account to monitor the status of the backups.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Turn on the cross-account management feature in AWS CloudFormation. From the management account, deploy a CloudFormation stack set that contains a backup plan from AWS Backup that specifies the frequency and retention requirements. Create an AWS Lambda function in the management account to monitor the status of the backups. Create an Amazon EventBridge rule in each account to run the Lambda function on a schedule.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure AWS Backup in each account. Create an Amazon Data Lifecycle Manager lifecycle policy that specifies the frequency and retention requirements. Specify the DB instances as the target resource Use the Amazon Data Lifecycle Manager console in each member account to monitor the status of the backups.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 309,
    "question": "A company is using AWS Organizations with a multi-account architecture. The company's current security configuration for the account architecture includes SCPs, resource-based policies, identity-based policies, trust policies, and session policies.<br><br>A solutions architect needs to allow an IAM user in Account A to assume a role in Account B.<br><br>Which combination of steps must the solutions architect take to meet this requirement? (Choose three.)",
    "answers": [
      {
        "id": 1,
        "answer": "Configure the SCP for Account A to allow the action.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Configure the resource-based policies to allow the action.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure the identity-based policy on the user in Account A to allow the action.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure the identity-based policy on the user in Account B to allow the action.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Configure the trust policy on the target role in Account B to allow the action.",
        "correct": true
      },
      {
        "id": 6,
        "answer": "Configure the session policy to allow the action and to be passed programmatically by the GetSessionToken API operation.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3,
      5
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 310,
    "question": "A company wants to use Amazon S3 to back up its on-premises file storage solution. The company’s on-premises file storage solution supports NFS, and the company wants its new solution to support NFS. The company wants to archive the backup files after 5 days. If the company needs archived files for disaster recovery, the company is willing to wait a few days for the retrieval of those files.<br><br>Which solution meets these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy an AWS Storage Gateway volume gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the volume gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy an AWS Storage Gateway tape gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the tape gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 311,
    "question": "A company runs its application on Amazon EC2 instances and AWS Lambda functions. The EC2 instances experience a continuous and stable load. The Lambda functions experience a varied and unpredictable load. The application includes a caching layer that uses an Amazon MemoryDB for Redis cluster.<br><br>A solutions architect must recommend a solution to minimize the company's overall monthly costs.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Purchase an EC2 instance Savings Plan to cover the EC2 instances. Purchase a Compute Savings Plan for Lambda to cover the minimum expected consumption of the Lambda functions. Purchase reserved nodes to cover the MemoryDB cache nodes.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Purchase a Compute Savings Plan to cover the EC2 instances. Purchase Lambda reserved concurrency to cover the expected Lambda usage. Purchase reserved nodes to cover the MemoryDB cache nodes.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Purchase a Compute Savings Plan to cover the entire expected cost of the EC2 instances, Lambda functions, and MemoryDB cache nodes.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Purchase a Compute Savings Plan to cover the EC2 instances and the MemoryDB cache nodes. Purchase Lambda reserved concurrency to cover the expected Lambda usage.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 312,
    "question": "A company is launching a new online game on Amazon EC2 instances. The game must be available globally. The company plans to run the game in three AWS Regions us-east-1, eu-west-1, and ap-southeast-1. The game's leaderboards, player inventory and event status must be available across Regions.<br><br>A solutions architect must design a solution that will give any Region the ability to scale to handle the load of all Regions. Additionally, users must automatically connect to the Region that provides the least latency.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an EC2 Spot Fleet. Attach the Spot Fleet to a Network Load Balancer (NLB) in each Region. Create an AWS Global Accelerator IP address that points to the NLB. Create an Amazon Route 53 latency-based routing entry for the Global Accelerator IP address. Save the game metadata to an Amazon RDS for MySQL DB instance in each Region. Set up a read replica in the other Regions.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Auto Scaling group for the EC2 instances Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses geoproximity routing and points to the NLB in that Region. Save the game metadata to MySQL databases on EC2 instances in each Region. Set up replication between the database EC2 instances in each Region.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Auto Scaling group for the EC2 instances. Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses latency-based routing and points to the NLB in that Region. Save the game metadata to an Amazon DynamoDB global table.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use EC2 Global View. Deploy the EC2 instances to each Region. Attach the instances to a Network Load Balancer (NLB). Deploy a DNS server on an EC2 instance in each Region. Set up custom logic on each DNS server to redirect the user to the Region that provides the lowest latency. Save the game metadata to an Amazon Aurora global database.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 313,
    "question": "A company is deploying a third-party firewall appliance solution from AWS Marketplace to monitor and protect traffic that leaves the company's AWS environments. The company wants to deploy this appliance into a shared services VPC and route all outbound internet-bound traffic through the appliances.<br><br>A solutions architect needs to recommend a deployment method that prioritizes reliability and minimizes failover time between firewall appliances within a single AWS Region. The company has set up routing from the shared services VPC to other VPCs.<br><br>Which steps should the solutions architect recommend to meet these requirements? (Choose three.)",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy two firewall appliances into the shared services VPC, each in a separate Availability Zone.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a new Network Load Balancer in the shared services VPC. Create a new target group, and attach it to the new Network Load Balancer. Add each of the firewall appliance instances to the target group.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a new Gateway Load Balancer in the shared services VPCreate a new target group, and attach it to the new Gateway Load Balancer Add each of the firewall appliance instances to the target group.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create a VPC interface endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Deploy two firewall appliances into the shared services VPC, each in the same Availability Zone.",
        "correct": false
      },
      {
        "id": 6,
        "answer": "Create a VPC Gateway Load Balancer endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs.",
        "correct": true
      }
    ],
    "corrects": [
      1,
      3,
      6
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 314,
    "question": "A solutions architect needs to migrate an on-premises legacy application to AWS. The application runs on two servers behind a load balancer. The application requires a license file that is associated with the MAC address of the server's network adapter It takes the software vendor 12 hours to send new license files. The application also uses configuration files with a static IP address to access a database server, host names are not supported.<br><br>Given these requirements, which combination of steps should be taken to implement highly available architecture for the application servers in AWS? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Create a pool of ENIs. Request license files from the vendor for the pool, and store the license files in Amazon S3. Create a bootstrap automation script to download a license file and attach the corresponding ENI to an Amazon EC2 instance.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a pool of ENIs. Request license files from the vendor for the pool, store the license files on an Amazon EC2 instance. Create an AMI from the instance and use this AMI for all future EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a bootstrap automation script to request a new license file from the vendor .When the response is received, apply the license file to an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Edit the bootstrap automation script to read the database server IP address from the AWS Systems Manager Parameter Store, and inject the value into the local configuration files.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Edit an Amazon EC2 instance to include the database server IP address in the configuration files and re-create the AMI to use for all future EC2 stances.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      4
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 315,
    "question": "A company runs its sales reporting application in an AWS Region in the United States. The application uses an Amazon API Gateway Regional API and AWS Lambda functions to generate on-demand reports from data in an Amazon RDS for MySQL database. The frontend of the application is hosted on Amazon S3 and is accessed by users through an Amazon CloudFront distribution. The company is using Amazon Route 53 as the DNS service for the domain. Route 53 is configured with a simple routing policy to route traffic to the API Gateway API.<br><br>In the next 6 months, the company plans to expand operations to Europe. More than 90% of the database traffic is read-only traffic. The company has already deployed an API Gateway API and Lambda functions in the new Region.<br><br>A solutions architect must design a solution that minimizes latency for users who download reports.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Use an AWS Database Migration Service (AWS DMS) task with full load to replicate the primary database in the original Region to the database in the new Region. Change the Route 53 record to latency-based routing to connect to the API Gateway API.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use an AWS Database Migration Service (AWS DMS) task with full load plus change data capture (CDC) to replicate the primary database in the original Region to the database in the new Region. Change the Route 53 record to geolocation routing to connect to the API Gateway API.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure a cross-Region read replica for the RDS database in the new Region Change the Route 53 record to latency-based routing to connect to the API Gateway API.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure a cross-Region read replica for the RDS database in the new Region. Change the Route 53 record to geolocation routing to connect to the API Gateway API.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 316,
    "question": "A software company needs to create short-lived test environments to test pull requests as part of its development process. Each test environment consists of a single Amazon EC2 instance that is in an Auto Scaling group.<br><br>The test environments must be able to communicate with a central server to report test results. The central server is located in an on-premises data center. A solutions architect must implement a solution so that the company can create and delete test environments without any manual intervention. The company has created a transit gateway with a VPN attachment to the on-premises network.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS CloudFormation template that contains a transit gateway attachment and related routing configurations. Create a CloudFormation stack set that includes this template. Use CloudFormation StackSets to deploy a new stack for each VPC in the account. Deploy a new VPC for each test environment.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a single VPC for the test environments. Include a transit gateway attachment and related routing configurations. Use AWS CloudFormation to deploy all test environments into the VPC.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a new OU in AWS Organizations for testing. Create an AWS CioudFormation template that contains a VPC, necessary networking resources, a transit gateway attachment, and related routing configurations. Create a CloudFormation stack set that includes this template. Use CloudFormation StackSets for deployments into each account under the testing OU. Create a new account for each test environment.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Convert the test environment EC2 instances into Docker images. Use AWS CloudFormation to configure an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in a new VPC, create a transit gateway attachment, and create related routing configurations. Use Kubernetes to manage the deployment and lifecycle of the test environments.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 317,
    "question": "A company is deploying a new API to AWS. The API uses Amazon API Gateway with a Regional API endpoint and an AWS Lambda function for hosting. The API retrieves data from an external vendor API, stores data in an Amazon DynamoDB global table, and retrieves data from the DynamoDB global table The API key for the vendor's API is stored in AWS Secrets Manager and is encrypted with a customer managed key in AWS Key Management Service (AWS KMS). The company has deployed its own API into a single AWS Region.<br><br>A solutions architect needs to change the API components of the company’s API to ensure that the components can run across multiple Regions in an active-active configuration.<br><br>Which combination of changes will meet this requirement with the LEAST operational overhead? (Choose three.)",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy the API to multiple Regions. Configure Amazon Route 53 with custom domain names that route traffic to each Regional API endpoint. Implement a Route 53 multivalue answer routing policy.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a new KMS multi-Region customer managed key. Create a new KMS customer managed replica key in each in-scope Region.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Replicate the existing Secrets Manager secret to other Regions. For each in-scope Region's replicated secret, select the appropriate KMS key.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create a new AWS managed KMS key in each in-scope Region. Convert an existing key to a multiRegion key. Use the multi-Region key in other Regions.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Create a new Secrets Manager secret in each in-scope Region. Copy the secret value from the existing Region to the new secret in each in-scope Region.",
        "correct": false
      },
      {
        "id": 6,
        "answer": "Modify the deployment process for the Lambda function to repeat the deployment across in-scope Regions. Turn on the multi-Region option for the existing API. Select the Lambda function that is deployed in each Region as the backend for the multi-Region API.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2,
      3
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 318,
    "question": "An online retail company hosts its stateful web-based application and MySQL database in an on-premises data center on a single server. The company wants to increase its customer base by conducting more marketing campaigns and promotions. In preparation, the company wants to migrate its application and database to AWS to increase the reliability of its architecture.<br><br>Which solution should provide the HIGHEST level of reliability?",
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the database to an Amazon RDS MySQL Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon Neptune",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Migrate the database to Amazon Aurora MySQL. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in an Amazon ElastiCache for Redis replication group.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Migrate the database to Amazon DocumentDB (with MongoDB compatibility). Deploy the application in an Auto Scaling group on Amazon EC2 instances behind a Network Load Balancer Store sessions in Amazon Kinesis Data Firehose.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Migrate the database to an Amazon RDS MariaDB Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon ElastiCache for Memcached.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 319,
    "question": "A company’s solutions architect needs to provide secure Remote Desktop connectivity to users for Amazon EC2 Windows instances that are hosted in a VPC. The solution must integrate centralized user management with the company's on-premises Active Directory. Connectivity to the VPC is through the internet. The company has hardware that can be used to establish an AWS Site-to-Site VPN connection.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy a managed Active Directory by using AWS Directory Service for Microsoft Active Directory. Establish a trust with the on-premises Active Directory. Deploy an EC2 instance as a bastion host in the VPC. Ensure that the EC2 instance is joined to the domain. Use the bastion host to access the target instances through RDP.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure AWS IAM Identity Center (AWS Single Sign-On) to integrate with the on-premises Active Directory by using the AWS Directory Service for Microsoft Active Directory AD Connector. Configure permission sets against user groups for access to AWS Systems Manager. Use Systems Manager Fleet Manager to access the target instances through RDP.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Implement a VPN between the on-premises environment and the target VPEnsure that the target instances are joined to the on-premises Active Directory domain over the VPN connection. Configure RDP access through the VPN. Connect from the company’s network to the target instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy a managed Active Directory by using AWS Directory Service for Microsoft Active Directory. Establish a trust with the on-premises Active Directory. Deploy a Remote Desktop Gateway on AWS by using an AWS Quick Start. Ensure that the Remote Desktop Gateway is joined to the domain. Use the Remote Desktop Gateway to access the target instances through RDP.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 320,
    "question": "A company's compliance audit reveals that some Amazon Elastic Block Store (Amazon EBS) volumes that were created in an AWS account were not encrypted. A solutions architect must implement a solution to encrypt all new EBS volumes at rest.<br><br>Which solution will meet this requirement with the LEAST effort?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon EventBridge rule to detect the creation of unencrypted EBS volumes. Invoke an AWS Lambda function to delete noncompliant volumes.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS Audit Manager with data encryption.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an AWS Config rule to detect the creation of a new EBS volume. Encrypt the volume by using AWS Systems Manager Automation.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Turn on EBS encryption by default in all AWS Regions.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 321,
    "question": "A research company is running daily simulations in the AWS Cloud to meet high demand. The simulations run on several hundred Amazon EC2 instances that are based on Amazon Linux 2. Occasionally, a simulation gets stuck and requires a cloud operations engineer to solve the problem by connecting to an EC2 instance through SSH.<br><br>Company policy states that no EC2 instance can use the same SSH key and that all connections must be logged in AWS CloudTrail.<br><br>How can a solutions architect meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Launch new EC2 instances, and generate an individual SSH key for each instance. Store the SSH key in AWS Secrets Manager. Create a new IAM policy, and attach it to the engineers’ IAM role with an Allow statement for the GetSecretValue action. Instruct the engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an AWS Systems Manager document to run commands on EC2 instances to set a new unique SSH key. Create a new IAM policy, and attach it to the engineers’ IAM role with an Allow statement to run Systems Manager documents. Instruct the engineers to run the document to set an SSH key and to connect through any SSH client.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Launch new EC2 instances without setting up any SSH key for the instances. Set up EC2 Instance Connect on each instance. Create a new IAM policy, and attach it to the engineers’ IAM role with an Allow statement for the SendSSHPublicKey action. Instruct the engineers to connect to the instance by using a browser-based SSH client from the EC2 console.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Set up AWS Secrets Manager to store the EC2 SSH key. Create a new AWS Lambda function to create a new SSH key and to call AWS Systems Manager Session Manager to set the SSH key on the EC2 instance. Configure Secrets Manager to use the Lambda function for automatic rotation once daily. Instruct the engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 322,
    "question": "A company is migrating mobile banking applications to run on Amazon EC2 instances in a VPC. Backend service applications run in an on-premises data center. The data center has an AWS Direct Connect connection into AWS. The applications that run in the VPC need to resolve DNS requests to an on-premises Active Directory domain that runs in the data center.<br><br>Which solution will meet these requirements with the LEAST administrative overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Provision a set of EC2 instances across two Availability Zones in the VPC as caching DNS servers to resolve DNS queries from the application servers within the VPC.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Provision an Amazon Route 53 private hosted zone. Configure NS records that point to on-premises DNS servers.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create DNS endpoints by using Amazon Route 53 Resolver. Add conditional forwarding rules to resolve DNS namespaces between the on-premises data center and the VPC.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Provision a new Active Directory domain controller in the VPC with a bidirectional trust between this new domain and the on-premises Active Directory domain.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 323,
    "question": "A company processes environmental data. The company has set up sensors to provide a continuous stream of data from different areas in a city. The data is available in JSON format.<br><br>The company wants to use an AWS solution to send the data to a database that does not require fixed schemas for storage. The data must be sent in real time.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Use Amazon Kinesis Data Firehose to send the data to Amazon Redshift.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Amazon Kinesis Data Streams to send the data to Amazon DynamoDB.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to send the data to Amazon Aurora.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use Amazon Kinesis Data Firehose to send the data to Amazon Keyspaces (for Apache Cassandra).",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 324,
    "question": "A company is migrating a legacy application from an on-premises data center to AWS. The application uses MongoDB as a key-value database. According to the company's technical guidelines, all Amazon EC2 instances must be hosted in a private subnet without an internet connection. In addition, all connectivity between applications and databases must be encrypted. The database must be able to scale based on demand.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the instance endpoint to connect to Amazon DocumentDB.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create new Amazon DynamoDB tables for the application with on-demand capacity. Use a gateway VPC endpoint for DynamoDB to connect to the DynamoDB tables.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create new Amazon DynamoDB tables for the application with on-demand capacity. Use an interface VPC endpoint for DynamoDB to connect to the DynamoDB tables.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the cluster endpoint to connect to Amazon DocumentDB.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 325,
    "question": "A company is running an application on Amazon EC2 instances in the AWS Cloud. The application is using a MongoDB database with a replica set as its data tier. The MongoDB database is installed on systems in the company’s on-premises data center and is accessible through an AWS Direct Connect connection to the data center environment.<br><br>A solutions architect must migrate the on-premises MongoDB database to Amazon DocumentDB (with MongoDB compatibility).<br><br>Which strategy should the solutions architect choose to perform this migration?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a fleet of EC2 instances. Install MongoDB Community Edition on the EC2 instances, and create a database. Configure continuous synchronous replication with the database that is running in the on-premises data center.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an AWS Database Migration Service (AWS DMS) replication instance. Create a source endpoint for the on-premises MongoDB database by using change data capture (CDC). Create a target endpoint for the Amazon DocumentDB database. Create and run a DMS migration task.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a data migration pipeline by using AWS Data Pipeline. Define data nodes for the on-premises MongoDB database and the Amazon DocumentDB database. Create a scheduled task to run the data pipeline.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a source endpoint for the on-premises MongoDB database by using AWS Glue crawlers. Configure continuous asynchronous replication between the MongoDB database and the Amazon DocumentDB database.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 326,
    "question": "A company is rearchitecting its applications to run on AWS. The company’s infrastructure includes multiple Amazon EC2 instances. The company's development team needs different levels of access. The company wants to implement a policy that requires all Windows EC2 instances to be joined to an Active Directory domain on AWS. The company also wants to implement enhanced security processes such as multi-factor authentication (MFA). The company wants to use managed AWS services wherever possible.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS Directory Service for Microsoft Active Directory implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an AWS Directory Service for Microsoft Active Directory implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an AWS Directory Service Simple AD implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an AWS Directory Service Simple AD implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 327,
    "question": "A company wants to migrate its on-premises application to AWS. The database for the application stores structured product data and temporary user session data. The company needs to decouple the product data from the user session data. The company also needs to implement replication in another AWS Region for disaster recovery.<br><br>Which solution will meet these requirements with the HIGHEST performance?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an Amazon RDS DB instance with separate schemas to host the product data and the user session data. Configure a read replica for the DB instance in another Region.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an Amazon RDS DB instance to host the product data. Configure a read replica for the DB instance in another Region. Create a global datastore in Amazon ElastiCache for Memcached to host the user session data.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create two Amazon DynamoDB global tables. Use one global table to host the product data. Use the other global table to host the user session data. Use DynamoDB Accelerator (DAX) for caching.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Amazon RDS DB instance to host the product data. Configure a read replica for the DB instance in another Region. Create an Amazon DynamoDB global table to host the user session data.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 328,
    "question": "A company orchestrates a multi-account structure on AWS by using AWS Control Tower. The company is using AWS Organizations, AWS Config, and AWS Trusted Advisor. The company has a specific OU for development accounts that developers use to experiment on AWS. The company has hundreds of developers, and each developer has an individual development account.<br><br>The company wants to optimize costs in these development accounts. Amazon EC2 instances and Amazon RDS instances in these accounts must be burstable. The company wants to disallow the use of other services that are not relevant.<br><br>What should a solutions architect recommend to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a custom SCP in AWS Organizations to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the SCP to the development OU.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a custom detective control (guardrail) in AWS Control Tower. Configure the control (guardrail) to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the control (guardrail) to the development OU.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a custom preventive control (guardrail) in AWS Control Tower. Configure the control (guardrail) to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the control (guardrail) to the development OU.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an AWS Config rule in the AWS Control Tower account. Configure the AWS Config rule to allow the deployment of only burstable instances and to disallow services that are not relevant. Deploy the AWS Config rule to the development OU by using AWS CloudFormation StackSets.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 329,
    "question": "A financial services company runs a complex, multi-tier application on Amazon EC2 instances and AWS Lambda functions. The application stores temporary data in Amazon S3. The S3 objects are valid for only 45 minutes and are deleted after 24 hours.<br><br>The company deploys each version of the application by launching an AWS CloudFormation stack. The stack creates all resources that are required to run the application. When the company deploys and validates a new application version, the company deletes the CloudFormation stack of the old version.<br><br>The company recently tried to delete the CloudFormation stack of an old application version, but the operation failed. An analysis shows that CloudFormation failed to delete an existing S3 bucket. A solutions architect needs to resolve this issue without making major changes to the application's architecture.<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Implement a Lambda function that deletes all files from a given S3 bucket. Integrate this Lambda function as a custom resource into the CloudFormation stack. Ensure that the custom resource has a DependsOn attribute that points to the S3 bucket's resource.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Modify the CloudFormation template to provision an Amazon Elastic File System (Amazon EFS) file system to store the temporary files there instead of in Amazon S3. Configure the Lambda functions to run in the same VPC as the file system. Mount the file system to the EC2 instances and Lambda functions.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Modify the CloudF ormation stack to create an S3 Lifecycle rule that expires all objects 45 minutes after creation. Add a DependsOn attribute that points to the S3 bucket’s resource.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 330,
    "question": "A company has developed a mobile game. The backend for the game runs on several virtual machines located in an on-premises data center. The business logic is exposed using a REST API with multiple functions. Player session data is stored in central file storage. Backend services use different API keys for throttling and to distinguish between live and test traffic.<br><br>The load on the game backend varies throughout the day. During peak hours, the server capacity is not sufficient. There are also latency issues when fetching player session data. Management has asked a solutions architect to present a cloud architecture that can handle the game’s varying load and provide low-latency data access. The API model should not be changed.<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Implement the REST API using a Network Load Balancer (NLB). Run the business logic on an Amazon EC2 instance behind the NLB. Store player session data in Amazon Aurora Serverless.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Implement the REST API using an Application Load Balancer (ALB). Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Implement the REST API using Amazon API Gateway. Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Implement the REST API using AWS AppSync. Run the business logic in AWS Lambda. Store player session data in Amazon Aurora Serverless.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 331,
    "question": "A company is migrating an application to the AWS Cloud. The application runs in an on-premises data center and writes thousands of images into a mounted NFS file system each night. After the company migrates the application, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system.<br><br>The company has established an AWS Direct Connect connection to AWS. Before the migration cutover, a solutions architect must build a process that will replicate the newly created on-premises images to the EFS file system.<br><br>What is the MOST operationally efficient way to replicate the images?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure a periodic process to run the aws s3 sync command from the on-premises file system to Amazon S3. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy an AWS Storage Gateway file gateway with an NFS mount point. Mount the file gateway file system on the on-premises server. Configure a process to periodically copy the images to the mount point.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an S3 bucket by using a public VIF. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Configure a DataSync scheduled task to send the images to the EFS file system every 24 hours.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 332,
    "question": "A company recently migrated a web application from an on-premises data center to the AWS Cloud. The web application infrastructure consists of an Amazon CloudFront distribution that routes to an Application Load Balancer (ALB), with Amazon Elastic Container Service (Amazon ECS) to process requests. A recent security audit revealed that the web application is accessible by using both CloudFront and ALB endpoints. However, the company requires that the web application must be accessible only by using the CloudFront endpoint.<br><br>Which solution will meet this requirement with the LEAST amount of effort?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a new security group and attach it to the CloudFront distribution. Update the ALB security group ingress to allow access only from the CloudFront security group.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Update ALB security group ingress to allow access only from the com.amazonaws.global.cloudfront.origin-facing CloudFront managed prefix list.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a com.amazonaws.region.elasticloadbalancing VPC interface endpoint for Elastic Load Balancing. Update the ALB scheme from internet-facing to internal.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Extract CloudFront IPs from the AWS provided ip-ranges.json document. Update ALB security group ingress to allow access only from CloudFront IPs.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 333,
    "question": "A company hosts a community forum site using an Application Load Balancer (ALB) and a Docker application hosted in an Amazon ECS cluster. The site data is stored in Amazon RDS for MySQL and the container image is stored in ECR. The company needs to provide their customers with a disaster recovery SLA with an RTO of no more than 24 hours and RPO of no more than 8 hours.<br><br>Which of the following solutions is the MOST cost-effective way to meet the requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Use AWS CloudFormation to deploy identical ALB, EC2, ECS and RDS resources in two regions. Schedule RDS snapshots every 8 hours. Use RDS multi-region replication to update the secondary region's copy of the database. In the event of a failure, restore from the latest snapshot, and use an Amazon Route 53 DNS failover policy to automatically redirect customers to the ALB in the secondary region.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Store the Docker image in ECR in two regions. Schedule RDS snapshots every 8 hours with snapshots copied to the secondary region. In the event of a failure, use AWS CloudFormation to deploy the ALB, EC2, ECS and RDS resources in the secondary region, restore from the latest snapshot, and update the DNS record to point to the ALB in the secondary region.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use AWS CloudFormation to deploy identical ALB, EC2, ECS, and RDS resources in a secondary region. Schedule hourly RDS MySQL backups to Amazon S3 and use cross-region replication to replicate data to a bucket in the secondary region. In the event of a failure, import the latest Docker image to Amazon ECR in the secondary region, deploy to the EC2 instance, restore the latest MySQL backup, and update the DNS record to point to the ALB in the secondary region.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy a pilot light environment in a secondary region with an ALB and a minimal resource EC2 deployment for Docker in an AWS Auto Scaling group with a scaling policy to increase instance size and number of nodes. Create a cross-region read replica of the RDS data. In the event of a failure, promote the replica to primary, and update the DNS record to point to the ALB in the secondary region.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 334,
    "question": "A company is migrating its infrastructure to the AWS Cloud. The company must comply with a variety of regulatory standards for different projects. The company needs a multi-account environment.<br><br>A solutions architect needs to prepare the baseline infrastructure. The solution must provide a consistent baseline of management and security, but it must allow flexibility for different compliance requirements within various AWS accounts. The solution also needs to integrate with the existing on-premises Active Directory Federation Services (AD FS) server.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an organization in AWS Organizations. Create a single SCP for least privilege access across all accounts. Create a single OU for all accounts. Configure an IAM identity provider for federation with the on-premises AD FS server. Configure a central logging account with a defined process for log generating services to send log events to the central account. Enable AWS Config in the central account with conformance packs for all accounts.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Add OUs as necessary. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an organization in AWS Organizations. Create SCPs for least privilege access. Create an OU structure, and use it to group AWS accounts. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server. Configure a central logging account with a defined process for log generating services to send log events to the central account. Enable AWS Config in the central account with aggregators and conformance packs.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Configure an IAM identity provider for federation with the on-premises AD FS server.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 335,
    "question": "An online magazine will launch its latest edition this month. This edition will be the first to be distributed globally. The magazine's dynamic website currently uses an Application Load Balancer in front of the web tier, a fleet of Amazon EC2 instances for web and application servers, and Amazon Aurora MySQL. Portions of the website include static content and almost all traffic is read-only.<br><br>The magazine is expecting a significant spike in internet traffic when the new edition is launched. Optimal performance is a top priority for the week following the launch.<br><br>Which combination of steps should a solutions architect take to reduce system response times for a global audience? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Use logical cross-Region replication to replicate the Aurora MySQL database to a secondary Region. Replace the web servers with Amazon S3. Deploy S3 buckets in cross-Region replication mode.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Ensure the web and application tiers are each in Auto Scaling groups. Introduce an AWS Direct Connect connection. Deploy the web and application tiers in Regions across the world.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Migrate the database from Amazon Aurora to Amazon RDS for MySQL. Ensure all three of the application tiers – web, application, and database – are in private subnets.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use an Aurora global database for physical cross-Region replication. Use Amazon S3 with cross-Region replication for static content and resources. Deploy the web and application tiers in Regions across the world.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Introduce Amazon Route 53 with latency-based routing and Amazon CloudFront distributions. Ensure the web and application tiers are each in Auto Scaling groups.",
        "correct": true
      }
    ],
    "corrects": [
      4,
      5
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 336,
    "question": "An online gaming company needs to optimize the cost of its workloads on AWS. The company uses a dedicated account to host the production environment for its online gaming application and an analytics application.<br><br>Amazon EC2 instances host the gaming application and must always be available. The EC2 instances run all year. The analytics application uses data that is stored in Amazon S3. The analytics application can be interrupted and resumed without issue.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Purchase an EC2 Instance Savings Plan for the online gaming application instances. Use On-Demand Instances for the analytics application.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Purchase an EC2 Instance Savings Plan for the online gaming application instances. Use Spot Instances for the analytics application.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use Spot Instances for the online gaming application and the analytics application. Set up a catalog in AWS Service Catalog to provision services at a discount.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use On-Demand Instances for the online gaming application. Use Spot Instances for the analytics application. Set up a catalog in AWS Service Catalog to provision services at a discount.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 337,
    "question": "A company runs applications in hundreds of production AWS accounts. The company uses AWS Organizations with all features enabled and has a centralized backup operation that uses AWS Backup.<br><br>The company is concerned about ransomware attacks. To address this concern, the company has created a new policy that all backups must be resilient to breaches of privileged-user credentials in any production account.<br><br>Which combination of steps will meet this new requirement? (Choose three.)",
    "answers": [
      {
        "id": 1,
        "answer": "Implement cross-account backup with AWS Backup vaults in designated non-production accounts.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Add an SCP that restricts the modification of AWS Backup vaults.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Implement AWS Backup Vault Lock in compliance mode.<br>C. Implement least privilege access for the IAM service role that is assigned to AWS Backup.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure the backup frequency, lifecycle, and retention period to ensure that at least one backup always exists in the cold tier.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Configure AWS Backup to write all backups to an Amazon S3 bucket in a designated non-production account. Ensure that the S3 bucket has S3 Object Lock enabled.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2,
      3
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 338,
    "question": "A company needs to aggregate Amazon CloudWatch logs from its AWS accounts into one central logging account. The collected logs must remain in the AWS Region of creation. The central logging account will then process the logs, normalize the logs into standard output format, and stream the output logs to a security tool for more processing.<br><br>A solutions architect must design a solution that can handle a large volume of logging data that needs to be ingested. Less logging will occur outside normal business hours than during normal business hours. The logging solution must scale with the anticipated load. The solutions architect has decided to use an AWS Control Tower design to handle the multi-account logging process.<br><br>Which combination of steps should the solutions architect take to meet the requirements? (Choose three.)",
    "answers": [
      {
        "id": 1,
        "answer": "Create a destination Amazon Kinesis data stream in the central logging account.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a destination Amazon Simple Queue Service (Amazon SQS) queue in the central logging account.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Kinesis data stream. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a subscription filter for each log group to send data to the Kinesis data stream.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Simple Queue Service (Amazon SQS) queue. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a single subscription filter for all log groups to send data to the SQS queue.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Create an AWS Lambda function. Program the Lambda function to normalize the logs in the central logging account and to write the logs to the security tool.",
        "correct": true
      },
      {
        "id": 6,
        "answer": "Create an AWS Lambda function. Program the Lambda function to normalize the logs in the member accounts and to write the logs to the security tool.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3,
      5
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 339,
    "question": "A company is migrating a legacy application from an on-premises data center to AWS. The application consists of a single application server and a Microsoft SQL Server database server. Each server is deployed on a VMware VM that consumes 500 TB of data across multiple attached volumes.<br><br>The company has established a 10 Gbps AWS Direct Connect connection from the closest AWS Region to its on-premises data center. The Direct Connect connection is not currently in use by other services.<br><br>Which combination of steps should a solutions architect take to migrate the application with the LEAST amount of downtime? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Use an AWS Server Migration Service (AWS SMS) replication job to migrate the database server VM to AWS.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use VM Import/Export to import the application server VM.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Export the VM images to an AWS Snowball Edge Storage Optimized device.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use an AWS Server Migration Service (AWS SMS) replication job to migrate the application server VM to AWS.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Use an AWS Database Migration Service (AWS DMS) replication instance to migrate the database to an Amazon RDS DB instance.",
        "correct": true
      }
    ],
    "corrects": [
      4,
      5
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 340,
    "question": "A company operates a fleet of servers on premises and operates a fleet of Amazon EC2 instances in its organization in AWS Organizations. The company's AWS accounts contain hundreds of VPCs. The company wants to connect its AWS accounts to its on-premises network. AWS Site-to-Site VPN connections are already established to a single AWS account. The company wants to control which VPCs can communicate with other VPCs.<br><br>Which combination of steps will achieve this level of control with the LEAST operational effort? (Choose three.)",
    "answers": [
      {
        "id": 1,
        "answer": "Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM).",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Configure attachments to all VPCs and VPNs.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Configure VPC peering between the VPCs.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Configure attachments between the VPCs and VPNs.",
        "correct": false
      },
      {
        "id": 6,
        "answer": "Setup route tables on the VPCs and VPNs.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2,
      3
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 341,
    "question": "A company needs to optimize the cost of its application on AWS. The application uses AWS Lambda functions and Amazon Elastic Container Service (Amazon ECS) containers that run on AWS Fargate. The application is write-heavy and stores data in an Amazon Aurora MySQL database.<br><br>The load on the application is not consistent. The application experiences long periods of no usage, followed by sudden and significant increases and decreases in traffic. The database runs on a memory optimized DB instance that cannot handle the load.<br><br>A solutions architect must design a solution that can scale to handle the changes in traffic.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "answers": [
      {
        "id": 1,
        "answer": "Add additional read replicas to the database. Purchase Instance Savings Plans and RDS Reserved Instances.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Migrate the database to an Aurora DB cluster that has multiple writer instances. Purchase Instance Savings Plans.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Migrate the database to an Aurora global database. Purchase Compute Savings Plans and RDS Reserved instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Migrate the database to Aurora Serverless v1. Purchase Compute Savings Plans.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 342,
    "question": "A company migrated an application to the AWS Cloud. The application runs on two Amazon EC2 instances behind an Application Load Balancer (ALB).<br>Application data is stored in a MySQL database that runs on an additional EC2 instance. The application's use of the database is read-heavy.<br><br>The application loads static content from Amazon Elastic Block Store (Amazon EBS) volumes that are attached to each EC2 instance. The static content is updated frequently and must be copied to each EBS volume.<br><br>The load on the application changes throughout the day. During peak hours, the application cannot handle all the incoming requests. Trace data shows that the database cannot handle the read load during peak hours.<br><br>Which solution will improve the reliability of the application?",
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the application to a set of AWS Lambda functions. Set the Lambda functions as targets for the ALB. Create a new single EBS volume for the static content. Configure the Lambda functions to read from the new EBS volume. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB cluster.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Migrate the application to a set of AWS Step Functions state machines. Set the state machines as targets for the ALCreate an Amazon Elastic File System (Amazon EFS) file system for the static content. Configure the state machines to read from the EFS file system. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Containerize the application. Migrate the application to an Amazon Elastic Container Service (Amazon ECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create a new single EBS volume for the static content. Mount the new EBS volume on the ECS cluster. Configure AWS Application Auto Scaling on the ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB cluster.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Containerize the application. Migrate the application to an Amazon Elastic Container Service (Amazon ECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create an Amazon Elastic File System (Amazon EFS) file system for the static content. Mount the EFS file system to each container. Configure AWS Application Auto Scaling on the ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 343,
    "question": "A solutions architect wants to make sure that only AWS users or roles with suitable permissions can access a new Amazon API Gateway endpoint. The solutions architect wants an end-to-end view of each request to analyze the latency of the request and create service maps.<br><br>How can the solutions architect design the API Gateway access control and perform request inspections?",
    "answers": [
      {
        "id": 1,
        "answer": "For the API Gateway method, set the authorization to AWS_IAM. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWS Signature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to API Gateway.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "For the API Gateway resource, set CORS to enabled and only return the company's domain in Access-Control-Allow-Origin headers. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Use Amazon CloudWatch to trace and analyze user requests to API Gateway.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an AWS Lambda function as the custom authorizer, ask the API client to pass the key and secret when making the call, and then use Lambda to validate the key/secret pair against the IAM system. Use AWS X-Ray to trace and analyze user requests to API Gateway.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a client certificate for API Gateway. Distribute the certificate to the AWS users and roles that need to access the endpoint. Enable the API caller to pass the client certificate when accessing the endpoint. Use Amazon CloudWatch to trace and analyze user requests to API Gateway.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 344,
    "question": "A company is using AWS CodePipeline for the CI/CD of an application to an Amazon EC2 Auto Scaling group. All AWS resources are defined in AWS CloudFormation templates. The application artifacts are stored in an Amazon S3 bucket and deployed to the Auto Scaling group using instance user data scripts. As the application has become more complex, recent resource changes in the CloudFormation templates have caused unplanned downtime.<br><br>How should a solutions architect improve the CI/CD pipeline to reduce the likelihood that changes in the templates will cause downtime?",
    "answers": [
      {
        "id": 1,
        "answer": "Adapt the deployment scripts to detect and report CloudFormation error conditions when performing deployments. Write test plans for a testing team to run in a non-production environment before approving the change for production.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Implement automated testing using AWS CodeBuild in a test environment. Use CloudFormation change sets to evaluate changes before deployment. Use AWS CodeDeploy to leverage blue/green deployment patterns to allow evaluations and the ability to revert changes, if needed.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use plugins for the integrated development environment (IDE) to check the templates for errors, and use the AWS CLI to validate that the templates are correct. Adapt the deployment code to check for error conditions and generate notifications on errors. Deploy to a test environment and run a manual test plan before approving the change for production.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS CodeDeploy and a blue/green deployment pattern with CloudFormation to replace the user data deployment scripts. Have the operators log in to running instances and go through a manual test plan to verify the application is running as expected.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 345,
    "question": "A North American company with headquarters on the East Coast is deploying a new web application running on Amazon EC2 in the us-east-1 Region. The application should dynamically scale to meet user demand and maintain resiliency. Additionally, the application must have disaster recovery capabilities in an active-passive configuration with the us-west-1 Region.<br><br>Which steps should a solutions architect take after creating a VPC in the us-east-1 Region?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs in each Region as part of an Auto Scaling group spanning both VPCs and served by the ALB.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALDeploy the same solution to the us-west-1 Region. Create an Amazon Route 53 record set with a failover routing policy and health checks enabled to provide high availability across both Regions.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) that spans both VPCs. Deploy EC2 instances across multiple Availability Zones as part of an Auto Scaling group in each VPC served by the ALB. Create an Amazon Route 53 record that points to the ALB.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALB. Deploy the same solution to the us-west-1 Region. Create separate Amazon Route 53 records in each Region that point to the ALB in the Region. Use Route 53 health checks to provide high availability across both Regions.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 346,
    "question": "A company has a legacy application that runs on multiple NET Framework components. The components share the same Microsoft SQL Server database and communicate with each other asynchronously by using Microsoft Message Queueing (MSMQ).<br><br>The company is starting a migration to containerized .NET Core components and wants to refactor the application to run on AWS. The .NET Core components require complex orchestration. The company must have full control over networking and host configuration. The application's database model is strongly relational.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Host the INET Core components on AWS App Runner. Host the database on Amazon RDS for SQL Server. Use Amazon EventBiridge for asynchronous messaging.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Host the .NET Core components on Amazon Elastic Container Service (Amazon ECS) with the AWS Fargate launch type. Host the database on Amazon DynamoDUse Amazon Simple Notification Service (Amazon SNS) for asynchronous messaging.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Host the .NET Core components on AWS Elastic Beanstalk. Host the database on Amazon Aurora PostgreSQL Serverless v2. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) for asynchronous messaging.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Host the NET Core components on Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type. Host the database on Amazon Aurora MySQL Serverless v2. Use Amazon Simple Queue Service (Amazon SQS) for asynchronous messaging.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 347,
    "question": "A solutions architect has launched multiple Amazon EC2 instances in a placement group within a single Availability Zone. Because of additional load on the system, the solutions architect attempts to add new instances to the placement group. However, the solutions architect receives an insufficient capacity error.<br><br>What should the solutions architect do to troubleshoot this issue?",
    "answers": [
      {
        "id": 1,
        "answer": "Use a spread placement group. Set a minimum of eight instances for each Availability Zone.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Stop and start all the instances in the placement group. Try the launch again.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a new placement group. Merge the new placement group with the original placement group.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Launch the additional instances as Dedicated Hosts in the placement groups.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 348,
    "question": "A company has used infrastructure as code (IaC) to provision a set of two Amazon EC2 instances. The instances have remained the same for several years.<br><br>The company's business has grown rapidly in the past few months. In response, the company’s operations team has implemented an Auto Scaling group to manage the sudden increases in traffic. Company policy requires a monthly installation of security updates on all operating systems that are running.<br><br>The most recent security update required a reboot. As a result, the Auto Scaling group terminated the instances and replaced them with new, unpatched instances.<br><br>Which combination of steps should a solutions architect recommend to avoid a recurrence of this issue? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Modify the Auto Scaling group by setting the Update policy to target the oldest launch configuration for replacement.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a new Auto Scaling group before the next patch maintenance. During the maintenance window, patch both groups and reboot the instances.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an Elastic Load Balancer in front of the Auto Scaling group. Configure monitoring to ensure that target group health checks return healthy after the Auto Scaling group replaces the terminated instances.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create automation scripts to patch an AMI, update the launch configuration, and invoke an Auto Scaling instance refresh.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Create an Elastic Load Balancer in front of the Auto Scaling group. Configure termination protection on the instances.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      4
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 349,
    "question": "A team of data scientists is using Amazon SageMaker instances and SageMaker APIs to train machine learning (ML) models. The SageMaker instances are deployed in a VPC that does not have access to or from the internet. Datasets for ML model training are stored in an Amazon S3 bucket. Interface VPC endpoints provide access to Amazon S3 and the SageMaker APIs.<br><br>Occasionally, the data scientists require access to the Python Package Index (PyPI) repository to update Python packages that they use as part of their workflow. A solutions architect must provide access to the PyPI repository while ensuring that the SageMaker instances remain isolated from the internet.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS CodeCommit repository for each package that the data scientists need to access. Configure code synchronization between the PyPI repository and the CodeCommit repository. Create a VPC endpoint for CodeCommit.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a NAT gateway in the VPC. Configure VPC routes to allow access to the internet with a network ACL that allows access to only the PyPI repository endpoint.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a NAT instance in the VPConfigure VPC routes to allow access to the internet. Configure SageMaker notebook instance firewall rules that allow access to only the PyPI repository endpoint.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an AWS CodeArtifact domain and repository. Add an external connection for public:pypi to the CodeArtifact repository. Configure the Python client to use the CodeArtifact repository. Create a VPC endpoint for CodeArtifact.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 350,
    "question": "A solutions architect works for a government agency that has strict disaster recovery requirements. All Amazon Elastic Block Store (Amazon EBS) snapshots are required to be saved in at least two additional AWS Regions. The agency also is required to maintain the lowest possible operational overhead.<br><br>Which solution meets these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure a policy in Amazon Data Lifecycle Manager (Amazon DLM) to run once daily to copy the EBS snapshots to the additional Regions.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use Amazon EventBridge to schedule an AWS Lambda function to copy the EBS snapshots to the additional Regions.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Setup AWS Backup to create the EBS snapshots. Configure Amazon S3 Cross-Region Replication to copy the EBS snapshots to the additional Regions.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Schedule Amazon EC2 Image Builder to run once daily to create an AMI and copy the AMI to the additional Regions.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 351,
    "question": "A company has a project that is launching Amazon EC2 instances that are larger than required. The project's account cannot be part of the company's organization in AWS Organizations due to policy restrictions to keep this activity outside of corporate IT. The company wants to allow only the launch of t3.small EC2 instances by developers in the project's account. These EC2 instances must be restricted to the us-east-2 Region.<br><br>What should a solutions architect do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a new developer account. Move all EC2 instances, users, and assets into us-east-2. Add the account to the company's organization in AWS Organizations. Enforce a tagging policy that denotes Region affinity.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an SCP that denies the launch of all EC2 instances except t3.small EC2 instances in us-east-2. Attach the SCP to the project's account.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create and purchase a t3.small EC2 Reserved Instance for each developer in us-east-2. Assign each developer a specific EC2 instance with their name as the tag.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an IAM policy than allows the launch of only t3.small EC2 instances in us-east-2. Attach the policy to the roles and groups that the developers use in the project's account.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 352,
    "question": "A scientific company needs to process text and image data from an Amazon S3 bucket. The data is collected from several radar stations during a live, time-critical phase of a deep space mission. The radar stations upload the data to the source S3 bucket. The data is prefixed by radar station identification number.<br><br>The company created a destination S3 bucket in a second account. Data must be copied from the source S3 bucket to the destination S3 bucket to meet a compliance objective. This replication occurs through the use of an S3 replication rule to cover all objects in the source S3 bucket.<br><br>One specific radar station is identified as having the most accurate data. Data replication at this radar station must be monitored for completion within 30 minutes after the radar station uploads the objects to the source S3 bucket.<br><br>What should a solutions architect do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Setup an AWS DataSync agent to replicate the prefixed data from the source S3 bucket to the destination S3 bucket. Select to use all available bandwidth on the task, and monitor the task to ensure that itis in the TRANSFERRING status. Create an Amazon EventBridge rule to initiate an alert if this status changes.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "In the second account, create another S3 bucket to receive data from the radar station with the most accurate data. Set up a new replication rule for this new S3 bucket to separate the replication from the other radar stations. Monitor the maximum replication time to the destination. Create an Amazon EventBridge rule to initiate an alert when the time exceeds the desired threshold.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Enable Amazon S3 Transfer Acceleration on the source S3 bucket, and configure the radar station with the most accurate data to use the new endpoint. Monitor the S3 destination bucket's TotalRequestLatency metric. Create an Amazon EventBridge rule to initiate an alert if this status changes.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a new S3 replication rule on the source S3 bucket that filters for the keys that use the prefix of the radar station with the most accurate data. Enable S3 Replication Time Control (S3 RTC). Monitor the maximum replication time to the destination. Create an Amazon EventBridge rule to initiate an alert when the time exceeds the desired threshold.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 353,
    "question": "A company wants to migrate its on-premises data center to the AWS Cloud. This includes thousands of virtualized Linux and Microsoft Windows servers, SAN storage, Java and PHP applications with MySQL, and Oracle databases. There are many dependent services hosted either in the same data center or externally. The technical documentation is incomplete and outdated. A solutions architect needs to understand the current environment and estimate the cloud resource costs after the migration.<br><br>Which tools or services should the solutions architect use to plan the cloud migration? (Choose three.)",
    "answers": [
      {
        "id": 1,
        "answer": "AWS Application Discovery Service",
        "correct": true
      },
      {
        "id": 2,
        "answer": "AWS SMS",
        "correct": false
      },
      {
        "id": 3,
        "answer": "AWS X-Ray",
        "correct": false
      },
      {
        "id": 4,
        "answer": "AWS Cloud Adoption Readiness Tool (CART)",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Amazon Inspector",
        "correct": false
      },
      {
        "id": 6,
        "answer": "AWS Migration Hub",
        "correct": true
      }
    ],
    "corrects": [
      1,
      4,
      6
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 354,
    "question": "A solutions architect is reviewing an application's resilience before launch. The application runs on an Amazon EC2 instance that is deployed in a private subnet of a VPC. The EC2 instance is provisioned by an Auto Scaling group that has a minimum capacity of 1 and a maximum capacity of 1. The application stores data on an Amazon RDS for MySQL DB instance. The VPC has subnets configured in three Availability Zones and is configured with a single NAT gateway.<br><br>The solutions architect needs to recommend a solution to ensure that the application will operate across multiple Availability Zones.<br><br>Which solution will meet this requirement?",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to a Multi-AZ configuration. Configure the Auto Scaling group to launch the instances across Availability Zones. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Replace the NAT gateway with a virtual private gateway. Replace the RDS for MySQL DB instance with an Amazon Aurora MySQL DB cluster. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Replace the NAT gateway with a NAT instance. Migrate the RDS for MySQL DB instance to an RDS for PostgreSQL DB instance. Launch a new EC2 instance in the other Availability Zones.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to turn on automatic backups and retain the backups for 7 days. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Keep the minimum capacity and the maximum capacity of the Auto Scaling group at 1.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 355,
    "question": "A company is planning to migrate its on-premises transaction-processing application to AWS. The application runs inside Docker containers that are hosted on VMs in the company's data center. The Docker containers have shared storage where the application records transaction data.<br><br>The transactions are time sensitive. The volume of transactions inside the application is unpredictable. The company must implement a low-latency storage solution that will automatically scale throughput to meet increased demand. The company cannot develop the application further and cannot continue to administer the Docker hosting environment.<br><br>How should the company migrate the application to AWS to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Migrate the containers that run the application to Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon S3 to store the transaction data that the containers share.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Migrate the containers that run the application to AWS Fargate for Amazon Elastic Container Service (Amazon ECS). Create an Amazon Elastic File System (Amazon EFS) file system. Create a Fargate task definition. Add a volume to the task definition to point to the EFS file system.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Migrate the containers that run the application to AWS Fargate for Amazon Elastic Container Service (Amazon ECS). Create an Amazon Elastic Block Store (Amazon EBS) volume. Create a Fargate task definition. Attach the EBS volume to each running task.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Launch Amazon EC2 instances. Install Docker on the EC2 instances. Migrate the containers to the EC2 instances. Create an Amazon Elastic File System (Amazon EFS) file system. Add a mount point to the EC2 instances for the EFS file system.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 356,
    "question": "A company is planning to migrate to the AWS Cloud. The company hosts many applications on Windows servers and Linux servers. Some of the servers are physical, and some of the servers are virtual. The company uses several types of databases in its on-premises environment. The company does not have an accurate inventory of its on-premises servers and applications.<br><br>The company wants to rightsize its resources during migration. A solutions architect needs to obtain information about the network connections and the application relationships. The solutions architect must assess the company’s current environment and develop a migration plan.<br><br>Which solution will provide the solutions architect with the required information to develop the migration plan?",
    "answers": [
      {
        "id": 1,
        "answer": "Use Migration Evaluator to request an evaluation of the environment from AWS. Use the AWS Application Discovery Service Agentless Collector to import the details into a Migration Evaluator Quick Insights report.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use AWS Migration Hub and install the AWS Application Discovery Agent on the servers. Deploy the Migration Hub Strategy Recommendations application data collector. Generate a report by using Migration Hub Strategy Recommendations.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Use AWS Migration Hub and run the AWS Application Discovery Service Agentless Collector on the servers. Group the servers and databases by using AWS Application Migration Service. Generate a report by using Migration Hub Strategy Recommendations.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use the AWS Migration Hub import tool to load the details of the company’s on-premises environment. Generate a report by using Migration Hub Strategy Recommendations.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 357,
    "question": "A financial services company sells its software-as-a-service (SaaS) platform for application compliance to large global banks. The SaaS platform runs on AWS and uses multiple AWS accounts that are managed in an organization in AWS Organizations. The SaaS platform uses many AWS resources globally.<br><br>For regulatory compliance, all API calls to AWS resources must be audited, tracked for changes, and stored in a durable and secure data store.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a new AWS CloudTrail trail. Use an existing Amazon S3 bucket in the organization's management account to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a new AWS CloudTrail trail in each member account of the organization. Create new Amazon S3 buckets to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 buckets.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a new AWS CloudTrail trail in the organization's management account. Create a new Amazon S3 bucket with versioning turned on to store the logs. Deploy the trail for all accounts in the organization. Enable MFA delete and encryption on the S3 bucket.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create a new AWS CloudTrail trail in the organization's management account. Create a new Amazon S3 bucket to store the logs. Configure Amazon Simple Notification Service (Amazon SNS) to send log-file delivery notifications to an external management system that will track the logs. Enable MFA delete and encryption on the S3 bucket.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 358,
    "question": "A company is deploying a distributed in-memory database on a fleet of Amazon EC2 instances. The fleet consists of a primary node and eight worker nodes. The primary node is responsible for monitoring cluster health, accepting user requests, distributing user requests to worker nodes, and sending an aggregate response back to a client. Worker nodes communicate with each other to replicate data partitions.<br><br>The company requires the lowest possible networking latency to achieve maximum performance.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Launch memory optimized EC2 instances in a partition placement group.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Launch compute optimized EC2 instances in a partition placement group.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Launch memory optimized EC2 instances in a cluster placement group.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Launch compute optimized EC2 instances in a spread placement group.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 359,
    "question": "A company maintains information on premises in approximately 1 million.csv files that are hosted on a VM. The data initially is 10 TB in size and grows at a rate of 1 TB each week. The company needs to automate backups of the data to the AWS Cloud.<br><br>Backups of the data must occur daily. The company needs a solution that applies custom filters to back up only a subset of the data that is located in designated source directories. The company has set up an AWS Direct Connect connection.<br><br>Which solution will meet the backup requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Use the Amazon S3 CopyObject API operation with multipart upload to copy the existing data to Amazon S3. Use the CopyObject API operation to replicate new data to Amazon S3 daily.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a backup plan in AWS Backup to back up the data to Amazon S3. Schedule the backup plan to run daily.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Install the AWS DataSync agent as a VM that runs on the on-premises hypervisor. Configure a DataSync task to replicate the data to Amazon S3 daily.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use an AWS Snowball Edge device for the initial backup. Use AWS DataSync for incremental backups to Amazon S3 daily.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 360,
    "question": "A financial services company has an asset management product that thousands of customers use around the world. The customers provide feedback about the product through surveys. The company is building a new analytical solution that runs on Amazon EMR to analyze the data from these surveys. The following user personas need to access the analytical solution to perform different actions:<br><br>•Administrator: Provisions the EMR cluster for the analytics team based on the team’s requirements<br>•Data engineer: Runs ETL scripts to process, transform, and enrich the datasets<br>•Data analyst: Runs SQL and Hive queries on the data<br><br>A solutions architect must ensure that all the user personas have least privilege access to only the resources that they need. The user personas must be able to launch only applications that are approved and authorized. The solution also must ensure tagging for all resources that the user personas create.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create IAM roles for each user persona. Attach identity-based policies to define which actions the user who assumes the role can perform. Create an AWS Config rule to check for noncompliant resources. Configure the rule to notify the administrator to remediate the noncompliant resources.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Setup Kerberos-based authentication for EMR clusters upon launch. Specify a Kerberos security configuration along with cluster-specific Kerberos options.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use AWS Service Catalog to control the Amazon EMR versions available for deployment, the cluster configuration, and the permissions for each user persona.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Launch the EMR cluster by using AWS CloudFormation, Attach resource-based policies to the EMR cluster during cluster creation. Create an AWS. Config rule to check for noncompliant clusters and noncompliant Amazon S3 buckets. Configure the rule to notify the administrator to remediate the noncompliant resources.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 361,
    "question": "A software as a service (SaaS) company uses AWS to host a service that is powered by AWS PrivateLink. The service consists of proprietary software that runs on three Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are in private subnets in multiple Availability Zones in the eu-west-2 Region. All the company's customers are in eu-west-2.<br><br>However, the company now acquires a new customer in the us-east-1 Region. The company creates a new VPC and new subnets in us-east-1. The company establishes inter-Region VPC peering between the VPCs in the two Regions.<br><br>The company wants to give the new customer access to the SaaS service, but the company does not want to immediately deploy new EC2 resources in us-east-1.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure a PrivateLink endpoint service in us-east-1 to use the existing NLB that is in eu-west-2. Grant specific AWS accounts access to connect to the SaaS service.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an NLB in us-east-1. Create an IP target group that uses the IP addresses of the company's instances in eu-west-2 that host the SaaS service. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create an Application Load Balancer (ALB) in front of the EC2 instances in eu-west-2. Create an NLB in us-east-1. Associate the NLB that is in us-east-1 with an ALB target group that uses the ALB that is in eu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use AWS Resource Access Manager (AWS RAM) to share the EC2 instances that are in eu-west-2. In us-east-1, create an NLB and an instance target group that includes the shared EC2 instances from eu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 362,
    "question": "A company needs to monitor a growing number of Amazon S3 buckets across two AWS Regions. The company also needs to track the percentage of objects that are encrypted in Amazon S3. The company needs a dashboard to display this information for internal compliance teams.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a new 3 Storage Lens dashboard in each Region to track bucket and encryption metrics. Aggregate data from both Region dashboards into a single dashboard in Amazon QuickSight for the compliance teams.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Deploy an AWS Lambda function in each Region to list the number of buckets and the encryption status of objects. Store this data in Amazon S3. Use Amazon Athena queries to display the data on a custom dashboard in Amazon QuickSight for the compliance teams.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use the S3 Storage Lens default dashboard to track bucket and encryption metrics. Give the compliance teams access to the dashboard directly in the S3 console.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an Amazon EventBridge rule to detect AWS CloudTrail events for S3 object creation. Configure the rule to invoke an AWS Lambda function to record encryption metrics in Amazon DynamoDB. Use Amazon QuickSight to display the metrics in a dashboard for the compliance teams.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 363,
    "question": "A company’s CISO has asked a solutions architect to re-engineer the company's current CI/CD practices to make sure patch deployments to its application can happen as quickly as possible with minimal downtime if vulnerabilities are discovered. The company must also be able to quickly roll back a change in case of errors.<br><br>The web application is deployed in a fleet of Amazon EC2 instances behind an Application Load Balancer. The company is currently using GitHub to host the application source code, and has configured an AWS CodeBuild project to build the application. The company also intends to use AWS CodePipeline to trigger builds from GitHub commits using the existing CodeBuild project.<br><br>What CI/CD configuration meets all of the requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for in-place deployment. Monitor the newly deployed code, and, if there are any issues, push another code update",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for blue/green deployments. Monitor the newly deployed code, and, if there are any issues, trigger a manual rollback using CodeDeploy.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Configure CodePipeline with a deploy stage using AWS CloudFormation to create a pipeline for test and production stacks. Monitor the newly deployed code, and, if there are any issues, push another code update.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Configure the CodePipeline with a deploy stage using AWS OpsWorks and in-place deployments. Monitor the newly deployed code, and, if there are any issues, push another code update.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 364,
    "question": "A company is managing many AWS accounts by using an organization in AWS Organizations. Different business units in the company run applications on Amazon EC2 instances. All the EC2 instances must have a BusinessUnit tag so that the company can track the cost for each business unit.<br><br>A recent audit revealed that some instances were missing this tag. The company manually added the missing tag to the instances.<br><br>What should a solutions architect do to enforce the tagging requirement in the future?",
    "answers": [
      {
        "id": 1,
        "answer": "Enable tag policies in the organization. Create a tag policy for the BusinessUnit tag. Ensure that compliance with tag key capitalization is turned off. Implement the tag policy for the ec2:instance resource type. Attach the tag policy to the root of the organization.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Enable tag policies in the organization. Create a tag policy for the BusinessUnit tag. Ensure that compliance with tag key capitalization is turned on. Implement the tag policy for the ec2:instance resource type. Attach the tag policy to the organization's management account.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an SCP and attach the SCP to the root of the organization. Include the following statement in the SCP:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image8.png\">",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an SCP and attach the SCP to the organization’s management account. Include the following statement in the SCP:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image9.png\">",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 365,
    "question": "A company is running a workload that consists of thousands of Amazon EC2 instances. The workload is running in a VPC that contains several public subnets and private subnets. The public subnets have a route for 0.0.0.0/0 to an existing internet gateway. The private subnets have a route for 0.0.0.0/0 to an existing NAT gateway.<br><br>A solutions architect needs to migrate the entire fleet of EC2 instances to use IPv6. The EC2 instances that are in private subnets must not be accessible from the public internet.<br><br>What should the solutions architect do to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnets. Update all the VPC route tables, and add a route for ::/0 to the internet gateway.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Update the VPC route tables for all private subnets, and add a route for ::/0 to the NAT gateway.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Create an egress-only internet gateway. Update the VPC route tables for all private subnets, and add a route for ::/0 to the egress-only internet gateway.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Update the existing VPC, and associate a custom IPV6 CIDR block with the VPC and all subnets. Create a new NAT gateway, and enable IPV6 support. Update the VPC route tables for all private subnets, and add a route for ::/0 to the IPv6-enabled NAT gateway.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 366,
    "question": "A company is using Amazon API Gateway to deploy a private REST API that will provide access to sensitive data. The API must be accessible only from an application that is deployed in a VPC. The company deploys the API successfully. However, the API is not accessible from an Amazon EC2 instance that is deployed in the VPC.<br><br>Which solution will provide connectivity between the EC2 instance and the API?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an interface VPC endpoint for API Gateway. Attach an endpoint policy that allows apigateway:* actions. Disable private DNS naming for the VPC endpoint. Configure an API resource policy that allows access from the VPC. Use the VPC endpoint's DNS name to access the API.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an interface VPC endpoint for API Gateway. Attach an endpoint policy that allows the execute-api:Invoke action. Enable private DNS naming for the VPC endpoint. Configure an API resource policy that allows access from the VPC endpoint. Use the API endpoint’s DNS names to access the API.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Create a Network Load Balancer (NLB) and a VPC link. Configure private integration between API Gateway and the NLB. Use the API endpoint’s DNS names to access the API.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an Application Load Balancer (ALB) and a VPC Link. Configure private integration between API Gateway and the ALB. Use the ALB endpoint’s DNS name to access the API.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 367,
    "question": "A large payroll company recently merged with a small staffing company. The unified company now has multiple business units, each with its own existing AWS account.<br><br>A solutions architect must ensure that the company can centrally manage the billing and access policies for all the AWS accounts. The solutions architect configures AWS Organizations by sending an invitation to all member accounts of the company from a centralized management account.<br><br>What should the solutions architect do next to meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create the OrganizationAccountAccess IAM group in each member account. Include the necessary IAM roles for each administrator.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create the OrganizationAccountAccessPolicy IAM policy in each member account. Connect the member accounts to the management account by using cross-account access.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create the OrganizationAccountAccessRole IAM role in each member account. Grant permission to the management account to assume the IAM role.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create the OrganizationAccountAccessRole IAM role in the management account. Attach the AdministratorAccess AWS managed policy to the IAM role. Assign the IAM role to the administrators in each member account.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 368,
    "question": "A company has application services that have been containerized and deployed on multiple Amazon EC2 instances with public IPs. An Apache Kafka cluster has been deployed to the EC2 instances. A PostgreSQL database has been migrated to Amazon RDS for PostgreSQL. The company expects a significant increase of orders on its platform when a new version of its flagship product is released.<br><br>What changes to the current architecture will reduce operational overhead and support the product release?",
    "answers": [
      {
        "id": 1,
        "answer": "Create an EC2 Auto Scaling group behind an Application Load Balancer. Create additional read replicas for the DB instance. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create an EC2 Auto Scaling group behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy the application on a Kubernetes cluster created on the EC2 instances behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicas for the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 369,
    "question": "A company hosts a VPN in an on-premises data center. Employees currently connect to the VPN to access files in their Windows home directories. Recently, there has been a large growth in the number of employees who work remotely. As a result, bandwidth usage for connections into the data center has begun to reach 100% during business hours.<br><br>The company must design a solution on AWS that will support the growth of the company's remote workforce, reduce the bandwidth usage for connections into the data center, and reduce operational overhead.<br><br>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Create an AWS Storage Gateway Volume Gateway. Mount a volume from the Volume Gateway to the on-premises file server.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Migrate the home directories to Amazon FSx for Windows File Server.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Migrate the home directories to Amazon FSx for Lustre.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Migrate remote users to AWS Client VPN.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "Create an AWS Direct Connect connection from the on-premises data center to AWS.",
        "correct": false
      }
    ],
    "corrects": [
      2,
      4
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 370,
    "question": "A company has multiple AWS accounts. The company recently had a security audit that revealed many unencrypted Amazon Elastic Block Store (Amazon EBS) volumes attached to Amazon EC2 instances.<br><br>A solutions architect must encrypt the unencrypted volumes and ensure that unencrypted volumes will be detected automatically in the future. Additionally, the company wants a solution that can centrally manage multiple AWS accounts with a focus on compliance and security.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "answers": [
      {
        "id": 1,
        "answer": "Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the strongly recommended controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Use the AWS CLI to list all the unencrypted volumes in all the AWS accounts. Run a script to encrypt all the unencrypted volumes in place.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create a snapshot of each unencrypted volume. Create a new encrypted volume from the unencrypted snapshot. Detach the existing volume, and replace it with the encrypted volume.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the mandatory controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Turn on AWS CloudTrail. Configure an Amazon EventBridge rule to detect and automatically encrypt unencrypted volumes.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 371,
    "question": "A company hosts an intranet web application on Amazon EC2 instances behind an Application Load Balancer (ALB). Currently, users authenticate to the application against an internal user database.<br><br>The company needs to authenticate users to the application by using an existing AWS Directory Service for Microsoft Active Directory directory. All users with accounts in the directory must have access to the application.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Create a new app client in the directory. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule. Configure the listener rule with the appropriate issuer, client ID and secret, and endpoint details for the Active Directory service. Configure the new app client with the callback URL that the ALB provides.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Configure an Amazon Cognito user pool. Configure the user pool with a federated identity provider (ldP) that has metadata from the directory. Create an app client. Associate the app client with the user pool. Create a listener rule for the ALSpecify the authenticate-cognito action for the listener rule. Configure the listener rule to use the user pool and app client.",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Add the directory as a new IAM identity provider (ldP). Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Configure the new role as the default authenticated user role for the ldP. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Enable AWS IAM Identity Center (AWS Single Sign-On). Configure the directory as an external identity provider (ldP) that uses SAML. Use the automatic provisioning method. Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Attach the new role to all groups. Create a listener rule for the ALB. Specify the authenticate-cognito action for the listener rule.",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 372,
    "question": "A company has a website that serves many visitors. The company deploys a backend service for the website in a primary AWS Region and a disaster recovery (DR) Region.<br><br>A single Amazon CloudFront distribution is deployed for the website. The company creates an Amazon Route 53 record set with health checks and a failover routing policy for the primary Region’s backend service. The company configures the Route 53 record set as an origin for the CloudFront distribution. The company configures another record set that points to the backend service's endpoint in the DR Region as a secondary failover record type. The TTL for both record sets is 60 seconds.<br><br>Currently, failover takes more than 1 minute. A solutions architect must design a solution that will provide the fastest failover time.<br><br>Which solution will achieve this goal?",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy an additional CloudFront distribution. Create a new Route 53 failover record set with health checks for both CloudFront distributions.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Set the TTL to 4 second for the existing Route 53 record sets that are used for the backend service in each Region.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create new record sets for the backend services by using a latency routing policy. Use the record sets as an origin in the CloudFront distribution.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create a CloudFront origin group that includes two origins, one for each backend service Region. Configure origin failover as a cache behavior for the CloudFront distribution.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 373,
    "question": "A company is using multiple AWS accounts and has multiple DevOps teams running production and non-production workloads in these accounts. The company would like to centrally-restrict access to some of the AWS services that the DevOps teams do not use. The company decided to use AWS Organizations and successfully invited all AWS accounts into the Organization. They would like to allow access to services that are currently in-use and deny a few specific services. Also they would like to administer multiple accounts together as a single unit.<br><br>What combination of steps should the solutions architect take to satisfy these requirements? (Choose three.)",
    "answers": [
      {
        "id": 1,
        "answer": "Use a Deny list strategy.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Review the Access Advisor in AWS IAM to determine services recently used",
        "correct": true
      },
      {
        "id": 3,
        "answer": "Review the AWS Trusted Advisor report to determine services recently used.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Remove the default FullAWSAccess SCP.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Define organizational units (OUs) and place the member accounts in the OUs.",
        "correct": true
      },
      {
        "id": 6,
        "answer": "Remove the default DenyAWSAccess SCP.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2,
      5
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 374,
    "question": "A live-events company is designing a scaling solution for its ticket application on AWS. The application has high peaks of utilization during sale events. Each sale event is a one-time event that is scheduled. The application runs on Amazon EC2 instances that are in an Auto Scaling group. The application uses PostgreSQL for the database layer.<br><br>The company needs a scaling solution to maximize availability during the sale events.<br><br>Which solution will meet these requirements?",
    "answers": [
      {
        "id": 1,
        "answer": "Use a predictive scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Serverless v2 Multi-AZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL Mulli-AZ DB instance with automatically scaling read replicas. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger read replica before a sale event. Fail over to the larger read replica. Create another EventBridge rule that invokes another Lambda function to scale down the read replica after the sale event.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use a predictive scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL MultiAZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Multi-AZ DB cluster. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger Aurora Replica before a sale event. Fail over to the larger Aurora Replica. Create another EventBridge rule that invokes another Lambda function to scale down the Aurora Replica after the sale event.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "multiple": false,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 375,
    "question": "A company runs an intranet application on premises. The company wants to configure a cloud backup of the application. The company has selected AWS Elastic Disaster Recovery for this solution.<br><br>The company requires that replication traffic does not travel through the public internet. The application also must not be accessible from the internet. The company does not want this solution to consume all available network bandwidth because other applications require bandwidth.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "answers": [
      {
        "id": 1,
        "answer": "Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Create an AWS Site-to-Site VPN connection between the on-premises network and the target AWS network.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "During configuration of the replication servers, select the option to use private IP addresses for data replication.",
        "correct": true
      },
      {
        "id": 6,
        "answer": "During configuration of the launch settings for the target servers, select the option to ensure that the Recovery instance’s private IP address matches the source server's private IP address.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      4,
      5
    ],
    "multiple": true,
    "domain": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  }
]