[
  {
    "id": 1,
    "question": "<p>A data engineering team has deployed a microservice to the Amazon Elastic Container Service (Amazon ECS). The application layer is in a Docker container that provides both static and dynamic content through an Application Load Balancer. With increasing load, the Amazon ECS cluster is experiencing higher network usage. The team has looked into the network usage and found that 90% of it is due to distributing static content of the application.</p>\n\n<p>What do you recommend to improve the application's network usage and decrease costs?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Distribute the static content through Amazon EFS</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Distribute the dynamic content through Amazon EFS</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Distribute the static content through Amazon S3</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Distribute the dynamic content through Amazon S3</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Distribute the static content through Amazon S3</strong></p>\n\n<p>You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you must enable website hosting, set permissions, and create and add an index document. Depending on your website requirements, you can also configure redirects, web traffic logging, and a custom error document.</p>\n\n<p>Distributing the static content through Amazon S3 allows us to offload most of the network usage to Amazon S3 and free up our applications running on Amazon ECS.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Distribute the dynamic content through Amazon S3</strong> - By contrast, a dynamic website relies on server-side processing, including server-side scripts such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting, but AWS has other resources for hosting dynamic websites.</p>\n\n<p><strong>Distribute the static content through Amazon EFS</strong></p>\n\n<p><strong>Distribute the dynamic content through Amazon EFS</strong></p>\n\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Using Amazon EFS for static or dynamic content will not change anything as static content on EFS would still have to be distributed by the Amazon ECS instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Distribute the static content through Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you must enable website hosting, set permissions, and create and add an index document. Depending on your website requirements, you can also configure redirects, web traffic logging, and a custom error document."
      },
      {
        "answer": "",
        "explanation": "Distributing the static content through Amazon S3 allows us to offload most of the network usage to Amazon S3 and free up our applications running on Amazon ECS."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Distribute the dynamic content through Amazon S3</strong> - By contrast, a dynamic website relies on server-side processing, including server-side scripts such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting, but AWS has other resources for hosting dynamic websites."
      },
      {
        "answer": "",
        "explanation": "<strong>Distribute the static content through Amazon EFS</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Distribute the dynamic content through Amazon EFS</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Using Amazon EFS for static or dynamic content will not change anything as static content on EFS would still have to be distributed by the Amazon ECS instances."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html"
    ]
  },
  {
    "id": 2,
    "question": "<p>A company uses Amazon Redshift as their data warehouse service in the cloud. The performance of the Redshift cluster needs improvement and the company decided to scale read and write capacity to meet the user demand. The cluster runs on RA3 nodes.</p>\n\n<p>As a data engineer, which solution will you use to turn on the concurrency scaling of the cluster?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable workload manager (WLM) queue as a concurrency scaling queue. Set the <code>Concurrency Scaling mode</code> value to <code>auto</code></p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable Short query acceleration (SQA) to concurrently run queries and thereby improve the concurrency scaling of the cluster</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Re-configure the cluster to DC2 nodes and enable workload manager (WLM) queue as a concurrency scaling queue</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage custom parameter groups for the Amazon Redshift cluster to turn on the concurrency scaling of the cluster</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable workload manager (WLM) queue as a concurrency scaling queue. Set the <code>Concurrency Scaling mode</code> value to <code>auto</code></strong></p>\n\n<p>With the Concurrency Scaling feature, you can support thousands of concurrent users and concurrent queries, with consistently fast query performance. When you turn on concurrency scaling, Amazon Redshift automatically adds additional cluster capacity to process an increase in both read and write queries. Users see the most current data, whether the queries run on the main cluster or a concurrency-scaling cluster.</p>\n\n<p>You route queries to concurrency scaling clusters by enabling a workload manager (WLM) queue as a concurrency scaling queue. To turn on concurrency scaling for a queue, set the Concurrency Scaling mode value to auto.</p>\n\n<p>When the number of queries routed to a concurrency scaling queue exceeds the queue's configured concurrency, eligible queries are sent to the concurrency scaling cluster. When slots become available, queries are run on the main cluster. The number of queues is limited only by the number of queues permitted per cluster. As with any WLM queue, you route queries to a concurrency scaling queue based on user groups or by labeling queries with query group labels. You can also route queries by defining WLM query monitoring rules. For example, you might route all queries that take longer than 5 seconds to a concurrency scaling queue.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage custom parameter groups for the Amazon Redshift cluster to turn on the concurrency scaling of the cluster</strong> - In Amazon Redshift, you associate a parameter group with each cluster that you create. A parameter group is a group of parameters that apply to all of the databases that you create in the cluster. These parameters configure database settings such as query timeout and date style. Custom parameter groups cannot be used to turn on the concurrency scaling of the cluster.</p>\n\n<p><strong>Enable Short query acceleration (SQA) to concurrently run queries and thereby improve the concurrency scaling of the cluster</strong> - Short query acceleration (SQA) prioritizes selected short-running queries ahead of longer-running queries. SQA runs short-running queries in a dedicated space so that SQA queries aren't forced to wait in queues behind longer queries. If you enable SQA, you can reduce workload management (WLM) queues that are dedicated to running short queries. In addition, long-running queries don't need to contend with short queries for slots in a queue, so you can configure your WLM queues to use fewer query slots. SQA does not affect write operations, so this option is incorrect for the given use case.</p>\n\n<p><strong>Re-configure the cluster to DC2 nodes and enable workload manager (WLM) queue as a concurrency scaling queue</strong> - Amazon Redshift supports concurrency scaling for write operations on only Amazon Redshift RA3 nodes. Concurrency scaling for write operations isn't supported on other node types.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling-queues.html\">https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling-queues.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html\">https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable workload manager (WLM) queue as a concurrency scaling queue. Set the <code>Concurrency Scaling mode</code> value to <code>auto</code></strong>"
      },
      {
        "answer": "",
        "explanation": "With the Concurrency Scaling feature, you can support thousands of concurrent users and concurrent queries, with consistently fast query performance. When you turn on concurrency scaling, Amazon Redshift automatically adds additional cluster capacity to process an increase in both read and write queries. Users see the most current data, whether the queries run on the main cluster or a concurrency-scaling cluster."
      },
      {
        "answer": "",
        "explanation": "You route queries to concurrency scaling clusters by enabling a workload manager (WLM) queue as a concurrency scaling queue. To turn on concurrency scaling for a queue, set the Concurrency Scaling mode value to auto."
      },
      {
        "answer": "",
        "explanation": "When the number of queries routed to a concurrency scaling queue exceeds the queue's configured concurrency, eligible queries are sent to the concurrency scaling cluster. When slots become available, queries are run on the main cluster. The number of queues is limited only by the number of queues permitted per cluster. As with any WLM queue, you route queries to a concurrency scaling queue based on user groups or by labeling queries with query group labels. You can also route queries by defining WLM query monitoring rules. For example, you might route all queries that take longer than 5 seconds to a concurrency scaling queue."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage custom parameter groups for the Amazon Redshift cluster to turn on the concurrency scaling of the cluster</strong> - In Amazon Redshift, you associate a parameter group with each cluster that you create. A parameter group is a group of parameters that apply to all of the databases that you create in the cluster. These parameters configure database settings such as query timeout and date style. Custom parameter groups cannot be used to turn on the concurrency scaling of the cluster."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable Short query acceleration (SQA) to concurrently run queries and thereby improve the concurrency scaling of the cluster</strong> - Short query acceleration (SQA) prioritizes selected short-running queries ahead of longer-running queries. SQA runs short-running queries in a dedicated space so that SQA queries aren't forced to wait in queues behind longer queries. If you enable SQA, you can reduce workload management (WLM) queues that are dedicated to running short queries. In addition, long-running queries don't need to contend with short queries for slots in a queue, so you can configure your WLM queues to use fewer query slots. SQA does not affect write operations, so this option is incorrect for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Re-configure the cluster to DC2 nodes and enable workload manager (WLM) queue as a concurrency scaling queue</strong> - Amazon Redshift supports concurrency scaling for write operations on only Amazon Redshift RA3 nodes. Concurrency scaling for write operations isn't supported on other node types."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling-queues.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html"
    ]
  },
  {
    "id": 3,
    "question": "<p>A company is transitioning its database servers from Amazon EC2 instances running Microsoft SQL Server to Amazon RDS for Microsoft SQL Server DB instances. During this migration, the data engineering team needs to export this data, which is derived from SQL joins across multiple tables, using a daily schedule. The migrated data should be stored in Apache Parquet format on Amazon S3.</p>\n\n<p>What is the most operationally efficient solution to meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Develop an AWS Lambda function that utilizes Java Database Connectivity (JDBC) to query databases hosted on EC2 instances. Set up this Lambda function to fetch the necessary data, convert it into Parquet format, and upload it to an S3 bucket. Set up a daily schedule via Amazon EventBridge to trigger the Lambda function</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Construct a view within the SQL Server databases on the EC2 instances that includes the essential data elements. Then, set up an AWS Glue job to fetch the data directly from this view and transfer it to an S3 bucket in Parquet format. Ensure this AWS Glue job is scheduled to execute daily</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a SQL query on the SQL Server database hosted on the EC2 instances to establish a view containing the necessary data elements. Then, configure an AWS Glue crawler to access and read this view. Set up an AWS Glue job to extract the data and convert it into Parquet format before transferring it to an S3 bucket. Configure this AWS Glue job to execute daily</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up the SQL Server Agent to execute a daily SQL query on the SQL Server databases hosted on the EC2 instances, extracting the specified data elements. Configure the query to output the results as .csv files directly into an S3 bucket. Additionally, establish an S3 event trigger to activate an AWS Lambda function, which will convert these .csv files into the Parquet format</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a SQL query on the SQL Server database hosted on the EC2 instances to establish a view containing the necessary data elements. Then, configure an AWS Glue crawler to access and read this view. Set up an AWS Glue job to extract the data and convert it into Parquet format before transferring it to an S3 bucket. Configure this AWS Glue job to execute daily</strong></p>\n\n<p>You need to set up a view containing the necessary data elements by using a SQL query on the SQL Server database, like so:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q4-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q4-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/\">https://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/</a><p></p>\n\n<p>Then, you can configure an AWS Glue crawler to access and read this view. Run the crawler to hydrate the AWS Glue Data Catalog table, which is subsequently used in the AWS Glue job as the source table for extracting data from SQL Server.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q4-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q4-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/\">https://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/</a><p></p>\n\n<p>Finally, you can create an AWS Glue job, that runs on a daily schedule, to extract the data from this view (accessible via the AWS Glue Catalog) and convert it into Parquet format while writing the output to an S3 bucket.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Construct a view within the SQL Server databases on the EC2 instances that includes the essential data elements. Then, set up an AWS Glue job to fetch the data directly from this view and transfer it to an S3 bucket in Parquet format. Ensure this AWS Glue job is scheduled to execute daily</strong> - This option does not use a Glue Crawler access and read the view data. Using a Glue crawler avoids the need to manage the view's metadata manually on each run. So, this option is operationally inefficient.</p>\n\n<p><strong>Set up the SQL Server Agent to execute a daily SQL query on the SQL Server databases hosted on the EC2 instances, extracting the specified data elements. Configure the query to output the results as .csv files directly into an S3 bucket. Additionally, establish an S3 event trigger to activate an AWS Lambda function, which will convert these .csv files into the Parquet format</strong> - This option is operationally inefficient as it adopts a two-step approach, by initially creating CSV files and later re-writing to Parquet format via a Lambda function.</p>\n\n<p><strong>Develop an AWS Lambda function that utilizes Java Database Connectivity (JDBC) to query databases hosted on EC2 instances. Set up this Lambda function to fetch the necessary data, convert it into Parquet format, and upload it to an S3 bucket. Set up a daily schedule via Amazon EventBridge to trigger the Lambda function</strong> - This option requires significant coding effort to fetch the necessary data via the Lambda function, so it is operationally inefficient.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/\">https://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a SQL query on the SQL Server database hosted on the EC2 instances to establish a view containing the necessary data elements. Then, configure an AWS Glue crawler to access and read this view. Set up an AWS Glue job to extract the data and convert it into Parquet format before transferring it to an S3 bucket. Configure this AWS Glue job to execute daily</strong>"
      },
      {
        "answer": "",
        "explanation": "You need to set up a view containing the necessary data elements by using a SQL query on the SQL Server database, like so:"
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/"
      },
      {
        "answer": "",
        "explanation": "Then, you can configure an AWS Glue crawler to access and read this view. Run the crawler to hydrate the AWS Glue Data Catalog table, which is subsequently used in the AWS Glue job as the source table for extracting data from SQL Server."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/"
      },
      {
        "answer": "",
        "explanation": "Finally, you can create an AWS Glue job, that runs on a daily schedule, to extract the data from this view (accessible via the AWS Glue Catalog) and convert it into Parquet format while writing the output to an S3 bucket."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Construct a view within the SQL Server databases on the EC2 instances that includes the essential data elements. Then, set up an AWS Glue job to fetch the data directly from this view and transfer it to an S3 bucket in Parquet format. Ensure this AWS Glue job is scheduled to execute daily</strong> - This option does not use a Glue Crawler access and read the view data. Using a Glue crawler avoids the need to manage the view's metadata manually on each run. So, this option is operationally inefficient."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up the SQL Server Agent to execute a daily SQL query on the SQL Server databases hosted on the EC2 instances, extracting the specified data elements. Configure the query to output the results as .csv files directly into an S3 bucket. Additionally, establish an S3 event trigger to activate an AWS Lambda function, which will convert these .csv files into the Parquet format</strong> - This option is operationally inefficient as it adopts a two-step approach, by initially creating CSV files and later re-writing to Parquet format via a Lambda function."
      },
      {
        "answer": "",
        "explanation": "<strong>Develop an AWS Lambda function that utilizes Java Database Connectivity (JDBC) to query databases hosted on EC2 instances. Set up this Lambda function to fetch the necessary data, convert it into Parquet format, and upload it to an S3 bucket. Set up a daily schedule via Amazon EventBridge to trigger the Lambda function</strong> - This option requires significant coding effort to fetch the necessary data via the Lambda function, so it is operationally inefficient."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/"
    ]
  },
  {
    "id": 4,
    "question": "<p>A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible.</p>\n\n<p>Can you suggest a way to lower the storage costs while fulfilling the business requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days</strong></p>\n\n<p>Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single Availability Zone (AZ) and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed and re-creatable data but do not require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. The minimum storage duration is 30 days before you can transition objects from Amazon S3 Standard to Amazon S3 One Zone-IA.</p>\n\n<p>Amazon S3 One Zone-IA offers the same high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. S3 Storage Classes can be configured at the object level, and a single bucket can contain objects stored across Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One Zone-IA. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes.</p>\n\n<p>Constraints for Lifecycle storage class transitions:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q42-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q42-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a><p></p>\n\n<p>Supported Amazon S3 lifecycle transitions:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q42-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q42-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days</strong></p>\n\n<p><strong>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days</strong></p>\n\n<p>As mentioned earlier, the minimum storage duration is 30 days before you can transition objects from Amazon S3 Standard to Amazon S3 One Zone-IA or Amazon S3 Standard-IA, so both these options are added as distractors.</p>\n\n<p><strong>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days</strong> - Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. However, it costs more than Amazon S3 One Zone-IA because of the redundant storage across Availability Zones (AZs). As the data is re-creatable, so you don't need to incur this additional cost.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single Availability Zone (AZ) and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed and re-creatable data but do not require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. The minimum storage duration is 30 days before you can transition objects from Amazon S3 Standard to Amazon S3 One Zone-IA."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 One Zone-IA offers the same high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. S3 Storage Classes can be configured at the object level, and a single bucket can contain objects stored across Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One Zone-IA. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q42-i1.jpg",
        "answer": "",
        "explanation": "Constraints for Lifecycle storage class transitions:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q42-i2.jpg",
        "answer": "",
        "explanation": "Supported Amazon S3 lifecycle transitions:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days</strong>"
      },
      {
        "answer": "",
        "explanation": "As mentioned earlier, the minimum storage duration is 30 days before you can transition objects from Amazon S3 Standard to Amazon S3 One Zone-IA or Amazon S3 Standard-IA, so both these options are added as distractors."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days</strong> - Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. However, it costs more than Amazon S3 One Zone-IA because of the redundant storage across Availability Zones (AZs). As the data is re-creatable, so you don't need to incur this additional cost."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html",
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "id": 5,
    "question": "<p>A financial services company stores confidential data on an Amazon Simple Storage Service (S3) bucket. The compliance guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage the encryption keys.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Server-side encryption with customer-provided keys (SSE-C)</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Client Side Encryption</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Server-side encryption with AWS KMS keys (SSE-KMS)</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Server-side encryption with Amazon S3 managed keys (SSE-S3)</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Server-side encryption with Amazon S3 managed keys (SSE-S3)</strong></p>\n\n<p>Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. There are no additional fees for using server-side encryption with Amazon S3-managed keys (SSE-S3). By default, Amazon S3 encrypts all objects using SSE-S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Server-side encryption with customer-provided keys (SSE-C)</strong> - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects.</p>\n\n<p><strong>Client Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>Server-side encryption with AWS KMS keys (SSE-KMS)</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself. Although SSE-KMS provides an option where AWS manages the encryption key on your behalf, however, this entails a usage fee for the KMS key. So this option is not the best fit for the given use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Server-side encryption with Amazon S3 managed keys (SSE-S3)</strong>"
      },
      {
        "answer": "",
        "explanation": "Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. There are no additional fees for using server-side encryption with Amazon S3-managed keys (SSE-S3). By default, Amazon S3 encrypts all objects using SSE-S3."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Server-side encryption with customer-provided keys (SSE-C)</strong> - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects."
      },
      {
        "answer": "",
        "explanation": "<strong>Client Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools."
      },
      {
        "answer": "",
        "explanation": "<strong>Server-side encryption with AWS KMS keys (SSE-KMS)</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself. Although SSE-KMS provides an option where AWS manages the encryption key on your behalf, however, this entails a usage fee for the KMS key. So this option is not the best fit for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html"
    ]
  },
  {
    "id": 6,
    "question": "<p>A data analytics job requires data from multiple sources like Amazon DynamoDB, Amazon RDS, and Amazon Redshift. The job is run on Amazon Athena.</p>\n\n<p>Which of the following is the MOST cost-effective way to join data from these sources?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Develop an AWS Glue job using Apache Spark to join the data from all the sources</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Athena Federated Query to join the data from all data sources</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Copy the data from all the sources into a single S3 bucket. Use Athena queries on the saved S3 data</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Provision an EMR cluster to join the data from all the sources. Configure Spark for Athena to run the data analysis job</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Athena Federated Query to join the data from all data sources</strong></p>\n\n<p>If you have data in sources other than Amazon S3, you can use Athena Federated Query to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data stored in relational, non-relational, object, and custom data sources.</p>\n\n<p>Athena uses data source connectors that run on AWS Lambda to run federated queries. A data source connector is a piece of code that can translate between your target data source and Athena. You can think of a connector as an extension of Athena's query engine. Prebuilt Athena data source connectors exist for data sources like Amazon CloudWatch Logs, Amazon DynamoDB, Amazon DocumentDB, and Amazon RDS, and JDBC-compliant relational data sources such MySQL, and PostgreSQL</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Copy the data from all the sources into a single S3 bucket. Use Athena queries on the saved S3 data</strong> - Copying data from all sources into Amazon S3 results in unnecessary storage costs on S3, therefore, this option is incorrect.</p>\n\n<p><strong>Develop an AWS Glue job using Apache Spark to join the data from all the sources</strong> - You can certainly leverage an AWS Glue job using Apache Spark to perform data analytics on these sources, however, this solution is not cost-effective.</p>\n\n<p><strong>Provision an EMR cluster to join the data from all the sources. Configure Spark for Athena to run the data analysis job</strong> - Provisioning and setting up an EMR cluster is time-consuming and it is also not cost-effective for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html\">https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/what-is.html\">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Athena Federated Query to join the data from all data sources</strong>"
      },
      {
        "answer": "",
        "explanation": "If you have data in sources other than Amazon S3, you can use Athena Federated Query to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data stored in relational, non-relational, object, and custom data sources."
      },
      {
        "answer": "",
        "explanation": "Athena uses data source connectors that run on AWS Lambda to run federated queries. A data source connector is a piece of code that can translate between your target data source and Athena. You can think of a connector as an extension of Athena's query engine. Prebuilt Athena data source connectors exist for data sources like Amazon CloudWatch Logs, Amazon DynamoDB, Amazon DocumentDB, and Amazon RDS, and JDBC-compliant relational data sources such MySQL, and PostgreSQL"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Copy the data from all the sources into a single S3 bucket. Use Athena queries on the saved S3 data</strong> - Copying data from all sources into Amazon S3 results in unnecessary storage costs on S3, therefore, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Develop an AWS Glue job using Apache Spark to join the data from all the sources</strong> - You can certainly leverage an AWS Glue job using Apache Spark to perform data analytics on these sources, however, this solution is not cost-effective."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision an EMR cluster to join the data from all the sources. Configure Spark for Athena to run the data analysis job</strong> - Provisioning and setting up an EMR cluster is time-consuming and it is also not cost-effective for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html",
      "https://docs.aws.amazon.com/athena/latest/ug/what-is.html"
    ]
  },
  {
    "id": 7,
    "question": "<p>A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures.</p>\n\n<p>Which of the following AWS services should be combined to handle this use case? (Select two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Snowball Edge</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Glue</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Schema Conversion Tool (AWS SCT)</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>AWS Database Migration Service (AWS DMS)</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Basic Schema Copy</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>AWS Schema Conversion Tool (AWS SCT)</strong></p>\n\n<p><strong>AWS Database Migration Service (AWS DMS)</strong></p>\n\n<p>AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. AWS Database Migration Service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora.</p>\n\n<p>Given the use case where the CTO at the company wants to move away from license-based, expensive, legacy commercial database solutions deployed at the on-premises data center to more efficient, open-source, and cost-effective options on AWS Cloud, this is an example of heterogeneous database migrations.</p>\n\n<p>For such a scenario, the source and target database engines are different, like in the case of Oracle to Amazon Aurora, Oracle to PostgreSQL, or Microsoft SQL Server to MySQL migrations. In this case, the schema structure, data types, and database code of source and target databases can be quite different, requiring a schema and code transformation before the data migration starts.</p>\n\n<p>That makes heterogeneous migrations a two-step process. First use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database. All the required data type conversions will automatically be done by the AWS Database Migration Service during the migration. The source database can be located on your on-premises environment outside of AWS, running on an Amazon EC2 instance, or it can be an Amazon RDS database. The target can be a database in Amazon EC2 or Amazon RDS.</p>\n\n<p>Heterogeneous Database Migrations:\n<img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram_AWS-DMS_heterogeneous-database-migrations-2.3616bac30ab86d4310ddadfdec5d6e6ba4d8b81d.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram_AWS-DMS_heterogeneous-database-migrations-2.3616bac30ab86d4310ddadfdec5d6e6ba4d8b81d.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Snowball Edge</strong> - AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large-scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80TB of data, you can order 10 such devices to take care of the data transfer for all applications. The original Snowball devices were transitioned out of service and AWS Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space. AWS Snowball Edge cannot be used for database migrations.</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. Therefore, it cannot be used for database migrations.</p>\n\n<p><strong>Basic Schema Copy</strong> - To quickly migrate a database schema to your target instance you can rely on the Basic Schema Copy feature of AWS Database Migration Service. Basic Schema Copy will automatically create tables and primary keys in the target instance if the target does not already contain tables with the same names. Basic Schema Copy is great for doing a test migration, or when you are migrating databases heterogeneously e.g. Oracle to MySQL or SQL Server to Oracle. Basic Schema Copy will not migrate secondary indexes, foreign keys or stored procedures. When you need to use a more customizable schema migration process (e.g. when you are migrating your production database and need to move your stored procedures and secondary database objects), you must use the AWS Schema Conversion Tool.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dms/faqs/\">https://aws.amazon.com/dms/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dms/schema-conversion-tool/\">https://aws.amazon.com/dms/schema-conversion-tool/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Schema Conversion Tool (AWS SCT)</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Database Migration Service (AWS DMS)</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. AWS Database Migration Service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora."
      },
      {
        "answer": "",
        "explanation": "Given the use case where the CTO at the company wants to move away from license-based, expensive, legacy commercial database solutions deployed at the on-premises data center to more efficient, open-source, and cost-effective options on AWS Cloud, this is an example of heterogeneous database migrations."
      },
      {
        "answer": "",
        "explanation": "For such a scenario, the source and target database engines are different, like in the case of Oracle to Amazon Aurora, Oracle to PostgreSQL, or Microsoft SQL Server to MySQL migrations. In this case, the schema structure, data types, and database code of source and target databases can be quite different, requiring a schema and code transformation before the data migration starts."
      },
      {
        "answer": "",
        "explanation": "That makes heterogeneous migrations a two-step process. First use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database. All the required data type conversions will automatically be done by the AWS Database Migration Service during the migration. The source database can be located on your on-premises environment outside of AWS, running on an Amazon EC2 instance, or it can be an Amazon RDS database. The target can be a database in Amazon EC2 or Amazon RDS."
      },
      {
        "image": "https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram_AWS-DMS_heterogeneous-database-migrations-2.3616bac30ab86d4310ddadfdec5d6e6ba4d8b81d.png",
        "answer": "",
        "explanation": "Heterogeneous Database Migrations:"
      },
      {
        "link": "https://aws.amazon.com/dms/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Snowball Edge</strong> - AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large-scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80TB of data, you can order 10 such devices to take care of the data transfer for all applications. The original Snowball devices were transitioned out of service and AWS Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space. AWS Snowball Edge cannot be used for database migrations."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Glue</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. Therefore, it cannot be used for database migrations."
      },
      {
        "answer": "",
        "explanation": "<strong>Basic Schema Copy</strong> - To quickly migrate a database schema to your target instance you can rely on the Basic Schema Copy feature of AWS Database Migration Service. Basic Schema Copy will automatically create tables and primary keys in the target instance if the target does not already contain tables with the same names. Basic Schema Copy is great for doing a test migration, or when you are migrating databases heterogeneously e.g. Oracle to MySQL or SQL Server to Oracle. Basic Schema Copy will not migrate secondary indexes, foreign keys or stored procedures. When you need to use a more customizable schema migration process (e.g. when you are migrating your production database and need to move your stored procedures and secondary database objects), you must use the AWS Schema Conversion Tool."
      }
    ],
    "references": [
      "https://aws.amazon.com/dms/",
      "https://aws.amazon.com/dms/faqs/",
      "https://aws.amazon.com/dms/schema-conversion-tool/"
    ]
  },
  {
    "id": 8,
    "question": "<p>The data engineering team at a company wants to analyze Amazon S3 storage access patterns to decide when to transition the right data to the right storage class.</p>\n\n<p>Which of the following represents a correct option regarding the capabilities of Amazon S3 Analytics storage class analysis?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Storage class analysis only provides recommendations for Standard to Standard IA classes</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Storage class analysis only provides recommendations for Standard to Standard IA classes</strong></p>\n\n<p>By using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class.</p>\n\n<p>Storage class analysis only provides recommendations for Standard to Standard IA classes.</p>\n\n<p>After storage class analysis observes the infrequent access patterns of a filtered set of data over a period of time, you can use the analysis results to help you improve your lifecycle configurations. You can configure storage class analysis to analyze all the objects in a bucket. Or, you can configure filters to group objects together for analysis by common prefix (that is, objects that have names that begin with a common string), by object tags, or by both prefix and tags.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes</strong></p>\n\n<p><strong>Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes</strong></p>\n\n<p><strong>Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Storage class analysis only provides recommendations for Standard to Standard IA classes</strong>"
      },
      {
        "answer": "",
        "explanation": "By using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class."
      },
      {
        "answer": "",
        "explanation": "Storage class analysis only provides recommendations for Standard to Standard IA classes."
      },
      {
        "answer": "",
        "explanation": "After storage class analysis observes the infrequent access patterns of a filtered set of data over a period of time, you can use the analysis results to help you improve your lifecycle configurations. You can configure storage class analysis to analyze all the objects in a bucket. Or, you can configure filters to group objects together for analysis by common prefix (that is, objects that have names that begin with a common string), by object tags, or by both prefix and tags."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html"
    ]
  },
  {
    "id": 9,
    "question": "<p>An e-commerce company runs its workloads on Amazon EMR clusters. The data engineering team at the company manually installs third-party libraries on the newly launched clusters by logging onto the master nodes. The team wants to develop an automated solution that will replace this human intervention.</p>\n\n<p>Which of the following options would you recommend for the given requirement? (Select two)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Upload the required installation scripts in Amazon S3 and execute them using AWS EMR CLI</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Provision an Amazon EC2 instance with Amazon Linux and install the required third-party libraries on the instance and then use this EC2 instance to launch the EMR cluster</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Provision an Amazon EC2 instance with Amazon Linux and install the required third-party libraries on the instance. Create an AMI using this EC2 instance and then use this AMI to launch the EMR cluster</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Upload the required installation scripts in DynamoDB and use a Lambda function to execute these scripts for installing the third-party libraries on the EMR cluster</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Upload the required installation scripts in Amazon S3 and execute them using custom bootstrap actions</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Upload the required installation scripts in Amazon S3 and execute them using custom bootstrap actions</strong></p>\n\n<p>You can use a bootstrap action to install additional software or customize the configuration of the EMR cluster instances. Bootstrap actions are scripts that run on the cluster after Amazon EMR launches the instance using the Amazon Linux Amazon Machine Image (AMI). Bootstrap actions run before Amazon EMR installs the applications that you specify when you create the cluster and before cluster nodes begin processing data.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q25-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q25-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html</a><p></p>\n\n<p><strong>Provision an Amazon EC2 instance with Amazon Linux and install the required third-party libraries on the instance. Create an AMI using this EC2 instance and then use this AMI to launch the EMR cluster</strong></p>\n\n<p>You can create Amazon EMR clusters that have custom Amazon Machine Images (AMI) running Amazon Linux. You can create the AMI from an EC2 instance running Amazon Linux. Make sure that you have installed all the required third-party libraries on this EC2 instance. This allows you to preload additional software on your AMI and use these AMIs to launch your EMR clusters.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload the required installation scripts in DynamoDB and use a Lambda function to execute these scripts for installing the third-party libraries on the EMR cluster</strong> - This option has been added as a distractor. You can only load installation scripts from Amazon S3 for custom bootstrap actions on the EMR cluster.</p>\n\n<p><strong>Provision an Amazon EC2 instance with Amazon Linux and install the required third-party libraries on the instance and then use this EC2 instance to launch the EMR cluster</strong> - You need to use an AMI to launch the EMR cluster. You cannot directly use an EC2 instance to launch an EMR cluster.</p>\n\n<p><strong>Upload the required installation scripts in Amazon S3 and execute them using AWS EMR CLI</strong> - You can automate the installation of libraries by executing the installation scripts on S3 via custom bootstrap actions. You cannot replace custom bootstrap actions with AWS EMR CLI for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-emr-now-supports-launching-clusters-with-custom-amazon-linux-amis/\">https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-emr-now-supports-launching-clusters-with-custom-amazon-linux-amis/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Upload the required installation scripts in Amazon S3 and execute them using custom bootstrap actions</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use a bootstrap action to install additional software or customize the configuration of the EMR cluster instances. Bootstrap actions are scripts that run on the cluster after Amazon EMR launches the instance using the Amazon Linux Amazon Machine Image (AMI). Bootstrap actions run before Amazon EMR installs the applications that you specify when you create the cluster and before cluster nodes begin processing data."
      },
      {
        "link": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Provision an Amazon EC2 instance with Amazon Linux and install the required third-party libraries on the instance. Create an AMI using this EC2 instance and then use this AMI to launch the EMR cluster</strong>"
      },
      {
        "answer": "",
        "explanation": "You can create Amazon EMR clusters that have custom Amazon Machine Images (AMI) running Amazon Linux. You can create the AMI from an EC2 instance running Amazon Linux. Make sure that you have installed all the required third-party libraries on this EC2 instance. This allows you to preload additional software on your AMI and use these AMIs to launch your EMR clusters."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Upload the required installation scripts in DynamoDB and use a Lambda function to execute these scripts for installing the third-party libraries on the EMR cluster</strong> - This option has been added as a distractor. You can only load installation scripts from Amazon S3 for custom bootstrap actions on the EMR cluster."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision an Amazon EC2 instance with Amazon Linux and install the required third-party libraries on the instance and then use this EC2 instance to launch the EMR cluster</strong> - You need to use an AMI to launch the EMR cluster. You cannot directly use an EC2 instance to launch an EMR cluster."
      },
      {
        "answer": "",
        "explanation": "<strong>Upload the required installation scripts in Amazon S3 and execute them using AWS EMR CLI</strong> - You can automate the installation of libraries by executing the installation scripts on S3 via custom bootstrap actions. You cannot replace custom bootstrap actions with AWS EMR CLI for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html",
      "https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-emr-now-supports-launching-clusters-with-custom-amazon-linux-amis/"
    ]
  },
  {
    "id": 10,
    "question": "<p>A company makes use of multiple AWS Lambda functions for implementing various business requirements. These Lambda functions use code from common custom Python scripts that are also maintained by a data engineer along with the Lambda functions. The data engineer is looking for a solution to reduce the operational/maintenance work of updating the code in the Lambda functions when a change has to be made in the scripts.</p>\n\n<p>What is the most efficient way of implementing this change ?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Package the Lambda functions as container images and add Lambda layers to the function configuration</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Package the custom Python scripts into Lambda ephemeral storage. Share these scripts via the ephemeral storage across all Lambda functions</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Package the custom Python scripts into Lambda layers. Apply the Lambda layers to all the AWS Lambda functions using the scripts</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Lambda Extensions to add the common custom Python scripts to all the Lambda functions</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Package the custom Python scripts into Lambda layers. Apply the Lambda layers to all the AWS Lambda functions using the scripts</strong></p>\n\n<p>A Lambda layer is a .zip file archive that contains supplementary code or data. Layers usually contain library dependencies, a custom runtime, or configuration files. There are multiple reasons why you might consider using layers: To reduce the size of your deployment packages, To separate core function logic from dependencies, To share dependencies across multiple functions, and To use the Lambda console code editor.</p>\n\n<p>You can include up to five layers per function. Also, you can use layers only with Lambda functions deployed as a .zip file archive. For functions defined as a container image, package your preferred runtime and all code dependencies when you create the container image.</p>\n\n<p>Working with Lambda layers:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q12-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q12-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html\">https://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Lambda Extensions to add the common custom Python scripts to all the Lambda functions</strong> - You can use Lambda extensions to augment your Lambda functions. For example, use Lambda extensions to integrate functions with your preferred monitoring, observability, security, and governance tools. Lambda Extensions cannot be used to include scripts that are common across several Lambda functions.</p>\n\n<p><strong>Package the Lambda functions as container images and add Lambda layers to the function configuration</strong> - Lambda functions packaged as container images do not support adding Lambda layers to the function configuration.</p>\n\n<p><strong>Package the custom Python scripts into Lambda ephemeral storage. Share these scripts via the ephemeral storage across all Lambda functions</strong> - AWS Lambda allows you to configure ephemeral storage (/tmp) between 512 MB and 10,240 MB. You can control the amount of ephemeral storage a function gets for reading or writing data, allowing you to use AWS Lambda for ETL jobs, ML inference, or other data-intensive workloads. Ephemeral storage is specific to a Lambda function and cannot be shared across multiple Lambda functions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html\">https://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/\">https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Package the custom Python scripts into Lambda layers. Apply the Lambda layers to all the AWS Lambda functions using the scripts</strong>"
      },
      {
        "answer": "",
        "explanation": "A Lambda layer is a .zip file archive that contains supplementary code or data. Layers usually contain library dependencies, a custom runtime, or configuration files. There are multiple reasons why you might consider using layers: To reduce the size of your deployment packages, To separate core function logic from dependencies, To share dependencies across multiple functions, and To use the Lambda console code editor."
      },
      {
        "answer": "",
        "explanation": "You can include up to five layers per function. Also, you can use layers only with Lambda functions deployed as a .zip file archive. For functions defined as a container image, package your preferred runtime and all code dependencies when you create the container image."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q12-i1.jpg",
        "answer": "",
        "explanation": "Working with Lambda layers:"
      },
      {
        "link": "https://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Lambda Extensions to add the common custom Python scripts to all the Lambda functions</strong> - You can use Lambda extensions to augment your Lambda functions. For example, use Lambda extensions to integrate functions with your preferred monitoring, observability, security, and governance tools. Lambda Extensions cannot be used to include scripts that are common across several Lambda functions."
      },
      {
        "answer": "",
        "explanation": "<strong>Package the Lambda functions as container images and add Lambda layers to the function configuration</strong> - Lambda functions packaged as container images do not support adding Lambda layers to the function configuration."
      },
      {
        "answer": "",
        "explanation": "<strong>Package the custom Python scripts into Lambda ephemeral storage. Share these scripts via the ephemeral storage across all Lambda functions</strong> - AWS Lambda allows you to configure ephemeral storage (/tmp) between 512 MB and 10,240 MB. You can control the amount of ephemeral storage a function gets for reading or writing data, allowing you to use AWS Lambda for ETL jobs, ML inference, or other data-intensive workloads. Ephemeral storage is specific to a Lambda function and cannot be shared across multiple Lambda functions."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html",
      "https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/"
    ]
  },
  {
    "id": 11,
    "question": "<p>A company has noticed that its Amazon EBS Elastic Volume (io1) accounts for 90% of the cost and the remaining 10% cost can be attributed to the Amazon EC2 instance. The Amazon CloudWatch metrics report that both the Amazon EC2 instance and the Amazon EBS volume are under-utilized. The Amazon CloudWatch metrics also show that the Amazon EBS volume has occasional I/O bursts. The entire infrastructure is managed by AWS CloudFormation.</p>\n\n<p>What do you propose to reduce the costs?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Convert the Amazon EC2 instance EBS volume to gp2</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Don't use an AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Change the Amazon EC2 instance type to something much smaller</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Keep the Amazon EBS volume to io1 and reduce the IOPS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p>Amazon EBS provides various volume types, that differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications. The volume types fall into two categories:</p>\n\n<p>SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS.</p>\n\n<p>HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS</p>\n\n<p>Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.</p>\n\n<p><strong>Convert the Amazon EC2 instance EBS volume to gp2</strong></p>\n\n<p>General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for an extended duration. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver a provisioned performance of 99% uptime. A gp2 volume can range in size from 1 GiB to 16 TiB.</p>\n\n<p>Therefore, gp2 is the right choice as it is more cost-effective than io1, and it also allows a burst in performance when needed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Keep the Amazon EBS volume to io1 and reduce the IOPS</strong> - Keeping the Amazon EBS volume to io1 and reducing the IOPS may interfere with the burst of performance we need, so this option is ruled out.</p>\n\n<p><strong>Change the Amazon EC2 instance type to something much smaller</strong> - Changing the Amazon EC2 instance type to something much smaller won't affect 90% of the costs that are incurred, therefore this option is also incorrect.</p>\n\n<p><strong>Don't use an AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges</strong> - This statement is incorrect as AWS CloudFormation is a free service. The resources that are invoked by CloudFormation are charged as per their utilization rates, but using AWS CloudFormation will not cost anything.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon EBS provides various volume types, that differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications. The volume types fall into two categories:"
      },
      {
        "answer": "",
        "explanation": "SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS."
      },
      {
        "answer": "",
        "explanation": "HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS"
      },
      {
        "answer": "",
        "explanation": "Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time."
      },
      {
        "answer": "",
        "explanation": "<strong>Convert the Amazon EC2 instance EBS volume to gp2</strong>"
      },
      {
        "answer": "",
        "explanation": "General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for an extended duration. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver a provisioned performance of 99% uptime. A gp2 volume can range in size from 1 GiB to 16 TiB."
      },
      {
        "answer": "",
        "explanation": "Therefore, gp2 is the right choice as it is more cost-effective than io1, and it also allows a burst in performance when needed."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Keep the Amazon EBS volume to io1 and reduce the IOPS</strong> - Keeping the Amazon EBS volume to io1 and reducing the IOPS may interfere with the burst of performance we need, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the Amazon EC2 instance type to something much smaller</strong> - Changing the Amazon EC2 instance type to something much smaller won't affect 90% of the costs that are incurred, therefore this option is also incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Don't use an AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges</strong> - This statement is incorrect as AWS CloudFormation is a free service. The resources that are invoked by CloudFormation are charged as per their utilization rates, but using AWS CloudFormation will not cost anything."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops"
    ]
  },
  {
    "id": 12,
    "question": "<p>A CRM company has a software as a service (SaaS) application that feeds updates to other in-house and third-party applications. The SaaS application and the in-house applications are being migrated to use AWS services for this inter-application communication.</p>\n\n<p>Which of the following would you suggest to asynchronously decouple the architecture?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Elastic Load Balancing (ELB) for effective decoupling of system architecture</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon EventBridge to decouple the system architecture</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon EventBridge to decouple the system architecture</strong></p>\n\n<p>Both Amazon EventBridge and Amazon SNS can be used to develop event-driven applications, but for this use case, EventBridge is the right fit.</p>\n\n<p>Amazon EventBridge is recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services. Amazon EventBridge is the only event-based service that integrates directly with third-party SaaS partners. Amazon EventBridge also automatically ingests events from over 90 AWS services without requiring developers to create any resources in their accounts. Further, Amazon EventBridge uses a defined JSON-based structure for events and allows you to create rules that are applied across the entire event body to select events to forward to a target. Amazon EventBridge currently supports over 15 AWS services as targets, including AWS Lambda, Amazon SQS, Amazon SNS, Amazon Kinesis Streams, and Firehose, among others. At launch, Amazon EventBridge has limited throughput (see Service Limits) which can be increased upon request, and typical latency of around half a second.</p>\n\n<p>How Amazon EventBridge works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q50-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q50-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/eventbridge/\">https://aws.amazon.com/eventbridge/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture</strong> - As discussed above, Amazon SNS can be used for event-based services. However, the given use case needs integration with third-party SaaS services, hence Amazon EventBridge is the right choice, as Amazon SNS does not support third-party services integration.</p>\n\n<p><strong>Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture</strong> - Amazon SQS is a message queuing service from Amazon and works well for decoupling applications. It does not directly integrate with third-party SaaS services.</p>\n\n<p><strong>Use Elastic Load Balancing (ELB) for effective decoupling of system architecture</strong> - Elastic Load Balancing (ELB) offers a synchronous decoupling of applications, which is not the right fit for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/eventbridge/\">https://aws.amazon.com/eventbridge/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EventBridge to decouple the system architecture</strong>"
      },
      {
        "answer": "",
        "explanation": "Both Amazon EventBridge and Amazon SNS can be used to develop event-driven applications, but for this use case, EventBridge is the right fit."
      },
      {
        "answer": "",
        "explanation": "Amazon EventBridge is recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services. Amazon EventBridge is the only event-based service that integrates directly with third-party SaaS partners. Amazon EventBridge also automatically ingests events from over 90 AWS services without requiring developers to create any resources in their accounts. Further, Amazon EventBridge uses a defined JSON-based structure for events and allows you to create rules that are applied across the entire event body to select events to forward to a target. Amazon EventBridge currently supports over 15 AWS services as targets, including AWS Lambda, Amazon SQS, Amazon SNS, Amazon Kinesis Streams, and Firehose, among others. At launch, Amazon EventBridge has limited throughput (see Service Limits) which can be increased upon request, and typical latency of around half a second."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q50-i1.jpg",
        "answer": "",
        "explanation": "How Amazon EventBridge works:"
      },
      {
        "link": "https://aws.amazon.com/eventbridge/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture</strong> - As discussed above, Amazon SNS can be used for event-based services. However, the given use case needs integration with third-party SaaS services, hence Amazon EventBridge is the right choice, as Amazon SNS does not support third-party services integration."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture</strong> - Amazon SQS is a message queuing service from Amazon and works well for decoupling applications. It does not directly integrate with third-party SaaS services."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Elastic Load Balancing (ELB) for effective decoupling of system architecture</strong> - Elastic Load Balancing (ELB) offers a synchronous decoupling of applications, which is not the right fit for the current use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/eventbridge/",
      "https://aws.amazon.com/sns/"
    ]
  },
  {
    "id": 13,
    "question": "<p>The IT department at a company is conducting a training workshop for new data engineers. As part of an evaluation exercise on Amazon S3, the new data engineers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3.</p>\n\n<p>Can you identify the INVALID lifecycle transitions from the options below? (Select two) ?</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon S3 Standard-IA =&gt; Amazon S3 One Zone-IA</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon S3 One Zone-IA =&gt; Amazon S3 Standard-IA</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon S3 Intelligent-Tiering =&gt; Amazon S3 Standard</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon S3 Standard =&gt; Amazon S3 Intelligent-Tiering</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Amazon S3 Standard-IA =&gt; Amazon S3 Intelligent-Tiering</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p>As the question wants to know about the INVALID lifecycle transitions, the following options are the correct answers -</p>\n\n<p><strong>Amazon S3 Intelligent-Tiering =&gt; Amazon S3 Standard</strong></p>\n\n<p><strong>Amazon S3 One Zone-IA =&gt; Amazon S3 Standard-IA</strong></p>\n\n<p>Following are the unsupported life cycle transitions for S3 storage classes -\nAny storage class to the Amazon S3 Standard storage class.\nAny storage class to the Reduced Redundancy storage class.\nThe Amazon S3 Intelligent-Tiering storage class to the Amazon S3 Standard-IA storage class.\nThe Amazon S3 One Zone-IA storage class to the Amazon S3 Standard-IA or Amazon S3 Intelligent-Tiering storage classes.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 Standard =&gt; Amazon S3 Intelligent-Tiering</strong></p>\n\n<p><strong>Amazon S3 Standard-IA =&gt; Amazon S3 Intelligent-Tiering</strong></p>\n\n<p><strong>Amazon S3 Standard-IA =&gt; Amazon S3 One Zone-IA</strong></p>\n\n<p>Here are the supported life cycle transitions for S3 storage classes -\nThe S3 Standard storage class to any other storage class.\nAny storage class to the S3 Glacier or S3 Glacier Deep Archive storage classes.\nThe S3 Standard-IA storage class to the S3 Intelligent-Tiering or S3 One Zone-IA storage classes.\nThe S3 Intelligent-Tiering storage class to the S3 One Zone-IA storage class.\nThe S3 Glacier storage class to the S3 Glacier Deep Archive storage class.</p>\n\n<p>Amazon S3 supports a waterfall model for transitioning between storage classes, as shown in the diagram below:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q43-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q43-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a><p></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "As the question wants to know about the INVALID lifecycle transitions, the following options are the correct answers -"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Intelligent-Tiering =&gt; Amazon S3 Standard</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 One Zone-IA =&gt; Amazon S3 Standard-IA</strong>"
      },
      {
        "answer": "",
        "explanation": "Following are the unsupported life cycle transitions for S3 storage classes -\nAny storage class to the Amazon S3 Standard storage class.\nAny storage class to the Reduced Redundancy storage class.\nThe Amazon S3 Intelligent-Tiering storage class to the Amazon S3 Standard-IA storage class.\nThe Amazon S3 One Zone-IA storage class to the Amazon S3 Standard-IA or Amazon S3 Intelligent-Tiering storage classes."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Standard =&gt; Amazon S3 Intelligent-Tiering</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Standard-IA =&gt; Amazon S3 Intelligent-Tiering</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Standard-IA =&gt; Amazon S3 One Zone-IA</strong>"
      },
      {
        "answer": "",
        "explanation": "Here are the supported life cycle transitions for S3 storage classes -\nThe S3 Standard storage class to any other storage class.\nAny storage class to the S3 Glacier or S3 Glacier Deep Archive storage classes.\nThe S3 Standard-IA storage class to the S3 Intelligent-Tiering or S3 One Zone-IA storage classes.\nThe S3 Intelligent-Tiering storage class to the S3 One Zone-IA storage class.\nThe S3 Glacier storage class to the S3 Glacier Deep Archive storage class."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q43-i1.jpg",
        "answer": "",
        "explanation": "Amazon S3 supports a waterfall model for transitioning between storage classes, as shown in the diagram below:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html"
    ]
  },
  {
    "id": 14,
    "question": "<p>A financial analytics company wants to gather insights from personal finance data stored on Amazon S3 in the Microsoft Excel workbook format.</p>\n\n<p>Which of the following represents a serverless solution to interactively discover, clean and transform this raw data for performing this analysis?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage Amazon Glue Data Catalog to analyze the data stored on Amazon S3</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Leverage Amazon Redshift Spectrum to analyze the data stored on Amazon S3</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Leverage Amazon Athena to analyze the data stored on Amazon S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage AWS Glue DataBrew to analyze the data stored on Amazon S3</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage AWS Glue DataBrew to analyze the data stored on Amazon S3</strong></p>\n\n<p>AWS Glue DataBrew is a visual data preparation tool that enables users to clean and normalize data. AWS Glue DataBrew is a serverless solution to get insights from raw data. You can interactively discover, visualize, clean, and transform raw data. DataBrew makes smart suggestions to help you identify data quality issues that can be difficult to find and time-consuming to fix. To prepare the data, you can choose from more than 250 point-and-click transformations. These include removing nulls, replacing missing values, fixing schema inconsistencies, creating columns based on functions, and many more.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q1-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q1-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/databrew/latest/dg/what-is.html\">https://docs.aws.amazon.com/databrew/latest/dg/what-is.html</a><p></p>\n\n<p>Regarding any files stored in Amazon S3 or any files that you upload from a local drive, DataBrew supports the following file formats: comma-separated value (CSV), Microsoft Excel, JSON, ORC, and Parquet.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon Athena to analyze the data stored on Amazon S3</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. Athena supports creating tables and querying data from CSV, TSV, custom-delimited, and JSON formats; data from Hadoop-related formats: ORC, Apache Avro and Parquet; logs from Logstash, AWS CloudTrail logs, and Apache WebServer logs. Athena does not support querying data from files stored in the Microsoft Excel workbook format, so this option is incorrect.</p>\n\n<p><strong>Leverage Amazon Redshift Spectrum to analyze the data stored on Amazon S3</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift does not support reading Excel files directly, as it can only support CSV, AVRO, JSON, PARQUET and ORC formats.</p>\n\n<p><strong>Leverage Amazon Glue Data Catalog to analyze the data stored on Amazon S3</strong> - The AWS Glue Data Catalog contains references to data that is used as sources and targets of your extract, transform, and load (ETL) jobs in AWS Glue. To create your data warehouse or data lake, you must catalog this data. The AWS Glue Data Catalog is an index to the location, schema, and runtime metrics of your data. The Amazon Glue Data Catalog itself cannot be used to analyze the data stored on Amazon S3.</p>\n\n<p>AWS Glue Data Catalog:\n<img src=\"https://docs.aws.amazon.com/images/glue/latest/dg/images/PopulateCatalog-overview.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/images/glue/latest/dg/images/PopulateCatalog-overview.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html\">https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html</a><p></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/databrew/latest/dg/what-is.html\">https://docs.aws.amazon.com/databrew/latest/dg/what-is.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/what-is.html\">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/databrew/latest/dg/supported-data-file-sources.html\">https://docs.aws.amazon.com/databrew/latest/dg/supported-data-file-sources.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/supported-serdes.html\">https://docs.aws.amazon.com/athena/latest/ug/supported-serdes.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html\">https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-format.html\">https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-format.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Glue DataBrew to analyze the data stored on Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Glue DataBrew is a visual data preparation tool that enables users to clean and normalize data. AWS Glue DataBrew is a serverless solution to get insights from raw data. You can interactively discover, visualize, clean, and transform raw data. DataBrew makes smart suggestions to help you identify data quality issues that can be difficult to find and time-consuming to fix. To prepare the data, you can choose from more than 250 point-and-click transformations. These include removing nulls, replacing missing values, fixing schema inconsistencies, creating columns based on functions, and many more."
      },
      {
        "link": "https://docs.aws.amazon.com/databrew/latest/dg/what-is.html"
      },
      {
        "answer": "",
        "explanation": "Regarding any files stored in Amazon S3 or any files that you upload from a local drive, DataBrew supports the following file formats: comma-separated value (CSV), Microsoft Excel, JSON, ORC, and Parquet."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Athena to analyze the data stored on Amazon S3</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. Athena supports creating tables and querying data from CSV, TSV, custom-delimited, and JSON formats; data from Hadoop-related formats: ORC, Apache Avro and Parquet; logs from Logstash, AWS CloudTrail logs, and Apache WebServer logs. Athena does not support querying data from files stored in the Microsoft Excel workbook format, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Redshift Spectrum to analyze the data stored on Amazon S3</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift does not support reading Excel files directly, as it can only support CSV, AVRO, JSON, PARQUET and ORC formats."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Glue Data Catalog to analyze the data stored on Amazon S3</strong> - The AWS Glue Data Catalog contains references to data that is used as sources and targets of your extract, transform, and load (ETL) jobs in AWS Glue. To create your data warehouse or data lake, you must catalog this data. The AWS Glue Data Catalog is an index to the location, schema, and runtime metrics of your data. The Amazon Glue Data Catalog itself cannot be used to analyze the data stored on Amazon S3."
      },
      {
        "image": "https://docs.aws.amazon.com/images/glue/latest/dg/images/PopulateCatalog-overview.png",
        "answer": "",
        "explanation": "AWS Glue Data Catalog:"
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/databrew/latest/dg/what-is.html",
      "https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html",
      "https://docs.aws.amazon.com/athena/latest/ug/what-is.html",
      "https://docs.aws.amazon.com/databrew/latest/dg/supported-data-file-sources.html",
      "https://docs.aws.amazon.com/athena/latest/ug/supported-serdes.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-format.html"
    ]
  },
  {
    "id": 15,
    "question": "<p>While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs.</p>\n\n<p>Which of the following represents the best solution for the given scenario?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team</strong></p>\n\n<p>AWS CloudTrail log data can be ingested into Amazon CloudWatch to monitor and identify your AWS account activity against security threats and create a governance framework for security best practices. You can analyze log trail event data in CloudWatch using features such as Logs Insight, Contributor Insights, Metric filters, and CloudWatch Alarms.</p>\n\n<p>AWS CloudTrail integrates with the Amazon CloudWatch service to publish the API calls being made to resources or services in the AWS account. The published event has invaluable information that can be used for compliance, auditing, and governance of your AWS accounts. Below we introduce several features available in CloudWatch to monitor API activity, analyze the logs at scale, and take action when malicious activity is discovered, without provisioning your infrastructure.</p>\n\n<p>For the AWS Cloudtrail logs available in Amazon CloudWatch Logs, you can begin searching and filtering the log data by creating one or more metric filters. Use these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set a CloudWatch Alarm on.</p>\n\n<p>Note: AWS CloudTrail Insights helps AWS users identify and respond to unusual activity associated with <code>write</code> API calls by continuously analyzing CloudTrail management events.</p>\n\n<p>Insights events are logged when AWS CloudTrail detects unusual <code>write</code> management API activity in your account. If you have AWS CloudTrail Insights enabled and CloudTrail detects unusual activity, Insights events are delivered to the destination Amazon S3 bucket for your trail. You can also see the type of insight and the incident time when you view Insights events on the CloudTrail console. Unlike other types of events captured in a CloudTrail trail, Insights events are logged only when CloudTrail detects changes in your account's API usage that differ significantly from the account's typical usage patterns.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow</strong> -  AWS CloudTrail cannot stream data to Amazon Kinesis. Amazon S3 buckets and Amazon CloudWatch logs are the only destinations possible.</p>\n\n<p><strong>Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards</strong> - Generating reports and visualizations help in understanding and analyzing patterns but is not useful as a near-real-time automatic solution for the given problem.</p>\n\n<p><strong>AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded</strong> - When AWS Trusted Advisor refreshes your checks, Trusted Advisor publishes metrics about your check results to Amazon CloudWatch. You can view the metrics in CloudWatch. You can also create alarms to detect status changes to Trusted Advisor checks and status changes for resources, and service quota usage (formerly referred to as limits). The alarm will then notify you when you reach or exceed a service quota for your AWS account. However, the alarm is triggered only when the service limit is reached. We need a solution that raises an alarm when the number of API calls randomly increases or an abnormal pattern is detected. Hence, this option is not the right fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-authorization-failures\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-authorization-failures</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-insights-events-with-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-insights-events-with-cloudtrail.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-metrics-ta.html\">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-metrics-ta.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS CloudTrail log data can be ingested into Amazon CloudWatch to monitor and identify your AWS account activity against security threats and create a governance framework for security best practices. You can analyze log trail event data in CloudWatch using features such as Logs Insight, Contributor Insights, Metric filters, and CloudWatch Alarms."
      },
      {
        "answer": "",
        "explanation": "AWS CloudTrail integrates with the Amazon CloudWatch service to publish the API calls being made to resources or services in the AWS account. The published event has invaluable information that can be used for compliance, auditing, and governance of your AWS accounts. Below we introduce several features available in CloudWatch to monitor API activity, analyze the logs at scale, and take action when malicious activity is discovered, without provisioning your infrastructure."
      },
      {
        "answer": "",
        "explanation": "For the AWS Cloudtrail logs available in Amazon CloudWatch Logs, you can begin searching and filtering the log data by creating one or more metric filters. Use these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set a CloudWatch Alarm on."
      },
      {
        "answer": "",
        "explanation": "Note: AWS CloudTrail Insights helps AWS users identify and respond to unusual activity associated with <code>write</code> API calls by continuously analyzing CloudTrail management events."
      },
      {
        "answer": "",
        "explanation": "Insights events are logged when AWS CloudTrail detects unusual <code>write</code> management API activity in your account. If you have AWS CloudTrail Insights enabled and CloudTrail detects unusual activity, Insights events are delivered to the destination Amazon S3 bucket for your trail. You can also see the type of insight and the incident time when you view Insights events on the CloudTrail console. Unlike other types of events captured in a CloudTrail trail, Insights events are logged only when CloudTrail detects changes in your account's API usage that differ significantly from the account's typical usage patterns."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow</strong> -  AWS CloudTrail cannot stream data to Amazon Kinesis. Amazon S3 buckets and Amazon CloudWatch logs are the only destinations possible."
      },
      {
        "answer": "",
        "explanation": "<strong>Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards</strong> - Generating reports and visualizations help in understanding and analyzing patterns but is not useful as a near-real-time automatic solution for the given problem."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded</strong> - When AWS Trusted Advisor refreshes your checks, Trusted Advisor publishes metrics about your check results to Amazon CloudWatch. You can view the metrics in CloudWatch. You can also create alarms to detect status changes to Trusted Advisor checks and status changes for resources, and service quota usage (formerly referred to as limits). The alarm will then notify you when you reach or exceed a service quota for your AWS account. However, the alarm is triggered only when the service limit is reached. We need a solution that raises an alarm when the number of API calls randomly increases or an abnormal pattern is detected. Hence, this option is not the right fit for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-authorization-failures",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-insights-events-with-cloudtrail.html",
      "https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-metrics-ta.html"
    ]
  },
  {
    "id": 16,
    "question": "<p>A financial services company runs its flagship web application on AWS. The application serves thousands of users during peak hours. The company needs a scalable near-real-time solution to share hundreds of thousands of financial transactions with multiple internal applications. The solution should also remove sensitive details from the transactions before storing the cleansed transactions in a document database for low-latency retrieval.</p>\n\n<p>Which of the following would you recommend?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transaction data with the internal applications</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transaction data with the internal applications</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream</strong></p>\n\n<p>You can use Amazon Kinesis Data Streams to build custom applications that process or analyze streaming data for specialized needs.\nAmazon Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams.</p>\n\n<p>How Amazon Kinesis Data Streams Work:\n<img src=\"https://d1.awsstatic.com/Digital%20Marketing/House/1up/products/kinesis/Product-Page-Diagram_Amazon-Kinesis-Data-Streams.e04132af59c6aa1e9372cabf44a17749f4a81b16.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Digital%20Marketing/House/1up/products/kinesis/Product-Page-Diagram_Amazon-Kinesis-Data-Streams.e04132af59c6aa1e9372cabf44a17749f4a81b16.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a><p></p>\n\n<p>Amazon Kinesis Data Streams Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q59-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q59-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q59-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q59-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span>\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a><p></p>\n\n<p>For the given use case, you can stream the raw financial transactions into Amazon Kinesis Data Streams, which in turn, are processed by the AWS Lambda function that is set up as one of the consumers of the data stream. The Lambda would remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can be configured as the other consumers of the data stream and ingest the raw transactions</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transaction data with the internal applications</strong>- The use case requires a near-real-time solution for cleansing, processing, and storing the transactions, so using a batch process would be incorrect.</p>\n\n<p><strong>Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms, and delivers streaming data to data lakes, data stores, and analytics services.</p>\n\n<p><img src=\"https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a><p></p>\n\n<p>You cannot set up multiple consumers for Amazon Kinesis Data Firehose delivery streams as it can dump data in a single data repository at a time, so this option is incorrect.</p>\n\n<p><strong>Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transaction data with the internal applications</strong> - There is no such rule within Amazon DynamoDB that can auto-update every time a new item is written in a DynamoDB table. You would need to use an Amazon DynamoDB trigger to invoke an external service like a Lambda function on every new write, which can then cleanse and update the item. In addition, this process introduces inefficiency in the workflow as the same item is written and then updated for cleansing purposes. Therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use Amazon Kinesis Data Streams to build custom applications that process or analyze streaming data for specialized needs.\nAmazon Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams."
      },
      {
        "image": "https://d1.awsstatic.com/Digital%20Marketing/House/1up/products/kinesis/Product-Page-Diagram_Amazon-Kinesis-Data-Streams.e04132af59c6aa1e9372cabf44a17749f4a81b16.png",
        "answer": "",
        "explanation": "How Amazon Kinesis Data Streams Work:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q59-i1.jpg",
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams Key Concepts:"
      },
      {
        "link": "https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can stream the raw financial transactions into Amazon Kinesis Data Streams, which in turn, are processed by the AWS Lambda function that is set up as one of the consumers of the data stream. The Lambda would remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can be configured as the other consumers of the data stream and ingest the raw transactions"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transaction data with the internal applications</strong>- The use case requires a near-real-time solution for cleansing, processing, and storing the transactions, so using a batch process would be incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms, and delivers streaming data to data lakes, data stores, and analytics services."
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-firehose/"
      },
      {
        "answer": "",
        "explanation": "You cannot set up multiple consumers for Amazon Kinesis Data Firehose delivery streams as it can dump data in a single data repository at a time, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transaction data with the internal applications</strong> - There is no such rule within Amazon DynamoDB that can auto-update every time a new item is written in a DynamoDB table. You would need to use an Amazon DynamoDB trigger to invoke an external service like a Lambda function on every new write, which can then cleanse and update the item. In addition, this process introduces inefficiency in the workflow as the same item is written and then updated for cleansing purposes. Therefore this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html",
      "https://aws.amazon.com/kinesis/data-firehose/"
    ]
  },
  {
    "id": 17,
    "question": "<p>An Internet of Things (IoT) company would like to have a streaming system that performs real-time analytics on the ingested IoT data. Once the analytics is done, the company would like to send notifications back to the mobile applications of the IoT device owners.</p>\n\n<p>Which of the following AWS technologies would you recommend to send these notifications to the mobile applications?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Kinesis Data Streams with Amazon Simple Email Service (Amazon SES)</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Kinesis Data Streams with Amazon Simple Queue Service (Amazon SQS)</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Kinesis Data Streams with Amazon Simple Notification Service (Amazon SNS)</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis Data Streams with Amazon Simple Notification Service (Amazon SNS)</strong></p>\n\n<p>Amazon Kinesis Data Streams makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis Data Streams offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.</p>\n\n<p>With Amazon Kinesis Data Streams, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis Data Streams enables you to process and analyze data as it arrives and responds instantly instead of having to wait until all your data is collected before the processing can begin.</p>\n\n<p>Amazon Kinesis Data Streams will be great for event streaming from IoT devices, but not for sending notifications as it doesn't have such a feature.</p>\n\n<p>Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Amazon SNS is a notification service and will be perfect for this use case.</p>\n\n<p>Streaming data with Amazon Kinesis Data Streams and using Amazon SNS to send the response notifications is the optimal solution for the given scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Kinesis is better for streaming data since queues aren't meant for real-time streaming of data.</p>\n\n<p><strong>Amazon Kinesis Data Streams with Amazon Simple Email Service (Amazon SES)</strong> - Amazon Simple Email Service (Amazon SES) is a cloud-based email-sending service designed to help digital marketers and application developers send marketing, notification, and transactional emails. It is a reliable, cost-effective service for businesses of all sizes that use email to keep in contact with their customers. It is an email service and not a notification service as is the requirement in the current use case.</p>\n\n<p><strong>Amazon Kinesis Data Streams with Amazon Simple Queue Service (Amazon SQS)</strong> - As explained above, Amazon Kinesis Data Streams works well for streaming real-time data. Amazon SQS is a queuing service that helps decouple system architecture by offering flexibility and ease of maintenance. It cannot send notifications.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/\">https://aws.amazon.com/kinesis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/ses/\">https://aws.amazon.com/ses/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Streams with Amazon Simple Notification Service (Amazon SNS)</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis Data Streams offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application."
      },
      {
        "answer": "",
        "explanation": "With Amazon Kinesis Data Streams, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis Data Streams enables you to process and analyze data as it arrives and responds instantly instead of having to wait until all your data is collected before the processing can begin."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams will be great for event streaming from IoT devices, but not for sending notifications as it doesn't have such a feature."
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Amazon SNS is a notification service and will be perfect for this use case."
      },
      {
        "answer": "",
        "explanation": "Streaming data with Amazon Kinesis Data Streams and using Amazon SNS to send the response notifications is the optimal solution for the given scenario."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Kinesis is better for streaming data since queues aren't meant for real-time streaming of data."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Streams with Amazon Simple Email Service (Amazon SES)</strong> - Amazon Simple Email Service (Amazon SES) is a cloud-based email-sending service designed to help digital marketers and application developers send marketing, notification, and transactional emails. It is a reliable, cost-effective service for businesses of all sizes that use email to keep in contact with their customers. It is an email service and not a notification service as is the requirement in the current use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Streams with Amazon Simple Queue Service (Amazon SQS)</strong> - As explained above, Amazon Kinesis Data Streams works well for streaming real-time data. Amazon SQS is a queuing service that helps decouple system architecture by offering flexibility and ease of maintenance. It cannot send notifications."
      }
    ],
    "references": [
      "https://aws.amazon.com/sns/",
      "https://aws.amazon.com/kinesis/",
      "https://aws.amazon.com/ses/",
      "https://aws.amazon.com/sqs/"
    ]
  },
  {
    "id": 18,
    "question": "<p>An application uses Kinesis Data Streams to process real-time data for business analytics. Monitoring this incoming and outgoing data stream from the Kinesis Data Streams is important for the performance of the system as well as the downstream applications. For a read-intensive requirement, the age for the last record in the data stream for all the  <code>GetRecords</code> requests need to be tracked.</p>\n\n<p>Which stream-level metric will help address this requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p><code>PutRecords.Latency</code></p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p><code>GetRecords.Latency</code></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p><code>GetRecords.IteratorAgeMilliseconds</code></p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p><code>ReadProvisionedThroughputExceeded</code></p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong><code>GetRecords.IteratorAgeMilliseconds</code></strong> - <code>GetRecords.IteratorAgeMilliseconds</code> measures the age in milliseconds of the last record in the stream for all GetRecords requests. A value of zero for this metric indicates that the records are current within the stream. A lower value is preferred. To monitor any performance issues, increase the number of consumers for your stream so that the data is processed more quickly. To optimize your application code, increase the number of consumers to reduce the delay in processing records.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>GetRecords.Latency</code></strong> - <code>GetRecords.Latency</code> measures the time taken for each GetRecords operation on the stream over a specified time period. Confirms sufficient physical resources or record processing logic for increased stream throughput. Processes larger batches of data to reduce network and other downstream latencies in your application. The <code>GetRecords.Latency</code> metric confirms that the IDLE_TIME_BETWEEN_READS_IN_MILLIS setting is set to keep up with stream processing.</p>\n\n<p><strong><code>PutRecords.Latency</code></strong> - <code>PutRecords.Latency</code> measures the time taken for each PutRecords operation on the stream over a specified time period. If the PutRecords.Latency value is high, aggregate records into a larger file to put batch data into the Kinesis data stream.</p>\n\n<p><strong><code>ReadProvisionedThroughputExceeded</code></strong> - <code>ReadProvisionedThroughputExceeded</code> measures the count of GetRecords calls that throttled during a given time period, exceeding the service or shard limits for Kinesis Data Streams. A value of zero indicates that the data consumers aren't exceeding service quotas. Any other value indicates that the throughput limit is exceeded, requiring additional shards.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-streams-troubleshoot/\">https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-streams-troubleshoot/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/monitoring.html\">https://docs.aws.amazon.com/streams/latest/dev/monitoring.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong><code>GetRecords.IteratorAgeMilliseconds</code></strong> - <code>GetRecords.IteratorAgeMilliseconds</code> measures the age in milliseconds of the last record in the stream for all GetRecords requests. A value of zero for this metric indicates that the records are current within the stream. A lower value is preferred. To monitor any performance issues, increase the number of consumers for your stream so that the data is processed more quickly. To optimize your application code, increase the number of consumers to reduce the delay in processing records."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong><code>GetRecords.Latency</code></strong> - <code>GetRecords.Latency</code> measures the time taken for each GetRecords operation on the stream over a specified time period. Confirms sufficient physical resources or record processing logic for increased stream throughput. Processes larger batches of data to reduce network and other downstream latencies in your application. The <code>GetRecords.Latency</code> metric confirms that the IDLE_TIME_BETWEEN_READS_IN_MILLIS setting is set to keep up with stream processing."
      },
      {
        "answer": "",
        "explanation": "<strong><code>PutRecords.Latency</code></strong> - <code>PutRecords.Latency</code> measures the time taken for each PutRecords operation on the stream over a specified time period. If the PutRecords.Latency value is high, aggregate records into a larger file to put batch data into the Kinesis data stream."
      },
      {
        "answer": "",
        "explanation": "<strong><code>ReadProvisionedThroughputExceeded</code></strong> - <code>ReadProvisionedThroughputExceeded</code> measures the count of GetRecords calls that throttled during a given time period, exceeding the service or shard limits for Kinesis Data Streams. A value of zero indicates that the data consumers aren't exceeding service quotas. Any other value indicates that the throughput limit is exceeded, requiring additional shards."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-streams-troubleshoot/",
      "https://docs.aws.amazon.com/streams/latest/dev/monitoring.html"
    ]
  },
  {
    "id": 19,
    "question": "<p>A company regularly extracts about 2 TB of data daily from various data sources - including MySQL, MSSQL Server, Oracle, Vertica, and Teradata Vantage. Some of these sources feature undefined or frequently changing data schemas. A data engineer is tasked with implementing a solution that can automatically detect the schema of these data sources and perform data extraction, transformation, and loading to an Amazon S3 bucket.</p>\n\n<p>What solution would meet these needs while minimizing operational overhead?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Utilize AWS Glue to detect the schema including any ongoing changes. Extract, transform, and load the data into the S3 bucket by creating the ETL pipeline in Apache Spark</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Utilize Redshift spectrum to detect the schema including any ongoing changes. Extract, transform, and load the data into the S3 bucket by creating a stored procedure in Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize Amazon EMR to detect the schema including any ongoing changes. Extract, transform, and load the data into the S3 bucket by creating the ETL pipeline in Apache Spark</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Utilize PySpark to detect the schema including any ongoing changes. Extract, transform, and load the data into the S3 bucket by creating the ETL pipeline in AWS Lambda</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Utilize AWS Glue to detect the schema including any ongoing changes. Extract, transform, and load the data into the S3 bucket by creating the ETL pipeline in Apache Spark</strong></p>\n\n<p>In many use cases, the data teams responsible for building the data pipeline don’t have any control of the source schema, and they need to build a solution to identify changes in the source schema in order to be able to build the process or automation around it.</p>\n\n<p>For example, assume you’re receiving claim files from different external partners in the form of flat files, and you’ve built a solution to process claims based on these files. However, because these files were sent by external partners, you don’t have much control over the schema and data format. For example, columns such as customer_id and claim_id were changed to customerid and claimid by one partner, and another partner added new columns such as customer_age and earning and kept the rest of the columns the same. You need to identify such changes in advance so you can edit the ETL job to accommodate the changes, such as changing the column name or adding new columns to process the claims.</p>\n\n<p>You can capture these schema changes in your data source using an AWS Glue crawler. You can use an AWS Glue crawler to extract the metadata from data in an S3 bucket. Then you can use an AWS Glue ETL job to extract the changes in the schema to the AWS Glue Data Catalog. You can develop the code for the AWS Glue ETL job using Apache Spark.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q5-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q5-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/identify-source-schema-changes-using-aws-glue/\">https://aws.amazon.com/blogs/big-data/identify-source-schema-changes-using-aws-glue/</a><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q5-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q5-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/identify-source-schema-changes-using-aws-glue/\">https://aws.amazon.com/blogs/big-data/identify-source-schema-changes-using-aws-glue/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Utilize Amazon EMR to detect the schema including any ongoing changes. Extract, transform, and load the data into the S3 bucket by creating the ETL pipeline in Apache Spark</strong> - You will have to write significant code using Apache Spark in Amazon EMR to be able to detect the schema including any ongoing changes. So, this option is not the best fit for the given use case.</p>\n\n<p><strong>Utilize PySpark to detect the schema including any ongoing changes. Extract, transform, and load the data into the S3 bucket by creating the ETL pipeline in AWS Lambda</strong> - AWS Lambda has a maximum execution time (timeout) of 15 minutes which is not sufficient to run an ETL pipeline to process 2 TB of data daily. As such, AWS Lambda is not designed to run big data ETL pipelines. So, this option just acts as a distractor.</p>\n\n<p><strong>Utilize Redshift spectrum to detect the schema including any ongoing changes. Extract, transform, and load the data into the S3 bucket by creating a stored procedure in Amazon Redshift</strong> - You can define an Amazon Redshift stored procedure using the PostgreSQL procedural language PL/pgSQL to perform a set of SQL queries and logical operations. The procedure is stored in the database and available for any user with sufficient database privileges. You cannot use a stored procedure in Amazon Redshift to perform ETL operations for the given use case. This option just acts as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/identify-source-schema-changes-using-aws-glue/\">https://aws.amazon.com/blogs/big-data/identify-source-schema-changes-using-aws-glue/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/add-job.html\">https://docs.aws.amazon.com/glue/latest/dg/add-job.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/stored-procedure-overview.html\">https://docs.aws.amazon.com/redshift/latest/dg/stored-procedure-overview.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Utilize AWS Glue to detect the schema including any ongoing changes. Extract, transform, and load the data into the S3 bucket by creating the ETL pipeline in Apache Spark</strong>"
      },
      {
        "answer": "",
        "explanation": "In many use cases, the data teams responsible for building the data pipeline don’t have any control of the source schema, and they need to build a solution to identify changes in the source schema in order to be able to build the process or automation around it."
      },
      {
        "answer": "",
        "explanation": "For example, assume you’re receiving claim files from different external partners in the form of flat files, and you’ve built a solution to process claims based on these files. However, because these files were sent by external partners, you don’t have much control over the schema and data format. For example, columns such as customer_id and claim_id were changed to customerid and claimid by one partner, and another partner added new columns such as customer_age and earning and kept the rest of the columns the same. You need to identify such changes in advance so you can edit the ETL job to accommodate the changes, such as changing the column name or adding new columns to process the claims."
      },
      {
        "answer": "",
        "explanation": "You can capture these schema changes in your data source using an AWS Glue crawler. You can use an AWS Glue crawler to extract the metadata from data in an S3 bucket. Then you can use an AWS Glue ETL job to extract the changes in the schema to the AWS Glue Data Catalog. You can develop the code for the AWS Glue ETL job using Apache Spark."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/identify-source-schema-changes-using-aws-glue/"
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/identify-source-schema-changes-using-aws-glue/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Utilize Amazon EMR to detect the schema including any ongoing changes. Extract, transform, and load the data into the S3 bucket by creating the ETL pipeline in Apache Spark</strong> - You will have to write significant code using Apache Spark in Amazon EMR to be able to detect the schema including any ongoing changes. So, this option is not the best fit for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Utilize PySpark to detect the schema including any ongoing changes. Extract, transform, and load the data into the S3 bucket by creating the ETL pipeline in AWS Lambda</strong> - AWS Lambda has a maximum execution time (timeout) of 15 minutes which is not sufficient to run an ETL pipeline to process 2 TB of data daily. As such, AWS Lambda is not designed to run big data ETL pipelines. So, this option just acts as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Utilize Redshift spectrum to detect the schema including any ongoing changes. Extract, transform, and load the data into the S3 bucket by creating a stored procedure in Amazon Redshift</strong> - You can define an Amazon Redshift stored procedure using the PostgreSQL procedural language PL/pgSQL to perform a set of SQL queries and logical operations. The procedure is stored in the database and available for any user with sufficient database privileges. You cannot use a stored procedure in Amazon Redshift to perform ETL operations for the given use case. This option just acts as a distractor."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/identify-source-schema-changes-using-aws-glue/",
      "https://docs.aws.amazon.com/glue/latest/dg/add-job.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/stored-procedure-overview.html"
    ]
  },
  {
    "id": 20,
    "question": "<p>An e-commerce company is looking to enhance its product recommendation system which relies on user behavior and preferences. The company wants to achieve this by integrating insights from third-party datasets into its existing analytics platform. The company aims to minimize the effort and time involved in this integration.</p>\n\n<p>What solution would achieve this with the least operational overhead?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Access and integrate third-party datasets available through AWS Data Exchange</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Access and integrate third-party datasets from AWS CodeCommit repositories</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Access and integrate third-party datasets available through AWS Marketplace</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Access and integrate third-party datasets available through AWS DataSync</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Access and integrate third-party datasets available through AWS Data Exchange</strong></p>\n\n<p>AWS Data Exchange is a service that helps AWS easily share and manage data entitlements from other organizations at scale.</p>\n\n<p>As a data receiver, you can track and manage all of your data grants and AWS Marketplace data subscriptions in one place. When you have access to an AWS Data Exchange data set, you can use compatible AWS or partner analytics and machine learning to extract insights from it. You can also discover and subscribe to new third-party data sets available through AWS Data Exchange from the AWS Marketplace catalog</p>\n\n<p>For data senders, AWS Data Exchange eliminates the need to build and maintain any data delivery and entitlement infrastructure. Anyone with an AWS account can create and send data grants to data receivers.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Access and integrate third-party datasets available through AWS DataSync</strong> - AWS DataSync is an online data movement and discovery service that simplifies and accelerates data migrations to AWS as well as moving data to and from on-premises storage, edge locations, other cloud providers, and AWS Storage services. Although you could access third-party datasets, it would require a custom integration effort to consume the data within AWS services. So, this option is incorrect.</p>\n\n<p><strong>Access and integrate third-party datasets available through AWS Marketplace</strong> - You can use AWS Marketplace to find third-party software, data, and services that run on AWS and manage from a centralized location. AWS Marketplace includes thousands of software listings and simplifies software licensing and procurement with flexible pricing options and multiple deployment methods. Although you could access third-party datasets, it would require a custom integration effort to consume the data within AWS services. So, this option is incorrect.</p>\n\n<p><strong>Access and integrate third-party datasets from AWS CodeCommit repositories</strong> - AWS CodeCommit is a secure, highly scalable, managed source control service that makes it easier for teams to collaborate on code. It is not meant to provide access to third-party datasets.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/data-exchange/latest/userguide/what-is.html\">https://docs.aws.amazon.com/data-exchange/latest/userguide/what-is.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/datasync/faqs/\">https://aws.amazon.com/datasync/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/mp/marketplace-service/overview/\">https://aws.amazon.com/mp/marketplace-service/overview/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codecommit/faqs/\">https://aws.amazon.com/codecommit/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Access and integrate third-party datasets available through AWS Data Exchange</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Data Exchange is a service that helps AWS easily share and manage data entitlements from other organizations at scale."
      },
      {
        "answer": "",
        "explanation": "As a data receiver, you can track and manage all of your data grants and AWS Marketplace data subscriptions in one place. When you have access to an AWS Data Exchange data set, you can use compatible AWS or partner analytics and machine learning to extract insights from it. You can also discover and subscribe to new third-party data sets available through AWS Data Exchange from the AWS Marketplace catalog"
      },
      {
        "answer": "",
        "explanation": "For data senders, AWS Data Exchange eliminates the need to build and maintain any data delivery and entitlement infrastructure. Anyone with an AWS account can create and send data grants to data receivers."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Access and integrate third-party datasets available through AWS DataSync</strong> - AWS DataSync is an online data movement and discovery service that simplifies and accelerates data migrations to AWS as well as moving data to and from on-premises storage, edge locations, other cloud providers, and AWS Storage services. Although you could access third-party datasets, it would require a custom integration effort to consume the data within AWS services. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Access and integrate third-party datasets available through AWS Marketplace</strong> - You can use AWS Marketplace to find third-party software, data, and services that run on AWS and manage from a centralized location. AWS Marketplace includes thousands of software listings and simplifies software licensing and procurement with flexible pricing options and multiple deployment methods. Although you could access third-party datasets, it would require a custom integration effort to consume the data within AWS services. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Access and integrate third-party datasets from AWS CodeCommit repositories</strong> - AWS CodeCommit is a secure, highly scalable, managed source control service that makes it easier for teams to collaborate on code. It is not meant to provide access to third-party datasets."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/data-exchange/latest/userguide/what-is.html",
      "https://aws.amazon.com/datasync/faqs/",
      "https://aws.amazon.com/mp/marketplace-service/overview/",
      "https://aws.amazon.com/codecommit/faqs/"
    ]
  },
  {
    "id": 21,
    "question": "<p>A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center for durable long-term storage.</p>\n\n<p>What is your recommendation to migrate this data in the MOST cost-optimal way?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier</strong></p>\n\n<p>AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large-scale data transfer and pre-processing use cases.\nThe data stored on the AWS Snowball Edge device can be copied into the Amazon S3 bucket and later transitioned into Amazon S3 Glacier via a lifecycle policy. You can't directly copy data from AWS Snowball Edge devices into Amazon S3 Glacier.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier</strong> - As mentioned earlier, you can't directly copy data from AWS Snowball Edge devices into Amazon S3 Glacier. Hence, this option is incorrect.</p>\n\n<p><strong>Set up AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. Direct Connect involves significant monetary investment and takes more than a month to set up, therefore it's not the correct fit for this use case where just a one-time data transfer has to be done.</p>\n\n<p><strong>Set up AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). VPN Connections are a good solution if you have an immediate need, and have low to modest bandwidth requirements. Because of the high data volume for the given use case, Site-to-Site VPN is not the correct choice.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large-scale data transfer and pre-processing use cases.\nThe data stored on the AWS Snowball Edge device can be copied into the Amazon S3 bucket and later transitioned into Amazon S3 Glacier via a lifecycle policy. You can't directly copy data from AWS Snowball Edge devices into Amazon S3 Glacier."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier</strong> - As mentioned earlier, you can't directly copy data from AWS Snowball Edge devices into Amazon S3 Glacier. Hence, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. Direct Connect involves significant monetary investment and takes more than a month to set up, therefore it's not the correct fit for this use case where just a one-time data transfer has to be done."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). VPN Connections are a good solution if you have an immediate need, and have low to modest bandwidth requirements. Because of the high data volume for the given use case, Site-to-Site VPN is not the correct choice."
      }
    ],
    "references": [
      "https://aws.amazon.com/snowball/"
    ]
  },
  {
    "id": 22,
    "question": "<p>A digital media company does not want to own and manage its own IT infrastructure so it can redeploy resources toward innovation in Artificial Intelligence and related areas to create a better customer experience. As part of this digital transformation, the media company wants to archive about 9 PB of data in its on-premises data center for durable long-term storage on the AWS cloud.</p>\n\n<p>What would you recommend for migrating and storing this data in the quickest and the MOST cost-optimal way?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Transfer the on-premises data into a Snowmobile device. Copy the Snowmobile data into Amazon S3 and create a lifecycle policy to transition the data into Amazon Glacier Deep Archive</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon Glacier Deep Archive</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Transfer the on-premises data into a Snowmobile device. Copy the Snowmobile data directly into Amazon Glacier Deep Archive</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data directly into Amazon Glacier Deep Archive</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon Glacier Deep Archive</strong></p>\n\n<p>Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large-scale data transfer and pre-processing use cases.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q26-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q26-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html</a><p></p>\n\n<p>The data stored on the Snowball Edge device can be copied into the S3 bucket and later transitioned into Amazon Glacier Deep Archive via a lifecycle policy. You can't directly copy data from Snowball Edge devices into Amazon Glacier Deep Archive.</p>\n\n<p>Glacier Deep Archive Lifecycle Rule:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q26-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q26-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/storage/using-aws-snowball-to-migrate-data-to-amazon-s3-glacier-for-long-term-storage/\">https://aws.amazon.com/blogs/storage/using-aws-snowball-to-migrate-data-to-amazon-s3-glacier-for-long-term-storage/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Transfer the on-premises data into a Snowmobile device. Copy the Snowmobile data into Amazon S3 and create a lifecycle policy to transition the data into Amazon Glacier</strong> - AWS Snowmobile is an exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100 PB per Snowmobile. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration.</p>\n\n<p>AWS recommends that you use Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball. So, this option is not the best fit for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q26-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q26-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/snowmobile/faqs/\">https://aws.amazon.com/snowmobile/faqs/</a><p></p>\n\n<p><strong>Transfer the on-premises data into a Snowmobile device. Copy the Snowmobile data directly into Amazon Glacier</strong> - As mentioned above, AWS recommends that you use Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball. So, this option is not the best fit for the given use case.</p>\n\n<p>However, you should note that for Snowmobile, you can import your data directly into Glacier.</p>\n\n<p><strong>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data directly into Amazon Glacier</strong> - As mentioned earlier, you can't directly copy data from Snowball Edge devices into Amazon Glacier. Hence, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/snowball/latest/ug/how-it-works.html\">https://docs.aws.amazon.com/snowball/latest/ug/how-it-works.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/snowmobile/faqs/\">https://aws.amazon.com/snowmobile/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon Glacier Deep Archive</strong>"
      },
      {
        "answer": "",
        "explanation": "Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large-scale data transfer and pre-processing use cases."
      },
      {
        "link": "https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html"
      },
      {
        "answer": "",
        "explanation": "The data stored on the Snowball Edge device can be copied into the S3 bucket and later transitioned into Amazon Glacier Deep Archive via a lifecycle policy. You can't directly copy data from Snowball Edge devices into Amazon Glacier Deep Archive."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q26-i2.jpg",
        "answer": "",
        "explanation": "Glacier Deep Archive Lifecycle Rule:"
      },
      {
        "link": "https://aws.amazon.com/blogs/storage/using-aws-snowball-to-migrate-data-to-amazon-s3-glacier-for-long-term-storage/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Transfer the on-premises data into a Snowmobile device. Copy the Snowmobile data into Amazon S3 and create a lifecycle policy to transition the data into Amazon Glacier</strong> - AWS Snowmobile is an exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100 PB per Snowmobile. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration."
      },
      {
        "answer": "",
        "explanation": "AWS recommends that you use Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball. So, this option is not the best fit for the given use case."
      },
      {
        "link": "https://aws.amazon.com/snowmobile/faqs/"
      },
      {
        "answer": "",
        "explanation": "<strong>Transfer the on-premises data into a Snowmobile device. Copy the Snowmobile data directly into Amazon Glacier</strong> - As mentioned above, AWS recommends that you use Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball. So, this option is not the best fit for the given use case."
      },
      {
        "answer": "",
        "explanation": "However, you should note that for Snowmobile, you can import your data directly into Glacier."
      },
      {
        "answer": "",
        "explanation": "<strong>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data directly into Amazon Glacier</strong> - As mentioned earlier, you can't directly copy data from Snowball Edge devices into Amazon Glacier. Hence, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html",
      "https://aws.amazon.com/blogs/storage/using-aws-snowball-to-migrate-data-to-amazon-s3-glacier-for-long-term-storage/",
      "https://aws.amazon.com/snowmobile/faqs/",
      "https://aws.amazon.com/snowball/",
      "https://docs.aws.amazon.com/snowball/latest/ug/how-it-works.html"
    ]
  },
  {
    "id": 23,
    "question": "<p>You would like to mount a network file system on Linux instances, where files will be stored and accessed frequently at first, and then infrequently. What solution is the MOST cost-effective?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon S3 Glacier Deep Archive</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon EFS Infrequent Access</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon EBS io1/io2</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon S3 Intelligent Tiering</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EFS Infrequent Access</strong></p>\n\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability.</p>\n\n<p>Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files, not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. Therefore, this is the correct option.</p>\n\n<p>How Amazon EFS works:\n<img src=\"https://d1.awsstatic.com/r2018/b/EFS/product-page-diagram-Amazon-EFS-Launch_How-It-Works.cf947858f0ef3557b9fc14077bdf3f65b3f9ff43.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/r2018/b/EFS/product-page-diagram-Amazon-EFS-Launch_How-It-Works.cf947858f0ef3557b9fc14077bdf3f65b3f9ff43.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 Intelligent Tiering</strong> - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.</p>\n\n<p>You can't mount a network file system on Amazon S3 Intelligent Tiering as it's an object storage service, so this option is incorrect.</p>\n\n<p><strong>Amazon S3 Glacier Deep Archive</strong> - Amazon S3 Glacier Deep Archive is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>You can't mount a network file system on Amazon S3 Glacier Deep Archive as it's an object storage/archival system. Hence this option is incorrect.</p>\n\n<p><strong>Amazon EBS io1/io2</strong> - An EBS (Elastic Block Store) Volume is a network drive you can attach to your EC2 instances while they run. It allows your EC2 instances to persist data, even after their termination. It is a block storage solution and not a file storage system.  So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/efs/features/infrequent-access/\">https://aws.amazon.com/efs/features/infrequent-access/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EFS Infrequent Access</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability."
      },
      {
        "answer": "",
        "explanation": "Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files, not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. Therefore, this is the correct option."
      },
      {
        "image": "https://d1.awsstatic.com/r2018/b/EFS/product-page-diagram-Amazon-EFS-Launch_How-It-Works.cf947858f0ef3557b9fc14077bdf3f65b3f9ff43.png",
        "answer": "",
        "explanation": "How Amazon EFS works:"
      },
      {
        "link": "https://aws.amazon.com/efs/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Intelligent Tiering</strong> - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access."
      },
      {
        "answer": "",
        "explanation": "You can't mount a network file system on Amazon S3 Intelligent Tiering as it's an object storage service, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Glacier Deep Archive</strong> - Amazon S3 Glacier Deep Archive is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements."
      },
      {
        "answer": "",
        "explanation": "You can't mount a network file system on Amazon S3 Glacier Deep Archive as it's an object storage/archival system. Hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EBS io1/io2</strong> - An EBS (Elastic Block Store) Volume is a network drive you can attach to your EC2 instances while they run. It allows your EC2 instances to persist data, even after their termination. It is a block storage solution and not a file storage system.  So, this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/efs/features/infrequent-access/"
    ]
  },
  {
    "id": 24,
    "question": "<p>Multiple teams within a company use Amazon Athena to run ad-hoc queries on its data stored in an Amazon S3 bucket. The usage costs for Athena have been running high and the company wants to set a limit on the amount of data that can be scanned by each team. Also, the teams should not have access to queries, query results, or query history of other teams.</p>\n\n<p>As a data engineer, how will you implement the requirement with the least operational and cost overhead?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage S3 Access Points to control access to query history of the teams by creating unique access control policies for each access point</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an IAM group for each team. Assign appropriate permissions to each of the IAM groups by associating custom IAM policies that restrict access to the query history and control costs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Athena workgroup for each team and apply tags. Use these tags in a new IAM policy to configure appropriate permissions to the workgroups</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Lake Formation to define access policies for each of the teams separately to restrict access to query history for the respective teams and control costs</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an Athena workgroup for each team and apply tags. Use these tags in a new IAM policy to configure appropriate permissions to the workgroups</strong></p>\n\n<p>Use workgroups to separate users, teams, applications, or workloads, to set limits on the amount of data each query or the entire workgroup can process, and to track costs. Because workgroups act as resources, you can use resource-level identity-based policies to control access to a specific workgroup. You can also view query-related metrics in Amazon CloudWatch, control costs by configuring limits on the amount of data scanned, create thresholds, and trigger actions, such as Amazon SNS, when these thresholds are breached.</p>\n\n<p>To further control costs, you can create capacity reservations with the number of data processing units that you specify and add one or more workgroups to the reservation.</p>\n\n<p>Benefits of using workgroups:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q11-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q11-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/workgroups-benefits.html\">https://docs.aws.amazon.com/athena/latest/ug/workgroups-benefits.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Lake Formation to define access policies for each of the teams separately to restrict access to query history for the respective teams and control costs</strong> - AWS Lake Formation allows you to define and enforce database, table, and column-level access policies when using Athena queries to read data stored in Amazon S3.</p>\n\n<p>Athena query results locations in Amazon S3 cannot be registered with Lake Formation and IAM permissions policies for Amazon S3 control access. In addition, Lake Formation permissions do not apply to Athena query history. You need to use Athena workgroups to control access to query history.</p>\n\n<p><strong>Create an IAM group for each team. Assign appropriate permissions to each of the IAM groups by associating custom IAM policies that restrict access to the query history and control costs</strong> - You cannot use IAM policies to restrict access to query history for different groups of Athena users. So, this option is incorrect.</p>\n\n<p><strong>Leverage S3 Access Points to control access to query history of the teams by creating unique access control policies for each access point</strong> - With S3 Access Points, customers can create unique access control policies for each access point to easily control access to shared datasets. Customers with shared datasets including data lakes, media archives, and user-generated content can easily scale access for hundreds of applications by creating individualized access points with names and permissions customized for each application. Any access point can be restricted to a Virtual Private Cloud (VPC) to firewall S3 data access within customers’ private networks, and AWS Service Control Policies can be used to ensure all access points are VPC restricted. You cannot use access control policies to control access to Athena query history, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/manage-queries-control-costs-with-workgroups.html\">https://docs.aws.amazon.com/athena/latest/ug/manage-queries-control-costs-with-workgroups.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/features/access-points/\">https://aws.amazon.com/s3/features/access-points/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an Athena workgroup for each team and apply tags. Use these tags in a new IAM policy to configure appropriate permissions to the workgroups</strong>"
      },
      {
        "answer": "",
        "explanation": "Use workgroups to separate users, teams, applications, or workloads, to set limits on the amount of data each query or the entire workgroup can process, and to track costs. Because workgroups act as resources, you can use resource-level identity-based policies to control access to a specific workgroup. You can also view query-related metrics in Amazon CloudWatch, control costs by configuring limits on the amount of data scanned, create thresholds, and trigger actions, such as Amazon SNS, when these thresholds are breached."
      },
      {
        "answer": "",
        "explanation": "To further control costs, you can create capacity reservations with the number of data processing units that you specify and add one or more workgroups to the reservation."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q11-i1.jpg",
        "answer": "",
        "explanation": "Benefits of using workgroups:"
      },
      {
        "link": "https://docs.aws.amazon.com/athena/latest/ug/workgroups-benefits.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Lake Formation to define access policies for each of the teams separately to restrict access to query history for the respective teams and control costs</strong> - AWS Lake Formation allows you to define and enforce database, table, and column-level access policies when using Athena queries to read data stored in Amazon S3."
      },
      {
        "answer": "",
        "explanation": "Athena query results locations in Amazon S3 cannot be registered with Lake Formation and IAM permissions policies for Amazon S3 control access. In addition, Lake Formation permissions do not apply to Athena query history. You need to use Athena workgroups to control access to query history."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an IAM group for each team. Assign appropriate permissions to each of the IAM groups by associating custom IAM policies that restrict access to the query history and control costs</strong> - You cannot use IAM policies to restrict access to query history for different groups of Athena users. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage S3 Access Points to control access to query history of the teams by creating unique access control policies for each access point</strong> - With S3 Access Points, customers can create unique access control policies for each access point to easily control access to shared datasets. Customers with shared datasets including data lakes, media archives, and user-generated content can easily scale access for hundreds of applications by creating individualized access points with names and permissions customized for each application. Any access point can be restricted to a Virtual Private Cloud (VPC) to firewall S3 data access within customers’ private networks, and AWS Service Control Policies can be used to ensure all access points are VPC restricted. You cannot use access control policies to control access to Athena query history, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/workgroups-benefits.html",
      "https://docs.aws.amazon.com/athena/latest/ug/manage-queries-control-costs-with-workgroups.html",
      "https://aws.amazon.com/s3/features/access-points/"
    ]
  },
  {
    "id": 25,
    "question": "<p>A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to the large volume of incoming data. A data engineer needs to design a scalable and serverless solution to enhance performance.</p>\n\n<p>Which combination of steps do you recommend? (Select two)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up Amazon Kinesis Data Streams to ingest the data</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up AWS Fargate with Amazon ECS to process the data</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up AWS Database Migration Service (AWS DMS) to ingest the data</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Provision Amazon EC2 instances in an Auto Scaling group to process the data</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Set up AWS Lambda with AWS Step Functions to process the data</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Set up Amazon Kinesis Data Streams to ingest the data</strong></p>\n\n<p><strong>Set up AWS Fargate with Amazon ECS to process the data</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.</p>\n\n<p>For the given use case, we can use Kinesis Data Streams as the ingestion layer and the containerized ECS application on AWS Fargate as the processing layer. Both these components are serverless and can scale to offer the desired performance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Database Migration Service (AWS DMS) to ingest the data</strong> - AWS Database Migration Service helps you migrate databases to AWS quickly and securely. DMS cannot be used for real-time data ingestion. Hence, this option is incorrect.</p>\n\n<p><strong>Set up AWS Lambda with AWS Step Functions to process the data</strong> - The maximum timeout value for any AWS Lambda function is 15 minutes. When the specified timeout is reached, AWS Lambda terminates the execution of your Lambda function. Since the use case talks about a job that runs for 30 minutes, AWS Lambda is not the right fit.</p>\n\n<p><strong>Provision Amazon EC2 instances in an Auto Scaling group to process the data</strong> - The given requirement is for a serverless solution to process the data. Hence, provisioning an Amazon EC2 instance is clearly not the right solution.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/\">https://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon Kinesis Data Streams to ingest the data</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Set up AWS Fargate with Amazon ECS to process the data</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more."
      },
      {
        "answer": "",
        "explanation": "AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design."
      },
      {
        "answer": "",
        "explanation": "For the given use case, we can use Kinesis Data Streams as the ingestion layer and the containerized ECS application on AWS Fargate as the processing layer. Both these components are serverless and can scale to offer the desired performance."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up AWS Database Migration Service (AWS DMS) to ingest the data</strong> - AWS Database Migration Service helps you migrate databases to AWS quickly and securely. DMS cannot be used for real-time data ingestion. Hence, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up AWS Lambda with AWS Step Functions to process the data</strong> - The maximum timeout value for any AWS Lambda function is 15 minutes. When the specified timeout is reached, AWS Lambda terminates the execution of your Lambda function. Since the use case talks about a job that runs for 30 minutes, AWS Lambda is not the right fit."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision Amazon EC2 instances in an Auto Scaling group to process the data</strong> - The given requirement is for a serverless solution to process the data. Hence, provisioning an Amazon EC2 instance is clearly not the right solution."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/"
    ]
  },
  {
    "id": 26,
    "question": "<p>A gaming company maintains a staging environment for its flagship application which uses a DynamoDB table to keep track of the gaming history of the players. This data needs to be kept for only a week and then it can be deleted. The IT manager has noticed that the table has several months of data in the table. The company wants to implement a cost-effective solution to keep only the latest week's data in the table.</p>\n\n<p>Which of the following solutions requires the MINIMUM development effort and ongoing maintenance?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add a new attribute in the table to track the expiration time and set up a Glue job to delete items that are more than a week old</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add a new attribute in the table to track the expiration time and enable time to live (TTL) on the table</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Add a created_at attribute in the table and then use a cron job on EC2 instance to invoke a Python script daily. The script deletes items older than a week on the basis of this attribute</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Add a created_at attribute in the table and then use a CloudWatch Events rule to invoke a Lambda function daily. The Lambda function deletes items older than a week on the basis of this attribute</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a new attribute in the table to track the expiration time and enable time to live (TTL) on the table</strong></p>\n\n<p>You can use DynamoDB Time to Live (TTL) to determine when an item is no longer needed. TTL uses a per-item timestamp to determine items for expiration. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. After you enable TTL on a table, a per-partition scanner background process automatically and continuously evaluates the expiry status of items in the table.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q34-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q34-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q34-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q34-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add a new attribute in the table to track the expiration time and set up a Glue job to delete items that are more than a week old</strong> - Although this solution is feasible, it would require a significant development effort to build and maintain a Glue job to trim the DynamoDB table. There is an additional cost of running the Glue job on a daily basis as well.</p>\n\n<p><strong>Add a created_at attribute in the table and then use a cron job on the EC2 instance to invoke a Python script daily. The script deletes items older than a week on the basis of this attribute</strong> - Provisioning an EC2 instance and developing Python script involves significant development effort, maintenance, and additional cost. So this option is not correct.</p>\n\n<p><strong>Add a created_at attribute in the table and then use a CloudWatch Events rule to invoke a Lambda function daily. The Lambda function deletes items older than a week on the basis of this attribute</strong> - Custom code needs to be written for the Lambda function to implement the functionality to trim the DynamoDB table. The lambda function execution is also charged separately. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Add a new attribute in the table to track the expiration time and enable time to live (TTL) on the table</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use DynamoDB Time to Live (TTL) to determine when an item is no longer needed. TTL uses a per-item timestamp to determine items for expiration. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. After you enable TTL on a table, a per-partition scanner background process automatically and continuously evaluates the expiry status of items in the table."
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Add a new attribute in the table to track the expiration time and set up a Glue job to delete items that are more than a week old</strong> - Although this solution is feasible, it would require a significant development effort to build and maintain a Glue job to trim the DynamoDB table. There is an additional cost of running the Glue job on a daily basis as well."
      },
      {
        "answer": "",
        "explanation": "<strong>Add a created_at attribute in the table and then use a cron job on the EC2 instance to invoke a Python script daily. The script deletes items older than a week on the basis of this attribute</strong> - Provisioning an EC2 instance and developing Python script involves significant development effort, maintenance, and additional cost. So this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Add a created_at attribute in the table and then use a CloudWatch Events rule to invoke a Lambda function daily. The Lambda function deletes items older than a week on the basis of this attribute</strong> - Custom code needs to be written for the Lambda function to implement the functionality to trim the DynamoDB table. The lambda function execution is also charged separately. So this option is not correct."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html"
    ]
  },
  {
    "id": 27,
    "question": "<p>An analytics organization has been acquired by a leading media company. The analytics organization has 10 independent applications with an on-premises data footprint of about 70 Terabytes for each application. The CTO of the media company has set a timeline of two weeks to carry out the data migration from the on-premises data center to the AWS Cloud and establish connectivity.</p>\n\n<p>Which of the following are the MOST cost-effective options for completing the data transfer and establishing connectivity? (Select two)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Order 10 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Setup AWS Direct Connect to establish connectivity between the on-premises data center and AWS Cloud</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Setup AWS Site-to-Site VPN to establish on-going connectivity between the on-premises data center and AWS Cloud</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Order 70 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Order 1 AWS Snowmobile to complete the one-time data transfer</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Order 10 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer</strong></p>\n\n<p>AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 Terabytes of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gigabytes of network connectivity to address large-scale data transfer and pre-processing use cases.</p>\n\n<p>As each Snowball Edge Storage Optimized device can handle 80 Terabytes of data, you can order 10 such devices to take care of the data transfer for all applications.</p>\n\n<p><strong>Setup AWS Site-to-Site VPN to establish on-going connectivity between the on-premises data center and AWS Cloud</strong></p>\n\n<p>AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.</p>\n\n<p>Therefore this option is the right fit for the given use-case as the connectivity can be easily established within the given timeframe.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Order 1 AWS Snowmobile to complete the one-time data transfer</strong> -  Each AWS Snowmobile has a total capacity of up to 100 petabytes. To migrate large datasets of 10 petabytes or more in a single location, you should use AWS Snowmobile. For datasets less than 10 petabytes or distributed in multiple locations, you should use Snowball. So AWS Snowmobile is not the right fit for this use case.</p>\n\n<p><strong>Setup AWS Direct Connect to establish connectivity between the on-premises data center and AWS Cloud</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. Direct Connect involves significant monetary investment and takes at least a month to set up, therefore it's not the correct fit for this use case.</p>\n\n<p><strong>Order 70 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer</strong> - As the data transfer can be completed with just 10 AWS Snowball Edge Storage Optimized devices, there is no need to order 70 devices.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/snowball/faqs/\">https://aws.amazon.com/snowball/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/vpn/\">https://aws.amazon.com/vpn/</a></p>\n\n<p><a href=\"https://aws.amazon.com/snowmobile/faqs/\">https://aws.amazon.com/snowmobile/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/directconnect/\">https://aws.amazon.com/directconnect/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Order 10 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 Terabytes of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gigabytes of network connectivity to address large-scale data transfer and pre-processing use cases."
      },
      {
        "answer": "",
        "explanation": "As each Snowball Edge Storage Optimized device can handle 80 Terabytes of data, you can order 10 such devices to take care of the data transfer for all applications."
      },
      {
        "answer": "",
        "explanation": "<strong>Setup AWS Site-to-Site VPN to establish on-going connectivity between the on-premises data center and AWS Cloud</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity."
      },
      {
        "answer": "",
        "explanation": "Therefore this option is the right fit for the given use-case as the connectivity can be easily established within the given timeframe."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Order 1 AWS Snowmobile to complete the one-time data transfer</strong> -  Each AWS Snowmobile has a total capacity of up to 100 petabytes. To migrate large datasets of 10 petabytes or more in a single location, you should use AWS Snowmobile. For datasets less than 10 petabytes or distributed in multiple locations, you should use Snowball. So AWS Snowmobile is not the right fit for this use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Setup AWS Direct Connect to establish connectivity between the on-premises data center and AWS Cloud</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. Direct Connect involves significant monetary investment and takes at least a month to set up, therefore it's not the correct fit for this use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Order 70 AWS Snowball Edge Storage Optimized devices to complete the one-time data transfer</strong> - As the data transfer can be completed with just 10 AWS Snowball Edge Storage Optimized devices, there is no need to order 70 devices."
      }
    ],
    "references": [
      "https://aws.amazon.com/snowball/faqs/",
      "https://aws.amazon.com/vpn/",
      "https://aws.amazon.com/snowmobile/faqs/",
      "https://aws.amazon.com/directconnect/"
    ]
  },
  {
    "id": 28,
    "question": "<p>A nightly cron job generates a customer data file of 1 GB size in .xls format and stores it in an Amazon S3 bucket. A data engineer is tasked with concatenating the column in the file that contains customer first names with the column that contains customer last names and then calculating the number of distinct customers in the file.</p>\n\n<p>Which of the following will you suggest to address this requirement with the least operational overhead?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage AWS Glue DataBrew to create a recipe that uses the COUNT_DISTINCT aggregate function to determine the number of distinct customers</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Query the customer data file in .xls format stored in Amazon S3 using SQL commands via Amazon Athena</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Leverage AWS Glue DataBrew to create a recipe that uses the FLAG_DUPLICATE_ROWS function to determine the number of distinct customers</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Develop an Apache Spark job in an AWS Glue notebook to read the file in Amazon S3 and determine the number of distinct customers</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage AWS Glue DataBrew to create a recipe that uses the COUNT_DISTINCT aggregate function to determine the number of distinct customers</strong></p>\n\n<p>AWS Glue DataBrew is a visual data preparation tool that enables users to clean and normalize data without writing any code. Using DataBrew helps reduce the time it takes to prepare data for analytics and machine learning (ML) by up to 80 percent, compared to custom-developed data preparation. You can choose from over 250 ready-made transformations to automate data preparation tasks, such as filtering anomalies, converting data to standard formats, and correcting invalid values.</p>\n\n<p>To prepare the data, you can choose from more than 250 point-and-click transformations. These include removing nulls, replacing missing values, fixing schema inconsistencies, creating columns based on functions, and many more. You can also use transformations to apply natural language processing (NLP) techniques to split sentences into phrases. Immediate previews show a portion of your data before and after transformation, so you can modify your recipe before applying it to the entire dataset.</p>\n\n<p>For the given use case, you can use the COUNT_DISTINCT aggregate function to determine the number of distinct customers</p>\n\n<p>COUNT_DISTINCT - Returns the total number of distinct values from the selected source columns in a new column. Empty and null values are ignored.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q9-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q9-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.functions.COUNT_DISTINCT.html\">https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.functions.COUNT_DISTINCT.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Query the customer data file in .xls format stored in Amazon S3 using SQL commands via Amazon Athena</strong> - Athena supports creating tables and querying data from CSV, TSV, custom-delimited, and JSON formats; data from Hadoop-related formats: ORC, Apache Avro and Parquet; logs from Logstash, AWS CloudTrail logs, and Apache WebServer logs. You cannot query files stored in Amazon S3 in the .xls format via Amazon Athena.</p>\n\n<p><strong>Develop an Apache Spark job in an AWS Glue notebook to read the file in Amazon S3 and determine the number of distinct customers</strong> - It is certainly possible to develop an Apache Spark job in an AWS Glue notebook to read the file in Amazon S3 and determine the number of distinct customers by using countDistinct Function. However, this option involves significant operational overhead to develop the script, so it is ruled out.</p>\n\n<p><strong>Leverage AWS Glue DataBrew to create a recipe that uses the FLAG_DUPLICATE_ROWS function to determine the number of distinct customers</strong></p>\n\n<p>FLAG_DUPLICATE_ROWS - Returns a new column with a specified value in each row that indicates whether that row is an exact match of an earlier row in the dataset. When matches are found, they are flagged as duplicates. The initial occurrence is not flagged, because it doesn't match an earlier row. So, this option is not relevant for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/databrew/latest/dg/what-is.html\">https://docs.aws.amazon.com/databrew/latest/dg/what-is.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.functions.COUNT_DISTINCT.html\">https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.functions.COUNT_DISTINCT.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.FLAG_DUPLICATE_ROWS.html\">https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.FLAG_DUPLICATE_ROWS.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/supported-serdes.html\">https://docs.aws.amazon.com/athena/latest/ug/supported-serdes.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Glue DataBrew to create a recipe that uses the COUNT_DISTINCT aggregate function to determine the number of distinct customers</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Glue DataBrew is a visual data preparation tool that enables users to clean and normalize data without writing any code. Using DataBrew helps reduce the time it takes to prepare data for analytics and machine learning (ML) by up to 80 percent, compared to custom-developed data preparation. You can choose from over 250 ready-made transformations to automate data preparation tasks, such as filtering anomalies, converting data to standard formats, and correcting invalid values."
      },
      {
        "answer": "",
        "explanation": "To prepare the data, you can choose from more than 250 point-and-click transformations. These include removing nulls, replacing missing values, fixing schema inconsistencies, creating columns based on functions, and many more. You can also use transformations to apply natural language processing (NLP) techniques to split sentences into phrases. Immediate previews show a portion of your data before and after transformation, so you can modify your recipe before applying it to the entire dataset."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can use the COUNT_DISTINCT aggregate function to determine the number of distinct customers"
      },
      {
        "answer": "",
        "explanation": "COUNT_DISTINCT - Returns the total number of distinct values from the selected source columns in a new column. Empty and null values are ignored."
      },
      {
        "link": "https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.functions.COUNT_DISTINCT.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Query the customer data file in .xls format stored in Amazon S3 using SQL commands via Amazon Athena</strong> - Athena supports creating tables and querying data from CSV, TSV, custom-delimited, and JSON formats; data from Hadoop-related formats: ORC, Apache Avro and Parquet; logs from Logstash, AWS CloudTrail logs, and Apache WebServer logs. You cannot query files stored in Amazon S3 in the .xls format via Amazon Athena."
      },
      {
        "answer": "",
        "explanation": "<strong>Develop an Apache Spark job in an AWS Glue notebook to read the file in Amazon S3 and determine the number of distinct customers</strong> - It is certainly possible to develop an Apache Spark job in an AWS Glue notebook to read the file in Amazon S3 and determine the number of distinct customers by using countDistinct Function. However, this option involves significant operational overhead to develop the script, so it is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Glue DataBrew to create a recipe that uses the FLAG_DUPLICATE_ROWS function to determine the number of distinct customers</strong>"
      },
      {
        "answer": "",
        "explanation": "FLAG_DUPLICATE_ROWS - Returns a new column with a specified value in each row that indicates whether that row is an exact match of an earlier row in the dataset. When matches are found, they are flagged as duplicates. The initial occurrence is not flagged, because it doesn't match an earlier row. So, this option is not relevant for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.functions.COUNT_DISTINCT.html",
      "https://docs.aws.amazon.com/databrew/latest/dg/what-is.html",
      "https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.FLAG_DUPLICATE_ROWS.html",
      "https://docs.aws.amazon.com/athena/latest/ug/supported-serdes.html"
    ]
  },
  {
    "id": 29,
    "question": "<p>The web development team at an IT company has about 200 TB of web-log data that is stored in an Amazon S3 bucket as raw text. Each log file is identified by a key of the type year-month-day_log_HHmmss.txt where HHmmss denotes the time the log file was created. The data engineering team has created an Amazon Athena table that links to the given S3 bucket. The team executes several queries every hour against a subset of the table's columns. The company wants a Hive-metastore compatible solution that costs less and requires less maintenance to support the ongoing analytics on this log data.</p>\n\n<p>As an AWS Certified Data Engineer Associate, which of the following solutions would you combine to address these requirements? (Select three)</p>",
    "corrects": [
      1,
      4,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Partition the data by using a key prefix of the form date=year-month-day/ to the S3 objects</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Partition the data by using a key prefix of the form year-month-day/ to the S3 objects</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Change the log files to Apache Avro format</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Drop and recreate the table with the PARTITIONED BY clause. Load the partitions by executing the <code>MSCK REPAIR TABLE</code> statement</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Drop and recreate the table with the PARTITIONED BY clause. Load the partitions by executing the <code>ALTER TABLE ADD PARTITION</code> statement</p>",
        "correct": false
      },
      {
        "id": 6,
        "answer": "<p>Change the log files to Apache Parquet format</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Change the log files to Apache Parquet format</strong></p>\n\n<p><strong>Partition the data by using a key prefix of the form date=year-month-day/ to the S3 objects</strong></p>\n\n<p><strong>Drop and recreate the table with the PARTITIONED BY clause. Load the partitions by executing the <code>MSCK REPAIR TABLE</code> statement</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data stored in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p>\n\n<p>By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme.</p>\n\n<p>Athena can use Apache Hive style partitions, whose data paths contain key-value pairs connected by equal signs (for example, country=us/... or year=2021/month=01/day=26/...). Thus, the paths include both the names of the partition keys and the values that each path represents.</p>\n\n<p>Athena can also use non-Hive style partitioning schemes. For example, CloudTrail logs and Kinesis Data Firehose delivery streams use separate path components for date parts such as data/2021/01/26/us/6fc7845e.json. For such non-Hive compatible data, you use ALTER TABLE ADD PARTITION to add the partitions manually.</p>\n\n<p>Since the given use case needs a hive-metastore compatible solution, you can use a key prefix of the form date=year-month-day/ for partitioning data and use <code>MSCK REPAIR TABLE</code> statement to load the partitions.</p>\n\n<p>Considerations and Limitations for Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q20-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q20-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a><p></p>\n\n<p>Avro is a row-based storage format whereas Parquet is a columnar-based storage format. Writing operations in Avro are more efficient than Parquet whereas Parquet is much better for analytical operations since the reads and querying are much more efficient than writing. Parquet is better suited for querying a subset of columns in a multi-column table whereas Avro is better suited for ETL operations where we need to query all the columns.</p>\n\n<p>For the given use case, several queries are executed every hour, so Parquet is a better format than Avro.</p>\n\n<p>Highly recommend the following blog on the top performance tuning tips for Amazon Athena:\n<a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Drop and recreate the table with the PARTITIONED BY clause. Load the partitions by executing the <code>ALTER TABLE ADD PARTITION</code> statement</strong></p>\n\n<p><strong>Partition the data by using a key prefix of the form year-month-day/ to the S3 objects</strong></p>\n\n<p><strong>Change the log files to Apache Avro format</strong></p>\n\n<p>Per the explanation provided above, these three options do not meet the requirements for the given use case, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://www.clairvoyant.ai/blog/big-data-file-formats\">https://www.clairvoyant.ai/blog/big-data-file-formats</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/connect-to-data-source-hive.html\">https://docs.aws.amazon.com/athena/latest/ug/connect-to-data-source-hive.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Change the log files to Apache Parquet format</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Partition the data by using a key prefix of the form date=year-month-day/ to the S3 objects</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Drop and recreate the table with the PARTITIONED BY clause. Load the partitions by executing the <code>MSCK REPAIR TABLE</code> statement</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Athena is an interactive query service that makes it easy to analyze data stored in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run."
      },
      {
        "answer": "",
        "explanation": "By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme."
      },
      {
        "answer": "",
        "explanation": "Athena can use Apache Hive style partitions, whose data paths contain key-value pairs connected by equal signs (for example, country=us/... or year=2021/month=01/day=26/...). Thus, the paths include both the names of the partition keys and the values that each path represents."
      },
      {
        "answer": "",
        "explanation": "Athena can also use non-Hive style partitioning schemes. For example, CloudTrail logs and Kinesis Data Firehose delivery streams use separate path components for date parts such as data/2021/01/26/us/6fc7845e.json. For such non-Hive compatible data, you use ALTER TABLE ADD PARTITION to add the partitions manually."
      },
      {
        "answer": "",
        "explanation": "Since the given use case needs a hive-metastore compatible solution, you can use a key prefix of the form date=year-month-day/ for partitioning data and use <code>MSCK REPAIR TABLE</code> statement to load the partitions."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q20-i1.jpg",
        "answer": "",
        "explanation": "Considerations and Limitations for Athena:"
      },
      {
        "link": "https://docs.aws.amazon.com/athena/latest/ug/partitions.html"
      },
      {
        "answer": "",
        "explanation": "Avro is a row-based storage format whereas Parquet is a columnar-based storage format. Writing operations in Avro are more efficient than Parquet whereas Parquet is much better for analytical operations since the reads and querying are much more efficient than writing. Parquet is better suited for querying a subset of columns in a multi-column table whereas Avro is better suited for ETL operations where we need to query all the columns."
      },
      {
        "answer": "",
        "explanation": "For the given use case, several queries are executed every hour, so Parquet is a better format than Avro."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
        "answer": "",
        "explanation": "Highly recommend the following blog on the top performance tuning tips for Amazon Athena:\n<a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Drop and recreate the table with the PARTITIONED BY clause. Load the partitions by executing the <code>ALTER TABLE ADD PARTITION</code> statement</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Partition the data by using a key prefix of the form year-month-day/ to the S3 objects</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Change the log files to Apache Avro format</strong>"
      },
      {
        "answer": "",
        "explanation": "Per the explanation provided above, these three options do not meet the requirements for the given use case, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/partitions.html",
      "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
      "https://www.clairvoyant.ai/blog/big-data-file-formats",
      "https://docs.aws.amazon.com/athena/latest/ug/connect-to-data-source-hive.html"
    ]
  },
  {
    "id": 30,
    "question": "<p>An IT company wants to process IoT data from the field devices of an agricultural sciences company. The data engineering team at the company is designing a new database infrastructure for capturing this IoT data. The database should be resilient with minimal operational overhead and require the least development effort. The application includes a device tracking system that stores the GPS data for all devices. Real-time IoT data, as well as metadata lookups, must be performed with high throughput and microsecond latency.</p>\n\n<p>Which of the following options would you recommend as the MOST efficient solution for these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use RDS MySQL as the database with ElastiCache</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use DynamoDB as the database with DynamoDB Accelerator (DAX)</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Aurora MySQL as the database with Aurora cluster cache</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use DocumentDB as the database with API Gateway</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use DynamoDB as the database with DynamoDB Accelerator (DAX)</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable NoSQL database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.</p>\n\n<p>DAX Overview:\n<img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html</a><p></p>\n\n<p>Typically, DynamoDB response times can be measured in single-digit milliseconds. Certain use cases require response times in microseconds. For these use cases, DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data. Additionally, DAX reduces operational and application complexity by providing a managed service that is API-compatible with DynamoDB. Therefore, it requires only minimal functional changes to use with an existing application. Therefore, this is the correct option.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q37-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q37-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html\">https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use RDS MySQL as the database with ElastiCache</strong></p>\n\n<p>The given use-case deals with IoT data which has a variable underlying data structure by its nature. Relational databases are not a good fit for capturing such data. Also integrating ElastiCache with RDS involves custom code at the application level as well as provisioning and maintaining a separate ElastiCache cluster. This operational overhead goes against the specified requirements.</p>\n\n<p><strong>Use Aurora MySQL as the database with Aurora cluster cache</strong></p>\n\n<p>The given use-case deals with IoT data which has a variable underlying data structure by its nature. Relational databases are not a good fit for capturing such data. In addition, the main benefit of the cluster cache feature is to improve the performance of the new primary/writer instance after failover occurs. So Aurora with cluster cache is not the right fit to handle IoT data with microsecond latency.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q37-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q37-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/database/introduction-to-aurora-postgresql-cluster-cache-management/\">https://aws.amazon.com/blogs/database/introduction-to-aurora-postgresql-cluster-cache-management/</a><p></p>\n\n<p><strong>Use DocumentDB as the database with API Gateway</strong></p>\n\n<p>Amazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.</p>\n\n<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the front door for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications.</p>\n\n<p>This option has been added as a distractor as API Gateway cannot be used to optimize the database infrastructure for the given use case.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a><p></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html\">https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/introduction-to-aurora-postgresql-cluster-cache-management/\">https://aws.amazon.com/blogs/database/introduction-to-aurora-postgresql-cluster-cache-management/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use DynamoDB as the database with DynamoDB Accelerator (DAX)</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable NoSQL database with built-in security, backup and restore, and in-memory caching for internet-scale applications."
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management."
      },
      {
        "image": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png",
        "answer": "",
        "explanation": "DAX Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html"
      },
      {
        "answer": "",
        "explanation": "Typically, DynamoDB response times can be measured in single-digit milliseconds. Certain use cases require response times in microseconds. For these use cases, DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data. Additionally, DAX reduces operational and application complexity by providing a managed service that is API-compatible with DynamoDB. Therefore, it requires only minimal functional changes to use with an existing application. Therefore, this is the correct option."
      },
      {
        "link": "https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use RDS MySQL as the database with ElastiCache</strong>"
      },
      {
        "answer": "",
        "explanation": "The given use-case deals with IoT data which has a variable underlying data structure by its nature. Relational databases are not a good fit for capturing such data. Also integrating ElastiCache with RDS involves custom code at the application level as well as provisioning and maintaining a separate ElastiCache cluster. This operational overhead goes against the specified requirements."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Aurora MySQL as the database with Aurora cluster cache</strong>"
      },
      {
        "answer": "",
        "explanation": "The given use-case deals with IoT data which has a variable underlying data structure by its nature. Relational databases are not a good fit for capturing such data. In addition, the main benefit of the cluster cache feature is to improve the performance of the new primary/writer instance after failover occurs. So Aurora with cluster cache is not the right fit to handle IoT data with microsecond latency."
      },
      {
        "link": "https://aws.amazon.com/blogs/database/introduction-to-aurora-postgresql-cluster-cache-management/"
      },
      {
        "answer": "",
        "explanation": "<strong>Use DocumentDB as the database with API Gateway</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud."
      },
      {
        "answer": "",
        "explanation": "Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the front door for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications."
      },
      {
        "answer": "",
        "explanation": "This option has been added as a distractor as API Gateway cannot be used to optimize the database infrastructure for the given use case."
      },
      {
        "image": "https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png",
        "answer": "",
        "explanation": "How API Gateway Works:"
      },
      {
        "link": "https://aws.amazon.com/api-gateway/"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html",
      "https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html",
      "https://aws.amazon.com/blogs/database/introduction-to-aurora-postgresql-cluster-cache-management/",
      "https://aws.amazon.com/api-gateway/"
    ]
  },
  {
    "id": 31,
    "question": "<p>A data engineer needs to set up a daily execution of Amazon Athena queries, each of which may take longer than 15 minutes to run. What are the two most cost-effective steps to achieve this requirement? (Select two)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an AWS Lambda function that uses the Athena Boto3 client start_query_execution API call to execute the Athena queries programmatically</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up an AWS Glue Python shell job that uses the Athena Boto3 client start_query_execution API call to execute the Athena queries programmatically</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up a workflow in AWS Step Functions that incorporates two states. Configure the initial state prior to triggering the AWS Glue Python shell job. Establish the subsequent state as a Wait state, designed to periodically verify the completion status of the Athena query via the Athena Boto3 get_query_execution API call. Ensure the workflow is configured to initiate the subsequent query once the preceding one concludes</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a workflow in AWS Step Functions that incorporates two states. Configure the initial state prior to triggering the Lambda function. Establish the subsequent state as a Wait state, designed to periodically verify the completion status of the Athena query via the Athena Boto3 get_query_execution API call. Ensure the workflow is configured to initiate the subsequent query once the preceding one concludes</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Develop a Python shell script in AWS Glue to implement a sleep timer that checks every 5 minutes if the current Athena query has successfully completed. Set up the script so that it triggers the subsequent query once the current one is confirmed to have finished</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Set up an AWS Lambda function that uses the Athena Boto3 client start_query_execution API call to execute the Athena queries programmatically</strong></p>\n\n<p>You can an AWS Lambda function to execute Athena queries programmatically by using the Athena Boto3 client start_query_execution API call, like so:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q2-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q2-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://repost.aws/knowledge-center/schedule-query-athena\">https://repost.aws/knowledge-center/schedule-query-athena</a><p></p>\n\n<p><strong>Set up a workflow in AWS Step Functions that incorporates two states. Configure the initial state prior to triggering the Lambda function. Establish the subsequent state as a Wait state, designed to periodically verify the completion status of the Athena query via the Athena Boto3 get_query_execution API call. Ensure the workflow is configured to initiate the subsequent query once the preceding one concludes</strong></p>\n\n<p>AWS Step Functions is a serverless orchestration service. It is based on state machines and tasks. In Step Functions, a workflow is called a state machine, which is a series of event-driven steps. Each step in a workflow is called a state. A Task state represents a unit of work that another AWS service, such as AWS Lambda, performs. A Task state can call any AWS service or API.</p>\n\n<p>States are elements in your state machine. A state is referred to by its name, which can be any string, but must be unique within the scope of the entire state machine.</p>\n\n<p>States can perform a variety of functions in your state machine:</p>\n\n<p>Do some work in your state machine (a Task state)</p>\n\n<p>Make a choice between branches of execution (a Choice state)</p>\n\n<p>Stop an execution with a failure or success (a Fail or Succeed state)</p>\n\n<p>Pass its input to its output, or inject some fixed data into the workflow (a Pass state)</p>\n\n<p>Provide a delay for a certain amount of time or until a specified date and time (a Wait state)</p>\n\n<p>Begin parallel branches of execution (a Parallel state)</p>\n\n<p>Dynamically iterate steps (a Map state)</p>\n\n<p>The AWS Step Functions service integration with Amazon Athena enables you to use Step Functions to start and stop query execution and get query results. Using Step Functions, you can run ad-hoc or scheduled data queries, and retrieve results targeting your S3 data lakes.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q2-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q2-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/connect-athena.html\">https://docs.aws.amazon.com/step-functions/latest/dg/connect-athena.html</a><p></p>\n\n<p>For the given use case, incorporating a Wait state into the workflow allows for regular monitoring of the Athena query's status, advancing to the subsequent step only after the query is finalized. Step Functions are better suited for managing long-running processes and maintaining the status across different stages of the workflow.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an AWS Glue Python shell job that uses the Athena Boto3 client start_query_execution API call to execute the Athena queries programmatically</strong> - Although you could certainly use an AWS Glue Python shell job to invoke the start_query_execution API call via the Athena Boto3 client, however, it turns out to be costlier than using a Lambda function to invoke the call. So, this option is incorrect.</p>\n\n<p><strong>Set up a workflow in AWS Step Functions that incorporates two states. Configure the initial state prior to triggering the AWS Glue Python shell job. Establish the subsequent state as a Wait state, designed to periodically verify the completion status of the Athena query via the Athena Boto3 get_query_execution API call. Ensure the workflow is configured to initiate the subsequent query once the preceding one concludes</strong> - As mentioned earlier, for the given use case - leveraging an AWS Glue Python shell job instead of a Lambda function is a costlier proposition, so this option is incorrect.</p>\n\n<p><strong>Develop a Python shell script in AWS Glue to implement a sleep timer that checks every 5 minutes if the current Athena query has successfully completed. Set up the script so that it triggers the subsequent query once the current one is confirmed to have finished</strong> - It is costlier as well as resource-inefficient to check for the status of Athena query execution every 5 minutes via a long-running Glue job. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/schedule-query-athena\">https://repost.aws/knowledge-center/schedule-query-athena</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\">https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/connect-athena.html\">https://docs.aws.amazon.com/step-functions/latest/dg/connect-athena.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an AWS Lambda function that uses the Athena Boto3 client start_query_execution API call to execute the Athena queries programmatically</strong>"
      },
      {
        "answer": "",
        "explanation": "You can an AWS Lambda function to execute Athena queries programmatically by using the Athena Boto3 client start_query_execution API call, like so:"
      },
      {
        "link": "https://repost.aws/knowledge-center/schedule-query-athena"
      },
      {
        "answer": "",
        "explanation": "<strong>Set up a workflow in AWS Step Functions that incorporates two states. Configure the initial state prior to triggering the Lambda function. Establish the subsequent state as a Wait state, designed to periodically verify the completion status of the Athena query via the Athena Boto3 get_query_execution API call. Ensure the workflow is configured to initiate the subsequent query once the preceding one concludes</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Step Functions is a serverless orchestration service. It is based on state machines and tasks. In Step Functions, a workflow is called a state machine, which is a series of event-driven steps. Each step in a workflow is called a state. A Task state represents a unit of work that another AWS service, such as AWS Lambda, performs. A Task state can call any AWS service or API."
      },
      {
        "answer": "",
        "explanation": "States are elements in your state machine. A state is referred to by its name, which can be any string, but must be unique within the scope of the entire state machine."
      },
      {
        "answer": "",
        "explanation": "States can perform a variety of functions in your state machine:"
      },
      {
        "answer": "",
        "explanation": "Do some work in your state machine (a Task state)"
      },
      {
        "answer": "",
        "explanation": "Make a choice between branches of execution (a Choice state)"
      },
      {
        "answer": "",
        "explanation": "Stop an execution with a failure or success (a Fail or Succeed state)"
      },
      {
        "answer": "",
        "explanation": "Pass its input to its output, or inject some fixed data into the workflow (a Pass state)"
      },
      {
        "answer": "",
        "explanation": "Provide a delay for a certain amount of time or until a specified date and time (a Wait state)"
      },
      {
        "answer": "",
        "explanation": "Begin parallel branches of execution (a Parallel state)"
      },
      {
        "answer": "",
        "explanation": "Dynamically iterate steps (a Map state)"
      },
      {
        "answer": "",
        "explanation": "The AWS Step Functions service integration with Amazon Athena enables you to use Step Functions to start and stop query execution and get query results. Using Step Functions, you can run ad-hoc or scheduled data queries, and retrieve results targeting your S3 data lakes."
      },
      {
        "link": "https://docs.aws.amazon.com/step-functions/latest/dg/connect-athena.html"
      },
      {
        "answer": "",
        "explanation": "For the given use case, incorporating a Wait state into the workflow allows for regular monitoring of the Athena query's status, advancing to the subsequent step only after the query is finalized. Step Functions are better suited for managing long-running processes and maintaining the status across different stages of the workflow."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an AWS Glue Python shell job that uses the Athena Boto3 client start_query_execution API call to execute the Athena queries programmatically</strong> - Although you could certainly use an AWS Glue Python shell job to invoke the start_query_execution API call via the Athena Boto3 client, however, it turns out to be costlier than using a Lambda function to invoke the call. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up a workflow in AWS Step Functions that incorporates two states. Configure the initial state prior to triggering the AWS Glue Python shell job. Establish the subsequent state as a Wait state, designed to periodically verify the completion status of the Athena query via the Athena Boto3 get_query_execution API call. Ensure the workflow is configured to initiate the subsequent query once the preceding one concludes</strong> - As mentioned earlier, for the given use case - leveraging an AWS Glue Python shell job instead of a Lambda function is a costlier proposition, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Develop a Python shell script in AWS Glue to implement a sleep timer that checks every 5 minutes if the current Athena query has successfully completed. Set up the script so that it triggers the subsequent query once the current one is confirmed to have finished</strong> - It is costlier as well as resource-inefficient to check for the status of Athena query execution every 5 minutes via a long-running Glue job. So, this option is incorrect."
      }
    ],
    "references": [
      "https://repost.aws/knowledge-center/schedule-query-athena",
      "https://docs.aws.amazon.com/step-functions/latest/dg/connect-athena.html",
      "https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html"
    ]
  },
  {
    "id": 32,
    "question": "<p>A company has an S3 bucket that contains files in two different folders - <code>s3://my-bucket/images</code> and <code>s3://my-bucket/thumbnails</code>. When an image is first uploaded and new, it is viewed several times. Post a detailed analysis, the data analytics team has noticed that after 45 days those image files are rarely requested, but the thumbnails still are. After 180 days, the company would like to archive the image files and the thumbnails. Overall, the company would like the solution to remain highly available to prevent disasters from happening against the entire AZ.</p>\n\n<p>Which of the following represents the best-fit solutions for an efficient cost strategy for the given S3 bucket? (Select two)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a Lifecycle Policy to transition all objects to Glacier Deep Archive after 180 days</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a Lifecycle Policy to transition all objects to S3 Standard IA after 45 days</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a Lifecycle Policy to transition objects to S3 Standard IA using a prefix after 45 days</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create a Lifecycle Policy to transition objects to Glacier Deep Archive using a prefix after 180 days</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Create a Lifecycle Policy to transition objects to S3 One Zone IA using a prefix after 45 days</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p>To manage your S3 objects, so they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:</p>\n\n<p>Transition actions — Define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier Deep Archive storage class one year after creating them.</p>\n\n<p>Expiration actions — Define when objects expire. Amazon S3 deletes expired objects on your behalf.</p>\n\n<p><strong>Create a Lifecycle Policy to transition objects to S3 Standard IA using a prefix after 45 days</strong></p>\n\n<p>S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.</p>\n\n<p>As the use-case mentions, after 45 days, image files are rarely requested, but the thumbnails still are. So you need to use a prefix while configuring the Lifecycle Policy so that only objects in the <code>s3://my-bucket/images</code> are transitioned to Standard IA and not all the objects in the bucket.</p>\n\n<p><strong>Create a Lifecycle Policy to transition all objects to Glacier Deep Archive after 180 days</strong></p>\n\n<p>Amazon S3 Glacier Deep Archive is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lifecycle Policy to transition all objects to S3 Standard IA after 45 days</strong> - As discussed above, you need to use a prefix while configuring the Lifecycle Policy so that only objects in the <code>s3://my-bucket/images</code> are transitioned to Standard IA and not all the objects in the bucket.</p>\n\n<p><strong>Create a Lifecycle Policy to transition objects to Glacier Deep Archive using a prefix after 180 days</strong> - After 180 days, you can move all the objects to Glacier Deep Archive storage as per the use case. Glacier Deep Archive doesn't need prefixes for the given use case.</p>\n\n<p><strong>Create a Lifecycle Policy to transition objects to S3 One Zone IA using a prefix after 45 days</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.</p>\n\n<p>S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. The minimum storage duration charge is 30 days.</p>\n\n<p>Finally, S3 One Zone IA will not offer the required availability in case an AZ goes down.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "To manage your S3 objects, so they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:"
      },
      {
        "answer": "",
        "explanation": "Transition actions — Define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier Deep Archive storage class one year after creating them."
      },
      {
        "answer": "",
        "explanation": "Expiration actions — Define when objects expire. Amazon S3 deletes expired objects on your behalf."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a Lifecycle Policy to transition objects to S3 Standard IA using a prefix after 45 days</strong>"
      },
      {
        "answer": "",
        "explanation": "S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days."
      },
      {
        "answer": "",
        "explanation": "As the use-case mentions, after 45 days, image files are rarely requested, but the thumbnails still are. So you need to use a prefix while configuring the Lifecycle Policy so that only objects in the <code>s3://my-bucket/images</code> are transitioned to Standard IA and not all the objects in the bucket."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a Lifecycle Policy to transition all objects to Glacier Deep Archive after 180 days</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Glacier Deep Archive is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a Lifecycle Policy to transition all objects to S3 Standard IA after 45 days</strong> - As discussed above, you need to use a prefix while configuring the Lifecycle Policy so that only objects in the <code>s3://my-bucket/images</code> are transitioned to Standard IA and not all the objects in the bucket."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a Lifecycle Policy to transition objects to Glacier Deep Archive using a prefix after 180 days</strong> - After 180 days, you can move all the objects to Glacier Deep Archive storage as per the use case. Glacier Deep Archive doesn't need prefixes for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a Lifecycle Policy to transition objects to S3 One Zone IA using a prefix after 45 days</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days."
      },
      {
        "answer": "",
        "explanation": "S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. The minimum storage duration charge is 30 days."
      },
      {
        "answer": "",
        "explanation": "Finally, S3 One Zone IA will not offer the required availability in case an AZ goes down."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html"
    ]
  },
  {
    "id": 33,
    "question": "<p>The data engineering team at a social media company wants to use Amazon CloudWatch alarms to automatically recover Amazon EC2 instances if they become impaired. The team has hired you to provide subject matter expertise.</p>\n\n<p>Which of the following statements would you identify as CORRECT regarding this automatic recovery process? (Select two)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>If your instance has a public IPv4 address, it retains the public IPv4 address after recovery</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>During instance recovery, the instance is migrated during an instance reboot, and any data that is in memory is retained</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata</strong></p>\n\n<p><strong>If your instance has a public IPv4 address, it retains the public IPv4 address after recovery</strong></p>\n\n<p>You can create an Amazon CloudWatch alarm to automatically recover the Amazon EC2 instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group. If your instance has a public IPv4 address, it retains the public IPv4 address after recovery. During instance recovery, the instance is migrated during an instance reboot, and any data that is in memory is lost.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance</strong> - This is incorrect as terminated instances cannot be recovered.</p>\n\n<p><strong>During instance recovery, the instance is migrated during an instance reboot, and any data that is in memory is retained</strong> - As mentioned above, during instance recovery, the instance is migrated during an instance reboot, and any data that is in memory is lost.</p>\n\n<p><strong>If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery</strong> - As mentioned above, if your instance has a public IPv4 address, it retains the public IPv4 address after recovery.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>If your instance has a public IPv4 address, it retains the public IPv4 address after recovery</strong>"
      },
      {
        "answer": "",
        "explanation": "You can create an Amazon CloudWatch alarm to automatically recover the Amazon EC2 instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group. If your instance has a public IPv4 address, it retains the public IPv4 address after recovery. During instance recovery, the instance is migrated during an instance reboot, and any data that is in memory is lost."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance</strong> - This is incorrect as terminated instances cannot be recovered."
      },
      {
        "answer": "",
        "explanation": "<strong>During instance recovery, the instance is migrated during an instance reboot, and any data that is in memory is retained</strong> - As mentioned above, during instance recovery, the instance is migrated during an instance reboot, and any data that is in memory is lost."
      },
      {
        "answer": "",
        "explanation": "<strong>If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery</strong> - As mentioned above, if your instance has a public IPv4 address, it retains the public IPv4 address after recovery."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html"
    ]
  },
  {
    "id": 34,
    "question": "<p>The data engineering team at an e-commerce company uses Apache Hive on Amazon EMR. The team has noticed sub-par performance for the cluster during the morning peak load hours when 95% of the daily queries are executed. The team has also observed that HDFS's (Hadoop Distributed File System) usage never surpasses 10%.</p>\n\n<p>As an AWS Certified Data Engineer Associate,  which of the following solutions would you recommend to resolve these performance issues?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up spot fleet configurations for core and task nodes. Leverage the CloudWatch YARNMemoryAvailablePercentage metric to configure automatic scaling policies to scale out/scale in the spot fleet</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up instance group configurations for core and task nodes. Leverage the CloudWatch YARNMemoryAvailablePercentage metric to configure automatic scaling policies to scale out/scale in the instance groups</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up instance group configurations for core and task nodes. Leverage the CloudWatch CapacityRemainingGB metric to configure automatic scaling policies to scale out/scale in the instance groups</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up spot fleet configurations for core and task nodes. Leverage the CloudWatch CapacityRemainingGB metric to configure automatic scaling policies to scale out/scale in the spot fleet</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up instance group configurations for core and task nodes. Leverage the CloudWatch YARNMemoryAvailablePercentage metric to configure automatic scaling policies to scale out/scale in the instance groups</strong></p>\n\n<p>Apache Hive is an open-source, distributed, fault-tolerant system that provides data warehouse-like query capabilities. It enables users to read, write, and manage petabytes of data using a SQL-like interface. Learn more about Apache Hive here.</p>\n\n<p>Apache Hive is natively supported in Amazon EMR, and you can quickly and easily create managed Apache Hive clusters from the AWS Management Console, AWS CLI, or the Amazon EMR API. When you create a cluster and specify the configuration of the master node, core nodes, and task nodes, you have two configuration options. You can use instance fleets or uniform instance groups.</p>\n\n<p>Apache Hive Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q18-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q18-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/emr/features/hive/\">https://aws.amazon.com/emr/features/hive/</a><p></p>\n\n<p>The instance fleets configuration offers the widest variety of provisioning options for Amazon EC2 instances. Each node type has a single instance fleet, and using a task instance fleet is optional. You can specify up to five EC2 instance types per fleet, or 30 EC2 instance types per fleet when you create a cluster using the AWS CLI or Amazon EMR API and an allocation strategy for On-Demand and Spot Instances.</p>\n\n<p>Each Amazon EMR cluster can include up to 50 instance groups: one master instance group that contains one Amazon EC2 instance, a core instance group that contains one or more EC2 instances, and up to 48 optional task instance groups. Each core and task instance group can contain any number of Amazon EC2 instances.</p>\n\n<p>Instance Groups vs Instance Fleets:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q18-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q18-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html</a><p></p>\n\n<p>For the given use case, the correct solution should support automatic scaling. You can set up automatic scaling in Amazon EMR for an instance group, adding and removing instances automatically based on the value of an Amazon CloudWatch metric that you specify. The metric <code>YARNMemoryAvailablePercentage</code> represents the percentage of remaining memory available to YARN (YARNMemoryAvailablePercentage = MemoryAvailableMB / MemoryTotalMB). This value is useful for scaling cluster resources based on YARN memory usage.</p>\n\n<p>Amazon EMR metrics:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q18-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q18-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up spot fleet configurations for core and task nodes. Leverage the CloudWatch YARNMemoryAvailablePercentage metric to configure automatic scaling policies to scale out/scale in the spot fleet</strong></p>\n\n<p><strong>Set up spot fleet configurations for core and task nodes. Leverage the CloudWatch CapacityRemainingGB metric to configure automatic scaling policies to scale out/scale in the spot fleet</strong></p>\n\n<p>A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that is launched based on criteria that you specify. The Spot Fleet selects the Spot capacity pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. Spot fleet is applicable to EC2 instances and cannot be used directly with EMR. So both these options are incorrect. With EMR, you need to use the instance fleet option which does support automatic scaling.</p>\n\n<p><strong>Set up instance group configurations for core and task nodes. Leverage the CloudWatch CapacityRemainingGB metric to configure automatic scaling policies to scale out/scale in the instance groups</strong> - The metric <code>CapacityRemainingGB</code> represents the amount of remaining HDFS disk capacity. It can be used to monitor cluster progress and monitor cluster health. It cannot be used to scale cluster resources. In addition, the use-case states that HDFS usage never surpasses 10%, so this metric cannot be a criterion for right-sizing the cluster.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/emr/features/hive/\">https://aws.amazon.com/emr/features/hive/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up instance group configurations for core and task nodes. Leverage the CloudWatch YARNMemoryAvailablePercentage metric to configure automatic scaling policies to scale out/scale in the instance groups</strong>"
      },
      {
        "answer": "",
        "explanation": "Apache Hive is an open-source, distributed, fault-tolerant system that provides data warehouse-like query capabilities. It enables users to read, write, and manage petabytes of data using a SQL-like interface. Learn more about Apache Hive here."
      },
      {
        "answer": "",
        "explanation": "Apache Hive is natively supported in Amazon EMR, and you can quickly and easily create managed Apache Hive clusters from the AWS Management Console, AWS CLI, or the Amazon EMR API. When you create a cluster and specify the configuration of the master node, core nodes, and task nodes, you have two configuration options. You can use instance fleets or uniform instance groups."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q18-i1.jpg",
        "answer": "",
        "explanation": "Apache Hive Overview:"
      },
      {
        "link": "https://aws.amazon.com/emr/features/hive/"
      },
      {
        "answer": "",
        "explanation": "The instance fleets configuration offers the widest variety of provisioning options for Amazon EC2 instances. Each node type has a single instance fleet, and using a task instance fleet is optional. You can specify up to five EC2 instance types per fleet, or 30 EC2 instance types per fleet when you create a cluster using the AWS CLI or Amazon EMR API and an allocation strategy for On-Demand and Spot Instances."
      },
      {
        "answer": "",
        "explanation": "Each Amazon EMR cluster can include up to 50 instance groups: one master instance group that contains one Amazon EC2 instance, a core instance group that contains one or more EC2 instances, and up to 48 optional task instance groups. Each core and task instance group can contain any number of Amazon EC2 instances."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q18-i2.jpg",
        "answer": "",
        "explanation": "Instance Groups vs Instance Fleets:"
      },
      {
        "link": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html"
      },
      {
        "answer": "",
        "explanation": "For the given use case, the correct solution should support automatic scaling. You can set up automatic scaling in Amazon EMR for an instance group, adding and removing instances automatically based on the value of an Amazon CloudWatch metric that you specify. The metric <code>YARNMemoryAvailablePercentage</code> represents the percentage of remaining memory available to YARN (YARNMemoryAvailablePercentage = MemoryAvailableMB / MemoryTotalMB). This value is useful for scaling cluster resources based on YARN memory usage."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q18-i3.jpg",
        "answer": "",
        "explanation": "Amazon EMR metrics:"
      },
      {
        "link": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up spot fleet configurations for core and task nodes. Leverage the CloudWatch YARNMemoryAvailablePercentage metric to configure automatic scaling policies to scale out/scale in the spot fleet</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Set up spot fleet configurations for core and task nodes. Leverage the CloudWatch CapacityRemainingGB metric to configure automatic scaling policies to scale out/scale in the spot fleet</strong>"
      },
      {
        "answer": "",
        "explanation": "A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that is launched based on criteria that you specify. The Spot Fleet selects the Spot capacity pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. Spot fleet is applicable to EC2 instances and cannot be used directly with EMR. So both these options are incorrect. With EMR, you need to use the instance fleet option which does support automatic scaling."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up instance group configurations for core and task nodes. Leverage the CloudWatch CapacityRemainingGB metric to configure automatic scaling policies to scale out/scale in the instance groups</strong> - The metric <code>CapacityRemainingGB</code> represents the amount of remaining HDFS disk capacity. It can be used to monitor cluster progress and monitor cluster health. It cannot be used to scale cluster resources. In addition, the use-case states that HDFS usage never surpasses 10%, so this metric cannot be a criterion for right-sizing the cluster."
      }
    ],
    "references": [
      "https://aws.amazon.com/emr/features/hive/",
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html",
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html"
    ]
  },
  {
    "id": 35,
    "question": "<p>A university has tie-ups with local hospitals to share anonymized health statistics of people. The data is stored in Amazon S3 as .csv files. Amazon Athena is used to run extensive analytics on the data for finding correlations between different parameters in the data. The university is facing high costs and performance-related issues as the volume of data is growing rapidly. The data in the S3 bucket is already partitioned by date and the university does not want to change this partition scheme.</p>\n\n<p>As a data engineer, how can you further improve query performance? (Select two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Remove partitions and perform <code>data bucketing</code> on the S3 bucket</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Transform .csv files to Parquet format by fetching only the data fields required for predicates</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The S3 bucket should be configured in the same AWS Region where the Athena queries are being run</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Transform .csv files to JSON format by fetching the required key-value pairs only</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>The S3 bucket should be configured in the same Availability Zone where the Athena queries are being run</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>The S3 bucket should be configured in the same AWS Region where the Athena queries are being run</strong></p>\n\n<p>Athena supports the ability to query Amazon S3 data in an AWS Region that is different from the Region in which you are using Athena. Querying across Regions can be an option when moving the data is not practical or permissible, or if you want to query data across multiple regions. Even if Athena is not available in a particular Region, data from that Region can be queried from another Region in which Athena is available.</p>\n\n<p>However, cross-Region access to Athena has several limitations. Most importantly, running a cross-region query can result in more data transferred than the size of the dataset. In addition, there are cross-Region data transfer charges also involved. Therefore, for further optimizations, the S3 bucket should be configured in the same AWS Region where the Athena queries are being run.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q10-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q10-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/querying-across-regions.html\">https://docs.aws.amazon.com/athena/latest/ug/querying-across-regions.html</a><p></p>\n\n<p><strong>Transform .csv files to Parquet format by fetching only the data fields required for predicates</strong></p>\n\n<p>Apache Parquet and Apache ORC are popular file formats for analytics workloads. They are often described as columnar file formats because they store data not by row, but by column. They also have features that allow query engines to reduce the amount of data that needs to be loaded in different ways. For example, by storing and compressing columns separately, you can achieve higher compression ratios and only the columns referenced in a query need to be read.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q10-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q10-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Transform .csv files to JSON format by fetching the required key-value pairs only</strong> - As mentioned above, Apache Parquet and ORC are columnar storage formats that are optimized for fast retrieval of data and used in AWS analytical applications. JSON format does not offer any such advantages, so this option is incorrect.</p>\n\n<p><strong>The S3 bucket should be configured in the same Availability Zone where the Athena queries are being run</strong> - Amazon Athena as well as Amazon S3 manage data at the AWS Region level, rather than the Availability Zone (AZ) level. Configuring an S3 bucket at the AZ level is not possible. This option has been added as a distractor.</p>\n\n<p><strong>Remove partitions and perform <code>data bucketing</code> on the S3 bucket</strong> - Bucketing is a way to organize the records of a dataset into categories called buckets. This meaning of bucket and bucketing is different from, and should not be confused with Amazon S3 buckets. In data bucketing, records that have the same value for a property go into the same bucket. Records are distributed as evenly as possible among buckets so that each bucket has roughly the same amount of data. In practice, the buckets are files, and a hash function determines the bucket that a record goes into. A bucketed dataset will have one or more files per bucket per partition. The bucket that a file belongs to is encoded in the file name. Bucketing is useful when a dataset is bucketed by a certain property and you want to retrieve records in which that property has a certain value. Because the data is bucketed, Athena can use the value to determine which files to look at. For example, suppose a dataset is bucketed by customer_id and you want to find all records for a specific customer. Athena determines the bucket that contains those records and only reads the files in that bucket.</p>\n\n<p>Good candidates for bucketing occur when you have columns that have high cardinality (that is, have many distinct values), are uniformly distributed, and that you frequently query for specific values.</p>\n\n<p>Since the use case clearly states that you should improve query performance within the constraints of maintaining the partition scheme, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/querying-across-regions.html\">https://docs.aws.amazon.com/athena/latest/ug/querying-across-regions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html\">https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html\">https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/ctas-partitioning-and-bucketing.html\">https://docs.aws.amazon.com/athena/latest/ug/ctas-partitioning-and-bucketing.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The S3 bucket should be configured in the same AWS Region where the Athena queries are being run</strong>"
      },
      {
        "answer": "",
        "explanation": "Athena supports the ability to query Amazon S3 data in an AWS Region that is different from the Region in which you are using Athena. Querying across Regions can be an option when moving the data is not practical or permissible, or if you want to query data across multiple regions. Even if Athena is not available in a particular Region, data from that Region can be queried from another Region in which Athena is available."
      },
      {
        "answer": "",
        "explanation": "However, cross-Region access to Athena has several limitations. Most importantly, running a cross-region query can result in more data transferred than the size of the dataset. In addition, there are cross-Region data transfer charges also involved. Therefore, for further optimizations, the S3 bucket should be configured in the same AWS Region where the Athena queries are being run."
      },
      {
        "link": "https://docs.aws.amazon.com/athena/latest/ug/querying-across-regions.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Transform .csv files to Parquet format by fetching only the data fields required for predicates</strong>"
      },
      {
        "answer": "",
        "explanation": "Apache Parquet and Apache ORC are popular file formats for analytics workloads. They are often described as columnar file formats because they store data not by row, but by column. They also have features that allow query engines to reduce the amount of data that needs to be loaded in different ways. For example, by storing and compressing columns separately, you can achieve higher compression ratios and only the columns referenced in a query need to be read."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Transform .csv files to JSON format by fetching the required key-value pairs only</strong> - As mentioned above, Apache Parquet and ORC are columnar storage formats that are optimized for fast retrieval of data and used in AWS analytical applications. JSON format does not offer any such advantages, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>The S3 bucket should be configured in the same Availability Zone where the Athena queries are being run</strong> - Amazon Athena as well as Amazon S3 manage data at the AWS Region level, rather than the Availability Zone (AZ) level. Configuring an S3 bucket at the AZ level is not possible. This option has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Remove partitions and perform <code>data bucketing</code> on the S3 bucket</strong> - Bucketing is a way to organize the records of a dataset into categories called buckets. This meaning of bucket and bucketing is different from, and should not be confused with Amazon S3 buckets. In data bucketing, records that have the same value for a property go into the same bucket. Records are distributed as evenly as possible among buckets so that each bucket has roughly the same amount of data. In practice, the buckets are files, and a hash function determines the bucket that a record goes into. A bucketed dataset will have one or more files per bucket per partition. The bucket that a file belongs to is encoded in the file name. Bucketing is useful when a dataset is bucketed by a certain property and you want to retrieve records in which that property has a certain value. Because the data is bucketed, Athena can use the value to determine which files to look at. For example, suppose a dataset is bucketed by customer_id and you want to find all records for a specific customer. Athena determines the bucket that contains those records and only reads the files in that bucket."
      },
      {
        "answer": "",
        "explanation": "Good candidates for bucketing occur when you have columns that have high cardinality (that is, have many distinct values), are uniformly distributed, and that you frequently query for specific values."
      },
      {
        "answer": "",
        "explanation": "Since the use case clearly states that you should improve query performance within the constraints of maintaining the partition scheme, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/querying-across-regions.html",
      "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
      "https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html",
      "https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html",
      "https://docs.aws.amazon.com/athena/latest/ug/ctas-partitioning-and-bucketing.html"
    ]
  },
  {
    "id": 36,
    "question": "<p>A company wants to publish an event into an Amazon Simple Queue Service (Amazon SQS) queue whenever a new object is uploaded on Amazon S3.</p>\n\n<p>Which of the following statements are true regarding this functionality?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Both Standard Amazon SQS queue and FIFO SQS queue are allowed as an Amazon S3 event notification destination</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Neither Standard Amazon SQS queue nor FIFO SQS queue is allowed as an Amazon S3 event notification destination</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Only FIFO Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas Standard SQS queue is not allowed</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Only Standard Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas FIFO SQS queue is not allowed</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Only Standard Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas FIFO SQS queue is not allowed</strong></p>\n\n<p>The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications.</p>\n\n<p>Amazon S3 supports the following destinations where it can publish events:</p>\n\n<p>Amazon Simple Notification Service (Amazon SNS) topic</p>\n\n<p>Amazon Simple Queue Service (Amazon SQS) queue</p>\n\n<p>AWS Lambda</p>\n\n<p>Currently, the Standard Amazon SQS queue is only allowed as an Amazon S3 event notification destination, whereas the FIFO SQS queue is not allowed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Both Standard Amazon SQS queue and FIFO SQS queue are allowed as an Amazon S3 event notification destination</strong></p>\n\n<p><strong>Neither Standard Amazon SQS queue nor FIFO SQS queue is allowed as an Amazon S3 event notification destination</strong></p>\n\n<p><strong>Only FIFO Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas Standard SQS queue is not allowed</strong></p>\n\n<p>These three options contradict the details provided in the explanation above. To summarize, the Standard Amazon SQS queue is only allowed as an Amazon S3 event notification destination, whereas the FIFO SQS queue is not allowed. Hence these three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Only Standard Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas FIFO SQS queue is not allowed</strong>"
      },
      {
        "answer": "",
        "explanation": "The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 supports the following destinations where it can publish events:"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Notification Service (Amazon SNS) topic"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (Amazon SQS) queue"
      },
      {
        "answer": "",
        "explanation": "AWS Lambda"
      },
      {
        "answer": "",
        "explanation": "Currently, the Standard Amazon SQS queue is only allowed as an Amazon S3 event notification destination, whereas the FIFO SQS queue is not allowed."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Both Standard Amazon SQS queue and FIFO SQS queue are allowed as an Amazon S3 event notification destination</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Neither Standard Amazon SQS queue nor FIFO SQS queue is allowed as an Amazon S3 event notification destination</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Only FIFO Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas Standard SQS queue is not allowed</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the details provided in the explanation above. To summarize, the Standard Amazon SQS queue is only allowed as an Amazon S3 event notification destination, whereas the FIFO SQS queue is not allowed. Hence these three options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html"
    ]
  },
  {
    "id": 37,
    "question": "<p>A data engineer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the network access control list (network ACL) of the subnet for the Amazon EC2 instance. However, the data engineer is unable to connect to the service running on the Amazon EC2 instance.</p>\n\n<p>How will you fix this issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) is stateless, so you must allow both inbound and outbound traffic</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Rules associated with network access control list (network ACL) should never be modified from the command line. An attempt to modify rules from the command line blocks the rule and results in an erratic behavior</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Network access control list (network ACL) is stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) is stateless, so you must allow both inbound and outbound traffic</strong></p>\n\n<p>Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic.</p>\n\n<p>To enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as outbound traffic from ephemeral ports. When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.</p>\n\n<p>The designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL.</p>\n\n<p>By default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range.</p>\n\n<p>If you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must establish a route through a virtual private gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Network access control list (network ACL) is stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic</strong> - This is incorrect, as discussed above.</p>\n\n<p><strong>IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)</strong> - This is a made-up option and just added as a distractor.</p>\n\n<p><strong>Rules associated with network access control list (network ACL) should never be modified from the command line. An attempt to modify rules from the command line blocks the rule and results in an erratic behavior</strong> - This option is a distractor. AWS does not support modifying rules of Network ACLs from the command line tool.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/\">https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) is stateless, so you must allow both inbound and outbound traffic</strong>"
      },
      {
        "answer": "",
        "explanation": "Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic."
      },
      {
        "answer": "",
        "explanation": "To enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as outbound traffic from ephemeral ports. When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port."
      },
      {
        "answer": "",
        "explanation": "The designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL."
      },
      {
        "answer": "",
        "explanation": "By default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range."
      },
      {
        "answer": "",
        "explanation": "If you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must establish a route through a virtual private gateway."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Network access control list (network ACL) is stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic</strong> - This is incorrect, as discussed above."
      },
      {
        "answer": "",
        "explanation": "<strong>IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)</strong> - This is a made-up option and just added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Rules associated with network access control list (network ACL) should never be modified from the command line. An attempt to modify rules from the command line blocks the rule and results in an erratic behavior</strong> - This option is a distractor. AWS does not support modifying rules of Network ACLs from the command line tool."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/"
    ]
  },
  {
    "id": 38,
    "question": "<p>A retail company is migrating its infrastructure from the on-premises data center to AWS Cloud. The company wants to deploy its two-tier application with the EC2 instance-based web servers in a public subnet and PostgreSQL RDS-based database layer in a private subnet. The company wants to ensure that the database access credentials used by the web servers are handled securely as well as these credentials are changed every 90 days in an automated way using a built-in integration.</p>\n\n<p>Which of the following solutions would you recommend for the given use case?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the database access credentials in a KMS encrypted text file on EFS. Configure the application web servers to retrieve the credentials from EFS on system boot. Write custom code to change the database access credentials stored on the encrypted file after 90 days</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store the database access credentials as the EC2 instance user data. Configure the application web servers to retrieve the credentials from the user data while bootstrapping. Write custom code to change the database access credentials stored in the user data after 90 days</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Secrets Manager to store the database access credentials with the rotation interval configured to 90 days. Set up the application web servers to retrieve the credentials from the Secrets Manager</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Store the database access credentials in an SSE-S3 encrypted text file on S3. Configure the application web servers to retrieve the credentials from S3 on system boot. Write custom code to change the database access credentials stored on the encrypted file after 90 days</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Secrets Manager to store the database access credentials with the rotation interval configured to 90 days. Set up the application web servers to retrieve the credentials from the Secrets Manager</strong></p>\n\n<p>AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.</p>\n\n<p>Benefits of Secrets Manager:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q40-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q40-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the database access credentials as the EC2 instance user data. Configure the application web servers to retrieve the credentials from the user data while bootstrapping. Write custom code to change the database access credentials stored in the user data after 90 days</strong> - The given use-case mandates that the database secrets are rotated every 90 days using a built-in integration in an automated way. Writing custom code to change these credentials after 90 days violates this key requirement.</p>\n\n<p><strong>Store the database access credentials in an SSE-S3 encrypted text file on S3. Configure the application web servers to retrieve the credentials from S3 on system boot. Write custom code to change the database access credentials stored on the encrypted file after 90 days</strong></p>\n\n<p><strong>Store the database access credentials in a KMS-encrypted text file on EFS. Configure the application web servers to retrieve the credentials from EFS on system boot. Write custom code to change the database access credentials stored on the encrypted file after 90 days</strong></p>\n\n<p>A key requirement of the use case is to automate the database access secrets rotation every 90 days using a built-in integration. Both these options involve writing custom code to change the database access credentials after 90 days. In addition, storing database access credentials in an external file (even if encrypted) is not a best practice, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Secrets Manager to store the database access credentials with the rotation interval configured to 90 days. Set up the application web servers to retrieve the credentials from the Secrets Manager</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q40-i1.jpg",
        "answer": "",
        "explanation": "Benefits of Secrets Manager:"
      },
      {
        "link": "https://aws.amazon.com/secrets-manager/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the database access credentials as the EC2 instance user data. Configure the application web servers to retrieve the credentials from the user data while bootstrapping. Write custom code to change the database access credentials stored in the user data after 90 days</strong> - The given use-case mandates that the database secrets are rotated every 90 days using a built-in integration in an automated way. Writing custom code to change these credentials after 90 days violates this key requirement."
      },
      {
        "answer": "",
        "explanation": "<strong>Store the database access credentials in an SSE-S3 encrypted text file on S3. Configure the application web servers to retrieve the credentials from S3 on system boot. Write custom code to change the database access credentials stored on the encrypted file after 90 days</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Store the database access credentials in a KMS-encrypted text file on EFS. Configure the application web servers to retrieve the credentials from EFS on system boot. Write custom code to change the database access credentials stored on the encrypted file after 90 days</strong>"
      },
      {
        "answer": "",
        "explanation": "A key requirement of the use case is to automate the database access secrets rotation every 90 days using a built-in integration. Both these options involve writing custom code to change the database access credentials after 90 days. In addition, storing database access credentials in an external file (even if encrypted) is not a best practice, so both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/secrets-manager/"
    ]
  },
  {
    "id": 39,
    "question": "<p>A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS.</p>\n\n<p>Which of the following options represent the correct capabilities of an encrypted Amazon EBS volume? (Select three)</p>",
    "corrects": [
      2,
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Any snapshot created from the volume is NOT encrypted</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Data at rest inside the volume is encrypted</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Data moving between the volume and the instance is NOT encrypted</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Any snapshot created from the volume is encrypted</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Data moving between the volume and the instance is encrypted</p>",
        "correct": true
      },
      {
        "id": 6,
        "answer": "<p>Data at rest inside the volume is NOT encrypted</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Data at rest inside the volume is encrypted</strong></p>\n\n<p><strong>Any snapshot created from the volume is encrypted</strong></p>\n\n<p><strong>Data moving between the volume and the instance is encrypted</strong></p>\n\n<p>Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with Amazon EC2 instances. When you create an encrypted Amazon EBS volume and attach it to a supported instance type, data stored at rest on the volume, data moving between the volume and the instance, snapshots created from the volume, and volumes created from those snapshots are all encrypted. It uses AWS Key Management Service (AWS KMS) customer master keys (CMK) when creating encrypted volumes and snapshots. Encryption operations occur on the servers that host Amazon EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached Amazon EBS storage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Data moving between the volume and the instance is NOT encrypted</strong></p>\n\n<p><strong>Any snapshot created from the volume is NOT encrypted</strong></p>\n\n<p><strong>Data at rest inside the volume is NOT encrypted</strong></p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Data at rest inside the volume is encrypted</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Any snapshot created from the volume is encrypted</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Data moving between the volume and the instance is encrypted</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with Amazon EC2 instances. When you create an encrypted Amazon EBS volume and attach it to a supported instance type, data stored at rest on the volume, data moving between the volume and the instance, snapshots created from the volume, and volumes created from those snapshots are all encrypted. It uses AWS Key Management Service (AWS KMS) customer master keys (CMK) when creating encrypted volumes and snapshots. Encryption operations occur on the servers that host Amazon EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached Amazon EBS storage."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Data moving between the volume and the instance is NOT encrypted</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Any snapshot created from the volume is NOT encrypted</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Data at rest inside the volume is NOT encrypted</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html"
    ]
  },
  {
    "id": 40,
    "question": "<p>You are a data engineer at an IT company that recently moved its production application to AWS and migrated data from PostgreSQL to AWS DynamoDB. You are adding new tables to AWS DynamoDB and need to allow your application to query your data by the primary key and an alternate key. This option must be added at the outset when you are first creating tables, otherwise, changes cannot be done once the table is created.</p>\n\n<p>Which of the following actions would you suggest?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create DynamoDB Streams</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a Local Secondary Index (LSI)</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Migrate away from DynamoDB</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a Global Secondary Index (GSI)</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a Local Secondary Index (LSI)</strong></p>\n\n<p>Some applications only need to query data using the base table's primary key; however, there may be situations where an alternate sort key would be helpful. To give your application a choice of sort keys, you can create one or more local secondary indexes on a table and issue Query or Scan requests against these indexes.</p>\n\n<p>Local secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist.</p>\n\n<p>Differences between GSI and LSI:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q39-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q39-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create DynamoDB Streams</strong> - DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real-time. This option is not relevant to the given use case.</p>\n\n<p><strong>Create a Global Secondary Index (GSI)</strong> - GSI is an index with a partition key and a sort key that can be different from those on the base table. Global secondary indexes can be created at the same time that you create a table. You can also add a new global secondary index to an existing table, or delete an existing global secondary index.</p>\n\n<p><strong>Migrate away from DynamoDB</strong> - Migrating to another database that is not NoSQL may cause you to make changes that require substantial code development.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a Local Secondary Index (LSI)</strong>"
      },
      {
        "answer": "",
        "explanation": "Some applications only need to query data using the base table's primary key; however, there may be situations where an alternate sort key would be helpful. To give your application a choice of sort keys, you can create one or more local secondary indexes on a table and issue Query or Scan requests against these indexes."
      },
      {
        "answer": "",
        "explanation": "Local secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q39-i1.jpg",
        "answer": "",
        "explanation": "Differences between GSI and LSI:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create DynamoDB Streams</strong> - DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real-time. This option is not relevant to the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a Global Secondary Index (GSI)</strong> - GSI is an index with a partition key and a sort key that can be different from those on the base table. Global secondary indexes can be created at the same time that you create a table. You can also add a new global secondary index to an existing table, or delete an existing global secondary index."
      },
      {
        "answer": "",
        "explanation": "<strong>Migrate away from DynamoDB</strong> - Migrating to another database that is not NoSQL may cause you to make changes that require substantial code development."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html"
    ]
  },
  {
    "id": 41,
    "question": "<p>A big data analytics company is working on a real-time vehicle tracking solution. The data processing workflow involves both I/O-intensive and throughput-intensive database workloads. The data engineering team needs to store this real-time data in a NoSQL database hosted on an Amazon EC2 instance and needs to support up to 25,000 IOPS per volume.</p>\n\n<p>Which of the following Amazon Elastic Block Store (Amazon EBS) volume types would you recommend for this use-case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Provisioned IOPS SSD (io1)</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Cold HDD (sc1)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>General Purpose SSD (gp2)</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Throughput Optimized HDD (st1)</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Provisioned IOPS SSD (io1)</strong></p>\n\n<p>Provisioned IOPS SSD (io1) is backed by solid-state drives (SSDs) and is a high-performance Amazon EBS storage option designed for critical, I/O intensive database and application workloads, as well as throughput-intensive database workloads. io1 is designed to deliver a consistent baseline performance of up to 50 IOPS/GB to a maximum of 64,000 IOPS and provide up to 1,000 MB/s of throughput per volume. Therefore, the io1 volume type would be able to meet the requirement of 25,000 IOPS per volume for the given use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>General Purpose SSD (gp2)</strong> - gp2 is backed by solid-state drives (SSDs) and is suitable for a broad range of transactional workloads, including dev/test environments, low-latency interactive applications, and boot volumes. It supports max IOPS/Volume of 16,000.</p>\n\n<p><strong>Cold HDD (sc1)</strong> - sc1 is backed by hard disk drives (HDDs). It is ideal for less frequently accessed workloads with large, cold datasets. It supports max IOPS/Volume of 250.</p>\n\n<p><strong>Throughput Optimized HDD (st1)</strong> - st1 is backed by hard disk drives (HDDs) and is ideal for frequently accessed, throughput-intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads. It supports max IOPS/Volume of 500.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/ebs/volume-types/\">https://aws.amazon.com/ebs/volume-types/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Provisioned IOPS SSD (io1)</strong>"
      },
      {
        "answer": "",
        "explanation": "Provisioned IOPS SSD (io1) is backed by solid-state drives (SSDs) and is a high-performance Amazon EBS storage option designed for critical, I/O intensive database and application workloads, as well as throughput-intensive database workloads. io1 is designed to deliver a consistent baseline performance of up to 50 IOPS/GB to a maximum of 64,000 IOPS and provide up to 1,000 MB/s of throughput per volume. Therefore, the io1 volume type would be able to meet the requirement of 25,000 IOPS per volume for the given use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>General Purpose SSD (gp2)</strong> - gp2 is backed by solid-state drives (SSDs) and is suitable for a broad range of transactional workloads, including dev/test environments, low-latency interactive applications, and boot volumes. It supports max IOPS/Volume of 16,000."
      },
      {
        "answer": "",
        "explanation": "<strong>Cold HDD (sc1)</strong> - sc1 is backed by hard disk drives (HDDs). It is ideal for less frequently accessed workloads with large, cold datasets. It supports max IOPS/Volume of 250."
      },
      {
        "answer": "",
        "explanation": "<strong>Throughput Optimized HDD (st1)</strong> - st1 is backed by hard disk drives (HDDs) and is ideal for frequently accessed, throughput-intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads. It supports max IOPS/Volume of 500."
      }
    ],
    "references": [
      "https://aws.amazon.com/ebs/volume-types/"
    ]
  },
  {
    "id": 42,
    "question": "<p>An online gaming application has a large chunk of its traffic coming from users who download static assets such as historic leaderboard reports and the game tactics for various games. The current infrastructure and design are unable to handle the traffic and application freezes on most of the pages.</p>\n\n<p>Which of the following is a cost-optimal solution that requires the LEAST operational overhead?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets</strong></p>\n\n<p>When you put your content in an Amazon S3 bucket in the cloud, a lot of things become much easier. First, you don’t need to plan for and allocate a specific amount of storage space because Amazon S3 buckets scale automatically. As Amazon S3 is a serverless service, you don’t need to manage or patch servers that store files yourself; you just put and get your content. Finally, even if you require a server for your application (for example, because you have a dynamic application), the server can be smaller because it doesn’t have to handle requests for static content.</p>\n\n<p>Amazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from Amazon S3 directly to your users. Amazon CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located.</p>\n\n<p>When a user requests content that you serve with Amazon CloudFront, their request is routed to a nearby Edge Location. If Amazon CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the Amazon S3 bucket where you’ve stored your content. Then, for the next local request for the same content, it’s already cached nearby and can be served immediately.</p>\n\n<p>By caching your content in Edge Locations, Amazon CloudFront reduces the load on your Amazon S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using Amazon CloudFront is often more cost-effective than serving files directly from Amazon S3, and there is no data transfer fee from Amazon S3 to Amazon CloudFront. You only pay for what is delivered to the internet from Amazon CloudFront, plus request fees.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture</strong> - Amazon RDS is not the right choice for the given scenario because of the overhead of a database management system, as the given use-case can be addressed by using the Amazon S3 storage solution.</p>\n\n<p><strong>Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. However, Amazon DynamoDB is overkill for the given use-case and will prove to be a very costly solution.</p>\n\n<p><strong>Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency</strong> - As discussed above, Amazon RDS is not needed for this use case where the web application needs to display static pages and facilitate downloads of historic data. Amazon S3 is much better suited for this requirement.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets</strong>"
      },
      {
        "answer": "",
        "explanation": "When you put your content in an Amazon S3 bucket in the cloud, a lot of things become much easier. First, you don’t need to plan for and allocate a specific amount of storage space because Amazon S3 buckets scale automatically. As Amazon S3 is a serverless service, you don’t need to manage or patch servers that store files yourself; you just put and get your content. Finally, even if you require a server for your application (for example, because you have a dynamic application), the server can be smaller because it doesn’t have to handle requests for static content."
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from Amazon S3 directly to your users. Amazon CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located."
      },
      {
        "answer": "",
        "explanation": "When a user requests content that you serve with Amazon CloudFront, their request is routed to a nearby Edge Location. If Amazon CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the Amazon S3 bucket where you’ve stored your content. Then, for the next local request for the same content, it’s already cached nearby and can be served immediately."
      },
      {
        "answer": "",
        "explanation": "By caching your content in Edge Locations, Amazon CloudFront reduces the load on your Amazon S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using Amazon CloudFront is often more cost-effective than serving files directly from Amazon S3, and there is no data transfer fee from Amazon S3 to Amazon CloudFront. You only pay for what is delivered to the internet from Amazon CloudFront, plus request fees."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture</strong> - Amazon RDS is not the right choice for the given scenario because of the overhead of a database management system, as the given use-case can be addressed by using the Amazon S3 storage solution."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. However, Amazon DynamoDB is overkill for the given use-case and will prove to be a very costly solution."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency</strong> - As discussed above, Amazon RDS is not needed for this use case where the web application needs to display static pages and facilitate downloads of historic data. Amazon S3 is much better suited for this requirement."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/"
    ]
  },
  {
    "id": 43,
    "question": "<p>You are a data engineer at an IT company. The company has multiple enterprise customers that manage their own mobile applications that capture and send data to Amazon Kinesis Data Streams. They have been getting a <code>ProvisionedThroughputExceededException</code> exception. Upon analysis, you notice that messages are being sent one by one at a high rate.</p>\n\n<p>Which of the following options will help with the exception while keeping costs at a minimum?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Decrease the Stream retention duration</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Increase the number of shards</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use batch messages</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Exponential Backoff</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use batch messages</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Amazon Kinesis Data Streams Overview:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a><p></p>\n\n<p>When a host needs to send many records per second (RPS) to Amazon Kinesis, simply calling the basic PutRecord API action in a loop is inadequate. To reduce overhead and increase throughput, the application must batch records and implement parallel HTTP requests. This will increase the efficiency overall and ensure you are optimally using the shards.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Exponential Backoff</strong> - While this may help in the short term, as soon as the request rate increases, you will see the <code>ProvisionedThroughputExceededException</code> exception again.</p>\n\n<p><strong>Increase the number of shards</strong> - Increasing shards could be a short-term fix but will substantially increase the cost, so this option is ruled out.</p>\n\n<p><strong>Decrease the Stream retention duration</strong> - This operation may result in data loss and won't help with the exceptions, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/\">https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use batch messages</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png",
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams Overview:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/"
      },
      {
        "answer": "",
        "explanation": "When a host needs to send many records per second (RPS) to Amazon Kinesis, simply calling the basic PutRecord API action in a loop is inadequate. To reduce overhead and increase throughput, the application must batch records and implement parallel HTTP requests. This will increase the efficiency overall and ensure you are optimally using the shards."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Exponential Backoff</strong> - While this may help in the short term, as soon as the request rate increases, you will see the <code>ProvisionedThroughputExceededException</code> exception again."
      },
      {
        "answer": "",
        "explanation": "<strong>Increase the number of shards</strong> - Increasing shards could be a short-term fix but will substantially increase the cost, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Decrease the Stream retention duration</strong> - This operation may result in data loss and won't help with the exceptions, so this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/"
    ]
  },
  {
    "id": 44,
    "question": "<p>Consider the following scenario on Amazon S3: A folder INPUT-FOLDER1 has 10 files, 8 files with schema SCH_A and 2 files with schema SCH_B, and another folder INPUT-FOLDER2 has 10 files, 7 files with the schema SCH_A and 3 files with the schema SCH_B. The schemas are defined as follows:</p>\n\n<pre><code>SCH_A:\n{ \"id\": 1, \"first_name\": \"John\", \"last_name\": \"Doe\"}\n{ \"id\": 2, \"first_name\": \"Li\", \"last_name\": \"Juan\"}\n</code></pre>\n\n<pre><code>SCH_B:\n{\"city\":\"Dublin\",\"country\":\"Ireland\"}\n{\"city\":\"Paris\",\"country\":\"France\"}\n</code></pre>\n\n<p>What is the outcome, when the crawler crawls the Amazon Simple Storage Service (Amazon S3) path s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2 separately?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>For both the S3 paths s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2, the crawler creates one table with columns of both the schemas</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>For S3 path s3://INPUT-FOLDER2, the crawler creates one table with columns of both the schemas. And for S3 path s3://INPUT-FOLDER1, the crawler creates two tables, each table having columns of one schema respectively</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>For the S3 path s3://INPUT-FOLDER1, the crawler creates one table with columns of both the schemas. For the S3 path s3://INPUT-FOLDER2, the crawler creates two tables, each table having columns of one schema respectively</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>For both the S3 paths s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2, the crawler creates two tables each, each table having columns of one schema respectively</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>For the S3 path s3://INPUT-FOLDER1, the crawler creates one table with columns of both schemas. For the S3 path s3://INPUT-FOLDER2, the crawler creates two tables, each table having columns of one schema respectively</strong></p>\n\n<p>For schemas to be considered similar, the following conditions must be true:\n1. The partition threshold is higher than 0.7 (70%).\n2. The maximum number of different schemas (also referred to as \"clusters\" in this context) doesn't exceed 5.</p>\n\n<p>The crawler infers the schema at the folder level and compares the schemas across all folders. If the schemas that are compared match, that is, if the partition threshold is higher than 70%, then the schemas are denoted as partitions of a table. If they don’t match, then the crawler creates a table for each folder, resulting in a higher number of tables.</p>\n\n<p>Suppose that the folder DOC-EXAMPLE-FOLDER1 has 10 files, 8 files with schema SCH_A and 2 files with SCH_B.</p>\n\n<p>Suppose that the files with the schema SHC_A are similar to the following:</p>\n\n<pre><code>{ \"id\": 1, \"first_name\": \"John\", \"last_name\": \"Doe\"}\n{ \"id\": 2, \"first_name\": \"Li\", \"last_name\": \"Juan\"}\n</code></pre>\n\n<p>Suppose that the files with the schema SCH_B are similar to the following:</p>\n\n<pre><code>{\"city\":\"Dublin\",\"country\":\"Ireland\"}\n{\"city\":\"Paris\",\"country\":\"France\"}\n</code></pre>\n\n<p>When the crawler crawls the Amazon Simple Storage Service (Amazon S3) path s3://DOC-EXAMPLE-FOLDER1, the crawler creates one table. The table comprises columns of both schemas SCH_A and SCH_B. This is because 80% of the files in the path belong to the SCH_A schema, and 20% of the files belong to the SCH_B schema. Therefore, the partition threshold value is met. Also, the number of different schemas hasn't exceeded the number of clusters, and the cluster size limit isn't exceeded.</p>\n\n<p>Suppose that the folder DOC-EXAMPLE-FOLDER2 has 10 files, 7 files with the schema SCH_A and 3 files with the schema SCH_B.</p>\n\n<p>When the crawler crawls the Amazon S3 path s3://DOC-EXAMPLE-FOLDER2, the crawler creates one table for each file. This is because 70% of the files belong to the schema SCH_A and 30% of the files belong to the schema SCH_B. This means that the partition threshold isn't met. You can check the crawler logs in Amazon CloudWatch to get information on the created tables.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>For both the S3 paths s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2, the crawler creates two tables each, each table having columns of one schema respectively</strong></p>\n\n<p><strong>For both the S3 paths s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2, the crawler creates one table with columns of both the schemas</strong></p>\n\n<p><strong>For S3 path s3://INPUT-FOLDER2, the crawler creates one table with columns of both schemas. For S3 path s3://INPUT-FOLDER1, the crawler creates two tables, each table having columns of one schema respectively</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/glue-crawler-detect-schema/\">https://aws.amazon.com/premiumsupport/knowledge-center/glue-crawler-detect-schema/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>For the S3 path s3://INPUT-FOLDER1, the crawler creates one table with columns of both schemas. For the S3 path s3://INPUT-FOLDER2, the crawler creates two tables, each table having columns of one schema respectively</strong>"
      },
      {
        "answer": "",
        "explanation": "For schemas to be considered similar, the following conditions must be true:\n1. The partition threshold is higher than 0.7 (70%).\n2. The maximum number of different schemas (also referred to as \"clusters\" in this context) doesn't exceed 5."
      },
      {
        "answer": "",
        "explanation": "The crawler infers the schema at the folder level and compares the schemas across all folders. If the schemas that are compared match, that is, if the partition threshold is higher than 70%, then the schemas are denoted as partitions of a table. If they don’t match, then the crawler creates a table for each folder, resulting in a higher number of tables."
      },
      {
        "answer": "",
        "explanation": "Suppose that the folder DOC-EXAMPLE-FOLDER1 has 10 files, 8 files with schema SCH_A and 2 files with SCH_B."
      },
      {
        "answer": "",
        "explanation": "Suppose that the files with the schema SHC_A are similar to the following:"
      },
      {},
      {
        "answer": "",
        "explanation": "Suppose that the files with the schema SCH_B are similar to the following:"
      },
      {},
      {
        "answer": "",
        "explanation": "When the crawler crawls the Amazon Simple Storage Service (Amazon S3) path s3://DOC-EXAMPLE-FOLDER1, the crawler creates one table. The table comprises columns of both schemas SCH_A and SCH_B. This is because 80% of the files in the path belong to the SCH_A schema, and 20% of the files belong to the SCH_B schema. Therefore, the partition threshold value is met. Also, the number of different schemas hasn't exceeded the number of clusters, and the cluster size limit isn't exceeded."
      },
      {
        "answer": "",
        "explanation": "Suppose that the folder DOC-EXAMPLE-FOLDER2 has 10 files, 7 files with the schema SCH_A and 3 files with the schema SCH_B."
      },
      {
        "answer": "",
        "explanation": "When the crawler crawls the Amazon S3 path s3://DOC-EXAMPLE-FOLDER2, the crawler creates one table for each file. This is because 70% of the files belong to the schema SCH_A and 30% of the files belong to the schema SCH_B. This means that the partition threshold isn't met. You can check the crawler logs in Amazon CloudWatch to get information on the created tables."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>For both the S3 paths s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2, the crawler creates two tables each, each table having columns of one schema respectively</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>For both the S3 paths s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2, the crawler creates one table with columns of both the schemas</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>For S3 path s3://INPUT-FOLDER2, the crawler creates one table with columns of both schemas. For S3 path s3://INPUT-FOLDER1, the crawler creates two tables, each table having columns of one schema respectively</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/glue-crawler-detect-schema/"
    ]
  },
  {
    "id": 45,
    "question": "<p>A data engineer is provisioning a DynamoDB table for an e-commerce application. The engineer is planning to allocate 500 Write Capacity Units, 5000 Read Capacity Units, and 50GB of space for this table.</p>\n\n<p>How many partitions will be created in the table for this requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>3 partitions</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>5 partitions</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>8 partitions</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>6 partitions</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>[ 50GB/10GB ] = 5, (500/1000) + (5000/3000) = 2.1 ~ 3, Maximum (5,3) = 5 partitions</strong></p>\n\n<p>DynamoDB supports access patterns using the throughput that you provisioned, as long as the traffic against a given partition does not exceed 3,000 RCUs or 1,000 WCUs.</p>\n\n<p>So, partitions required to support throughput = Roundup[(500WCU/1000WCU) + (5000RCU/3000RCU)] = 3 partitions</p>\n\n<p>10GB is the maximum supported size of a partition, so to support the given size requirements, you will need -</p>\n\n<p>50GB/10GB = 5 partitions</p>\n\n<p>Total number of partitions = Max(5, 3) = 5 partitions</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>8 partitions</strong></p>\n\n<p><strong>6 partitions</strong></p>\n\n<p><strong>3 partitions</strong></p>\n\n<p>These three options contradict the details mentioned in the explanation above, hence these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>[ 50GB/10GB ] = 5, (500/1000) + (5000/3000) = 2.1 ~ 3, Maximum (5,3) = 5 partitions</strong>"
      },
      {
        "answer": "",
        "explanation": "DynamoDB supports access patterns using the throughput that you provisioned, as long as the traffic against a given partition does not exceed 3,000 RCUs or 1,000 WCUs."
      },
      {
        "answer": "",
        "explanation": "So, partitions required to support throughput = Roundup[(500WCU/1000WCU) + (5000RCU/3000RCU)] = 3 partitions"
      },
      {
        "answer": "",
        "explanation": "10GB is the maximum supported size of a partition, so to support the given size requirements, you will need -"
      },
      {
        "answer": "",
        "explanation": "50GB/10GB = 5 partitions"
      },
      {
        "answer": "",
        "explanation": "Total number of partitions = Max(5, 3) = 5 partitions"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>8 partitions</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>6 partitions</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>3 partitions</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the details mentioned in the explanation above, hence these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html"
    ]
  },
  {
    "id": 46,
    "question": "<p>An application writes real-time streaming data of chats into Amazon Kinesis Data Streams partitioned by user id. Before writing this data into an Amazon Elasticsearch Service cluster (now Amazon OpenSearch Service), an AWS Lambda function checks the content for validation. The validation procedure must receive the data of a specific user in the sequence in which the Kinesis data stream received it without changing the order. However, during peak hours, the lag between data received in Kinesis Data Streams to the data reaching OpenSearch Service is very high, thereby resulting in data anomalies.</p>\n\n<p>Which of the following is the best way to fix this issue with the least amount of operational overhead?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Increase the number of shards in the Kinesis data stream to accommodate the increased data during peak hours</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>The validation process should be moved from AWS Lambda to Amazon Firehose to accommodate the high volumes of data</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Replace Amazon Data Streams functionality with Apache Kafka to deal with the high volume of data</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Multiple consumer applications must be reading from the Data Stream exceeding the per-shard limits. Define different Data Streams for different consumer applications</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Increase the number of shards in the Kinesis data stream to accommodate the increased data during peak hours</strong></p>\n\n<p>Amazon Kinesis Data Streams supports resharding, which lets you adjust the number of shards in your stream to adapt to changes in the rate of data flow through the stream. Resharding is considered an advanced operation.</p>\n\n<p>Splitting increases the number of shards in your stream and therefore increases the data capacity of the stream. Because you are charged on a per-shard basis, splitting increases the cost of your stream.</p>\n\n<p>The capacity limits of a Kinesis stream are defined by the number of shards within the stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a <code>ProvisionedThroughputExceeded</code> exception. If this is due to a temporary rise of the stream’s input data rate, retry by the data producer will eventually lead to the completion of the requests. If this is due to a sustained rise of the stream’s input data rate, you should increase the number of shards within your stream to provide enough capacity for the put data calls to consistently succeed. In both cases, Amazon CloudWatch metrics allow you to learn about the change of the stream’s input data rate and the occurrence of <code>ProvisionedThroughputExceeded</code> exceptions.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q29-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q29-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q29-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q29-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The validation process should be moved from AWS Lambda to Amazon Firehose to accommodate the high volumes of data</strong> - AWS Lambda is a highly scalable compute service. You can also increase the concurrency by processing multiple batches from each shard in parallel with Lambda. Lambda can process up to 10 batches in each shard simultaneously. If you increase the number of concurrent batches per shard, Lambda still ensures in-order processing at the partition-key level. Hence, replacing Amazon Lambda with Amazon Firehose is not a correct option.</p>\n\n<p><strong>Replace Amazon Data Streams functionality with Apache Kafka to deal with the high volume of data</strong> - Shifting to Apache Kafka is not an option here since the user wants a solution with the least amount of operational overhead. Adding Kafka into the existing solution would involve significant development effort.</p>\n\n<p><strong>Multiple consumer applications must be reading from the Data Stream exceeding the per-shard limits. Define different Data Streams for different consumer applications</strong> - The use case does not talk about multiple consumers. Hence, this option is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Increase the number of shards in the Kinesis data stream to accommodate the increased data during peak hours</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams supports resharding, which lets you adjust the number of shards in your stream to adapt to changes in the rate of data flow through the stream. Resharding is considered an advanced operation."
      },
      {
        "answer": "",
        "explanation": "Splitting increases the number of shards in your stream and therefore increases the data capacity of the stream. Because you are charged on a per-shard basis, splitting increases the cost of your stream."
      },
      {
        "answer": "",
        "explanation": "The capacity limits of a Kinesis stream are defined by the number of shards within the stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a <code>ProvisionedThroughputExceeded</code> exception. If this is due to a temporary rise of the stream’s input data rate, retry by the data producer will eventually lead to the completion of the requests. If this is due to a sustained rise of the stream’s input data rate, you should increase the number of shards within your stream to provide enough capacity for the put data calls to consistently succeed. In both cases, Amazon CloudWatch metrics allow you to learn about the change of the stream’s input data rate and the occurrence of <code>ProvisionedThroughputExceeded</code> exceptions."
      },
      {
        "link": "https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html"
      },
      {
        "link": "https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The validation process should be moved from AWS Lambda to Amazon Firehose to accommodate the high volumes of data</strong> - AWS Lambda is a highly scalable compute service. You can also increase the concurrency by processing multiple batches from each shard in parallel with Lambda. Lambda can process up to 10 batches in each shard simultaneously. If you increase the number of concurrent batches per shard, Lambda still ensures in-order processing at the partition-key level. Hence, replacing Amazon Lambda with Amazon Firehose is not a correct option."
      },
      {
        "answer": "",
        "explanation": "<strong>Replace Amazon Data Streams functionality with Apache Kafka to deal with the high volume of data</strong> - Shifting to Apache Kafka is not an option here since the user wants a solution with the least amount of operational overhead. Adding Kafka into the existing solution would involve significant development effort."
      },
      {
        "answer": "",
        "explanation": "<strong>Multiple consumer applications must be reading from the Data Stream exceeding the per-shard limits. Define different Data Streams for different consumer applications</strong> - The use case does not talk about multiple consumers. Hence, this option is irrelevant to the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html",
      "https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html"
    ]
  },
  {
    "id": 47,
    "question": "<p>A company is experimenting with DynamoDB in its new test environment. The data engineering team has discovered that some of the write operations have been overwriting existing items having that specific primary key. This has corrupted the data leading to data discrepancies.</p>\n\n<p>Which DynamoDB write option would you select to prevent this kind of overwriting?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Batch writes</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Scan operation</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Atomic Counters</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Conditional writes</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Conditional writes</strong> - DynamoDB optionally supports conditional writes for write operations (PutItem, UpdateItem, DeleteItem). A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error.</p>\n\n<p>For example, you might want a PutItem operation to succeed only if there is no other item with that same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. This is the right choice for the current scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Batch writes</strong> - Bath operations (read and write) help reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Applications benefit from this parallelism without having to manage concurrency or threading. But, this is of no use in the given scenario of overwriting changes.</p>\n\n<p><strong>Atomic Counters</strong> - Atomic Counters is a numeric attribute that is incremented, unconditionally, without interfering with other write requests. You might use an atomic counter to track the number of visitors to a website. This functionality is not useful for the given scenario.</p>\n\n<p><strong>Scan operation</strong> - A Scan operation in Amazon DynamoDB reads every item in a table or a secondary index. By default, a Scan operation returns all of the data attributes for every item in the table or index. This option is given as a distractor and is not related to the DynamoDB item updates.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Conditional writes</strong> - DynamoDB optionally supports conditional writes for write operations (PutItem, UpdateItem, DeleteItem). A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error."
      },
      {
        "answer": "",
        "explanation": "For example, you might want a PutItem operation to succeed only if there is no other item with that same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. This is the right choice for the current scenario."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Batch writes</strong> - Bath operations (read and write) help reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Applications benefit from this parallelism without having to manage concurrency or threading. But, this is of no use in the given scenario of overwriting changes."
      },
      {
        "answer": "",
        "explanation": "<strong>Atomic Counters</strong> - Atomic Counters is a numeric attribute that is incremented, unconditionally, without interfering with other write requests. You might use an atomic counter to track the number of visitors to a website. This functionality is not useful for the given scenario."
      },
      {
        "answer": "",
        "explanation": "<strong>Scan operation</strong> - A Scan operation in Amazon DynamoDB reads every item in a table or a secondary index. By default, a Scan operation returns all of the data attributes for every item in the table or index. This option is given as a distractor and is not related to the DynamoDB item updates."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate"
    ]
  },
  {
    "id": 48,
    "question": "<p>A company uses Amazon Simple Storage Service (Amazon S3) as a storage service for storing various media files, log files, audit files, etc. The company has hired you as an AWS Certified Data Engineer Associate to also configure Amazon EMR to use Amazon S3 as the Hadoop storage layer instead of the Hadoop Distributed File System (HDFS).</p>\n\n<p>How will you configure this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>You can configure Amazon EMR to use Amazon S3 as the Hadoop storage layer while launching the EMR cluster</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>You can configure Amazon EMR to use Amazon S3 instead of HDFS for the Hadoop storage layer by launching the cluster as a long-running cluster</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>You can configure Amazon EMR to use the Amazon S3 block file system for this requirement</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>You can't configure Amazon EMR to use Amazon S3 instead of HDFS as the Hadoop storage layer</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>You can't configure Amazon EMR to use Amazon S3 instead of HDFS as the Hadoop storage layer</strong></p>\n\n<p>You can't configure Amazon EMR to use Amazon S3 instead of HDFS for the Hadoop storage layer. HDFS and the EMR File System (EMRFS), which uses Amazon S3, are both compatible with Amazon EMR, but they're not interchangeable. HDFS is an implementation of the Hadoop FileSystem API, which models POSIX file system behavior. EMRFS is an object store, not a file system.</p>\n\n<p>The EMR File System (EMRFS) is an implementation of HDFS that all Amazon EMR clusters use for reading and writing regular files from Amazon EMR directly to Amazon S3. EMRFS provides the convenience of storing persistent data in Amazon S3 for use with Hadoop while also providing features like data encryption.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can configure Amazon EMR to use the Amazon S3 block file system for this requirement</strong> - The Amazon S3 block file system is a legacy file system that was used to support uploads to Amazon S3 that were larger than 5 GB in size. With the multipart upload functionality Amazon EMR provides through the AWS Java SDK, you can upload files of up to 5 TB in size to the Amazon S3 native file system, and the Amazon S3 block file system is deprecated. Since this legacy file system can create race conditions that can corrupt the file system, you should avoid this format and use EMRFS instead. This option is not relevant to the given use case.</p>\n\n<p><strong>You can configure Amazon EMR to use Amazon S3 as the Hadoop storage layer while launching the EMR cluster</strong> - This statement is incorrect. It is not possible to replace HDFS with Amazon S3 as the Hadoop storage layer.</p>\n\n<p><strong>You can configure Amazon EMR to use Amazon S3 instead of HDFS for the Hadoop storage layer by launching the cluster as a long-running cluster</strong> - This statement is incorrect and given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/configure-emr-s3-hadoop-storage/\">https://aws.amazon.com/premiumsupport/knowledge-center/configure-emr-s3-hadoop-storage/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You can't configure Amazon EMR to use Amazon S3 instead of HDFS as the Hadoop storage layer</strong>"
      },
      {
        "answer": "",
        "explanation": "You can't configure Amazon EMR to use Amazon S3 instead of HDFS for the Hadoop storage layer. HDFS and the EMR File System (EMRFS), which uses Amazon S3, are both compatible with Amazon EMR, but they're not interchangeable. HDFS is an implementation of the Hadoop FileSystem API, which models POSIX file system behavior. EMRFS is an object store, not a file system."
      },
      {
        "answer": "",
        "explanation": "The EMR File System (EMRFS) is an implementation of HDFS that all Amazon EMR clusters use for reading and writing regular files from Amazon EMR directly to Amazon S3. EMRFS provides the convenience of storing persistent data in Amazon S3 for use with Hadoop while also providing features like data encryption."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You can configure Amazon EMR to use the Amazon S3 block file system for this requirement</strong> - The Amazon S3 block file system is a legacy file system that was used to support uploads to Amazon S3 that were larger than 5 GB in size. With the multipart upload functionality Amazon EMR provides through the AWS Java SDK, you can upload files of up to 5 TB in size to the Amazon S3 native file system, and the Amazon S3 block file system is deprecated. Since this legacy file system can create race conditions that can corrupt the file system, you should avoid this format and use EMRFS instead. This option is not relevant to the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>You can configure Amazon EMR to use Amazon S3 as the Hadoop storage layer while launching the EMR cluster</strong> - This statement is incorrect. It is not possible to replace HDFS with Amazon S3 as the Hadoop storage layer."
      },
      {
        "answer": "",
        "explanation": "<strong>You can configure Amazon EMR to use Amazon S3 instead of HDFS for the Hadoop storage layer by launching the cluster as a long-running cluster</strong> - This statement is incorrect and given only as a distractor."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/configure-emr-s3-hadoop-storage/",
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html"
    ]
  },
  {
    "id": 49,
    "question": "<p>A company uses Amazon Redshift as its data warehouse solution. The company runs certain complex queries repeatedly over a large amount of data and hence uses Amazon Redshift materialized views. The company wants these materialized views to refresh automatically per a defined schedule.</p>\n\n<p>Which of the following represents an optimal solution that can automate the process with the LEAST effort?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Materialized views cannot be refreshed automatically and need a manual refresh</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>You can set auto-refresh for materialized views using CREATE MATERIALIZED VIEW</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure an Amazon EventBridge to trigger an AWS Lambda function based on the defined schedule. The Lambda function will call the Amazon Redshift Data API to refresh the materialized views</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a schedule to refresh the materialized views with Amazon Redshift query editor v2</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a schedule to refresh the materialized views with Amazon Redshift query editor v2</strong></p>\n\n<p>In a data warehouse environment, applications often must perform complex queries on large tables. An example is SELECT statements that perform multi-table joins and aggregations on tables that contain billions of rows. Processing these queries can be expensive, in terms of system resources and the time it takes to compute the results.</p>\n\n<p>Materialized views in Amazon Redshift provide a way to address these issues. A materialized view contains a precomputed result set, based on an SQL query over one or more base tables. You can issue SELECT statements to query a materialized view, in the same way that you can query other tables or views in the database. Amazon Redshift returns the precomputed results from the materialized view, without having to access the base tables at all. From the user standpoint, the query results are returned much faster compared to when retrieving the same data from the base tables.</p>\n\n<p>Materialized views in Amazon Redshift:\n<img src=\"https://docs.aws.amazon.com/images/redshift/latest/dg/images/materialized-view.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/images/redshift/latest/dg/images/materialized-view.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.html\">https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.html</a><p></p>\n\n<p>You can create a schedule to run an SQL statement to refresh a materialized view with Amazon Redshift query editor v2. You create a schedule to run your SQL statement at the time intervals that match your business needs. When it's time for the scheduled query to run, the query is started by Amazon EventBridge and uses the Amazon Redshift Data API.</p>\n\n<p>To schedule queries, the AWS Identity and Access Management (IAM) user defining the schedule and the IAM role associated with the schedule must be configured with the IAM permissions to use Amazon EventBridge and Amazon Redshift Data API.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can set auto-refresh for materialized views using CREATE MATERIALIZED VIEW</strong> - Amazon Redshift can automatically refresh materialized views with up-to-date data from its base tables when materialized views are created or altered by enabling the autorefresh option. Amazon Redshift auto-refreshes materialized views as soon as possible after base tables changes. But, we want the materialized view to be refreshed as per a schedule and not whenever the base table changes. Hence, this option is not correct for the given use case.</p>\n\n<p><strong>Configure an Amazon EventBridge to trigger an AWS Lambda function based on the defined schedule. The Lambda function will call the Amazon Redshift Data API to refresh the materialized views</strong> - These steps are internally run by Amazon Redshift query editor v2 when you schedule a query. Operationally it is more efficient to leverage the Redshift query editor v2 option.</p>\n\n<p><strong>Materialized views cannot be refreshed automatically and need a manual refresh</strong> - This statement is incorrect and given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-schedule-query.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-schedule-query.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.html\">https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-create-sql-command.html\">https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-create-sql-command.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a schedule to refresh the materialized views with Amazon Redshift query editor v2</strong>"
      },
      {
        "answer": "",
        "explanation": "In a data warehouse environment, applications often must perform complex queries on large tables. An example is SELECT statements that perform multi-table joins and aggregations on tables that contain billions of rows. Processing these queries can be expensive, in terms of system resources and the time it takes to compute the results."
      },
      {
        "answer": "",
        "explanation": "Materialized views in Amazon Redshift provide a way to address these issues. A materialized view contains a precomputed result set, based on an SQL query over one or more base tables. You can issue SELECT statements to query a materialized view, in the same way that you can query other tables or views in the database. Amazon Redshift returns the precomputed results from the materialized view, without having to access the base tables at all. From the user standpoint, the query results are returned much faster compared to when retrieving the same data from the base tables."
      },
      {
        "image": "https://docs.aws.amazon.com/images/redshift/latest/dg/images/materialized-view.png",
        "answer": "",
        "explanation": "Materialized views in Amazon Redshift:"
      },
      {
        "link": "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.html"
      },
      {
        "answer": "",
        "explanation": "You can create a schedule to run an SQL statement to refresh a materialized view with Amazon Redshift query editor v2. You create a schedule to run your SQL statement at the time intervals that match your business needs. When it's time for the scheduled query to run, the query is started by Amazon EventBridge and uses the Amazon Redshift Data API."
      },
      {
        "answer": "",
        "explanation": "To schedule queries, the AWS Identity and Access Management (IAM) user defining the schedule and the IAM role associated with the schedule must be configured with the IAM permissions to use Amazon EventBridge and Amazon Redshift Data API."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You can set auto-refresh for materialized views using CREATE MATERIALIZED VIEW</strong> - Amazon Redshift can automatically refresh materialized views with up-to-date data from its base tables when materialized views are created or altered by enabling the autorefresh option. Amazon Redshift auto-refreshes materialized views as soon as possible after base tables changes. But, we want the materialized view to be refreshed as per a schedule and not whenever the base table changes. Hence, this option is not correct for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure an Amazon EventBridge to trigger an AWS Lambda function based on the defined schedule. The Lambda function will call the Amazon Redshift Data API to refresh the materialized views</strong> - These steps are internally run by Amazon Redshift query editor v2 when you schedule a query. Operationally it is more efficient to leverage the Redshift query editor v2 option."
      },
      {
        "answer": "",
        "explanation": "<strong>Materialized views cannot be refreshed automatically and need a manual refresh</strong> - This statement is incorrect and given only as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.html",
      "https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-schedule-query.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-create-sql-command.html"
    ]
  },
  {
    "id": 50,
    "question": "<p>A company stores its data in Amazon DynamoDb. The company needs to access this data in Amazon DynamoDb from an Amazon Sagemaker notebook for running machine learning models.</p>\n\n<p>Which of the following represents a solution to address this requirement with the LEAST operational effort?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Directly export your DynamoDB table into a .csv file and upload this file to Amazon S3. Access the S3 data from Sagemaker</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Data Pipeline console to export the DynamoDB table to Amazon S3. Exported JSON files are converted to comma-separated value (CSV) format to use as a data source for Amazon SageMaker</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Glue to transfer your table from DynamoDB to Amazon S3. Access the S3 data from Sagemaker</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Access the data from SageMaker Notebook by reading it using the boto3 client. Initialize a DynamoDB Client and do a <code>Scan</code> which will return all the data you need</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Access the data from SageMaker Notebook by reading it using the boto3 client. Initialize a DynamoDB Client and do a <code>Scan</code> which will return all the data you need</strong></p>\n\n<p>You can access the data in your SageMaker Notebook by reading it using the boto3 client. Initialize a DynamoDB Client and do a <code>Scan</code> which will return all the data that you require.</p>\n\n<p>Note that this will be in JSON format and you would need to convert that to a format that is useful for the rest of your notebook, such as converting to a pandas dataframe.</p>\n\n<p>The Scan operation returns one or more items and item attributes by accessing every item in a table or a secondary index. To have DynamoDB return fewer items, you can provide a FilterExpression operation.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the AWS Data Pipeline console to export the DynamoDB table to Amazon S3. Exported JSON files are converted to comma-separated value (CSV) format to use as a data source for Amazon SageMaker</strong> - Console-based access to the AWS Data Pipeline service has been deprecated since 2023. Continued access to AWS Data Pipeline is through the command line interface and API. Hence, this option is irrelevant for the given use case.</p>\n\n<p><strong>Use AWS Glue to transfer your table from DynamoDB to Amazon S3. Access the S3 data from Sagemaker</strong> - AWS Glue will need a Glue Crawler to populate the AWS Glue Data Catalog with tables. Also, using Amazon S3 is unnecessary for the solution, since Sagemaker can directly read from DynamoDB.</p>\n\n<p><strong>Directly export your DynamoDB table into a .csv file and upload this file to Amazon S3. Access the S3 data from Sagemaker</strong> - Transferring data to S3 is an unnecessary step that increases execution time as well as cost. Reading directly from DynamoDB into SageMaker is the straightforward, cost-effective way of implementing the solution.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://repost.aws/questions/QUowJvrZpiRjOS5v0OhDkEXQ/accessing-dynamodb-data-from-sagemaker\">https://repost.aws/questions/QUowJvrZpiRjOS5v0OhDkEXQ/accessing-dynamodb-data-from-sagemaker</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Access the data from SageMaker Notebook by reading it using the boto3 client. Initialize a DynamoDB Client and do a <code>Scan</code> which will return all the data you need</strong>"
      },
      {
        "answer": "",
        "explanation": "You can access the data in your SageMaker Notebook by reading it using the boto3 client. Initialize a DynamoDB Client and do a <code>Scan</code> which will return all the data that you require."
      },
      {
        "answer": "",
        "explanation": "Note that this will be in JSON format and you would need to convert that to a format that is useful for the rest of your notebook, such as converting to a pandas dataframe."
      },
      {
        "answer": "",
        "explanation": "The Scan operation returns one or more items and item attributes by accessing every item in a table or a secondary index. To have DynamoDB return fewer items, you can provide a FilterExpression operation."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the AWS Data Pipeline console to export the DynamoDB table to Amazon S3. Exported JSON files are converted to comma-separated value (CSV) format to use as a data source for Amazon SageMaker</strong> - Console-based access to the AWS Data Pipeline service has been deprecated since 2023. Continued access to AWS Data Pipeline is through the command line interface and API. Hence, this option is irrelevant for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue to transfer your table from DynamoDB to Amazon S3. Access the S3 data from Sagemaker</strong> - AWS Glue will need a Glue Crawler to populate the AWS Glue Data Catalog with tables. Also, using Amazon S3 is unnecessary for the solution, since Sagemaker can directly read from DynamoDB."
      },
      {
        "answer": "",
        "explanation": "<strong>Directly export your DynamoDB table into a .csv file and upload this file to Amazon S3. Access the S3 data from Sagemaker</strong> - Transferring data to S3 is an unnecessary step that increases execution time as well as cost. Reading directly from DynamoDB into SageMaker is the straightforward, cost-effective way of implementing the solution."
      }
    ],
    "references": [
      "https://repost.aws/questions/QUowJvrZpiRjOS5v0OhDkEXQ/accessing-dynamodb-data-from-sagemaker"
    ]
  },
  {
    "id": 51,
    "question": "<p>A photo-sharing company is storing user profile pictures in an Amazon S3 bucket and an image analysis application is deployed on four Amazon EC2 instances. A data engineer would like to trigger an image analysis procedure only on one of the four Amazon EC2 instances for each photo uploaded.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon EventBridge event that reacts to object uploads in Amazon S3 and invokes one of the Amazon EC2 instances</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon S3 Event Notification that sends a message to an Amazon SQS queue. Make the Amazon EC2 instances read from the Amazon SQS queue</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Subscribe the Amazon EC2 instances to Amazon S3 Analytics - storage class analysis</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon S3 Event Notification that sends a message to an Amazon SNS topic. Subscribe the Amazon EC2 instances to the Amazon SNS topic</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an Amazon S3 Event Notification that sends a message to an Amazon SQS queue. Make the Amazon EC2 instances read from the Amazon SQS queue</strong></p>\n\n<p>The Amazon S3 event notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications.</p>\n\n<p>Amazon S3 supports the following destinations where it can publish events:</p>\n\n<p>Amazon Simple Notification Service (Amazon SNS) topic</p>\n\n<p>Amazon Simple Queue Service (Amazon SQS) queue</p>\n\n<p>AWS Lambda</p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Here we have to use Amazon S3 Event Notifications (which can send a message to either AWS Lambda, Amazon SNS, or Amazon SQS) to send a message to the Amazon SQS queue. By using Amazon SQS, we know only one Amazon EC2 instance among the four will pick up a message and process it.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Subscribe the Amazon EC2 instances to Amazon S3 Analytics - storage class analysis</strong> - By using the Amazon S3 analytics storage class analysis tool, you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. Storage class analysis observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. This option acts as a distractor.</p>\n\n<p><strong>Create an Amazon EventBridge event that reacts to object uploads in Amazon S3 and invokes one of the Amazon EC2 instances</strong>- Amazon EventBridge events cannot invoke applications on Amazon EC2 instances, so we have to rule out that answer.</p>\n\n<p><strong>Create an Amazon S3 Event Notification that sends a message to an Amazon SNS topic. Subscribe the Amazon EC2 instances to the Amazon SNS topic</strong>- Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Using Amazon SNS would send a message to each Amazon EC2 instance via the Amazon SNS topic, therefore making all of them work for each upload. This is not the intended behavior.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon S3 Event Notification that sends a message to an Amazon SQS queue. Make the Amazon EC2 instances read from the Amazon SQS queue</strong>"
      },
      {
        "answer": "",
        "explanation": "The Amazon S3 event notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 supports the following destinations where it can publish events:"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Notification Service (Amazon SNS) topic"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (Amazon SQS) queue"
      },
      {
        "answer": "",
        "explanation": "AWS Lambda"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent."
      },
      {
        "answer": "",
        "explanation": "Here we have to use Amazon S3 Event Notifications (which can send a message to either AWS Lambda, Amazon SNS, or Amazon SQS) to send a message to the Amazon SQS queue. By using Amazon SQS, we know only one Amazon EC2 instance among the four will pick up a message and process it."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Subscribe the Amazon EC2 instances to Amazon S3 Analytics - storage class analysis</strong> - By using the Amazon S3 analytics storage class analysis tool, you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. Storage class analysis observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. This option acts as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon EventBridge event that reacts to object uploads in Amazon S3 and invokes one of the Amazon EC2 instances</strong>- Amazon EventBridge events cannot invoke applications on Amazon EC2 instances, so we have to rule out that answer."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an Amazon S3 Event Notification that sends a message to an Amazon SNS topic. Subscribe the Amazon EC2 instances to the Amazon SNS topic</strong>- Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Using Amazon SNS would send a message to each Amazon EC2 instance via the Amazon SNS topic, therefore making all of them work for each upload. This is not the intended behavior."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html",
      "https://aws.amazon.com/sqs/"
    ]
  },
  {
    "id": 52,
    "question": "<p>A data engineer has been tasked to optimize Amazon Athena queries that are underperforming. Upon analysis, the data engineer realized that the files queried by Athena were not compressed and just stored as .csv files. The data engineer also noticed that users perform most queries by selecting a specific column.</p>\n\n<p>What do you recommend to improve the query performance?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Change the data format from comma-separated text files to JSON format. Apply Snappy compression</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Change the data format from comma-separated text files to Apache ORC</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Change the data format from comma-separated text files to ZIP format</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Change the data format from comma-separated text files to Apache Parquet. Compress the files using Snappy compression</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Change the data format from comma-separated text files to Apache Parquet. Compress the files using Snappy compression</strong></p>\n\n<p>Amazon Athena query performance improves if you convert your data into open-source columnar formats, such as Apache parquet or ORC.</p>\n\n<p>Why choose Columnar storage formats:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q13-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q13-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html\">https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html</a><p></p>\n\n<p>To convert your existing raw data from other storage formats to Parquet or ORC, you can run CREATE TABLE AS SELECT (CTAS) queries in Athena and specify a data storage format as Parquet or ORC, or use the AWS Glue Crawler.</p>\n\n<p>Compressing your data can speed up your queries significantly. The smaller data sizes reduce the data scanned from Amazon S3, resulting in lower costs of running queries. It also reduces the network traffic from Amazon S3 to Athena. Athena supports a variety of compression formats, including common formats like gzip, Snappy, and zstd.</p>\n\n<p>SNAPPY – Compression algorithm that is part of the Lempel-Ziv 77 (LZ7) family. Snappy focuses on high compression and decompression speed rather than the maximum compression of data. This will improve the performance of the queries for the current use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the data format from comma-separated text files to JSON format. Apply Snappy compression</strong> - JSON is not a columnar format and is not as well optimized as Apache Parquet for Athena queries.</p>\n\n<p><strong>Change the data format from comma-separated text files to ORC</strong> - ORC is also a columnar format and provides an efficient way to store Hive data. ORC files are often smaller than Parquet files, and ORC indexes can make querying faster. We still need compression to speed up the Athena queries, so this option is not the best fit.</p>\n\n<p><strong>Change the data format from comma-separated text files to ZIP format</strong> - The ZIP file format is not supported by Athena.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html\">https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/compression-formats.html\">https://docs.aws.amazon.com/athena/latest/ug/compression-formats.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Change the data format from comma-separated text files to Apache Parquet. Compress the files using Snappy compression</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Athena query performance improves if you convert your data into open-source columnar formats, such as Apache parquet or ORC."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q13-i1.jpg",
        "answer": "",
        "explanation": "Why choose Columnar storage formats:"
      },
      {
        "link": "https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html"
      },
      {
        "answer": "",
        "explanation": "To convert your existing raw data from other storage formats to Parquet or ORC, you can run CREATE TABLE AS SELECT (CTAS) queries in Athena and specify a data storage format as Parquet or ORC, or use the AWS Glue Crawler."
      },
      {
        "answer": "",
        "explanation": "Compressing your data can speed up your queries significantly. The smaller data sizes reduce the data scanned from Amazon S3, resulting in lower costs of running queries. It also reduces the network traffic from Amazon S3 to Athena. Athena supports a variety of compression formats, including common formats like gzip, Snappy, and zstd."
      },
      {
        "answer": "",
        "explanation": "SNAPPY – Compression algorithm that is part of the Lempel-Ziv 77 (LZ7) family. Snappy focuses on high compression and decompression speed rather than the maximum compression of data. This will improve the performance of the queries for the current use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Change the data format from comma-separated text files to JSON format. Apply Snappy compression</strong> - JSON is not a columnar format and is not as well optimized as Apache Parquet for Athena queries."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the data format from comma-separated text files to ORC</strong> - ORC is also a columnar format and provides an efficient way to store Hive data. ORC files are often smaller than Parquet files, and ORC indexes can make querying faster. We still need compression to speed up the Athena queries, so this option is not the best fit."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the data format from comma-separated text files to ZIP format</strong> - The ZIP file format is not supported by Athena."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html",
      "https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html",
      "https://docs.aws.amazon.com/athena/latest/ug/compression-formats.html"
    ]
  },
  {
    "id": 53,
    "question": "<p>A financial services company is looking for a solution that detects anomalies in order to identify fraudulent transactions. The company utilizes Amazon Kinesis to transfer JSON-formatted transaction records from its on-premises database to Amazon S3. The existing dataset comprises 100-column-wide records for each transaction. To identify fraudulent transactions, the solution needs to analyze just ten of these columns.</p>\n\n<p>As an AWS Certified Data Engineer Associate, which of the following would you suggest as the lowest-cost solution that needs the least development work and offers out-of-the-box anomaly detection functionality?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon SageMaker to build an anomaly detection model that can detect fraudulent transactions by ingesting data directly from Amazon S3</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Leverage Kinesis Data Analytics to detect anomalies on a data stream from Kinesis Streams by running SQL queries which compute an anomaly score for all transactions and then store all fraudulent transactions in Amazon S3. Use Amazon QuickSight to visualize the results from Amazon S3</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon Athena to create a table with a subset of columns. Set up Amazon QuickSight for visual analysis of the data and identify fraudulent transactions using QuickSight's built-in machine learning-powered anomaly detection</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Leverage Kinesis Data Firehose to detect anomalies on a data stream from Kinesis Streams via a Lambda function which computes an anomaly score for all transactions and stores all fraudulent transactions in Amazon RDS. Use Amazon QuickSight to visualize the results from RDS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon Athena to create a table with a subset of columns. Set up Amazon QuickSight for visual analysis of the data and identify fraudulent transactions using QuickSight's built-in machine learning-powered anomaly detection</strong></p>\n\n<p>For the given use case, you can use an AWS Glue job to extract, transform, and load (ETL) data from the data source (in JSON format) to the data target (in Parquet format). You can then use an AWS Glue crawler, which is a program that connects to a data store (source or target) such as Amazon S3, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in the AWS Glue Data Catalog.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q21-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q21-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a><p></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run, thereby making this solution really low cost. You can also use Athena to build a table with only the subset of columns that are required for downstream analysis.</p>\n\n<p>Finally, you can read the data in the given Athena table via Amazon QuickSight to identify fraudulent transactions using QuickSight's built-in machine learning-powered anomaly detection. QuickSight uses proven Amazon technology to continuously run ML-powered anomaly detection across millions of metrics to discover hidden trends and outliers in your data. This anomaly detection enables you to get deep insights that are often buried in the aggregates and not scalable with manual analysis. With ML-powered anomaly detection, you can find outliers in your data without the need for manual analysis, custom development, or ML domain expertise.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Kinesis Data Analytics to detect anomalies on a data stream from Kinesis Streams by running SQL queries which compute an anomaly score for all transactions and then store all fraudulent transactions in Amazon S3. Use Amazon QuickSight to visualize the results from Amazon S3</strong> - Using Kinesis Data Analytics involves some custom query development to analyze the incoming data to compute an anomaly score for all transactions. In addition, this solution processes all columns of the data instead of just the subset of columns required for the analysis. Therefore, this option is not the best fit for the given use case.</p>\n\n<p><strong>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon SageMaker to build an anomaly detection model that can detect fraudulent transactions by ingesting data directly from Amazon S3</strong> - Amazon SageMaker is a fully managed service to build, train, and deploy machine learning (ML) models with fully managed infrastructure, tools, and workflows. Using SageMaker involves custom code development to build, develop, test, and deploy the anomaly detection model that is relevant to the given scenario. Instead, you can directly use QuickSight to identify fraudulent transactions using QuickSight's built-in machine learning-based anomaly detection functionality. Therefore, this option is not the right fit for the given use case.</p>\n\n<p><strong>Leverage Kinesis Data Firehose to detect anomalies on a data stream from Kinesis Streams via a Lambda function which computes an anomaly score for all transactions and stores all fraudulent transactions in Amazon RDS. Use Amazon QuickSight to visualize the results from RDS</strong> - This option involves significant custom code development on a Lambda function to examine the incoming stream from Firehose and then compute an anomaly score for all transactions. In addition, the lambda looks at all the fields in the data instead of just the subset of fields required for the analysis. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html\">https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/accessing-and-visualizing-data-from-multiple-data-sources-with-amazon-athena-and-amazon-quicksight/\">https://aws.amazon.com/blogs/big-data/accessing-and-visualizing-data-from-multiple-data-sources-with-amazon-athena-and-amazon-quicksight/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/anomaly-detection-function.html\">https://docs.aws.amazon.com/quicksight/latest/user/anomaly-detection-function.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sagemaker/faqs/\">https://aws.amazon.com/sagemaker/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon Athena to create a table with a subset of columns. Set up Amazon QuickSight for visual analysis of the data and identify fraudulent transactions using QuickSight's built-in machine learning-powered anomaly detection</strong>"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can use an AWS Glue job to extract, transform, and load (ETL) data from the data source (in JSON format) to the data target (in Parquet format). You can then use an AWS Glue crawler, which is a program that connects to a data store (source or target) such as Amazon S3, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in the AWS Glue Data Catalog."
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html"
      },
      {
        "answer": "",
        "explanation": "Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run, thereby making this solution really low cost. You can also use Athena to build a table with only the subset of columns that are required for downstream analysis."
      },
      {
        "answer": "",
        "explanation": "Finally, you can read the data in the given Athena table via Amazon QuickSight to identify fraudulent transactions using QuickSight's built-in machine learning-powered anomaly detection. QuickSight uses proven Amazon technology to continuously run ML-powered anomaly detection across millions of metrics to discover hidden trends and outliers in your data. This anomaly detection enables you to get deep insights that are often buried in the aggregates and not scalable with manual analysis. With ML-powered anomaly detection, you can find outliers in your data without the need for manual analysis, custom development, or ML domain expertise."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Kinesis Data Analytics to detect anomalies on a data stream from Kinesis Streams by running SQL queries which compute an anomaly score for all transactions and then store all fraudulent transactions in Amazon S3. Use Amazon QuickSight to visualize the results from Amazon S3</strong> - Using Kinesis Data Analytics involves some custom query development to analyze the incoming data to compute an anomaly score for all transactions. In addition, this solution processes all columns of the data instead of just the subset of columns required for the analysis. Therefore, this option is not the best fit for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon SageMaker to build an anomaly detection model that can detect fraudulent transactions by ingesting data directly from Amazon S3</strong> - Amazon SageMaker is a fully managed service to build, train, and deploy machine learning (ML) models with fully managed infrastructure, tools, and workflows. Using SageMaker involves custom code development to build, develop, test, and deploy the anomaly detection model that is relevant to the given scenario. Instead, you can directly use QuickSight to identify fraudulent transactions using QuickSight's built-in machine learning-based anomaly detection functionality. Therefore, this option is not the right fit for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Kinesis Data Firehose to detect anomalies on a data stream from Kinesis Streams via a Lambda function which computes an anomaly score for all transactions and stores all fraudulent transactions in Amazon RDS. Use Amazon QuickSight to visualize the results from RDS</strong> - This option involves significant custom code development on a Lambda function to examine the incoming stream from Firehose and then compute an anomaly score for all transactions. In addition, the lambda looks at all the fields in the data instead of just the subset of fields required for the analysis. Therefore, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html",
      "https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html",
      "https://aws.amazon.com/blogs/big-data/accessing-and-visualizing-data-from-multiple-data-sources-with-amazon-athena-and-amazon-quicksight/",
      "https://docs.aws.amazon.com/quicksight/latest/user/anomaly-detection-function.html",
      "https://aws.amazon.com/kinesis/data-analytics/",
      "https://aws.amazon.com/sagemaker/faqs/"
    ]
  },
  {
    "id": 54,
    "question": "<p>A company produces a huge volume of data on a daily basis and it is stored in the form of .csv files on Amazon S3. The company also needs to run queries on historical data on a regular basis for reporting purposes. Currently, the company uses Amazon Athena to run SQL queries for analysis. Although Athena has worked well for the company, the volume of data fed into Amazon S3 has risen drastically leading to query lags as well as performance deterioration.</p>\n\n<p>What will you recommend to boost the query performance?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>When joining two tables in Athena, specify the smaller table on the left side of the join and the larger table on the right side of the join to consume less memory and run queries faster</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure a daily AWS Glue ETL job to convert the data files to ZIP format and partition these converted files. Create a periodic AWS Glue crawler to automatically crawl the partitioned data on a daily basis</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure a daily AWS Glue ETL job to convert the data files to Apache Parquet format and partition these converted files. Create a periodic AWS Glue crawler to automatically crawl the partitioned data on a daily basis</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Athena to extract the data and store it in Apache Parquet format daily. Query the extracted data</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a daily AWS Glue ETL job to convert the data files to Apache Parquet format and partition these converted files. Create a periodic AWS Glue crawler to automatically crawl the partitioned data on a daily basis</strong></p>\n\n<p>AWS states that you can improve the performance of your query by compressing, partitioning, or converting your data into columnar formats. Amazon Athena supports open-source columnar data formats such as Apache Parquet and Apache ORC. Converting your data into a compressed, columnar format lowers your cost and improves query performance by enabling Athena to scan fewer data from S3 when executing your query.</p>\n\n<p>Therefore, converting the CSV files to Parquet format and partitioning will help improve the query performance in Amazon Athena.</p>\n\n<p>Also, you can use AWS Glue crawlers to automatically infer database and table schema from your data in Amazon S3 and store the associated metadata in the AWS Glue Data Catalog. Athena uses the AWS Glue Data Catalog to store and retrieve table metadata for the Amazon S3 data in your Amazon Web Services account. The table metadata lets the Athena query engine know how to find, read, and process the data that you want to query.</p>\n\n<p>Partitioning divides your table into parts and keeps the related data together based on column values such as date, country, region, etc. Partitions act as virtual columns. You define them at table creation, and they can help reduce the amount of data scanned per query, thereby improving performance. You can restrict the amount of data scanned by a query by specifying filters based on the partition.</p>\n\n<p>Highly recommend the following blog on performance-tuning tips for Amazon Athena:\n<a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a daily AWS Glue ETL job to convert the data files to ZIP format and partition these converted files. Create a periodic AWS Glue crawler to automatically crawl the partitioned data on a daily basis</strong> - The ZIP file format is not supported by Amazon Athena.</p>\n\n<p><strong>When joining two tables in Athena, specify the smaller table on the left side of the join and the larger table on the right side of the join to consume less memory and run queries faster</strong> - This statement is incorrect. When you join two tables, specify the larger table on the left side of the join and the smaller table on the right side of the join. Presto distributes the table on the right to worker nodes and then streams the table on the left to do the join. If the table on the right is smaller, then there is less memory used and the query runs faster.</p>\n\n<p><strong>Use Athena to extract the data and store it in Apache Parquet format daily. Query the extracted data</strong> - You cannot use Athena to store the existing .csv data in Apache Parquet format, as it is not an ETL tool.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-data-to-apache-parquet.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-data-to-apache-parquet.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html\">https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure a daily AWS Glue ETL job to convert the data files to Apache Parquet format and partition these converted files. Create a periodic AWS Glue crawler to automatically crawl the partitioned data on a daily basis</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS states that you can improve the performance of your query by compressing, partitioning, or converting your data into columnar formats. Amazon Athena supports open-source columnar data formats such as Apache Parquet and Apache ORC. Converting your data into a compressed, columnar format lowers your cost and improves query performance by enabling Athena to scan fewer data from S3 when executing your query."
      },
      {
        "answer": "",
        "explanation": "Therefore, converting the CSV files to Parquet format and partitioning will help improve the query performance in Amazon Athena."
      },
      {
        "answer": "",
        "explanation": "Also, you can use AWS Glue crawlers to automatically infer database and table schema from your data in Amazon S3 and store the associated metadata in the AWS Glue Data Catalog. Athena uses the AWS Glue Data Catalog to store and retrieve table metadata for the Amazon S3 data in your Amazon Web Services account. The table metadata lets the Athena query engine know how to find, read, and process the data that you want to query."
      },
      {
        "answer": "",
        "explanation": "Partitioning divides your table into parts and keeps the related data together based on column values such as date, country, region, etc. Partitions act as virtual columns. You define them at table creation, and they can help reduce the amount of data scanned per query, thereby improving performance. You can restrict the amount of data scanned by a query by specifying filters based on the partition."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
        "answer": "",
        "explanation": "Highly recommend the following blog on performance-tuning tips for Amazon Athena:\n<a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure a daily AWS Glue ETL job to convert the data files to ZIP format and partition these converted files. Create a periodic AWS Glue crawler to automatically crawl the partitioned data on a daily basis</strong> - The ZIP file format is not supported by Amazon Athena."
      },
      {
        "answer": "",
        "explanation": "<strong>When joining two tables in Athena, specify the smaller table on the left side of the join and the larger table on the right side of the join to consume less memory and run queries faster</strong> - This statement is incorrect. When you join two tables, specify the larger table on the left side of the join and the smaller table on the right side of the join. Presto distributes the table on the right to worker nodes and then streams the table on the left to do the join. If the table on the right is smaller, then there is less memory used and the query runs faster."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Athena to extract the data and store it in Apache Parquet format daily. Query the extracted data</strong> - You cannot use Athena to store the existing .csv data in Apache Parquet format, as it is not an ETL tool."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-data-to-apache-parquet.html",
      "https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html"
    ]
  },
  {
    "id": 55,
    "question": "<p>The data engineering team at a company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the reporting, the team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p>\n\n<p>Which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS EMR to replicate the data from the databases into Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</strong></p>\n\n<p>AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3.</p>\n\n<p>Continuous Data Replication\n<img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram-AWS-DMS_continuous-data-replication.a0e3bd328d2a4bd9b40a83e767199dcc13cf678f.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram-AWS-DMS_continuous-data-replication.a0e3bd328d2a4bd9b40a83e767199dcc13cf678f.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a><p></p>\n\n<p>You can migrate data to Amazon Redshift databases using AWS Database Migration Service. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. With an Amazon Redshift database as a target, you can migrate data from all of the other supported source databases.</p>\n\n<p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance.\nDuring a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database. The AWS DMS replication instance must be located in that same region.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Glue to replicate the data from the databases into Amazon Redshift</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing.</p>\n\n<p>Using AWS Glue involves significant development efforts to write custom migration scripts to copy the database data into Redshift.</p>\n\n<p><strong>Use AWS EMR to replicate the data from the databases into Amazon Redshift</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p>Using EMR involves significant infrastructure management efforts to set up and maintain the EMR cluster. Additionally, this option involves a major development effort to write custom migration jobs to copy the database data into Redshift.</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>However, the user is expected to manually provision an appropriate number of shards to process the expected volume of the incoming data stream. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Therefore Kinesis Data Streams is not the right fit for this use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/use-the-aws-database-migration-service-to-stream-change-data-to-amazon-kinesis-data-streams/\">https://aws.amazon.com/blogs/database/use-the-aws-database-migration-service-to-stream-change-data-to-amazon-kinesis-data-streams/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3."
      },
      {
        "image": "https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram-AWS-DMS_continuous-data-replication.a0e3bd328d2a4bd9b40a83e767199dcc13cf678f.png",
        "answer": "",
        "explanation": "Continuous Data Replication"
      },
      {
        "link": "https://aws.amazon.com/dms/"
      },
      {
        "answer": "",
        "explanation": "You can migrate data to Amazon Redshift databases using AWS Database Migration Service. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. With an Amazon Redshift database as a target, you can migrate data from all of the other supported source databases."
      },
      {
        "answer": "",
        "explanation": "The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance.\nDuring a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database. The AWS DMS replication instance must be located in that same region."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue to replicate the data from the databases into Amazon Redshift</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing."
      },
      {
        "answer": "",
        "explanation": "Using AWS Glue involves significant development efforts to write custom migration scripts to copy the database data into Redshift."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS EMR to replicate the data from the databases into Amazon Redshift</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances."
      },
      {
        "answer": "",
        "explanation": "Using EMR involves significant infrastructure management efforts to set up and maintain the EMR cluster. Additionally, this option involves a major development effort to write custom migration jobs to copy the database data into Redshift."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events."
      },
      {
        "answer": "",
        "explanation": "However, the user is expected to manually provision an appropriate number of shards to process the expected volume of the incoming data stream. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Therefore Kinesis Data Streams is not the right fit for this use-case."
      }
    ],
    "references": [
      "https://aws.amazon.com/dms/",
      "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html",
      "https://aws.amazon.com/blogs/database/use-the-aws-database-migration-service-to-stream-change-data-to-amazon-kinesis-data-streams/"
    ]
  },
  {
    "id": 56,
    "question": "<p>A company runs multiple gaming platforms that need to store game state, player data, session history, and leaderboards. The company is looking to move to AWS Cloud to scale reliably to millions of concurrent users and requests while ensuring consistently low latency measured in single-digit milliseconds. The data engineering team at the company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of its users.</p>\n\n<p>Which of the following solutions can be used to address the given requirements? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Develop the leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Develop the leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, and low latency requirements</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Develop the leaderboard using AWS Neptune as it meets the in-memory, high availability, low latency requirements</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Develop the leaderboard using RDS Aurora as it meets the in-memory, high availability, low latency requirements</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Develop the leaderboard using ElastiCache Redis as it meets the in-memory, high availability, low latency requirements</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Develop the leaderboard using ElastiCache Redis as it meets the in-memory, high availability, and low latency requirements</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.\nElastiCache for Redis can be used to power the live leaderboard, so this option is correct.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p><strong>Develop the leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. So DynamoDB with DAX can be used to power the live leaderboard.</p>\n\n<p>DAX Overview:\n<img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Develop the leaderboard using Amazon Redshift as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale. Redshift is not an in-memory database, so this option is not correct.</p>\n\n<p><strong>Develop the leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements</strong> - DynamoDB is not an in-memory database, so this option is not correct.</p>\n\n<p><strong>Develop the leaderboard using RDS Aurora as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Develop the leaderboard using ElastiCache Redis as it meets the in-memory, high availability, and low latency requirements</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.\nElastiCache for Redis can be used to power the live leaderboard, so this option is correct."
      },
      {
        "image": "https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png",
        "answer": "",
        "explanation": "ElastiCache for Redis Overview:"
      },
      {
        "answer": "",
        "explanation": "<strong>Develop the leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications."
      },
      {
        "answer": "",
        "explanation": "DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. So DynamoDB with DAX can be used to power the live leaderboard."
      },
      {
        "image": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png",
        "answer": "",
        "explanation": "DAX Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Develop the leaderboard using Amazon Redshift as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale. Redshift is not an in-memory database, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Develop the leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements</strong> - DynamoDB is not an in-memory database, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Develop the leaderboard using RDS Aurora as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database, so this option is not correct."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html",
      "https://aws.amazon.com/elasticache/",
      "https://aws.amazon.com/elasticache/redis/",
      "https://aws.amazon.com/dynamodb/dax/"
    ]
  },
  {
    "id": 57,
    "question": "<p>A financial services company is moving its IT infrastructure to AWS Cloud and wants to enforce adequate data protection mechanisms on Amazon Simple Storage Service (Amazon S3) to meet compliance guidelines. The data engineering team has hired you to build a solution for this requirement.</p>\n\n<p>Can you help the team identify the INCORRECT option from the choices below?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon S3 can protect data at rest using Server-Side Encryption</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon S3 can encrypt object metadata by using Server-Side Encryption</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon S3 can encrypt data in transit using HTTPS (TLS)</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon S3 can protect data at rest using Client-Side Encryption</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon S3 can encrypt object metadata by using Server-Side Encryption</strong></p>\n\n<p>Amazon S3 is a simple key-value store designed to store as many objects as you want. You store these objects in one or more buckets, and each object can be up to 5 TB in size.</p>\n\n<p>An object consists of the following:</p>\n\n<p>Key – The name that you assign to an object. You use the object key to retrieve the object.</p>\n\n<p>Version ID – Within a bucket, a key and version ID uniquely identify an object.</p>\n\n<p>Value – The content that you are storing.</p>\n\n<p>Metadata – A set of name-value pairs with which you can store information regarding the object.</p>\n\n<p>Subresources – Amazon S3 uses the subresource mechanism to store object-specific additional information.</p>\n\n<p>Access Control Information – You can control access to the objects you store in Amazon S3.</p>\n\n<p>Metadata, which can be included with the object, is not encrypted while being stored on Amazon S3. Therefore, AWS recommends that customers not place sensitive information in Amazon S3 metadata.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 can protect data at rest using Server-Side Encryption</strong>  - This is possible and AWS provides three different ways of doing this - Server-side encryption with Amazon S3‐managed keys (SSE-S3), Server-side encryption with customer master keys stored in AWS Key Management Service (SSE-KMS), Server-side encryption with customer-provided keys (SSE-C).</p>\n\n<p><strong>Amazon S3 can protect data at rest using Client-Side Encryption</strong> - This is a possible scenario too. You can encrypt data on the client side and upload the encrypted data to Amazon S3. In this case, the client manages the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>Amazon S3 can encrypt data in transit using HTTPS (TLS)</strong> - This is also possible and you can use HTTPS (TLS) to help prevent potential attackers from eavesdropping on or manipulating network traffic using person-in-the-middle or similar attacks.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html#server-side\">https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html#server-side</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf?did=wp_card&amp;trk=wp_card\">https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf?did=wp_card&amp;trk=wp_card</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 can encrypt object metadata by using Server-Side Encryption</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 is a simple key-value store designed to store as many objects as you want. You store these objects in one or more buckets, and each object can be up to 5 TB in size."
      },
      {
        "answer": "",
        "explanation": "An object consists of the following:"
      },
      {
        "answer": "",
        "explanation": "Key – The name that you assign to an object. You use the object key to retrieve the object."
      },
      {
        "answer": "",
        "explanation": "Version ID – Within a bucket, a key and version ID uniquely identify an object."
      },
      {
        "answer": "",
        "explanation": "Value – The content that you are storing."
      },
      {
        "answer": "",
        "explanation": "Metadata – A set of name-value pairs with which you can store information regarding the object."
      },
      {
        "answer": "",
        "explanation": "Subresources – Amazon S3 uses the subresource mechanism to store object-specific additional information."
      },
      {
        "answer": "",
        "explanation": "Access Control Information – You can control access to the objects you store in Amazon S3."
      },
      {
        "answer": "",
        "explanation": "Metadata, which can be included with the object, is not encrypted while being stored on Amazon S3. Therefore, AWS recommends that customers not place sensitive information in Amazon S3 metadata."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 can protect data at rest using Server-Side Encryption</strong>  - This is possible and AWS provides three different ways of doing this - Server-side encryption with Amazon S3‐managed keys (SSE-S3), Server-side encryption with customer master keys stored in AWS Key Management Service (SSE-KMS), Server-side encryption with customer-provided keys (SSE-C)."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 can protect data at rest using Client-Side Encryption</strong> - This is a possible scenario too. You can encrypt data on the client side and upload the encrypted data to Amazon S3. In this case, the client manages the encryption process, the encryption keys, and related tools."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 can encrypt data in transit using HTTPS (TLS)</strong> - This is also possible and you can use HTTPS (TLS) to help prevent potential attackers from eavesdropping on or manipulating network traffic using person-in-the-middle or similar attacks."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html#server-side",
      "https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf?did=wp_card&amp;trk=wp_card"
    ]
  },
  {
    "id": 58,
    "question": "<p>A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Data Engineer Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution.</p>\n\n<p>Which of the following solutions will you suggest to address these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Push score updates to an SNS topic, subscribe a Lambda function to this SNS topic to process the updates, and then store these processed updates in a SQL database running on Amazon EC2</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Push score updates to an SQS queue which uses a fleet of EC2 instances (with Auto Scaling) to process these updates in the SQS queue and then store these processed updates in an RDS MySQL database</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Push score updates to Kinesis Data Streams which uses a fleet of EC2 instances (with Auto Scaling) to process the updates in Kinesis Data Streams and then store these processed updates in DynamoDB</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Push score updates to Kinesis Data Streams which uses a Lambda function to process these updates and then store these processed updates in DynamoDB</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Push score updates to Kinesis Data Streams which uses a Lambda function to process these updates and then store these processed updates in DynamoDB</strong></p>\n\n<p>To help ingest real-time data or streaming data at large scales, you can use Amazon Kinesis Data Streams (KDS). KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. The data collected is available in milliseconds, enabling real-time analytics. KDS provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications.</p>\n\n<p>Lambda integrates natively with Kinesis Data Streams. The polling, checkpointing, and error-handling complexities are abstracted when you use this native integration. The processed data can then be configured to be saved in DynamoDB.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Push score updates to an SQS queue which uses a fleet of EC2 instances (with Auto Scaling) to process these updates in the SQS queue and then store these processed updates in an RDS MySQL database</strong></p>\n\n<p><strong>Push score updates to Kinesis Data Streams which uses a fleet of EC2 instances (with Auto Scaling) to process the updates in Kinesis Data Streams and then store these processed updates in DynamoDB</strong></p>\n\n<p><strong>Push score updates to an SNS topic, subscribe a Lambda function to this SNS topic to process the updates, and then store these processed updates in an SQL database running on Amazon EC2</strong></p>\n\n<p>These three options use EC2 instances as part of the solution architecture. The use case seeks to minimize the management overhead required to maintain the solution. However, EC2 instances involve several maintenance activities such as managing the guest operating system and software deployed to the guest operating system, including updates and security patches, etc. Hence these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/\">https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Push score updates to Kinesis Data Streams which uses a Lambda function to process these updates and then store these processed updates in DynamoDB</strong>"
      },
      {
        "answer": "",
        "explanation": "To help ingest real-time data or streaming data at large scales, you can use Amazon Kinesis Data Streams (KDS). KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. The data collected is available in milliseconds, enabling real-time analytics. KDS provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications."
      },
      {
        "answer": "",
        "explanation": "Lambda integrates natively with Kinesis Data Streams. The polling, checkpointing, and error-handling complexities are abstracted when you use this native integration. The processed data can then be configured to be saved in DynamoDB."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Push score updates to an SQS queue which uses a fleet of EC2 instances (with Auto Scaling) to process these updates in the SQS queue and then store these processed updates in an RDS MySQL database</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Push score updates to Kinesis Data Streams which uses a fleet of EC2 instances (with Auto Scaling) to process the updates in Kinesis Data Streams and then store these processed updates in DynamoDB</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Push score updates to an SNS topic, subscribe a Lambda function to this SNS topic to process the updates, and then store these processed updates in an SQL database running on Amazon EC2</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options use EC2 instances as part of the solution architecture. The use case seeks to minimize the management overhead required to maintain the solution. However, EC2 instances involve several maintenance activities such as managing the guest operating system and software deployed to the guest operating system, including updates and security patches, etc. Hence these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/"
    ]
  },
  {
    "id": 59,
    "question": "<p>A data engineer has configured an AWS Glue job to read data from an Amazon S3 bucket by setting up the AWS Glue connection details and the associated IAM role. However, when the AWS Glue job is run, it fails with an error pointing to the Amazon S3 VPC gateway endpoint that has been configured for accessing the data in Amazon S3.</p>\n\n<p>How should the data engineer troubleshoot this issue?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure an Internet Gateway in the VPC for the AWS Glue job to access the Amazon S3 bucket via the public endpoint</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure private DNS options for the VPC gateway endpoint</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Verify that the route table of the VPC has inbound and outbound routes for the Amazon S3 VPC gateway endpoint</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Attach a bucket policy to the S3 bucket that will explicitly grant access permissions to the IAM role associated with the AWS Glue job</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Verify that the route table of the VPC has inbound and outbound routes for the Amazon S3 VPC gateway endpoint</strong></p>\n\n<p>A gateway VPC endpoint enables you to create a private connection between your VPC and another AWS service. When you create a gateway endpoint, you specify the subnet route tables in your VPC that are used by the gateway endpoint. A route is automatically added to each of the route tables with a destination that specifies the prefix list ID of the service (pl-xxxxxxxx), and a target with the endpoint ID (vpce-xxxxxxxxxxxxxxxxx). You cannot explicitly delete or modify the endpoint route, but you can change the route tables that are used by the endpoint.</p>\n\n<p>A VPC endpoint for Amazon S3 enables AWS Glue to use private IP addresses to access Amazon S3 with no exposure to the public internet. AWS Glue does not require public IP addresses, and you don't need an internet gateway, a NAT device, or a virtual private gateway in your VPC. You use endpoint policies to control access to Amazon S3. Traffic between your VPC and the AWS service does not leave the Amazon network.</p>\n\n<p>AWS Glue with VPC endpoints:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q7-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q7-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/vpc-endpoints-s3.html\">https://docs.aws.amazon.com/glue/latest/dg/vpc-endpoints-s3.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Attach a bucket policy to the S3 bucket that will explicitly grant access permissions to the IAM role associated with the AWS Glue job</strong> - An Amazon S3 bucket policy that does not have sufficient permissions for the IAM role associated with the AWS Glue job will not result in an error pointing to the Amazon S3 VPC gateway endpoint. So, this option is ruled out.</p>\n\n<p><strong>Configure private DNS options for the VPC gateway endpoint</strong> - For private connectivity, you use private DNS options for your Amazon S3 when you use an interface VPC endpoint. This configuration does not apply to the gateway endpoint.</p>\n\n<p><strong>Configure an Internet Gateway in the VPC for the AWS Glue job to access the Amazon S3 bucket via the public endpoint</strong> - The correct solution is to set up a VPC endpoint for Amazon S3 that enables AWS Glue to use private IP addresses to access Amazon S3 with no exposure to the public internet.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/route-table-options.html\">https://docs.aws.amazon.com/vpc/latest/userguide/route-table-options.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.htm\">https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.htm</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/storage/introducing-private-dns-support-for-amazon-s3-with-aws-privatelink/\">https://aws.amazon.com/blogs/storage/introducing-private-dns-support-for-amazon-s3-with-aws-privatelink/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Verify that the route table of the VPC has inbound and outbound routes for the Amazon S3 VPC gateway endpoint</strong>"
      },
      {
        "answer": "",
        "explanation": "A gateway VPC endpoint enables you to create a private connection between your VPC and another AWS service. When you create a gateway endpoint, you specify the subnet route tables in your VPC that are used by the gateway endpoint. A route is automatically added to each of the route tables with a destination that specifies the prefix list ID of the service (pl-xxxxxxxx), and a target with the endpoint ID (vpce-xxxxxxxxxxxxxxxxx). You cannot explicitly delete or modify the endpoint route, but you can change the route tables that are used by the endpoint."
      },
      {
        "answer": "",
        "explanation": "A VPC endpoint for Amazon S3 enables AWS Glue to use private IP addresses to access Amazon S3 with no exposure to the public internet. AWS Glue does not require public IP addresses, and you don't need an internet gateway, a NAT device, or a virtual private gateway in your VPC. You use endpoint policies to control access to Amazon S3. Traffic between your VPC and the AWS service does not leave the Amazon network."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q7-i1.jpg",
        "answer": "",
        "explanation": "AWS Glue with VPC endpoints:"
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/vpc-endpoints-s3.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Attach a bucket policy to the S3 bucket that will explicitly grant access permissions to the IAM role associated with the AWS Glue job</strong> - An Amazon S3 bucket policy that does not have sufficient permissions for the IAM role associated with the AWS Glue job will not result in an error pointing to the Amazon S3 VPC gateway endpoint. So, this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure private DNS options for the VPC gateway endpoint</strong> - For private connectivity, you use private DNS options for your Amazon S3 when you use an interface VPC endpoint. This configuration does not apply to the gateway endpoint."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure an Internet Gateway in the VPC for the AWS Glue job to access the Amazon S3 bucket via the public endpoint</strong> - The correct solution is to set up a VPC endpoint for Amazon S3 that enables AWS Glue to use private IP addresses to access Amazon S3 with no exposure to the public internet."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/vpc-endpoints-s3.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/route-table-options.html",
      "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.htm",
      "https://aws.amazon.com/blogs/storage/introducing-private-dns-support-for-amazon-s3-with-aws-privatelink/"
    ]
  },
  {
    "id": 60,
    "question": "<p>The data engineering team at an e-commerce company processes transactions into Amazon Kinesis Data Streams using the Kinesis Producer Library (KPL). The Data Streams are managed via Auto Scaling configuration. On the other hand, the Kinesis Client Library (KCL) ingests the incoming data into the company's warehousing system to be used for downstream analytics. Lately, the data engineering team has come across issues arising out of duplicate records.</p>\n\n<p>Which of the following would you identify as the most likely reason for this behavior?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The Kinesis Producer Library (KPL) is aggregating smaller records into larger records of up to 1 MB, sometimes resulting in duplicate records</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>If <code>PutRecords.Bytes</code> metric exceeds the provisioned write capacity, throttling for the stream kicks in, which results in record failures leading to re-writing of data by Kinesis Producer Library (KPL)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>If the <code>GetRecord</code> call fails without an acknowledgment from Amazon Kinesis Data Streams, the Kinesis Producer Library (KPL) will write the same data again</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The producer is experiencing network-related timeouts, forcing duplicate entries into the Kinesis Data Streams</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The producer is experiencing network-related timeouts, forcing duplicate entries into the Kinesis Data Stream</strong> - There are two primary reasons why records may be delivered more than once to your Amazon Kinesis Data Streams application: producer retries and consumer retries.</p>\n\n<p>Consider a producer that experiences a network-related timeout after it makes a call to <code>PutRecord</code>, but before it can receive an acknowledgment from Amazon Kinesis Data Streams. The producer cannot be sure if the record was delivered to Kinesis Data Streams. Assuming that every record is important to the application, the producer would have been written to retry the call with the same data. If both <code>PutRecord</code> calls on that same data were successfully committed to Kinesis Data Streams, then there will be two Kinesis Data Streams records.</p>\n\n<p>Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing. Note that the number of duplicates due to producer retries is usually low compared to the number of duplicates due to consumer retries.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Kinesis Producer Library (KPL) is aggregating smaller records into larger records of up to 1 MB, sometimes resulting in duplicate records</strong> - The Kinesis Producer Library (KPL) aggregates small user-formatted records into larger records up to 1 MB to make better use of Amazon Kinesis Data Streams throughput. This has no bearing on the duplicate records issue.</p>\n\n<p><strong>If the <code>GetRecord</code> call fails without an acknowledgment from Amazon Kinesis Data Streams, the Kinesis Producer Library (KPL) will write the same data again</strong> - It's the <code>PutRecord</code> call that results in writing duplicate records if the acknowledgment from Amazon Kinesis Data Streams is not received.</p>\n\n<p><strong>If <code>PutRecords.Bytes</code> metric exceeds the provisioned write capacity, throttling for the stream kicks in, which results in record failures leading to re-writing of data by Kinesis Producer Library (KPL)</strong> - Kinesis Data Streams sends these stream-level metrics to CloudWatch every minute. The PutRecords.Bytes` metric depicts the number of bytes put into the Kinesis stream using the PutRecords operation over the specified period. This metric has no relation to duplicate records.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-cloudwatch.html\">https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-cloudwatch.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The producer is experiencing network-related timeouts, forcing duplicate entries into the Kinesis Data Stream</strong> - There are two primary reasons why records may be delivered more than once to your Amazon Kinesis Data Streams application: producer retries and consumer retries."
      },
      {
        "answer": "",
        "explanation": "Consider a producer that experiences a network-related timeout after it makes a call to <code>PutRecord</code>, but before it can receive an acknowledgment from Amazon Kinesis Data Streams. The producer cannot be sure if the record was delivered to Kinesis Data Streams. Assuming that every record is important to the application, the producer would have been written to retry the call with the same data. If both <code>PutRecord</code> calls on that same data were successfully committed to Kinesis Data Streams, then there will be two Kinesis Data Streams records."
      },
      {
        "answer": "",
        "explanation": "Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing. Note that the number of duplicates due to producer retries is usually low compared to the number of duplicates due to consumer retries."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The Kinesis Producer Library (KPL) is aggregating smaller records into larger records of up to 1 MB, sometimes resulting in duplicate records</strong> - The Kinesis Producer Library (KPL) aggregates small user-formatted records into larger records up to 1 MB to make better use of Amazon Kinesis Data Streams throughput. This has no bearing on the duplicate records issue."
      },
      {
        "answer": "",
        "explanation": "<strong>If the <code>GetRecord</code> call fails without an acknowledgment from Amazon Kinesis Data Streams, the Kinesis Producer Library (KPL) will write the same data again</strong> - It's the <code>PutRecord</code> call that results in writing duplicate records if the acknowledgment from Amazon Kinesis Data Streams is not received."
      },
      {
        "answer": "",
        "explanation": "<strong>If <code>PutRecords.Bytes</code> metric exceeds the provisioned write capacity, throttling for the stream kicks in, which results in record failures leading to re-writing of data by Kinesis Producer Library (KPL)</strong> - Kinesis Data Streams sends these stream-level metrics to CloudWatch every minute. The PutRecords.Bytes` metric depicts the number of bytes put into the Kinesis stream using the PutRecords operation over the specified period. This metric has no relation to duplicate records."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html",
      "https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-cloudwatch.html"
    ]
  },
  {
    "id": 61,
    "question": "<p>A streaming service company uses AWS Cloud for analytics, recommendation engines, and video transcoding. To monitor and optimize this network, the data engineering team at the company has developed a solution for ingesting, augmenting, and analyzing the multiple terabytes of data its network generates daily in the form of virtual private cloud (VPC) flow logs. This would enable the company to identify performance-improvement opportunities such as identifying apps that are communicating across regions and collocating them. The VPC flow logs data is funneled into Kinesis Data Streams which further acts as the source of a delivery stream for Kinesis Firehose.</p>\n\n<p>The data engineering team has now configured a Kinesis Agent to send the VPC flow logs data from another set of network devices to the same Firehose delivery stream. They noticed that this log data is not reaching Firehose.</p>\n\n<p>Which of the following options would you identify as the MOST plausible root cause behind this issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The data sent by Kinesis Agent is lost because of a configuration error</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Kinesis Agent cannot write to a Kinesis Firehose for which the delivery stream source is already set as Kinesis Data Streams</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Kinesis Agent can only write to Kinesis Data Streams, not to Kinesis Firehose</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Kinesis Firehose delivery stream has reached its limit and needs to be scaled manually</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Kinesis Agent cannot write to a Kinesis Firehose for which the delivery stream source is already set as Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</p>\n\n<p>When a Kinesis data stream is configured as the source of a Firehose delivery stream, Firehose’s PutRecord and PutRecordBatch operations are disabled and the Kinesis Agent cannot write to the Firehose delivery stream directly. Data needs to be added to the Kinesis data stream through the Kinesis Data Streams PutRecord and PutRecords operations instead. Therefore, this option is correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Kinesis Agent can only write to Kinesis Data Streams, not to Kinesis Firehose</strong> - Kinesis Agent is a stand-alone Java-based software application that offers an easy way to collect and send data to Kinesis Data Streams or Kinesis Firehose. So this option is incorrect.</p>\n\n<p><strong>Kinesis Firehose delivery stream has reached its limit and needs to be scaled manually</strong> - Kinesis Firehose is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. Therefore this option is not correct.</p>\n\n<p>How Kinesis Firehose works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a><p></p>\n\n<p><strong>The data sent by Kinesis Agent is lost because of a configuration error</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Kinesis Agent cannot write to a Kinesis Firehose for which the delivery stream source is already set as Kinesis Data Streams</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security."
      },
      {
        "answer": "",
        "explanation": "When a Kinesis data stream is configured as the source of a Firehose delivery stream, Firehose’s PutRecord and PutRecordBatch operations are disabled and the Kinesis Agent cannot write to the Firehose delivery stream directly. Data needs to be added to the Kinesis data stream through the Kinesis Data Streams PutRecord and PutRecords operations instead. Therefore, this option is correct."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Kinesis Agent can only write to Kinesis Data Streams, not to Kinesis Firehose</strong> - Kinesis Agent is a stand-alone Java-based software application that offers an easy way to collect and send data to Kinesis Data Streams or Kinesis Firehose. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Kinesis Firehose delivery stream has reached its limit and needs to be scaled manually</strong> - Kinesis Firehose is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. Therefore this option is not correct."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png",
        "answer": "",
        "explanation": "How Kinesis Firehose works:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-firehose/"
      },
      {
        "answer": "",
        "explanation": "<strong>The data sent by Kinesis Agent is lost because of a configuration error</strong> - This is a made-up option and has been added as a distractor."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://aws.amazon.com/kinesis/data-firehose/faqs/"
    ]
  },
  {
    "id": 62,
    "question": "<p>A healthcare company has significant investments in running Oracle and PostgreSQL services on Amazon RDS which provide their data engineers with near real-time analysis of millions of rows of health data having 2,000 data points per row. The data engineering team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The team lead has observed that the database performance takes a hit whenever these reports are run. To facilitate the reporting process, the team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p>\n\n<p>Which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon EMR to replicate the data from the databases into Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</strong></p>\n\n<p>AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3.</p>\n\n<p>Continuous Data Replication\n<img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram-AWS-DMS_continuous-data-replication.a0e3bd328d2a4bd9b40a83e767199dcc13cf678f.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram-AWS-DMS_continuous-data-replication.a0e3bd328d2a4bd9b40a83e767199dcc13cf678f.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a><p></p>\n\n<p>You can migrate data to Amazon Redshift databases using AWS Database Migration Service. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. With an Amazon Redshift database as a target, you can migrate data from all of the other supported source databases.</p>\n\n<p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance.\nDuring a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database. The AWS DMS replication instance must be located in that same region.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Glue to replicate the data from the databases into Amazon Redshift</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing.</p>\n\n<p>Using AWS Glue involves significant development efforts to write custom migration scripts to copy the database data into Redshift, so this option is not correct.</p>\n\n<p><strong>Use Amazon EMR to replicate the data from the databases into Amazon Redshift</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p>Using EMR involves significant infrastructure management efforts to set up and maintain the EMR cluster. Additionally, this option involves a major development effort to write custom migration jobs to copy the database data into Redshift.</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>However, this option involves a major development effort to write a custom migration job to process the incoming data streams from the source database and reliably write into Redshift via S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3."
      },
      {
        "image": "https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram-AWS-DMS_continuous-data-replication.a0e3bd328d2a4bd9b40a83e767199dcc13cf678f.png",
        "answer": "",
        "explanation": "Continuous Data Replication"
      },
      {
        "link": "https://aws.amazon.com/dms/"
      },
      {
        "answer": "",
        "explanation": "You can migrate data to Amazon Redshift databases using AWS Database Migration Service. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. With an Amazon Redshift database as a target, you can migrate data from all of the other supported source databases."
      },
      {
        "answer": "",
        "explanation": "The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance.\nDuring a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database. The AWS DMS replication instance must be located in that same region."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue to replicate the data from the databases into Amazon Redshift</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing."
      },
      {
        "answer": "",
        "explanation": "Using AWS Glue involves significant development efforts to write custom migration scripts to copy the database data into Redshift, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EMR to replicate the data from the databases into Amazon Redshift</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances."
      },
      {
        "answer": "",
        "explanation": "Using EMR involves significant infrastructure management efforts to set up and maintain the EMR cluster. Additionally, this option involves a major development effort to write custom migration jobs to copy the database data into Redshift."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events."
      },
      {
        "answer": "",
        "explanation": "However, this option involves a major development effort to write a custom migration job to process the incoming data streams from the source database and reliably write into Redshift via S3."
      }
    ],
    "references": [
      "https://aws.amazon.com/dms/",
      "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html"
    ]
  },
  {
    "id": 63,
    "question": "<p>Your company runs a web portal to match developers to clients who need their help. As a data engineer, you've designed the architecture of the website to be fully serverless with Amazon API Gateway and AWS Lambda. The backend uses an Amazon DynamoDB table. You would like to automatically congratulate your developers on important milestones, such as - their first paid contract. All the contracts are stored in Amazon DynamoDB.</p>\n\n<p>Which of the following options can you use to implement this functionality such that there is LEAST delay in sending automatic notifications?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon DynamoDB DAX + Amazon API Gateway</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon DynamoDB Streams + AWS Lambda</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon EventBridge events + AWS Lambda</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Simple Queue Service (Amazon SQS) + AWS Lambda</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p><strong>Amazon DynamoDB Streams + AWS Lambda</strong></p>\n\n<p>Amazon DynamoDB stream is an ordered flow of information about changes to items in the Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table.</p>\n\n<p>Amazon DynamoDB Streams will contain a stream of all the changes that happen to an Amazon DynamoDB table. It can be chained with an AWS Lambda function that will be triggered to react to these changes, one of which is the developer's milestone. Therefore, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon DynamoDB DAX + Amazon API Gateway</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.</p>\n\n<p>DAX is a caching layer and Amazon API Gateway is used to deploy APIs at scale, so this won't help.</p>\n\n<p><strong>Amazon Simple Queue Service (Amazon SQS) + AWS Lambda</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Amazon SQS and AWS Lambda could also work, but one would need to write extra logic to send messages to SQS, whereas our data already lives on Amazon DynamoDB so Amazon DynamoDB Streams is a much better choice.</p>\n\n<p><strong>Amazon EventBridge events + AWS Lambda</strong> - You cannot use Amazon DynamoDB as a target for an Amazon EventBridge event, so this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon DynamoDB Streams + AWS Lambda</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB stream is an ordered flow of information about changes to items in the Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table."
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB Streams will contain a stream of all the changes that happen to an Amazon DynamoDB table. It can be chained with an AWS Lambda function that will be triggered to react to these changes, one of which is the developer's milestone. Therefore, this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon DynamoDB DAX + Amazon API Gateway</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management."
      },
      {
        "answer": "",
        "explanation": "DAX is a caching layer and Amazon API Gateway is used to deploy APIs at scale, so this won't help."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (Amazon SQS) + AWS Lambda</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent."
      },
      {
        "answer": "",
        "explanation": "Amazon SQS and AWS Lambda could also work, but one would need to write extra logic to send messages to SQS, whereas our data already lives on Amazon DynamoDB so Amazon DynamoDB Streams is a much better choice."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EventBridge events + AWS Lambda</strong> - You cannot use Amazon DynamoDB as a target for an Amazon EventBridge event, so this option is ruled out."
      }
    ],
    "references": [
      "https://aws.amazon.com/dynamodb/dax/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html"
    ]
  },
  {
    "id": 64,
    "question": "<p>A company has integration with a third-party service to export its data to an Amazon S3 bucket. After each export, the incremental data from the S3 bucket needs to be transferred to Amazon Redshift and this data should be available to filter on specific keys.</p>\n\n<p>Which AWS service/solution is a good fit to address this requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Amazon S3 Event notifications to trigger an AWS Lambda function to copy the data from the S3 bucket to Redshift</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon S3 Replication to copy the data from the S3 bucket to Redshift</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Load data from Amazon S3 to Amazon Redshift using AWS Glue</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Redshift Spectrum to copy data from S3 bucket to Redshift</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Load data from Amazon S3 to Amazon Redshift using AWS Glue</strong></p>\n\n<p>With AWS Glue, you can discover and connect to more than 70 diverse data sources and manage your data in a centralized data catalog. You can visually create, run, and monitor extract, transform, and load (ETL) pipelines to load data into your data lakes. Also, you can immediately search and query cataloged data using Amazon Athena, Amazon EMR, and Amazon Redshift Spectrum.</p>\n\n<p>AWS Glue provides serverless data integration service and is the best fit of the options given.</p>\n\n<p>For further deep-dive on AWS Glue, Amazon S3, and Redshift, refer to the following example:</p>\n\n<p>Reference Example:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q15-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q15-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/build-an-etl-service-pipeline-to-load-data-incrementally-from-amazon-s3-to-amazon-redshift-using-aws-glue.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/build-an-etl-service-pipeline-to-load-data-incrementally-from-amazon-s3-to-amazon-redshift-using-aws-glue.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Redshift Spectrum to copy data from S3 bucket to Redshift</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. The given use case needs to move incremental data from S3 to Redshift, so this option is incorrect.</p>\n\n<p><strong>Configure Amazon S3 Event notifications to trigger an AWS Lambda function to copy the data from the S3 bucket to Redshift</strong> - Using the AWS Lambda function to copy data from S3 to Redshift is not effective since Lambda functions have a maximum execution time of 15 minutes which does not suit incremental data transfers from Amazon S3 to Redshift.</p>\n\n<p><strong>Use Amazon S3 Replication to copy the data from the S3 bucket to Redshift</strong> - Amazon S3 Replication is an elastic, fully managed, low-cost feature that replicates objects between S3 buckets. So, you cannot use it to transfer data into Redshift.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/build-an-etl-service-pipeline-to-load-data-incrementally-from-amazon-s3-to-amazon-redshift-using-aws-glue.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/build-an-etl-service-pipeline-to-load-data-incrementally-from-amazon-s3-to-amazon-redshift-using-aws-glue.html</a></p>\n\n<p><a href=\"https://repost.aws/questions/QUsYL4rbe8RAmdqWrE4WWabQ/transfer-from-s3-to-redshift-and-filter-specific-key\">https://repost.aws/questions/QUsYL4rbe8RAmdqWrE4WWabQ/transfer-from-s3-to-redshift-and-filter-specific-key</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Load data from Amazon S3 to Amazon Redshift using AWS Glue</strong>"
      },
      {
        "answer": "",
        "explanation": "With AWS Glue, you can discover and connect to more than 70 diverse data sources and manage your data in a centralized data catalog. You can visually create, run, and monitor extract, transform, and load (ETL) pipelines to load data into your data lakes. Also, you can immediately search and query cataloged data using Amazon Athena, Amazon EMR, and Amazon Redshift Spectrum."
      },
      {
        "answer": "",
        "explanation": "AWS Glue provides serverless data integration service and is the best fit of the options given."
      },
      {
        "answer": "",
        "explanation": "For further deep-dive on AWS Glue, Amazon S3, and Redshift, refer to the following example:"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt1-q15-i1.jpg",
        "answer": "",
        "explanation": "Reference Example:"
      },
      {
        "link": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/build-an-etl-service-pipeline-to-load-data-incrementally-from-amazon-s3-to-amazon-redshift-using-aws-glue.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Redshift Spectrum to copy data from S3 bucket to Redshift</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. The given use case needs to move incremental data from S3 to Redshift, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon S3 Event notifications to trigger an AWS Lambda function to copy the data from the S3 bucket to Redshift</strong> - Using the AWS Lambda function to copy data from S3 to Redshift is not effective since Lambda functions have a maximum execution time of 15 minutes which does not suit incremental data transfers from Amazon S3 to Redshift."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Replication to copy the data from the S3 bucket to Redshift</strong> - Amazon S3 Replication is an elastic, fully managed, low-cost feature that replicates objects between S3 buckets. So, you cannot use it to transfer data into Redshift."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/build-an-etl-service-pipeline-to-load-data-incrementally-from-amazon-s3-to-amazon-redshift-using-aws-glue.html",
      "https://repost.aws/questions/QUsYL4rbe8RAmdqWrE4WWabQ/transfer-from-s3-to-redshift-and-filter-specific-key"
    ]
  },
  {
    "id": 65,
    "question": "<p>A data engineer is working on modeling data for a DynamoDB production database. To address certain access patterns for the application, the data engineer is in the process of creating secondary indexes.</p>\n\n<p>Which of the following options should the data engineer consider for these requirements? (Select three)</p>",
    "corrects": [
      1,
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Applications that need to perform many kinds of queries, using a variety of different attributes for their query criteria should use Global Secondary Indexes</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>A Global Secondary Index contains a selection of attributes from the base table which are organized by a primary key that is different from that of the table</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Global secondary indexes support both data models - eventually consistent or strongly consistent reads</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>If the writes are throttled on the Global Secondary Indexes, then the main table will be throttled, even though the Write Capacity Units on the main tables are fine</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>For greater query or scan flexibility, you can create up to twenty Local Secondary Indexes per table</p>",
        "correct": false
      },
      {
        "id": 6,
        "answer": "<p>A Local Secondary Index maintains an alternate primary key for a given partition key value</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Applications that need to perform many kinds of queries, using a variety of different attributes for their query criteria should use Global Secondary Indexes</strong> - Some applications might need to perform many kinds of queries, using a variety of different attributes as query criteria. To support these requirements, you can create one or more global secondary indexes and issue Query requests against these indexes in Amazon DynamoDB.</p>\n\n<p><strong>If the writes are throttled on the Global Secondary Indexes, then the main table will be throttled, even though the Write Capacity Units on the main tables are fine</strong> - For a table write to succeed, the provisioned throughput settings for the table and all of its global secondary indexes must have enough write capacity to accommodate the write. Otherwise, the write to the main table itself is throttled.</p>\n\n<p><strong>A Global Secondary Index contains a selection of attributes from the base table which are organized by a primary key that is different from that of the table</strong> - To speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table. It doesn't even need to have the same key schema as a table.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A Local Secondary Index maintains an alternate primary key for a given partition key value</strong> - This is an invalid statement. A local secondary index maintains an alternate sort key for a given partition key value. The data in a local secondary index is organized by the same partition key as the base table but with a different sort key.</p>\n\n<p><strong>Global secondary indexes support both data models - eventually consistent or strongly consistent reads</strong> - This statement is incorrect. Global secondary indexes support only eventually consistent data model.</p>\n\n<p><strong>For greater query or scan flexibility, you can create up to twenty Local Secondary Indexes per table</strong> - This statement is not correct. Each table in DynamoDB can have up to 20 global secondary indexes (default quota) and 5 local secondary indexes.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Applications that need to perform many kinds of queries, using a variety of different attributes for their query criteria should use Global Secondary Indexes</strong> - Some applications might need to perform many kinds of queries, using a variety of different attributes as query criteria. To support these requirements, you can create one or more global secondary indexes and issue Query requests against these indexes in Amazon DynamoDB."
      },
      {
        "answer": "",
        "explanation": "<strong>If the writes are throttled on the Global Secondary Indexes, then the main table will be throttled, even though the Write Capacity Units on the main tables are fine</strong> - For a table write to succeed, the provisioned throughput settings for the table and all of its global secondary indexes must have enough write capacity to accommodate the write. Otherwise, the write to the main table itself is throttled."
      },
      {
        "answer": "",
        "explanation": "<strong>A Global Secondary Index contains a selection of attributes from the base table which are organized by a primary key that is different from that of the table</strong> - To speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table. It doesn't even need to have the same key schema as a table."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>A Local Secondary Index maintains an alternate primary key for a given partition key value</strong> - This is an invalid statement. A local secondary index maintains an alternate sort key for a given partition key value. The data in a local secondary index is organized by the same partition key as the base table but with a different sort key."
      },
      {
        "answer": "",
        "explanation": "<strong>Global secondary indexes support both data models - eventually consistent or strongly consistent reads</strong> - This statement is incorrect. Global secondary indexes support only eventually consistent data model."
      },
      {
        "answer": "",
        "explanation": "<strong>For greater query or scan flexibility, you can create up to twenty Local Secondary Indexes per table</strong> - This statement is not correct. Each table in DynamoDB can have up to 20 global secondary indexes (default quota) and 5 local secondary indexes."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html"
    ]
  }
]