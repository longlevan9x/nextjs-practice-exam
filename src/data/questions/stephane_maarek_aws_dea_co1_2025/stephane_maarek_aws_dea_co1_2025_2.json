[
  {
    "id": 1,
    "question": "<p>A multi-national retail company uses separate AWS accounts for their business units. The finance team has an encrypted snapshot of an Amazon Relational Database Service (Amazon RDS) instance that uses the default AWS Key Management Service (AWS KMS) key. The finance team wants to share the encrypted snapshot with their Audit team that uses another AWS account.</p>\n\n<p>Which of the following solutions would you recommend to address the given use-case?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add the target account to the default AWS KMS key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add the target account to a customer-managed key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add the target account to the default AWS KMS key. Copy the snapshot using the customer-managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Add the target account to a customer-managed key. Copy the snapshot using the customer-managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Add the target account to a customer-managed key. Copy the snapshot using the customer-managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</strong> - You can't share a snapshot that's encrypted using the default AWS KMS encryption key.</p>\n\n<p>To share an encrypted Amazon RDS DB snapshot:</p>\n\n<p>Add the target account to a custom (non-default) KMS key.</p>\n\n<p>Copy the snapshot using the customer-managed key, and then share the snapshot with the target account.</p>\n\n<p>Copy the shared DB snapshot from the target account.</p>\n\n<p>Detailed steps to implement this solution:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q34-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q34-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/\">https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Add the target account to the default AWS KMS key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account\"</p>\n\n<p>\"Add the target account to a customer-managed key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account\"</p>\n\n<p>\"Add the target account to the default AWS KMS key. Copy the snapshot using the customer-managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account\"</p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/\">https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Add the target account to a customer-managed key. Copy the snapshot using the customer-managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</strong> - You can't share a snapshot that's encrypted using the default AWS KMS encryption key."
      },
      {
        "answer": "",
        "explanation": "To share an encrypted Amazon RDS DB snapshot:"
      },
      {
        "answer": "",
        "explanation": "Add the target account to a custom (non-default) KMS key."
      },
      {
        "answer": "",
        "explanation": "Copy the snapshot using the customer-managed key, and then share the snapshot with the target account."
      },
      {
        "answer": "",
        "explanation": "Copy the shared DB snapshot from the target account."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q34-i1.jpg",
        "answer": "",
        "explanation": "Detailed steps to implement this solution:"
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "\"Add the target account to the default AWS KMS key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account\""
      },
      {
        "answer": "",
        "explanation": "\"Add the target account to a customer-managed key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account\""
      },
      {
        "answer": "",
        "explanation": "\"Add the target account to the default AWS KMS key. Copy the snapshot using the customer-managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account\""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/"
    ]
  },
  {
    "id": 2,
    "question": "<p>A research agency has deployed two autonomous underwater vehicles in the ocean to track parameters such as salinity, temperature, speed and direction of currents, etc. Vehicle A has twenty sensors whereas Vehicle B has ten sensors. Each sensor is identified by a unique ID. Amazon Kinesis Data Streams is being used to gather data from each sensor. A single Amazon Kinesis Data Stream with two shards is configured based on the total incoming and outgoing data throughput. Two partition keys are generated based on the name of the vehicles. During initial testing, data from Vehicle A experiences a bottleneck whereas data from Vehicle B does not. The overall stream throughput has been validated to be less than the assigned Kinesis Data Streams throughput.</p>\n\n<p>Which of the following solutions would you use to address this bottleneck without increasing the total cost and complexity of the system?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up another Kinesis Data Stream for Vehicle A with twenty shards and then direct Vehicle A sensor data to this new Kinesis Data Stream</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Increase the number of shards in Kinesis Data Streams to support throughput from both vehicles</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Change the partition key to use the sensor ID instead of the name of the vehicle</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up another Kinesis Data Stream for Vehicle B with ten shards and then direct Vehicle B sensor data to this new Kinesis Data Stream</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Change the partition key to use the sensor ID instead of the name of the vehicle</strong></p>\n\n<p>Amazon Kinesis Data Streams is a fully managed, serverless streaming data service that makes it easy to elastically ingest and store logs, events, clickstreams, and other forms of streaming data in real time. Kinesis Data Streams has two capacity modes: on-demand and provisioned, and both come with specific billing options.</p>\n\n<p>In the on-demand mode, pricing is based on the volume of data ingested and retrieved along with a per-hour charge for each data stream in your account.</p>\n\n<p>With provisioned capacity mode, you specify the number of shards necessary for your application based on its write and read request rate. A shard is a unit of capacity that provides 1 MB/second of write and 2 MB/second of read throughout. A record is the data that your data producer adds to your Amazon Kinesis data stream. A PUT Payload Unit is counted in 25 KB payload “chunks” that comprise a record. For example, a 5 KB record contains one PUT Payload Unit, a 45 KB record contains two PUT Payload Units, and a 1 MB record contains 40 PUT Payload Units. PUT Payload Unit is charged a per-million PUT Payload Units rate. In the provisioned mode (applicable for the given scenario), you pay by the shard hour (1MB/second ingress, 2MB/second egress) and PUT Payload Units, per 1,000,000 units.</p>\n\n<p>For the existing use case, the Kinesis Data Stream has been partitioned into two shards, one for each of the two vehicles. Therefore, data from the 20 sensors of Vehicle A is going to the first shard and the data from the 10 sensors of Vehicle B is going to the other shard, causing an imbalance and thereby resulting in a throughput bottleneck for Vehicle A. The correct solution is to partition the shards on the basis of the sensor ID which will result in an even distribution across the two shards. The use-case already states that the overall stream throughput is less than the assigned Kinesis Data Streams throughput, so there is no need to provision extra shards.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the number of shards in Kinesis Data Streams to support throughput from both vehicles</strong> - As mentioned in the explanation above, there is no need to provision extra shards as it would result in additional costs. The right solution is to partition the shards on the basis of the sensor ID which will result in an even distribution across the two shards already provisioned.</p>\n\n<p><strong>Set up another Kinesis Data Stream for Vehicle A with twenty shards and then direct Vehicle A sensor data to this new Kinesis Data Stream</strong></p>\n\n<p><strong>Set up another Kinesis Data Stream for Vehicle B with ten shards and then direct Vehicle B sensor data to this new Kinesis Data Stream</strong></p>\n\n<p>Both these options have been added as distractors. There is no need to provision extra shards for any of the vehicles, as it would result in additional costs. The right solution is to partition the shards on the basis of the sensor ID which will result in an even distribution across the two shards already provisioned.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/pricing/\">https://aws.amazon.com/kinesis/data-streams/pricing/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Change the partition key to use the sensor ID instead of the name of the vehicle</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams is a fully managed, serverless streaming data service that makes it easy to elastically ingest and store logs, events, clickstreams, and other forms of streaming data in real time. Kinesis Data Streams has two capacity modes: on-demand and provisioned, and both come with specific billing options."
      },
      {
        "answer": "",
        "explanation": "In the on-demand mode, pricing is based on the volume of data ingested and retrieved along with a per-hour charge for each data stream in your account."
      },
      {
        "answer": "",
        "explanation": "With provisioned capacity mode, you specify the number of shards necessary for your application based on its write and read request rate. A shard is a unit of capacity that provides 1 MB/second of write and 2 MB/second of read throughout. A record is the data that your data producer adds to your Amazon Kinesis data stream. A PUT Payload Unit is counted in 25 KB payload “chunks” that comprise a record. For example, a 5 KB record contains one PUT Payload Unit, a 45 KB record contains two PUT Payload Units, and a 1 MB record contains 40 PUT Payload Units. PUT Payload Unit is charged a per-million PUT Payload Units rate. In the provisioned mode (applicable for the given scenario), you pay by the shard hour (1MB/second ingress, 2MB/second egress) and PUT Payload Units, per 1,000,000 units."
      },
      {
        "answer": "",
        "explanation": "For the existing use case, the Kinesis Data Stream has been partitioned into two shards, one for each of the two vehicles. Therefore, data from the 20 sensors of Vehicle A is going to the first shard and the data from the 10 sensors of Vehicle B is going to the other shard, causing an imbalance and thereby resulting in a throughput bottleneck for Vehicle A. The correct solution is to partition the shards on the basis of the sensor ID which will result in an even distribution across the two shards. The use-case already states that the overall stream throughput is less than the assigned Kinesis Data Streams throughput, so there is no need to provision extra shards."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Increase the number of shards in Kinesis Data Streams to support throughput from both vehicles</strong> - As mentioned in the explanation above, there is no need to provision extra shards as it would result in additional costs. The right solution is to partition the shards on the basis of the sensor ID which will result in an even distribution across the two shards already provisioned."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up another Kinesis Data Stream for Vehicle A with twenty shards and then direct Vehicle A sensor data to this new Kinesis Data Stream</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Set up another Kinesis Data Stream for Vehicle B with ten shards and then direct Vehicle B sensor data to this new Kinesis Data Stream</strong>"
      },
      {
        "answer": "",
        "explanation": "Both these options have been added as distractors. There is no need to provision extra shards for any of the vehicles, as it would result in additional costs. The right solution is to partition the shards on the basis of the sensor ID which will result in an even distribution across the two shards already provisioned."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/pricing/"
    ]
  },
  {
    "id": 3,
    "question": "<p>A data engineer is tasked with managing the real-time ingestion of streaming data into AWS. The solution must have the capability to perform real-time analytics, including time-based aggregations over periods of up to 45 minutes with high fault tolerance.</p>\n\n<p>Which of the following options fulfills these criteria while minimizing the operational overhead?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage Amazon Kinesis Firehose with a data transformation Lambda function to conduct real-time data analysis including time-based aggregations over periods of up to 45 minutes</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Leverage Amazon Managed Service for Apache Flink to conduct real-time data analysis including time-based aggregations over periods of up to 45 minutes</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Leverage Amazon Kinesis Streams having a consumer Lambda function to conduct real-time data analysis including time-based aggregations over periods of up to 45 minutes</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage Amazon Kinesis Streams having a consumer app running on an EC2 instance to conduct real-time data analysis including time-based aggregations over periods of up to 45 minutes</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage Amazon Managed Service for Apache Flink to conduct real-time data analysis including time-based aggregations over periods of up to 45 minutes</strong></p>\n\n<p>With Amazon Managed Service for Apache Flink, you can transform and analyze streaming data in real time with Apache Flink. Amazon Managed Service for Apache Flink takes care of everything required to continuously run streaming applications and scales automatically to match the volume and throughput of your incoming data. Amazon Managed Service for Apache Flink supports all operators from Apache Flink that can be used to solve a wide variety of use cases including map, KeyBy, aggregations, windows, joins, and more. Aggregations performs processing across multiple keys such as sum, min, and max. Window Join joins two data streams together on a given key and window.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q3-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q3-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/managed-service-apache-flink/faqs/\">https://aws.amazon.com/managed-service-apache-flink/faqs/</a><p></p>\n\n<p>For the given use case, Amazon Managed Service for Apache Flink is the best fit to perform real-time analytics including time-based aggregations over periods of up to 45 minutes with high fault tolerance, since this solution involves minimal operation overhead.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon Kinesis Streams having a consumer Lambda function to conduct real-time data analysis including time-based aggregations over periods of up to 45 minutes</strong> - A Kinesis data stream is a set of shards. Each shard contains a sequence of data records. A consumer is an application that processes the data from a Kinesis data stream. You can map a Lambda function to a shared-throughput consumer (standard iterator), or to a dedicated-throughput consumer with enhanced fan-out. For this option, you will need to integrate a Lambda function as a consumer and develop custom code to perform time-based aggregations over periods of up to 45 minutes. This represents additional operational overhead, so this option is incorrect.</p>\n\n<p><strong>Leverage Amazon Kinesis Streams having a consumer app running on an EC2 instance to conduct real-time data analysis including time-based aggregations over periods of up to 45 minutes</strong> - A Kinesis data stream is a set of shards. Each shard contains a sequence of data records. A consumer is an application that processes the data from a Kinesis data stream. You can set up a consumer app on an EC2 instance. However, you will need to develop custom code to perform time-based aggregations over periods of up to 45 minutes. In addition, you will need to monitor and maintain the EC2 instance. So, this option represents additional operational overhead and hence it is incorrect.</p>\n\n<p><strong>Leverage Amazon Kinesis Firehose with a data transformation Lambda function to conduct real-time data analysis including time-based aggregations over periods of up to 45 minutes</strong> - Amazon Kinesis Firehose can only be used for near real-time analysis and NOT real-time analysis, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/managed-service-apache-flink/faqs/\">https://aws.amazon.com/managed-service-apache-flink/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/firehose/faqs/\">https://aws.amazon.com/firehose/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Managed Service for Apache Flink to conduct real-time data analysis including time-based aggregations over periods of up to 45 minutes</strong>"
      },
      {
        "answer": "",
        "explanation": "With Amazon Managed Service for Apache Flink, you can transform and analyze streaming data in real time with Apache Flink. Amazon Managed Service for Apache Flink takes care of everything required to continuously run streaming applications and scales automatically to match the volume and throughput of your incoming data. Amazon Managed Service for Apache Flink supports all operators from Apache Flink that can be used to solve a wide variety of use cases including map, KeyBy, aggregations, windows, joins, and more. Aggregations performs processing across multiple keys such as sum, min, and max. Window Join joins two data streams together on a given key and window."
      },
      {
        "link": "https://aws.amazon.com/managed-service-apache-flink/faqs/"
      },
      {
        "answer": "",
        "explanation": "For the given use case, Amazon Managed Service for Apache Flink is the best fit to perform real-time analytics including time-based aggregations over periods of up to 45 minutes with high fault tolerance, since this solution involves minimal operation overhead."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Kinesis Streams having a consumer Lambda function to conduct real-time data analysis including time-based aggregations over periods of up to 45 minutes</strong> - A Kinesis data stream is a set of shards. Each shard contains a sequence of data records. A consumer is an application that processes the data from a Kinesis data stream. You can map a Lambda function to a shared-throughput consumer (standard iterator), or to a dedicated-throughput consumer with enhanced fan-out. For this option, you will need to integrate a Lambda function as a consumer and develop custom code to perform time-based aggregations over periods of up to 45 minutes. This represents additional operational overhead, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Kinesis Streams having a consumer app running on an EC2 instance to conduct real-time data analysis including time-based aggregations over periods of up to 45 minutes</strong> - A Kinesis data stream is a set of shards. Each shard contains a sequence of data records. A consumer is an application that processes the data from a Kinesis data stream. You can set up a consumer app on an EC2 instance. However, you will need to develop custom code to perform time-based aggregations over periods of up to 45 minutes. In addition, you will need to monitor and maintain the EC2 instance. So, this option represents additional operational overhead and hence it is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Kinesis Firehose with a data transformation Lambda function to conduct real-time data analysis including time-based aggregations over periods of up to 45 minutes</strong> - Amazon Kinesis Firehose can only be used for near real-time analysis and NOT real-time analysis, so this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/managed-service-apache-flink/faqs/",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
      "https://aws.amazon.com/firehose/faqs/"
    ]
  },
  {
    "id": 4,
    "question": "<p>A company is looking at developing an Internet-of-Things (IoT) solution that would analyze real-time clickstream events from embedded sensors in consumer electronic devices. The company has hired you as an AWS Certified Data Engineer Associate to advise the data engineering team and develop a solution using the AWS Cloud. The company wants to use clickstream data to perform data science, develop algorithms, and create visualizations and dashboards to support the business stakeholders. Each of these groups would work independently and would need real-time access to this clickstream data for their applications.</p>\n\n<p>Which solution would provide a highly available and fault-tolerant solution to capture the clickstream events from the source and also provide a simultaneous feed of the data stream to the downstream applications?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Kinesis Data Analytics to facilitate multiple applications to consume and analyze the same streaming data concurrently and independently</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Kinesis Data Firehose to allow applications to consume the same streaming data concurrently and independently</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Kinesis Data Streams to facilitate multiple applications to consume the same streaming data concurrently and independently</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon SQS to facilitate multiple applications to process the same streaming data concurrently and independently</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Kinesis Data Streams to facilitate multiple applications to consume the same streaming data concurrently and independently</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</p>\n\n<p>Amazon Kinesis Data Streams is recommended when you need the ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another application that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.</p>\n\n<p>KDS provides the ability for multiple applications to consume the same stream concurrently\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q22-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q22-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Kinesis Data Firehose to allow applications to consume the same streaming data concurrently and independently</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.</p>\n\n<p><strong>Use AWS Kinesis Data Analytics to facilitate multiple applications to consume and analyze the same streaming data concurrently and independently</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: set up your streaming data sources, write your queries or streaming applications, and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries on streaming data, therefore this option is incorrect.</p>\n\n<p><strong>Use Amazon SQS to facilitate multiple applications to process the same streaming data concurrently and independently</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers at the same time, therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Kinesis Data Streams to facilitate multiple applications to consume the same streaming data concurrently and independently</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering)."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams is recommended when you need the ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another application that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q22-i1.jpg",
        "answer": "",
        "explanation": "KDS provides the ability for multiple applications to consume the same stream concurrently"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/faqs/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Kinesis Data Firehose to allow applications to consume the same streaming data concurrently and independently</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Kinesis Data Analytics to facilitate multiple applications to consume and analyze the same streaming data concurrently and independently</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: set up your streaming data sources, write your queries or streaming applications, and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries on streaming data, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS to facilitate multiple applications to process the same streaming data concurrently and independently</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers at the same time, therefore this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/faqs/",
      "https://aws.amazon.com/kinesis/data-firehose/faqs/",
      "https://aws.amazon.com/kinesis/data-analytics/faqs/"
    ]
  },
  {
    "id": 5,
    "question": "<p>As part of the on-premises data center migration to AWS Cloud, a company is looking at using multiple AWS Snow Family devices to move their on-premises data.</p>\n\n<p>Which AWS Snow Family service offers the feature of storage clustering?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Snowcone</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Snowmobile</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Snowmobile Storage Compute</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Snowball Edge Compute Optimized</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Snowball Edge Compute Optimized</strong></p>\n\n<p>AWS Snowball is a data migration and edge computing device that comes in two device options: Compute Optimized and Storage Optimized. AWS Snowball Edge Storage Optimized devices provide 40 vCPUs of compute capacity coupled with 80 terabytes of usable block or Amazon S3-compatible object storage. It is well-suited for local storage and large-scale data transfer. AWS Snowball Edge Compute Optimized devices provide 52 vCPUs, 42 terabytes of usable block or object storage, and an optional GPU for use cases such as advanced machine learning and full-motion video analysis in disconnected environments.</p>\n\n<p>Customers can use these two options for data collection, machine learning and processing, and storage in environments with intermittent connectivity (such as manufacturing, industrial, and transportation) or in extremely remote locations (such as military or maritime operations) before shipping it back to AWS. These devices may also be rack-mounted and clustered together to build larger, temporary installations.</p>\n\n<p>Therefore, both AWS Snowball Edge Storage Optimized and AWS Snowball Edge Compute Optimized offer the storage clustering feature.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q51-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q51-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/snow/#Feature_comparison\">https://aws.amazon.com/snow/#Feature_comparison</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Snowcone</strong> - AWS Snowcone is the smallest member of the AWS Snow Family of edge computing and data transfer devices. Snowcone is portable, rugged, and secure. You can use Snowcone to collect, process, and move data to AWS, either offline by shipping the device or online with AWS DataSync. Snowcone does not offer a storage Clustering option.</p>\n\n<p><strong>AWS Snowmobile</strong> - AWS Snowmobile moves up to 100 PB of data in a 45-foot-long ruggedized shipping container and is ideal for multi-petabyte or Exabyte-scale digital media migrations and data center shutdowns. AWS Snowmobile arrives at the customer site and appears as a network-attached data store for more secure, high-speed data transfer. After data is transferred to Snowmobile, it is driven back to an AWS Region where the data is loaded into Amazon S3. AWS Snowmobile does not offer a storage Clustering option.</p>\n\n<p><strong>AWS Snowmobile Storage Compute</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/snow/#Feature_comparison\">https://aws.amazon.com/snow/#Feature_comparison</a></p>\n\n<p><a href=\"https://aws.amazon.com/snow/\">https://aws.amazon.com/snow/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Snowball Edge Compute Optimized</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Snowball is a data migration and edge computing device that comes in two device options: Compute Optimized and Storage Optimized. AWS Snowball Edge Storage Optimized devices provide 40 vCPUs of compute capacity coupled with 80 terabytes of usable block or Amazon S3-compatible object storage. It is well-suited for local storage and large-scale data transfer. AWS Snowball Edge Compute Optimized devices provide 52 vCPUs, 42 terabytes of usable block or object storage, and an optional GPU for use cases such as advanced machine learning and full-motion video analysis in disconnected environments."
      },
      {
        "answer": "",
        "explanation": "Customers can use these two options for data collection, machine learning and processing, and storage in environments with intermittent connectivity (such as manufacturing, industrial, and transportation) or in extremely remote locations (such as military or maritime operations) before shipping it back to AWS. These devices may also be rack-mounted and clustered together to build larger, temporary installations."
      },
      {
        "answer": "",
        "explanation": "Therefore, both AWS Snowball Edge Storage Optimized and AWS Snowball Edge Compute Optimized offer the storage clustering feature."
      },
      {
        "link": "https://aws.amazon.com/snow/#Feature_comparison"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Snowcone</strong> - AWS Snowcone is the smallest member of the AWS Snow Family of edge computing and data transfer devices. Snowcone is portable, rugged, and secure. You can use Snowcone to collect, process, and move data to AWS, either offline by shipping the device or online with AWS DataSync. Snowcone does not offer a storage Clustering option."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Snowmobile</strong> - AWS Snowmobile moves up to 100 PB of data in a 45-foot-long ruggedized shipping container and is ideal for multi-petabyte or Exabyte-scale digital media migrations and data center shutdowns. AWS Snowmobile arrives at the customer site and appears as a network-attached data store for more secure, high-speed data transfer. After data is transferred to Snowmobile, it is driven back to an AWS Region where the data is loaded into Amazon S3. AWS Snowmobile does not offer a storage Clustering option."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Snowmobile Storage Compute</strong> - This is a made-up option, given only as a distractor."
      }
    ],
    "references": [
      "https://aws.amazon.com/snow/#Feature_comparison",
      "https://aws.amazon.com/snow/"
    ]
  },
  {
    "id": 6,
    "question": "<p>An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last year. However, the analysts want to retain the ability to cross-reference this historical data along with the daily reports.</p>\n\n<p>The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. Which option would you recommend to facilitate this use-case?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Glue ETL job to load the Amazon S3-based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the Amazon Redshift COPY command to load the Amazon S3-based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team needs to export these in flat files and then do further analysis</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift</strong></p>\n\n<p>Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis.</p>\n\n<p>Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables.</p>\n\n<p>Amazon Redshift Spectrum resides on dedicated Amazon Redshift servers that are independent of your cluster. The Redshift Spectrum pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to the Redshift Spectrum layer. Thus, Amazon Redshift Spectrum queries use much less of your cluster's processing capacity than other queries.</p>\n\n<p>Redshift Spectrum Overview:\n<img src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2017/07/18/redshift_spectrum-1.gif\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2017/07/18/redshift_spectrum-1.gif\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team needs to export these in flat files and then do further analysis</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.\nProviding access to historical data via Athena would mean that historical data reconciliation would become difficult as the daily report would still be produced via Redshift. Such a setup is cumbersome to maintain on a day-to-day basis. Hence the option to use Athena is ruled out.</p>\n\n<p><strong>Use the Amazon Redshift COPY command to load the Amazon S3-based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift</strong></p>\n\n<p><strong>Use AWS Glue ETL job to load the Amazon S3-based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift</strong></p>\n\n<p>Loading historical data into Amazon Redshift via COPY command or AWS Glue ETL job would cost heavily for a one-time ad-hoc process. The same result can be achieved more cost-efficiently by using Amazon Redshift Spectrum. Therefore both these options to load historical data into Redshift are also incorrect for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#c-spectrum-overview\nhttps://aws.amazon.com/blogs/big-data/\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#c-spectrum-overview\nhttps://aws.amazon.com/blogs/big-data/</a></p>\n\n<p><a href=\"#\">amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis."
      },
      {
        "answer": "",
        "explanation": "Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables."
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift Spectrum resides on dedicated Amazon Redshift servers that are independent of your cluster. The Redshift Spectrum pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to the Redshift Spectrum layer. Thus, Amazon Redshift Spectrum queries use much less of your cluster's processing capacity than other queries."
      },
      {
        "image": "https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2017/07/18/redshift_spectrum-1.gif",
        "answer": "",
        "explanation": "Redshift Spectrum Overview:"
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team needs to export these in flat files and then do further analysis</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.\nProviding access to historical data via Athena would mean that historical data reconciliation would become difficult as the daily report would still be produced via Redshift. Such a setup is cumbersome to maintain on a day-to-day basis. Hence the option to use Athena is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the Amazon Redshift COPY command to load the Amazon S3-based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue ETL job to load the Amazon S3-based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift</strong>"
      },
      {
        "answer": "",
        "explanation": "Loading historical data into Amazon Redshift via COPY command or AWS Glue ETL job would cost heavily for a one-time ad-hoc process. The same result can be achieved more cost-efficiently by using Amazon Redshift Spectrum. Therefore both these options to load historical data into Redshift are also incorrect for the given use-case."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/",
      "#"
    ]
  },
  {
    "id": 7,
    "question": "<p>A financial services company has to retain the activity logs for each of their customers to meet compliance guidelines. Depending on the business line, the company wants to retain the logs for 5-10 years in highly available and durable storage on AWS. The overall data size is expected to be in Petabytes. In case of an audit, the data would need to be accessible within a timeframe of up to 48 hours.</p>\n\n<p>Which AWS storage option is the MOST cost-effective for the given compliance requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon S3 Glacier Flexible Retrieval</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon S3 Glacier Instant Retrieval</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon S3 Standard storage</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon S3 Glacier Deep Archive</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon S3 Glacier Deep Archive</strong></p>\n\n<p>Amazon S3 Glacier Deep Archive is an Amazon S3 storage class that provides secure and durable object storage for long-term retention of data that is accessed once or twice a year. From just $0.00099 per GB-month (less than one-tenth of one cent, or about $1 per TB-month), Amazon S3 Glacier Deep Archive offers the lowest cost storage in the cloud, at prices significantly lower than storing and maintaining data in on-premises magnetic tape libraries or archiving data off-site.</p>\n\n<p>S3 Glacier Deep Archive is the lowest-cost storage option in AWS. Storage costs for the S3 Glacier Deep Archive are less expensive than using the S3 Glacier Flexible Retrieval storage class. Amazon S3 Glacier Deep Archive provides retrieval within 12 hours using the Standard retrieval speed. You may also reduce retrieval costs by selecting Bulk retrieval, which will return data within 48 hours.</p>\n\n<p>Therefore, Amazon S3 Glacier Deep Archive is the correct choice.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q41-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q41-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 Glacier Instant Retrieval</strong> - Amazon S3 Glacier Instant Retrieval is used for archiving data that is rarely accessed and requires milliseconds retrieval. Data stored in the S3 Glacier Instant Retrieval storage class offers cost savings compared to the S3 Standard-IA storage class, with the same latency and throughput performance as the S3 Standard-IA storage class. S3 Glacier Instant Retrieval has higher data access costs than S3 Glacier Deep Archive.</p>\n\n<p><strong>Amazon S3 Glacier Flexible Retrieval</strong> - Amazon S3 Glacier Flexible Retrieval is used for archives where portions of the data might need to be retrieved in minutes. Data stored in the S3 Glacier Flexible Retrieval storage class has a minimum storage duration period of 90 days and can be accessed in as little as 1-5 minutes by using expedited retrieval. The retrieval time is flexible, and you can request free bulk retrievals in up to 5-12 hours. S3 Glacier Instant Retrieval has higher data access costs than S3 Glacier Deep Archive.</p>\n\n<p><strong>Amazon S3 Standard storage</strong> - Given the relaxed retrieval times, Amazon S3 standard storage would be much costlier than the Amazon S3 Glacier Deep Archive, so Amazon S3 standard storage is not the correct option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Glacier Deep Archive</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Glacier Deep Archive is an Amazon S3 storage class that provides secure and durable object storage for long-term retention of data that is accessed once or twice a year. From just $0.00099 per GB-month (less than one-tenth of one cent, or about $1 per TB-month), Amazon S3 Glacier Deep Archive offers the lowest cost storage in the cloud, at prices significantly lower than storing and maintaining data in on-premises magnetic tape libraries or archiving data off-site."
      },
      {
        "answer": "",
        "explanation": "S3 Glacier Deep Archive is the lowest-cost storage option in AWS. Storage costs for the S3 Glacier Deep Archive are less expensive than using the S3 Glacier Flexible Retrieval storage class. Amazon S3 Glacier Deep Archive provides retrieval within 12 hours using the Standard retrieval speed. You may also reduce retrieval costs by selecting Bulk retrieval, which will return data within 48 hours."
      },
      {
        "answer": "",
        "explanation": "Therefore, Amazon S3 Glacier Deep Archive is the correct choice."
      },
      {
        "link": "https://aws.amazon.com/s3/storage-classes/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Glacier Instant Retrieval</strong> - Amazon S3 Glacier Instant Retrieval is used for archiving data that is rarely accessed and requires milliseconds retrieval. Data stored in the S3 Glacier Instant Retrieval storage class offers cost savings compared to the S3 Standard-IA storage class, with the same latency and throughput performance as the S3 Standard-IA storage class. S3 Glacier Instant Retrieval has higher data access costs than S3 Glacier Deep Archive."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Glacier Flexible Retrieval</strong> - Amazon S3 Glacier Flexible Retrieval is used for archives where portions of the data might need to be retrieved in minutes. Data stored in the S3 Glacier Flexible Retrieval storage class has a minimum storage duration period of 90 days and can be accessed in as little as 1-5 minutes by using expedited retrieval. The retrieval time is flexible, and you can request free bulk retrievals in up to 5-12 hours. S3 Glacier Instant Retrieval has higher data access costs than S3 Glacier Deep Archive."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Standard storage</strong> - Given the relaxed retrieval times, Amazon S3 standard storage would be much costlier than the Amazon S3 Glacier Deep Archive, so Amazon S3 standard storage is not the correct option."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/s3/faqs/"
    ]
  },
  {
    "id": 8,
    "question": "<p>A company maintains its datasets in JSON and .csv formats in an Amazon S3 bucket and utilizes Amazon RDS for Microsoft SQL Server, Amazon DynamoDB (in provisioned capacity mode), and an Amazon Redshift cluster. The data engineering team is tasked with creating a solution that enables data scientists to query all these data sources using an SQL-like syntax.</p>\n\n<p>What solution would fulfill these requirements while incurring the least operational overhead?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage AWS Glue to crawl the various data sources and store the resultant metadata in the AWS Glue Data Catalog. Utilize Amazon Athena for querying the data, employing standard SQL for structured data sources and PartiQL for handling data stored in JSON format</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Leverage AWS Glue to crawl the various data sources and store the resultant metadata in the AWS Glue Data Catalog. Utilize AWS Glue jobs to transform the JSON data to .csv format and query the resultant data in .csv format using Amazon Athena</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Leverage AWS Glue to crawl the various data sources and store the resultant metadata in the AWS Glue Data Catalog. Utilize Amazon Redshift Spectrum for querying the data, employing standard SQL for structured data sources and PartiQL for handling data stored in JSON format</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage AWS Glue to crawl the various data sources and store the resultant metadata in the AWS Glue Data Catalog. Utilize AWS Glue jobs to transform both the JSON and .csv format data to parquet format and query the resultant data using Amazon Athena</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage AWS Glue to crawl the various data sources and store the resultant metadata in the AWS Glue Data Catalog. Utilize Amazon Athena for querying the data, employing standard SQL for structured data sources and PartiQL for handling data stored in JSON format</strong></p>\n\n<p>The AWS Glue Data Catalog contains references to data that is used as sources and targets of your extract, transform, and load (ETL) jobs in AWS Glue.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q5-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q5-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html\">https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html</a><p></p>\n\n<p>If you have data in sources other than Amazon S3, you can use Athena Federated Query to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data stored in relational, non-relational, object, and custom data sources.</p>\n\n<p>Athena uses data source connectors that run on AWS Lambda to run federated queries. A data source connector is a piece of code that can translate between your target data source and Athena. You can think of a connector as an extension of Athena's query engine. Prebuilt Athena data source connectors exist for data sources like Amazon CloudWatch Logs, Amazon DynamoDB, Amazon DocumentDB, and Amazon RDS, and JDBC-compliant relational data sources such MySQL, and PostgreSQL under the Apache 2.0 license.</p>\n\n<p>Amazon Athena also allows federated query pass-through, which allows entire queries to be executed directly on the underlying data source. Federated query pass-through allows you to take advantage of the unique functions, query language, and performance capabilities of different data sources, and can result in faster query execution and less data processed by Athena. You can run Athena queries on DynamoDB using the PartiQL language. Federated query pass-through is also useful when you want to run SELECT queries that aggregate, join, or invoke functions of your data source that are not available in Athena.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage AWS Glue to crawl the various data sources and store the resultant metadata in the AWS Glue Data Catalog. Utilize Amazon Redshift Spectrum for querying the data, employing standard SQL for structured data sources and PartiQL for handling data stored in JSON format</strong> - Using Amazon Redshift Spectrum to query the data would involve more operational overhead since you need to spin up a Redshift cluster to get Redshift spectrum to work.</p>\n\n<p><strong>Leverage AWS Glue to crawl the various data sources and store the resultant metadata in the AWS Glue Data Catalog. Utilize AWS Glue jobs to transform the JSON data to .csv format and query the resultant data in .csv format using Amazon Athena</strong> - Using AWS Glue jobs to transform the JSON data to .csv format represents unnecessary work to build a solution, so this option is incorrect.</p>\n\n<p><strong>Leverage AWS Glue to crawl the various data sources and store the resultant metadata in the AWS Glue Data Catalog. Utilize AWS Glue jobs to transform both the JSON and .csv format data to parquet format and query the resultant data using Amazon Athena</strong> - Using AWS Glue jobs to transform both the JSON and .csv format data to parquet format represents unnecessary work to build a solution, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html\">https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2024/04/amazon-athena-federated-query-pass-through/\">https://aws.amazon.com/about-aws/whats-new/2024/04/amazon-athena-federated-query-pass-through/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html\">https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Glue to crawl the various data sources and store the resultant metadata in the AWS Glue Data Catalog. Utilize Amazon Athena for querying the data, employing standard SQL for structured data sources and PartiQL for handling data stored in JSON format</strong>"
      },
      {
        "answer": "",
        "explanation": "The AWS Glue Data Catalog contains references to data that is used as sources and targets of your extract, transform, and load (ETL) jobs in AWS Glue."
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html"
      },
      {
        "answer": "",
        "explanation": "If you have data in sources other than Amazon S3, you can use Athena Federated Query to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data stored in relational, non-relational, object, and custom data sources."
      },
      {
        "answer": "",
        "explanation": "Athena uses data source connectors that run on AWS Lambda to run federated queries. A data source connector is a piece of code that can translate between your target data source and Athena. You can think of a connector as an extension of Athena's query engine. Prebuilt Athena data source connectors exist for data sources like Amazon CloudWatch Logs, Amazon DynamoDB, Amazon DocumentDB, and Amazon RDS, and JDBC-compliant relational data sources such MySQL, and PostgreSQL under the Apache 2.0 license."
      },
      {
        "answer": "",
        "explanation": "Amazon Athena also allows federated query pass-through, which allows entire queries to be executed directly on the underlying data source. Federated query pass-through allows you to take advantage of the unique functions, query language, and performance capabilities of different data sources, and can result in faster query execution and less data processed by Athena. You can run Athena queries on DynamoDB using the PartiQL language. Federated query pass-through is also useful when you want to run SELECT queries that aggregate, join, or invoke functions of your data source that are not available in Athena."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Glue to crawl the various data sources and store the resultant metadata in the AWS Glue Data Catalog. Utilize Amazon Redshift Spectrum for querying the data, employing standard SQL for structured data sources and PartiQL for handling data stored in JSON format</strong> - Using Amazon Redshift Spectrum to query the data would involve more operational overhead since you need to spin up a Redshift cluster to get Redshift spectrum to work."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Glue to crawl the various data sources and store the resultant metadata in the AWS Glue Data Catalog. Utilize AWS Glue jobs to transform the JSON data to .csv format and query the resultant data in .csv format using Amazon Athena</strong> - Using AWS Glue jobs to transform the JSON data to .csv format represents unnecessary work to build a solution, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Glue to crawl the various data sources and store the resultant metadata in the AWS Glue Data Catalog. Utilize AWS Glue jobs to transform both the JSON and .csv format data to parquet format and query the resultant data using Amazon Athena</strong> - Using AWS Glue jobs to transform both the JSON and .csv format data to parquet format represents unnecessary work to build a solution, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html",
      "https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html",
      "https://aws.amazon.com/about-aws/whats-new/2024/04/amazon-athena-federated-query-pass-through/"
    ]
  },
  {
    "id": 9,
    "question": "<p>A media company has around 8 TB of data on its on-premises data center that is stored in the form of video files, text files, image files, and multiple other formats. The company wants to move this data to Amazon S3 buckets. Approximately 2% of the data changes every day and needs to be updated to Amazon S3. The company wants to automate the entire process to run on a schedule.</p>\n\n<p>Which AWS service is best suited to address this requirement in the MOST operationally efficient manner?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Glue</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon S3 Transfer Acceleration</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Global Accelerator</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS DataSync</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS DataSync</strong></p>\n\n<p>AWS DataSync is a secure, online service that automates and accelerates moving data between on-premises and AWS Storage services. DataSync can copy data between Network File System (NFS) shares, Server Message Block (SMB) shares, Hadoop Distributed File Systems (HDFS), self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, Amazon FSx for Windows File Server file systems, Amazon FSx for Lustre file systems, Amazon FSz for OpenZFS file systems, and Amazon FSx for NetApp ONTAP file systems.</p>\n\n<p>You can use AWS DataSync to migrate data located on-premises, at the edge, or in other clouds to Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon FSx for Lustre, Amazon FSx for OpenZFS, and Amazon FSx for NetApp ONTAP. You can use AWS DataSync for ongoing transfers from on-premises systems into or out of AWS for processing. DataSync can help speed up your critical hybrid cloud storage workflows in industries that need to move active files into AWS quickly. This includes machine learning in life sciences, video production in media and entertainment, big data analytics in financial services, and seismic research in oil and gas.</p>\n\n<p>Transfer data between on-premises and AWS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q13-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q13-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/datasync/\">https://aws.amazon.com/datasync/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 Transfer Acceleration</strong> - Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. We need a migration service and not a data access optimizer.</p>\n\n<p><strong>AWS Global Accelerator</strong> - AWS Global Accelerator is a networking service that helps you improve the availability, performance, and security of your public applications. Global Accelerator provides two global static public IPs that act as a fixed entry point to your application endpoints, such as Application Load Balancers, Network Load Balancers, Amazon Elastic Compute Cloud (EC2) instances, and elastic IPs. It cannot be used as a data migration service for the given use case.</p>\n\n<p><strong>AWS Glue</strong> - Using AWS Glue to read data from on-premises servers is an operationally expensive job requiring several AWS resources like VPN or AWS Direct Connect connection, JDBC connection, configuring elastic network interfaces (ENIs) and their security groups, and several other configuration steps. This option is not the best fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html\">https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/\">https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/</a></p>\n\n<p><a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS DataSync</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS DataSync is a secure, online service that automates and accelerates moving data between on-premises and AWS Storage services. DataSync can copy data between Network File System (NFS) shares, Server Message Block (SMB) shares, Hadoop Distributed File Systems (HDFS), self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, Amazon FSx for Windows File Server file systems, Amazon FSx for Lustre file systems, Amazon FSz for OpenZFS file systems, and Amazon FSx for NetApp ONTAP file systems."
      },
      {
        "answer": "",
        "explanation": "You can use AWS DataSync to migrate data located on-premises, at the edge, or in other clouds to Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon FSx for Lustre, Amazon FSx for OpenZFS, and Amazon FSx for NetApp ONTAP. You can use AWS DataSync for ongoing transfers from on-premises systems into or out of AWS for processing. DataSync can help speed up your critical hybrid cloud storage workflows in industries that need to move active files into AWS quickly. This includes machine learning in life sciences, video production in media and entertainment, big data analytics in financial services, and seismic research in oil and gas."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q13-i1.jpg",
        "answer": "",
        "explanation": "Transfer data between on-premises and AWS:"
      },
      {
        "link": "https://aws.amazon.com/datasync/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Transfer Acceleration</strong> - Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. We need a migration service and not a data access optimizer."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Global Accelerator</strong> - AWS Global Accelerator is a networking service that helps you improve the availability, performance, and security of your public applications. Global Accelerator provides two global static public IPs that act as a fixed entry point to your application endpoints, such as Application Load Balancers, Network Load Balancers, Amazon Elastic Compute Cloud (EC2) instances, and elastic IPs. It cannot be used as a data migration service for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Glue</strong> - Using AWS Glue to read data from on-premises servers is an operationally expensive job requiring several AWS resources like VPN or AWS Direct Connect connection, JDBC connection, configuring elastic network interfaces (ENIs) and their security groups, and several other configuration steps. This option is not the best fit for the given use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/datasync/",
      "https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html",
      "https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/",
      "https://aws.amazon.com/global-accelerator/"
    ]
  },
  {
    "id": 10,
    "question": "<p>The HR department at a company has hired a data engineering team to develop a dashboard with data visualizations that will allow stakeholders to see the historical hiring patterns of the employees. Access to all dashboards should be through Microsoft Active Directory which should cater to the company's security policy of encrypting data-in-transit and at-rest.</p>\n\n<p>Which option would you identify as the right solution that satisfies the criteria given by the company?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon QuickSight Enterprise edition configured to perform identity federation using SAML 2.0 along with the default encryption settings</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon QuickSight Standard edition configured to perform identity federation using SAML 2.0 along with the default encryption settings</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon QuckSight Standard edition using AD Connector to authenticate using Active Directory. Configure QuickSight to use customer-provided keys imported into AWS KMS</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon QuickSight Enterprise edition using AD Connector to authenticate using Active Directory. Configure Amazon QuickSight to use customer-provided keys imported into AWS KMS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon QuickSight Enterprise edition configured to perform identity federation using SAML 2.0 along with the default encryption settings</strong></p>\n\n<p>Amazon QuickSight Enterprise edition integrates with your existing directories, using either Microsoft Active Directory or single sign-on (SSO) using Security Assertion Markup Language (SAML).</p>\n\n<p>Amazon QuickSight supports identity federation in both Standard and Enterprise editions. When you use federated identities, you can manage users with your enterprise identity provider (IdP) and use AWS Identity and Access Management (IAM) to authenticate users when they sign in to Amazon QuickSight.</p>\n\n<p>You can use a third-party identity provider that supports Security Assertion Markup Language 2.0 (SAML 2.0) to provide a simple onboarding flow for your Amazon QuickSight users. Such identity providers include Microsoft Active Directory Federation Services, Okta, and Ping One Federation Server.</p>\n\n<p>With identity federation, your users get one-click access to their Amazon QuickSight applications using their existing identity credentials. You also have the security benefit of identity authentication by your identity provider. You can control which users have access to Amazon QuickSight using your existing identity provider.</p>\n\n<p>Enterprise edition additionally offers encryption at rest and Microsoft Active Directory integration. In the Enterprise Edition, you select a Microsoft Active Directory directory in AWS Directory Service. You use that active directory to identify and manage your Amazon QuickSight users and administrators</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon QuckSight Standard edition using AD Connector to authenticate using Active Directory. Configure QuickSight to use customer-provided keys imported into AWS KMS</strong></p>\n\n<p><strong>Use Amazon QuickSight Standard edition configured to perform identity federation using SAML 2.0 along with the default encryption settings</strong></p>\n\n<p>Amazon QuickSight Standard edition is not integrated with Microsoft Active Directory. Also, the encryption at rest feature is not available for the standard edition. Therefore, both these options are incorrect.</p>\n\n<p><strong>Use Amazon QuickSight Enterprise edition using AD Connector to authenticate using Active Directory. Configure Amazon QuickSight to use customer-provided keys imported into AWS KMS</strong> - This option is incorrect since all keys associated with Amazon QuickSight are managed by AWS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/editions.html\">https://docs.aws.amazon.com/quicksight/latest/user/editions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers-setting-up-saml.html#external-identity-providers-config-idp\">https://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers-setting-up-saml.html#external-identity-providers-config-idp</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html\">https://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers.html\">https://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon QuickSight Enterprise edition configured to perform identity federation using SAML 2.0 along with the default encryption settings</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon QuickSight Enterprise edition integrates with your existing directories, using either Microsoft Active Directory or single sign-on (SSO) using Security Assertion Markup Language (SAML)."
      },
      {
        "answer": "",
        "explanation": "Amazon QuickSight supports identity federation in both Standard and Enterprise editions. When you use federated identities, you can manage users with your enterprise identity provider (IdP) and use AWS Identity and Access Management (IAM) to authenticate users when they sign in to Amazon QuickSight."
      },
      {
        "answer": "",
        "explanation": "You can use a third-party identity provider that supports Security Assertion Markup Language 2.0 (SAML 2.0) to provide a simple onboarding flow for your Amazon QuickSight users. Such identity providers include Microsoft Active Directory Federation Services, Okta, and Ping One Federation Server."
      },
      {
        "answer": "",
        "explanation": "With identity federation, your users get one-click access to their Amazon QuickSight applications using their existing identity credentials. You also have the security benefit of identity authentication by your identity provider. You can control which users have access to Amazon QuickSight using your existing identity provider."
      },
      {
        "answer": "",
        "explanation": "Enterprise edition additionally offers encryption at rest and Microsoft Active Directory integration. In the Enterprise Edition, you select a Microsoft Active Directory directory in AWS Directory Service. You use that active directory to identify and manage your Amazon QuickSight users and administrators"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon QuckSight Standard edition using AD Connector to authenticate using Active Directory. Configure QuickSight to use customer-provided keys imported into AWS KMS</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon QuickSight Standard edition configured to perform identity federation using SAML 2.0 along with the default encryption settings</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon QuickSight Standard edition is not integrated with Microsoft Active Directory. Also, the encryption at rest feature is not available for the standard edition. Therefore, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon QuickSight Enterprise edition using AD Connector to authenticate using Active Directory. Configure Amazon QuickSight to use customer-provided keys imported into AWS KMS</strong> - This option is incorrect since all keys associated with Amazon QuickSight are managed by AWS."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/quicksight/latest/user/editions.html",
      "https://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers-setting-up-saml.html#external-identity-providers-config-idp",
      "https://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html",
      "https://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers.html"
    ]
  },
  {
    "id": 11,
    "question": "<p>Which of the following AWS services provides a highly available and fault-tolerant solution to capture the clickstream events from the source and then provide a concurrent feed of the data stream to the downstream applications?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Kinesis Data Firehose</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon Simple Queue Service (Amazon SQS)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Kinesis Data Analytics</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Kinesis Data Streams</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</p>\n\n<p>Amazon Kinesis Data Streams is recommended when you need the ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another application that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.</p>\n\n<p>KDS provides the ability for multiple applications to consume the same stream concurrently\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q52-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q52-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.</p>\n\n<p><strong>Amazon Kinesis Data Analytics</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: set up your streaming data sources, write your queries or streaming applications, and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.</p>\n\n<p><strong>Amazon Simple Queue Service (Amazon SQS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers at the same time, therefore this option is incorrect.</p>\n\n<p>Exam alert:</p>\n\n<p>You should remember that Amazon Kinesis Data Firehose is used to load streaming data into data stores (Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk) whereas Kinesis Data Streams provides support for real-time processing of streaming data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple downstream Amazon Kinesis Applications.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Streams</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering)."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams is recommended when you need the ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another application that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q52-i1.jpg",
        "answer": "",
        "explanation": "KDS provides the ability for multiple applications to consume the same stream concurrently"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/faqs/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Analytics</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: set up your streaming data sources, write your queries or streaming applications, and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (Amazon SQS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers at the same time, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "Exam alert:"
      },
      {
        "answer": "",
        "explanation": "You should remember that Amazon Kinesis Data Firehose is used to load streaming data into data stores (Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk) whereas Kinesis Data Streams provides support for real-time processing of streaming data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple downstream Amazon Kinesis Applications."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/faqs/",
      "https://aws.amazon.com/kinesis/data-firehose/faqs/",
      "https://aws.amazon.com/kinesis/data-analytics/faqs/"
    ]
  },
  {
    "id": 12,
    "question": "<p>An Internet-of-Things (IoT) solutions company wants to migrate its on-premises infrastructure into the AWS Cloud. The data engineering team is looking for a fully managed NoSQL persistent data store with in-memory caching to maintain low latency which is critical for real-time data processing. The team is well versed with the access patterns for the underlying database and expects the number of concurrent users to touch up to a million so the database should be able to scale elastically.</p>\n\n<p>Which of the following AWS services would you recommend for this use-case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>RDS</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>DynamoDB</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>DocumentDB</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>ElastiCache</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. Companies use caching through DynamoDB Accelerator (DAX) when they have high read volumes or need submillisecond read latency.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DocumentDB</strong> - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. Although DocumentDB is fully managed, it does not have an in-memory caching layer.</p>\n\n<p><strong>ElastiCache</strong> - Amazon ElastiCache allows you to set up popular open-source compatible in-memory data stores in the cloud. You can build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores such as Redis and Memcached. Elasticache is used as a caching layer. It's not a fully managed NoSQL database.</p>\n\n<p><strong>RDS</strong> - RDS makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. It's not a NoSQL database.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>DynamoDB</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. Companies use caching through DynamoDB Accelerator (DAX) when they have high read volumes or need submillisecond read latency."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>DocumentDB</strong> - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. Although DocumentDB is fully managed, it does not have an in-memory caching layer."
      },
      {
        "answer": "",
        "explanation": "<strong>ElastiCache</strong> - Amazon ElastiCache allows you to set up popular open-source compatible in-memory data stores in the cloud. You can build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores such as Redis and Memcached. Elasticache is used as a caching layer. It's not a fully managed NoSQL database."
      },
      {
        "answer": "",
        "explanation": "<strong>RDS</strong> - RDS makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. It's not a NoSQL database."
      }
    ],
    "references": [
      "https://aws.amazon.com/dynamodb/"
    ]
  },
  {
    "id": 13,
    "question": "<p>A company wants to adopt a hybrid cloud infrastructure where it uses some AWS services such as Amazon S3 alongside its on-premises data center. The company wants a dedicated private connection between the on-premise data center and AWS. In case of failures though, the company needs to guarantee uptime and is willing to use the public internet for an encrypted connection.</p>\n\n<p>What do you recommend? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Site-to-Site VPN as a primary connection</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Site-to-Site VPN as a backup connection</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Internet Gateway as a backup connection</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Direct Connect connection as a backup connection</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use AWS Direct Connect connection as a primary connection</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use AWS Direct Connect connection as a primary connection</strong></p>\n\n<p>AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC.</p>\n\n<p><strong>Use AWS Site-to-Site VPN as a backup connection</strong></p>\n\n<p>AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.</p>\n\n<p>AWS Direct Connect as a primary connection guarantees great performance and security (as the connection is private). Using Direct Connect as a backup solution would work but probably carries a risk it would fail as well. As we don't mind going over the public internet (which is reliable, but less secure as connections are going over the public route), we should use a Site to Site VPN which offers an encrypted connection to handle failover scenarios.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Internet Gateway as a backup connection</strong> - An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that can help EC2 instances connect with the internet. Internet Gateway cannot be used to connect on-premises data centers to AWS Cloud.</p>\n\n<p><strong>Use AWS Site-to-Site VPN as a primary connection</strong> - AWS Site-to-Site VPN as a primary connection is not advisable since the use of internet-based connection is only for failover scenarios, as stated in the problem.</p>\n\n<p><strong>Use AWS Direct Connect connection as a backup connection</strong> - AWS Direct Connect connection is a highly secure, physical connection. It is also a costly solution and hence does not make much sense to set up the connection and keep it only as a backup.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/directconnect/\">https://aws.amazon.com/directconnect/</a></p>\n\n<p><a href=\"https://aws.amazon.com/vpn/\">https://aws.amazon.com/vpn/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Direct Connect connection as a primary connection</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Site-to-Site VPN as a backup connection</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity."
      },
      {
        "answer": "",
        "explanation": "AWS Direct Connect as a primary connection guarantees great performance and security (as the connection is private). Using Direct Connect as a backup solution would work but probably carries a risk it would fail as well. As we don't mind going over the public internet (which is reliable, but less secure as connections are going over the public route), we should use a Site to Site VPN which offers an encrypted connection to handle failover scenarios."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Internet Gateway as a backup connection</strong> - An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that can help EC2 instances connect with the internet. Internet Gateway cannot be used to connect on-premises data centers to AWS Cloud."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Site-to-Site VPN as a primary connection</strong> - AWS Site-to-Site VPN as a primary connection is not advisable since the use of internet-based connection is only for failover scenarios, as stated in the problem."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Direct Connect connection as a backup connection</strong> - AWS Direct Connect connection is a highly secure, physical connection. It is also a costly solution and hence does not make much sense to set up the connection and keep it only as a backup."
      }
    ],
    "references": [
      "https://aws.amazon.com/directconnect/",
      "https://aws.amazon.com/vpn/"
    ]
  },
  {
    "id": 14,
    "question": "<p>A company runs its database on Amazon RDS for MySQL. The application using the RDS is facing performance lag and initial research has confirmed a high CPU utilization on the RDS instance.</p>\n\n<p>As a data engineer, how will you troubleshoot this issue?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the Performance Insights feature of RDS to identify the queries that are causing high CPU usage. Optimize the queries to reduce CPU usage</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Improve performance of Amazon RDS by using RDS Optimized Writes for MySQL</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Reconfigure Amazon RDS storage to <code>Provisioned IOPS</code> storage from the standard <code>General Purpose</code> storage for a fast, predictable, and consistent I/O performance</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Check the <code>long_query_time</code> parameter to know the top 5 queries that took the longest time. Optimize the queries to reduce CPU usage</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the Performance Insights feature of RDS to identify the queries that are causing high CPU usage. Optimize the queries to reduce CPU usage</strong></p>\n\n<p>Amazon RDS Performance Insights is a database performance tuning and monitoring feature that helps you quickly assess the load on your database, and determine when and where to take action. Performance Insights allows non-experts to detect performance problems with an easy-to-understand dashboard that visualizes database load.</p>\n\n<p>Performance Insights uses lightweight data collection methods that don’t impact the performance of your applications and make it easy to see which SQL statements are causing the load, and why. It requires no configuration or maintenance and is currently available for Amazon Aurora (PostgreSQL- and MySQL-compatible editions), Amazon RDS for PostgreSQL, MySQL, MariaDB, SQL Server, and Oracle.</p>\n\n<p>Monitoring with Performance Insights:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q8-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q8-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/rds/performance-insights/\">https://aws.amazon.com/rds/performance-insights/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Reconfigure Amazon RDS storage to <code>Provisioned IOPS</code> storage from the standard <code>General Purpose</code> storage for a fast, predictable, and consistent I/O performance</strong> - Changing storage type consumes I/O capacity and might reduce DB instance performance while in progress. Also, there is a downtime if the instance is a single AZ, without offering a CPU usage advantage. Changing the storage type does not point toward the root cause behind the issue, so this option is incorrect.</p>\n\n<p><strong>Improve performance of Amazon RDS by using RDS Optimized Writes for MySQL</strong> - You can improve the performance of write transactions with RDS Optimized Writes for MySQL. When your RDS for MySQL database uses RDS Optimized Writes, it can achieve up to two times higher write transaction throughput. This is irrelevant to the given use case since there is no reference of the database usage being only write-heavy.</p>\n\n<p><strong>Check the <code>long_query_time</code> parameter to know the top 5 queries that took the longest time. Optimize the queries to reduce CPU usage</strong> - This statement is incorrect. <code>long_query_time</code> parameter value is set by you. You analyze the MySQL Slow Query Logs to find queries that take longer to run than the seconds that you set for <code>long_query_time</code>.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/rds-instance-high-cpu\">https://repost.aws/knowledge-center/rds-instance-high-cpu</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/\">https://aws.amazon.com/rds/features/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-optimized-writes.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-optimized-writes.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the Performance Insights feature of RDS to identify the queries that are causing high CPU usage. Optimize the queries to reduce CPU usage</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon RDS Performance Insights is a database performance tuning and monitoring feature that helps you quickly assess the load on your database, and determine when and where to take action. Performance Insights allows non-experts to detect performance problems with an easy-to-understand dashboard that visualizes database load."
      },
      {
        "answer": "",
        "explanation": "Performance Insights uses lightweight data collection methods that don’t impact the performance of your applications and make it easy to see which SQL statements are causing the load, and why. It requires no configuration or maintenance and is currently available for Amazon Aurora (PostgreSQL- and MySQL-compatible editions), Amazon RDS for PostgreSQL, MySQL, MariaDB, SQL Server, and Oracle."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q8-i1.jpg",
        "answer": "",
        "explanation": "Monitoring with Performance Insights:"
      },
      {
        "link": "https://aws.amazon.com/rds/performance-insights/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Reconfigure Amazon RDS storage to <code>Provisioned IOPS</code> storage from the standard <code>General Purpose</code> storage for a fast, predictable, and consistent I/O performance</strong> - Changing storage type consumes I/O capacity and might reduce DB instance performance while in progress. Also, there is a downtime if the instance is a single AZ, without offering a CPU usage advantage. Changing the storage type does not point toward the root cause behind the issue, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Improve performance of Amazon RDS by using RDS Optimized Writes for MySQL</strong> - You can improve the performance of write transactions with RDS Optimized Writes for MySQL. When your RDS for MySQL database uses RDS Optimized Writes, it can achieve up to two times higher write transaction throughput. This is irrelevant to the given use case since there is no reference of the database usage being only write-heavy."
      },
      {
        "answer": "",
        "explanation": "<strong>Check the <code>long_query_time</code> parameter to know the top 5 queries that took the longest time. Optimize the queries to reduce CPU usage</strong> - This statement is incorrect. <code>long_query_time</code> parameter value is set by you. You analyze the MySQL Slow Query Logs to find queries that take longer to run than the seconds that you set for <code>long_query_time</code>."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/performance-insights/",
      "https://repost.aws/knowledge-center/rds-instance-high-cpu",
      "https://aws.amazon.com/rds/features/",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-optimized-writes.html"
    ]
  },
  {
    "id": 15,
    "question": "<p>A trading firm wants to migrate its on-premises Apache Hadoop cluster to an Amazon Elastic Map Reduce (EMR) cluster. The cluster is only operational during normal business hours. The EMR cluster must be highly available to prevent intraday cluster failures. The data must survive when the cluster is terminated at the end of each business day.</p>\n\n<p>Which of the following options would you recommend to address these requirements? (Select three)</p>",
    "corrects": [
      3,
      5,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Hadoop Distributed File System (HDFS) for storage</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up multiple master nodes in multiple Availability Zones</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up multiple master nodes in a single Availability Zone</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up MySQL database on the master node as the metastore for Apache Hive</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Set up AWS Glue Data Catalog as the metastore for Apache Hive</p>",
        "correct": true
      },
      {
        "id": 6,
        "answer": "<p>Use EMR File System (EMRFS) for storage</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use EMR File System (EMRFS) for storage</strong></p>\n\n<p>The EMR File System (EMRFS) is an implementation of HDFS that all Amazon EMR clusters use for reading and writing regular files from Amazon EMR directly to Amazon S3. EMRFS provides the convenience of storing persistent data in Amazon S3 for use with Hadoop while also providing features like data encryption. This ensures that the data persists even when the cluster is terminated at the end of each business day.</p>\n\n<p><strong>Set up multiple master nodes in a single Availability Zone</strong></p>\n\n<p>An EMR cluster with multiple master nodes ensures that the master node is no longer a single point of failure. If one of the master nodes fails, the cluster uses the other two master nodes and runs without interruption. This would ensure high availability to prevent intraday cluster failures.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q24-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q24-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha.html</a><p></p>\n\n<p><strong>Set up AWS Glue Data Catalog as the metastore for Apache Hive</strong> - The AWS Glue Data Catalog is an index to the location, schema, and runtime metrics of your data. You use the information in the Data Catalog to create and monitor your ETL jobs. The catalog would persist even when the cluster is terminated at the end of each business day.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q24-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q24-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html\">https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Hadoop Distributed File System (HDFS) for storage</strong> - Hadoop Distributed File System (HDFS) is a distributed, scalable file system for Hadoop. HDFS distributes the data it stores across instances in the cluster, storing multiple copies of data on different instances to ensure that no data is lost if an individual instance fails. HDFS is ephemeral storage that is reclaimed when you terminate a cluster. So this option is incorrect for the given use case.</p>\n\n<p><strong>Set up MySQL database on the master node as the metastore for Apache Hive</strong> - Since the cluster is terminated at the end of each business day, the MySQL database running on the master node would also terminate, hence this option is incorrect.</p>\n\n<p><strong>Set up multiple master nodes in multiple Availability Zones</strong> - An EMR cluster can only reside in a single Availability Zone, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-fs.html\">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-fs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-arch.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-arch.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html\">https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use EMR File System (EMRFS) for storage</strong>"
      },
      {
        "answer": "",
        "explanation": "The EMR File System (EMRFS) is an implementation of HDFS that all Amazon EMR clusters use for reading and writing regular files from Amazon EMR directly to Amazon S3. EMRFS provides the convenience of storing persistent data in Amazon S3 for use with Hadoop while also providing features like data encryption. This ensures that the data persists even when the cluster is terminated at the end of each business day."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up multiple master nodes in a single Availability Zone</strong>"
      },
      {
        "answer": "",
        "explanation": "An EMR cluster with multiple master nodes ensures that the master node is no longer a single point of failure. If one of the master nodes fails, the cluster uses the other two master nodes and runs without interruption. This would ensure high availability to prevent intraday cluster failures."
      },
      {
        "link": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Set up AWS Glue Data Catalog as the metastore for Apache Hive</strong> - The AWS Glue Data Catalog is an index to the location, schema, and runtime metrics of your data. You use the information in the Data Catalog to create and monitor your ETL jobs. The catalog would persist even when the cluster is terminated at the end of each business day."
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Hadoop Distributed File System (HDFS) for storage</strong> - Hadoop Distributed File System (HDFS) is a distributed, scalable file system for Hadoop. HDFS distributes the data it stores across instances in the cluster, storing multiple copies of data on different instances to ensure that no data is lost if an individual instance fails. HDFS is ephemeral storage that is reclaimed when you terminate a cluster. So this option is incorrect for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up MySQL database on the master node as the metastore for Apache Hive</strong> - Since the cluster is terminated at the end of each business day, the MySQL database running on the master node would also terminate, hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up multiple master nodes in multiple Availability Zones</strong> - An EMR cluster can only reside in a single Availability Zone, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha.html",
      "https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html",
      "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-fs.html",
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-arch.html"
    ]
  },
  {
    "id": 16,
    "question": "<p>A healthcare company has recently migrated to Amazon Redshift. The technology team at the company is now working on the Disaster Recovery (DR) plans for the Redshift cluster deployed in the eu-west-1 Region. The existing cluster is encrypted via AWS KMS and the team wants to copy the Redshift snapshots to another Region to meet the DR requirements.</p>\n\n<p>Which of the following solutions would you recommend to meet the given requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region replication in the source Region</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a snapshot copy grant in the source Region for a KMS key in the source Region. Configure Redshift cross-Region snapshots in the destination Region</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region snapshots in the source Region</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an IAM role in the destination Region with access to the KMS key in the source Region. Create a snapshot copy grant in the destination Region for this KMS key in the source Region. Configure Redshift cross-Region snapshots in the source Region</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region snapshots in the source Region</strong></p>\n\n<p>To copy snapshots for AWS KMS–encrypted clusters to another AWS Region, you need to create a grant for Redshift to use a KMS customer master key (CMK) in the destination AWS Region. Then choose that grant when you enable copying of snapshots in the source AWS Region. You cannot use a KMS key from the source Region as AWS KMS keys are specific to an AWS Region.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q23-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q23-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a snapshot copy grant in the source Region for a KMS key in the source Region. Configure Redshift cross-Region snapshots in the destination Region</strong> - As described above, you need to configure the Redshift cross-Region snapshot in the source Region and not the destination Region. Also, the snapshot copy grant must be set up in the destination Region for a KMS key in the destination Region.</p>\n\n<p><strong>Create an IAM role in the destination Region with access to the KMS key in the source Region. Create a snapshot copy grant in the destination Region for this KMS key in the source Region. Configure Redshift cross-Region snapshots in the source Region</strong> - This has been added as a distractor as AWS KMS keys are specific to an AWS Region. You cannot create a snapshot copy grant in the destination Region for a KMS key in the source Region.</p>\n\n<p><strong>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region replication in the source Region</strong> - This has been added as a distractor as there is no such thing as cross-Region replication for Redshift. The concept of cross-Region replication (CRR) applies to Amazon S3.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region snapshots in the source Region</strong>"
      },
      {
        "answer": "",
        "explanation": "To copy snapshots for AWS KMS–encrypted clusters to another AWS Region, you need to create a grant for Redshift to use a KMS customer master key (CMK) in the destination AWS Region. Then choose that grant when you enable copying of snapshots in the source AWS Region. You cannot use a KMS key from the source Region as AWS KMS keys are specific to an AWS Region."
      },
      {
        "link": "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a snapshot copy grant in the source Region for a KMS key in the source Region. Configure Redshift cross-Region snapshots in the destination Region</strong> - As described above, you need to configure the Redshift cross-Region snapshot in the source Region and not the destination Region. Also, the snapshot copy grant must be set up in the destination Region for a KMS key in the destination Region."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an IAM role in the destination Region with access to the KMS key in the source Region. Create a snapshot copy grant in the destination Region for this KMS key in the source Region. Configure Redshift cross-Region snapshots in the source Region</strong> - This has been added as a distractor as AWS KMS keys are specific to an AWS Region. You cannot create a snapshot copy grant in the destination Region for a KMS key in the source Region."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region replication in the source Region</strong> - This has been added as a distractor as there is no such thing as cross-Region replication for Redshift. The concept of cross-Region replication (CRR) applies to Amazon S3."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant"
    ]
  },
  {
    "id": 17,
    "question": "<p>A company is looking to archive the on-premises data into a POSIX-compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year.</p>\n\n<p>Which of the following AWS services would you recommend as the MOST cost-optimal solution?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon S3 Standard</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon EFS Infrequent Access</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon EFS Standard</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon S3 Standard-IA</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EFS Infrequent Access</strong></p>\n\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed, elastic, NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. The EFS IA storage class costs only $0.025/GB-month. To get started with EFS IA, simply enable EFS Lifecycle Management for your file system by selecting a lifecycle policy that matches your needs.</p>\n\n<p>How Amazon EFS Infrequent Access Works:\n<img src=\"https://d1.awsstatic.com/EFS/product-page-diagram-Amazon-EFS-Infrequent-Access-How-It-Works.83f88e30a40c27f38abae1ff157712a336dd1320.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/EFS/product-page-diagram-Amazon-EFS-Infrequent-Access-How-It-Works.83f88e30a40c27f38abae1ff157712a336dd1320.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/efs/features/infrequent-access/\">https://aws.amazon.com/efs/features/infrequent-access/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EFS Standard</strong> - Amazon EFS Infrequent Access is more cost-effective than EFS Standard for the given use case, therefore this option is incorrect.</p>\n\n<p><strong>Amazon S3 Standard</strong></p>\n\n<p><strong>Amazon S3 Standard-IA</strong></p>\n\n<p>Both these options are object-based storage, whereas the given use case requires a POSIX-compliant file storage solution. Hence these two options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/efs/features/infrequent-access/\">https://aws.amazon.com/efs/features/infrequent-access/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EFS Infrequent Access</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed, elastic, NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. The EFS IA storage class costs only $0.025/GB-month. To get started with EFS IA, simply enable EFS Lifecycle Management for your file system by selecting a lifecycle policy that matches your needs."
      },
      {
        "image": "https://d1.awsstatic.com/EFS/product-page-diagram-Amazon-EFS-Infrequent-Access-How-It-Works.83f88e30a40c27f38abae1ff157712a336dd1320.png",
        "answer": "",
        "explanation": "How Amazon EFS Infrequent Access Works:"
      },
      {
        "link": "https://aws.amazon.com/efs/features/infrequent-access/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EFS Standard</strong> - Amazon EFS Infrequent Access is more cost-effective than EFS Standard for the given use case, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Standard</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Standard-IA</strong>"
      },
      {
        "answer": "",
        "explanation": "Both these options are object-based storage, whereas the given use case requires a POSIX-compliant file storage solution. Hence these two options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/efs/features/infrequent-access/"
    ]
  },
  {
    "id": 18,
    "question": "<p>An ad-tech company runs a leading ad targeting platform that captures various kinds of marketing data such as user profiles, user events, clicks, and visited links. The company is looking to migrate its IT infrastructure from the on-premises data center to AWS Cloud. The technical requirements for the advertising platform mandate a high request rate (millions of requests per second), low predictable latency, and high reliability. The company also needs to deploy this ad-targeting platform in more than one AWS Region. The maximum size of an event is 200 KB and the average size is 10 KB. The structure of the incoming data varies depending on the event.</p>\n\n<p>Which of the following database solutions would you recommend to address these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>DynamoDB Global Tables</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon RDS</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>DocumentDB</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Redshift</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB Global Tables</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>Global Tables builds upon DynamoDB’s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read, and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions.</p>\n\n<p>DynamoDB Global Tables Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q39-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q39-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a><p></p>\n\n<p>For the given use case, DynamoDB is the right choice as it supports a high request rate (millions of requests per second) with low predictable latency and high reliability. DynamoDB supports a maximum item size of 400 KB which meets the requirement for the maximum size of an event is 200 KB and the average size is 10 KB. As DynamoDB is a NoSQL database, it can be used to store the input data even with the variable structure of the metadata for different events. As the company wants to deploy this ad-targeting platform in more than one AWS Region, therefore DynamoDB Global Tables fits the overall requirement.</p>\n\n<p>A deep dive on ad-tech use cases for DynamoDB:\n<a href=\"https://aws.amazon.com/blogs/database/amazon-dynamodb-ad-tech-use-cases-and-design-patterns/\">https://aws.amazon.com/blogs/database/amazon-dynamodb-ad-tech-use-cases-and-design-patterns/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DocumentDB</strong> - Amazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud. Although DocumentDB is a NoSQL database just like DynamoDB, it can only be deployed to a specific AWS Region. In addition, an Amazon DocumentDB Global Cluster also consists of one primary region and up to five read-only secondary regions. Therefore it does not meet all the requirements for the given use-case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q39-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q39-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/documentdb/latest/developerguide/replication.html\">https://docs.aws.amazon.com/documentdb/latest/developerguide/replication.html</a><p></p>\n\n<p><strong>Amazon RDS</strong> - Amazon RDS is not the right fit as it does not support a high request rate (millions of requests per second) with low predictable latency.</p>\n\n<p><strong>Amazon Redshift</strong> - Amazon Redshift is an industry-leading data warehouse designed to handle OLAP workloads. Redshift is not the right fit to meet the high request rates and low latency requirements mandated by the advertising platform. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/amazon-dynamodb-ad-tech-use-cases-and-design-patterns/\">https://aws.amazon.com/blogs/database/amazon-dynamodb-ad-tech-use-cases-and-design-patterns/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/documentdb/latest/developerguide/replication.html\">https://docs.aws.amazon.com/documentdb/latest/developerguide/replication.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>DynamoDB Global Tables</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications."
      },
      {
        "answer": "",
        "explanation": "Global Tables builds upon DynamoDB’s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read, and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q39-i1.jpg",
        "answer": "",
        "explanation": "DynamoDB Global Tables Overview:"
      },
      {
        "link": "https://aws.amazon.com/dynamodb/global-tables/"
      },
      {
        "answer": "",
        "explanation": "For the given use case, DynamoDB is the right choice as it supports a high request rate (millions of requests per second) with low predictable latency and high reliability. DynamoDB supports a maximum item size of 400 KB which meets the requirement for the maximum size of an event is 200 KB and the average size is 10 KB. As DynamoDB is a NoSQL database, it can be used to store the input data even with the variable structure of the metadata for different events. As the company wants to deploy this ad-targeting platform in more than one AWS Region, therefore DynamoDB Global Tables fits the overall requirement."
      },
      {
        "link": "https://aws.amazon.com/blogs/database/amazon-dynamodb-ad-tech-use-cases-and-design-patterns/",
        "answer": "",
        "explanation": "A deep dive on ad-tech use cases for DynamoDB:\n<a href=\"https://aws.amazon.com/blogs/database/amazon-dynamodb-ad-tech-use-cases-and-design-patterns/\">https://aws.amazon.com/blogs/database/amazon-dynamodb-ad-tech-use-cases-and-design-patterns/</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>DocumentDB</strong> - Amazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud. Although DocumentDB is a NoSQL database just like DynamoDB, it can only be deployed to a specific AWS Region. In addition, an Amazon DocumentDB Global Cluster also consists of one primary region and up to five read-only secondary regions. Therefore it does not meet all the requirements for the given use-case."
      },
      {
        "link": "https://docs.aws.amazon.com/documentdb/latest/developerguide/replication.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon RDS</strong> - Amazon RDS is not the right fit as it does not support a high request rate (millions of requests per second) with low predictable latency."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Redshift</strong> - Amazon Redshift is an industry-leading data warehouse designed to handle OLAP workloads. Redshift is not the right fit to meet the high request rates and low latency requirements mandated by the advertising platform. So, this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/dynamodb/global-tables/",
      "https://aws.amazon.com/blogs/database/amazon-dynamodb-ad-tech-use-cases-and-design-patterns/",
      "https://docs.aws.amazon.com/documentdb/latest/developerguide/replication.html"
    ]
  },
  {
    "id": 19,
    "question": "<p>A company operates thousands of hardware devices like switches, routers, cables, etc. The real-time status data for these devices must be fed into a communications application for notifications. Simultaneously, another analytics application needs to read the same real-time status data and analyze all the connecting lines that may go down because of any device failures.</p>\n\n<p>Which of the following solutions would you suggest, so that both the applications can consume the real-time status data concurrently?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon Simple Notification Service (SNS)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Kinesis Data Streams</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon Simple Queue Service (SQS) with Amazon AppFlow</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</p>\n\n<p>AWS recommends Amazon Kinesis Data Streams for use cases with requirements that are similar to the following:</p>\n\n<ol>\n<li>Routing related records to the same record processor (as in streaming MapReduce). For example, counting and aggregation are simpler when all records for a given key are routed to the same record processor.</li>\n<li>Ordering of records. For example, you want to transfer log data from the application host to the processing/archival host while maintaining the order of log statements.</li>\n<li>Ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.</li>\n<li>Ability to consume records in the same order a few hours later. For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for up to 365 days, you can run the audit application up to 365 days behind the billing application.</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Amazon SNS is a notification service and cannot be used for real-time processing of data.</p>\n\n<p><strong>Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. Since multiple applications need to consume the same data stream concurrently, Kinesis is a better choice when compared to the combination of SQS with SNS.</p>\n\n<p><strong>Amazon Simple Queue Service (SQS) with Amazon AppFlow</strong> - Amazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications and AWS.</p>\n\n<p>Amazon AppFlow is not the right fit for the given use case. Hence, this option is an incorrect answer.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Streams</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering)."
      },
      {
        "answer": "",
        "explanation": "AWS recommends Amazon Kinesis Data Streams for use cases with requirements that are similar to the following:"
      },
      {}
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Amazon SNS is a notification service and cannot be used for real-time processing of data."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. Since multiple applications need to consume the same data stream concurrently, Kinesis is a better choice when compared to the combination of SQS with SNS."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (SQS) with Amazon AppFlow</strong> - Amazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications and AWS."
      },
      {
        "answer": "",
        "explanation": "Amazon AppFlow is not the right fit for the given use case. Hence, this option is an incorrect answer."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 20,
    "question": "<p>A data engineering team wants to orchestrate multiple Amazon ECS task types running on Amazon EC2 instances that are part of the Amazon ECS cluster. The output and state data for all tasks need to be stored. The amount of data output by each task is approximately 20 megabytes and there could be hundreds of tasks running at a time. As old outputs are archived, the storage size is not expected to exceed 1 terabyte.</p>\n\n<p>Which of the following would you recommend as an optimized solution for high-frequency reading and writing?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an Amazon EBS volume mounted to the Amazon ECS cluster instances</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon EFS with Provisioned Throughput mode</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon EFS with Bursting Throughput mode</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon DynamoDB table that is accessible by all ECS cluster instances</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p>Amazon EFS file systems are distributed across an unconstrained number of storage servers. This distributed data storage design enables file systems to grow elastically to petabyte scale. It also enables massively parallel access from compute instances, including Amazon EC2, Amazon ECS, and AWS Lambda, to your data.</p>\n\n<p><strong>Use Amazon EFS with Provisioned Throughput mode</strong></p>\n\n<p>Provisioned Throughput mode is available for applications with high throughput-to-storage (MiB/s per TiB) ratios, or with requirements greater than those allowed by the Bursting Throughput mode. For example, say you're using Amazon EFS for development tools, web serving, or content management applications where the amount of data in your file system is low relative to throughput demands. Your file system can now get the high levels of throughput your applications require without having to pad your file system.</p>\n\n<p>If your file system is in the Provisioned Throughput mode, you can increase the Provisioned Throughput of your file system as often as you want. You can decrease your file system throughput in Provisioned Throughput mode as long as it's been more than 24 hours since the last decrease. Additionally, you can change between Provisioned Throughput mode and the default Bursting Throughput mode as long as it’s been more than 24 hours since the last throughput mode change.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q65-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q65-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon EFS with Bursting Throughput mode</strong> - With Bursting Throughput mode, a file system's throughput scales as the amount of data stored in the standard storage class grows. File-based workloads are typically spiky, driving high levels of throughput for short periods of time, and low levels of throughput the rest of the time. To accommodate this, Amazon EFS is designed to burst to high throughput levels for periods of time. By default, AWS recommends that you run your application in the Bursting Throughput mode. But, if you're planning to migrate large amounts of data into your file system, consider switching to Provisioned Throughput mode.</p>\n\n<p>The use-case mentions that the solution should be optimized for high-frequency reading and writing even when the old outputs are archived, therefore Provisioned Throughput mode is a better fit as it guarantees high levels of throughput your applications require without having to pad your file system.</p>\n\n<p><strong>Use an Amazon EBS volume mounted to the Amazon ECS cluster instances</strong> - Amazon EFS has a higher throughput than Amazon EBS. In addition, Amazon EBS can be attached to multiple Amazon EC2 instances when the underlying EBS type is io1/io2 and the instance is of Nitro type. The use case does not provide any such details, so this option is ruled out.</p>\n\n<p><strong>Use Amazon DynamoDB table that is accessible by all ECS cluster instances</strong> - Amazon DynamoDB is not a fit for this scenario as each task output is 20 MB but the storage limit for each item in an Amazon DynamoDB table is 400 KB. You could write custom code to split the task output data into multiple items but it is not an optimal solution compared to using Amazon EFS in Provisioned Throughput mode.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon EFS file systems are distributed across an unconstrained number of storage servers. This distributed data storage design enables file systems to grow elastically to petabyte scale. It also enables massively parallel access from compute instances, including Amazon EC2, Amazon ECS, and AWS Lambda, to your data."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EFS with Provisioned Throughput mode</strong>"
      },
      {
        "answer": "",
        "explanation": "Provisioned Throughput mode is available for applications with high throughput-to-storage (MiB/s per TiB) ratios, or with requirements greater than those allowed by the Bursting Throughput mode. For example, say you're using Amazon EFS for development tools, web serving, or content management applications where the amount of data in your file system is low relative to throughput demands. Your file system can now get the high levels of throughput your applications require without having to pad your file system."
      },
      {
        "answer": "",
        "explanation": "If your file system is in the Provisioned Throughput mode, you can increase the Provisioned Throughput of your file system as often as you want. You can decrease your file system throughput in Provisioned Throughput mode as long as it's been more than 24 hours since the last decrease. Additionally, you can change between Provisioned Throughput mode and the default Bursting Throughput mode as long as it’s been more than 24 hours since the last throughput mode change."
      },
      {
        "link": "https://docs.aws.amazon.com/efs/latest/ug/performance.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EFS with Bursting Throughput mode</strong> - With Bursting Throughput mode, a file system's throughput scales as the amount of data stored in the standard storage class grows. File-based workloads are typically spiky, driving high levels of throughput for short periods of time, and low levels of throughput the rest of the time. To accommodate this, Amazon EFS is designed to burst to high throughput levels for periods of time. By default, AWS recommends that you run your application in the Bursting Throughput mode. But, if you're planning to migrate large amounts of data into your file system, consider switching to Provisioned Throughput mode."
      },
      {
        "answer": "",
        "explanation": "The use-case mentions that the solution should be optimized for high-frequency reading and writing even when the old outputs are archived, therefore Provisioned Throughput mode is a better fit as it guarantees high levels of throughput your applications require without having to pad your file system."
      },
      {
        "answer": "",
        "explanation": "<strong>Use an Amazon EBS volume mounted to the Amazon ECS cluster instances</strong> - Amazon EFS has a higher throughput than Amazon EBS. In addition, Amazon EBS can be attached to multiple Amazon EC2 instances when the underlying EBS type is io1/io2 and the instance is of Nitro type. The use case does not provide any such details, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon DynamoDB table that is accessible by all ECS cluster instances</strong> - Amazon DynamoDB is not a fit for this scenario as each task output is 20 MB but the storage limit for each item in an Amazon DynamoDB table is 400 KB. You could write custom code to split the task output data into multiple items but it is not an optimal solution compared to using Amazon EFS in Provisioned Throughput mode."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/efs/latest/ug/performance.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items"
    ]
  },
  {
    "id": 21,
    "question": "<p>You are using AWS Lambda to implement a batch job for a big data analytics workflow. Based on historical trends, a similar job runs for 30 minutes on average. The AWS Lambda function pulls data from Amazon S3, processes it, and then writes the results back to Amazon S3. When you deployed your AWS Lambda function, you noticed an issue where the AWS Lambda function abruptly failed after 15 minutes of execution.</p>\n\n<p>Which of the following would you identify as the root cause of the issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The AWS Lambda function is running out of memory</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The AWS Lambda function is timing out</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The AWS Lambda function is missing IAM permissions</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The AWS Lambda function chosen runtime is wrong</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>With AWS Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app. AWS Lambda functions can be configured to run up to 15 minutes per execution. You can set the timeout to any value between 1 second and 15 minutes.</p>\n\n<p><strong>The AWS Lambda function is timing out</strong></p>\n\n<p>AWS Lambda functions have a maximum execution time of 15 minutes and are not meant for long-running jobs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The AWS Lambda function is running out of memory</strong> - Memory errors will not result in the abrupt termination of the function with no error message.</p>\n\n<p><strong>The AWS Lambda function chosen runtime is wrong</strong> - AWS Lambda function execution will fail if there is an issue with runtime. So, this is not the issue for the given use case.</p>\n\n<p><strong>The AWS Lambda function is missing IAM permissions</strong> - Without enough permissions, AWS Lambda would not have been able to start its execution at all. So, permissions are not an issue here.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/lambda/faqs/\">https://aws.amazon.com/lambda/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume."
      },
      {
        "answer": "",
        "explanation": "With AWS Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app. AWS Lambda functions can be configured to run up to 15 minutes per execution. You can set the timeout to any value between 1 second and 15 minutes."
      },
      {
        "answer": "",
        "explanation": "<strong>The AWS Lambda function is timing out</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Lambda functions have a maximum execution time of 15 minutes and are not meant for long-running jobs."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The AWS Lambda function is running out of memory</strong> - Memory errors will not result in the abrupt termination of the function with no error message."
      },
      {
        "answer": "",
        "explanation": "<strong>The AWS Lambda function chosen runtime is wrong</strong> - AWS Lambda function execution will fail if there is an issue with runtime. So, this is not the issue for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>The AWS Lambda function is missing IAM permissions</strong> - Without enough permissions, AWS Lambda would not have been able to start its execution at all. So, permissions are not an issue here."
      }
    ],
    "references": [
      "https://aws.amazon.com/lambda/faqs/"
    ]
  },
  {
    "id": 22,
    "question": "<p>The data engineering team at a company is moving the static content from the company's logistics website hosted on Amazon EC2 instances to an Amazon S3 bucket. The team wants to use an Amazon CloudFront distribution to deliver the static content. The security group used by the Amazon EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post-migration to Amazon CloudFront, access to the static content should only be allowed from the aforementioned IP addresses.</p>\n\n<p>Which options would you combine to build a solution to meet these requirements? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure an origin access control (OAC) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAC can read the objects</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure an origin access control (OAC) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAC can read the objects</strong></p>\n\n<p>When you use Amazon CloudFront with an Amazon S3 bucket as the origin, you can configure Amazon CloudFront and Amazon S3 in a way that provides the following benefits:</p>\n\n<p>Restricts access to the Amazon S3 bucket so that it's not publicly accessible</p>\n\n<p>Makes sure that viewers (users) can access the content in the bucket only through the specified Amazon CloudFront distribution—that is, prevents them from accessing the content directly from the bucket, or through an unintended CloudFront distribution.</p>\n\n<p>To do this, configure Amazon CloudFront to send authenticated requests to Amazon S3, and configure Amazon S3 to only allow access to authenticated requests from Amazon CloudFront. Amazon CloudFront provides two ways to send authenticated requests to an Amazon S3 origin via origin access control (OAC).</p>\n\n<p>Exam Alert:</p>\n\n<p>You should note that AWS recommends using OAC because it supports:</p>\n\n<p>All Amazon S3 buckets in all AWS Regions, including opt-in Regions launched after December 2022</p>\n\n<p>Amazon S3 server-side encryption with AWS KMS (SSE-KMS)</p>\n\n<p>Dynamic requests (POST, PUT, etc.) to Amazon S3</p>\n\n<p><strong>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution</strong></p>\n\n<p>AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to your protected web application resources. You can protect the following resource types:</p>\n\n<p>Amazon CloudFront distribution</p>\n\n<p>Amazon API Gateway REST API</p>\n\n<p>Application Load Balancer</p>\n\n<p>AWS AppSync GraphQL API</p>\n\n<p>Amazon Cognito user pool</p>\n\n<p>AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, your protected resource responds to requests either with the requested content, with an HTTP 403 status code (Forbidden), or with a custom response.</p>\n\n<p>If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions via your AWS WAF. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from.</p>\n\n<p>For the given use case, you should add those IP addresses that are allowed in the Amazon EC2 security group into the IP match condition.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy</strong> - You cannot associate an AWS WAF ACL with an Amazon S3 bucket policy.</p>\n\n<p><strong>Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution</strong> - NACL is associated with a subnet within a VPC. Amazon CloudFront delivers your content through a worldwide network of data centers called edge locations. So a NACL cannot be associated with an Amazon CloudFront distribution.</p>\n\n<p><strong>Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution</strong> - A security group acts as a virtual firewall for your Amazon EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. Amazon CloudFront delivers your content through a worldwide network of data centers called edge locations. So a security group cannot be associated with Amazon CloudFront distribution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html\">https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure an origin access control (OAC) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAC can read the objects</strong>"
      },
      {
        "answer": "",
        "explanation": "When you use Amazon CloudFront with an Amazon S3 bucket as the origin, you can configure Amazon CloudFront and Amazon S3 in a way that provides the following benefits:"
      },
      {
        "answer": "",
        "explanation": "Restricts access to the Amazon S3 bucket so that it's not publicly accessible"
      },
      {
        "answer": "",
        "explanation": "Makes sure that viewers (users) can access the content in the bucket only through the specified Amazon CloudFront distribution—that is, prevents them from accessing the content directly from the bucket, or through an unintended CloudFront distribution."
      },
      {
        "answer": "",
        "explanation": "To do this, configure Amazon CloudFront to send authenticated requests to Amazon S3, and configure Amazon S3 to only allow access to authenticated requests from Amazon CloudFront. Amazon CloudFront provides two ways to send authenticated requests to an Amazon S3 origin via origin access control (OAC)."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "answer": "",
        "explanation": "You should note that AWS recommends using OAC because it supports:"
      },
      {
        "answer": "",
        "explanation": "All Amazon S3 buckets in all AWS Regions, including opt-in Regions launched after December 2022"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 server-side encryption with AWS KMS (SSE-KMS)"
      },
      {
        "answer": "",
        "explanation": "Dynamic requests (POST, PUT, etc.) to Amazon S3"
      },
      {
        "answer": "",
        "explanation": "<strong>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to your protected web application resources. You can protect the following resource types:"
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront distribution"
      },
      {
        "answer": "",
        "explanation": "Amazon API Gateway REST API"
      },
      {
        "answer": "",
        "explanation": "Application Load Balancer"
      },
      {
        "answer": "",
        "explanation": "AWS AppSync GraphQL API"
      },
      {
        "answer": "",
        "explanation": "Amazon Cognito user pool"
      },
      {
        "answer": "",
        "explanation": "AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, your protected resource responds to requests either with the requested content, with an HTTP 403 status code (Forbidden), or with a custom response."
      },
      {
        "answer": "",
        "explanation": "If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions via your AWS WAF. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you should add those IP addresses that are allowed in the Amazon EC2 security group into the IP match condition."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy</strong> - You cannot associate an AWS WAF ACL with an Amazon S3 bucket policy."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution</strong> - NACL is associated with a subnet within a VPC. Amazon CloudFront delivers your content through a worldwide network of data centers called edge locations. So a NACL cannot be associated with an Amazon CloudFront distribution."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution</strong> - A security group acts as a virtual firewall for your Amazon EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. Amazon CloudFront delivers your content through a worldwide network of data centers called edge locations. So a security group cannot be associated with Amazon CloudFront distribution."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html",
      "https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html",
      "https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html"
    ]
  },
  {
    "id": 23,
    "question": "<p>A business is moving their data to Amazon Redshift. A core table with billions of rows needs to be moved to Redshift. This table contains certain columns that have sensitive data that can only be accessed by the finance team. Once the data is moved to Redshift, queries will be run on this table by multiple teams.</p>\n\n<p>How will you configure a solution for this requirement such that the columns holding sensitive data are only accessible to members of the finance team?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Grant the finance team (defined as a group) permission to read from the table. Create a second table having data only for columns with non-sensitive data. Grant read-only permissions to the second table for the rest of the users</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Grant all users read-only permissions to the non-sensitive columns. Add the finance team to the administrator group so they have complete access to the table</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Grant the finance group permission to read from the table. Create a view of the new table with only those columns having non-sensitive data. Grant the other users read-only permissions to this view</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Grant the finance team (defined as a group) permission to read from the table. Use the GRANT SQL command to allow read-only access to a subset of columns having non-sensitive data to the other users</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Grant the finance team (defined as a group) permission to read from the table. Use the GRANT SQL command to allow read-only access to a subset of columns having non-sensitive data to the other users</strong></p>\n\n<p>Since March 2020, Amazon Redshift supports column-level access control for data in Redshift. Customers can use column-level GRANT and REVOKE statements to help meet their security and compliance needs.</p>\n\n<p>Redshift's table-level access controls for the data in Redshift are already in use by many customers, but they also want the ability to control access in more detail. You can now control access to columns without having to implement view-based access control or use another system. Column-level access control is available in all Amazon Redshift regions.</p>\n\n<p>GRANT command defines access privileges for a user or user group. Privileges include access options such as being able to read data in tables and views, write data, create tables, and drop tables. Use this command to give specific privileges for a table, database, schema, function, procedure, language, or column.</p>\n\n<p>The syntax for column-level privileges on Amazon Redshift tables and views looks like the below:</p>\n\n<pre><code>GRANT { { SELECT | UPDATE } ( column_name [, ...] ) [, ...] | ALL [ PRIVILEGES ] ( column_name [,...] ) }\n    ON { [ TABLE ] table_name [, ...] }\n    TO { username | GROUP group_name | PUBLIC } [, ...]\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Grant all users read-only permissions to the non-sensitive columns. Add the finance team to the administrator group so they have complete access to the table</strong> - This is a security concern as the finance team do not need access privileges as an administrator.</p>\n\n<p><strong>Grant the finance team (defined as a group) permission to read from the table. Create a second table having data only for columns with non-sensitive data. Grant read-only permissions to the second table for the rest of the users</strong> - This is not an efficient solution as the storage space would be wasted to create the second table. It is better to use column-level access controls on the original table.</p>\n\n<p><strong>Grant the finance group permission to read from the table. Create a view of the new table with only those columns having non-sensitive data. Grant the other users read-only permissions to this view</strong> - AWS recommends that you should use column-level access control, instead of views, to manage access to sensitive columns within a table.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html#r_GRANT-usage-notes-clp\">https://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html#r_GRANT-usage-notes-clp</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/\">https://aws.amazon.com/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Grant the finance team (defined as a group) permission to read from the table. Use the GRANT SQL command to allow read-only access to a subset of columns having non-sensitive data to the other users</strong>"
      },
      {
        "answer": "",
        "explanation": "Since March 2020, Amazon Redshift supports column-level access control for data in Redshift. Customers can use column-level GRANT and REVOKE statements to help meet their security and compliance needs."
      },
      {
        "answer": "",
        "explanation": "Redshift's table-level access controls for the data in Redshift are already in use by many customers, but they also want the ability to control access in more detail. You can now control access to columns without having to implement view-based access control or use another system. Column-level access control is available in all Amazon Redshift regions."
      },
      {
        "answer": "",
        "explanation": "GRANT command defines access privileges for a user or user group. Privileges include access options such as being able to read data in tables and views, write data, create tables, and drop tables. Use this command to give specific privileges for a table, database, schema, function, procedure, language, or column."
      },
      {
        "answer": "",
        "explanation": "The syntax for column-level privileges on Amazon Redshift tables and views looks like the below:"
      },
      {}
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Grant all users read-only permissions to the non-sensitive columns. Add the finance team to the administrator group so they have complete access to the table</strong> - This is a security concern as the finance team do not need access privileges as an administrator."
      },
      {
        "answer": "",
        "explanation": "<strong>Grant the finance team (defined as a group) permission to read from the table. Create a second table having data only for columns with non-sensitive data. Grant read-only permissions to the second table for the rest of the users</strong> - This is not an efficient solution as the storage space would be wasted to create the second table. It is better to use column-level access controls on the original table."
      },
      {
        "answer": "",
        "explanation": "<strong>Grant the finance group permission to read from the table. Create a view of the new table with only those columns having non-sensitive data. Grant the other users read-only permissions to this view</strong> - AWS recommends that you should use column-level access control, instead of views, to manage access to sensitive columns within a table."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html#r_GRANT-usage-notes-clp",
      "https://aws.amazon.com/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/"
    ]
  },
  {
    "id": 24,
    "question": "<p>A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes.</p>\n\n<p>Which of the following is the fastest way to upload the daily compressed file into Amazon S3?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Upload the compressed file using multipart upload</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Upload the compressed file in a single operation</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)</strong></p>\n\n<p>Amazon S3 Transfer Acceleration (Amazon S3TA) enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p>\n\n<p>Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload the compressed file in a single operation</strong> - In general, when your object size reaches 100 megabytes, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput - you can upload parts in parallel to improve throughput. Therefore, this option is not correct.</p>\n\n<p><strong>Upload the compressed file using multipart upload</strong> - Although using multipart upload would certainly speed up the process, combining it with Amazon S3 Transfer Acceleration (Amazon S3TA) would further improve the transfer speed. Therefore just using multipart upload is not the correct option.</p>\n\n<p><strong>FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket</strong> -  This is a roundabout process of getting the file into Amazon S3 and added as a distractor. Although it is technically feasible to follow this process, it would involve a lot of scripting and certainly would not be the fastest way to get the file into Amazon S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Transfer Acceleration (Amazon S3TA) enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path."
      },
      {
        "answer": "",
        "explanation": "Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Upload the compressed file in a single operation</strong> - In general, when your object size reaches 100 megabytes, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput - you can upload parts in parallel to improve throughput. Therefore, this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Upload the compressed file using multipart upload</strong> - Although using multipart upload would certainly speed up the process, combining it with Amazon S3 Transfer Acceleration (Amazon S3TA) would further improve the transfer speed. Therefore just using multipart upload is not the correct option."
      },
      {
        "answer": "",
        "explanation": "<strong>FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket</strong> -  This is a roundabout process of getting the file into Amazon S3 and added as a distractor. Although it is technically feasible to follow this process, it would involve a lot of scripting and certainly would not be the fastest way to get the file into Amazon S3."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html"
    ]
  },
  {
    "id": 25,
    "question": "<p>A company uploads all of its data to an Amazon S3 bucket. While multiple formats of data can be uploaded to the bucket, the downstream applications need to consume data only the .csv files are uploaded. These .csv files have to be transformed into Apache Parquet format for downstream consumption.</p>\n\n<p>Which of the following options meets these requirements with the LEAST operational overhead?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an Amazon S3 event notification with event type as <code>s3:*</code>. Create a suffix filter for the notification configuration to generate notifications only when the suffix includes .csv. Set the destination as Amazon EventBridge and trigger the Lambda function from EventBridge</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an Amazon S3 event notification with event type as <code>s3:ObjectCreated:*</code>. Create a suffix filter for the notification configuration to generate notifications only when the suffix includes .csv. Set the ARN of the Lambda function as the destination for the event notification</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up an Amazon S3 event notification with event type as <code>s3:ObjectCreated:*</code>. Create a suffix filter for the notification configuration to generate notifications only when the suffix includes .csv. Set the destination as Amazon EventBridge and trigger the Lambda function from EventBridge</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up an Amazon S3 event notification with event type as <code>s3:*</code>. Create a suffix filter for the notification configuration to generate notifications only when the suffix includes .csv. Set the ARN of the Lambda function as the destination for the event notification</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an Amazon S3 event notification with event type as <code>s3:ObjectCreated:*</code>. Create a suffix filter for the notification configuration to generate notifications only when the suffix includes .csv. Set the ARN of the Lambda function as the destination for the event notification</strong></p>\n\n<p>s3:ObjectCreated:* - This event type will publish the following events : s3:ObjectCreated:Put, s3:ObjectCreated:Post, s3:ObjectCreated:Copy, s3:ObjectCreated:CompleteMultipartUpload.</p>\n\n<p>Amazon S3 API operations such as PUT, POST, and COPY can create an object. With these event types, you can enable notifications when an object is created using a specific API operation. Alternatively, you can use the s3:ObjectCreated:* event type to request notification regardless of the API that was used to create an object.</p>\n\n<p>s3:ObjectCreated:CompleteMultipartUpload includes objects that are created using UploadPartCopy for Copy operations.</p>\n\n<p>This is the correct option since we want to process the data only when .csv files are uploaded.</p>\n\n<p>You can configure notifications to be filtered by the prefix and suffix of the key name of objects. For example, you can set up a configuration where you're sent a notification only when image files with a \".jpg\" file name extension are added to a bucket.</p>\n\n<p>Example notification configuration for multiple non-overlapping suffixes:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q11-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q11-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-filtering.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-filtering.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon S3 event notification with event type as <code>s3:*</code>. Create a suffix filter for the notification configuration to generate notifications only when the suffix includes .csv. Set the ARN of the Lambda function as the destination for the event notification</strong></p>\n\n<p><strong>Set up an Amazon S3 event notification with event type as <code>s3:*</code>. Create a suffix filter for the notification configuration to generate notifications only when the suffix includes .csv. Set the ARN of the Lambda function as the destination for the event notification</strong></p>\n\n<p>Both these options have the event type as <code>s3:*</code>, which means ALL S3 event types will trigger a notification for downstream processing, which is unnecessary. The given use case states that you need to process only when .csv files are uploaded (that is, you need to set up processing only for <code>s3:ObjectCreated:*</code> type of events that have a suffix of .csv). So, both these options are incorrect.</p>\n\n<p><strong>Set up an Amazon S3 event notification with event type as <code>s3:ObjectCreated:*</code>. Create a suffix filter for the notification configuration to generate notifications only when the suffix includes .csv. Set the destination as Amazon EventBridge and trigger the Lambda function from EventBridge</strong> - Unlike other destinations, you can either enable or disable events to be delivered to EventBridge for a bucket. If you enable delivery, all events are sent to EventBridge.</p>\n\n<p>Using Eventbridge to process S3 event notifications turns out to be a roundabout way of doing things for the given use case. There is no need to invoke the Lambda function via Eventbridge when you can configure Amazon S3 to directly trigger the Lambda function using the S3 event notifications. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-filtering.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-filtering.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-use-amazon-s3-event-notifications-with-amazon-eventbridge/\">https://aws.amazon.com/blogs/aws/new-use-amazon-s3-event-notifications-with-amazon-eventbridge/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon S3 event notification with event type as <code>s3:ObjectCreated:*</code>. Create a suffix filter for the notification configuration to generate notifications only when the suffix includes .csv. Set the ARN of the Lambda function as the destination for the event notification</strong>"
      },
      {
        "answer": "",
        "explanation": "s3:ObjectCreated:* - This event type will publish the following events : s3:ObjectCreated:Put, s3:ObjectCreated:Post, s3:ObjectCreated:Copy, s3:ObjectCreated:CompleteMultipartUpload."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 API operations such as PUT, POST, and COPY can create an object. With these event types, you can enable notifications when an object is created using a specific API operation. Alternatively, you can use the s3:ObjectCreated:* event type to request notification regardless of the API that was used to create an object."
      },
      {
        "answer": "",
        "explanation": "s3:ObjectCreated:CompleteMultipartUpload includes objects that are created using UploadPartCopy for Copy operations."
      },
      {
        "answer": "",
        "explanation": "This is the correct option since we want to process the data only when .csv files are uploaded."
      },
      {
        "answer": "",
        "explanation": "You can configure notifications to be filtered by the prefix and suffix of the key name of objects. For example, you can set up a configuration where you're sent a notification only when image files with a \".jpg\" file name extension are added to a bucket."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q11-i1.jpg",
        "answer": "",
        "explanation": "Example notification configuration for multiple non-overlapping suffixes:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-filtering.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon S3 event notification with event type as <code>s3:*</code>. Create a suffix filter for the notification configuration to generate notifications only when the suffix includes .csv. Set the ARN of the Lambda function as the destination for the event notification</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon S3 event notification with event type as <code>s3:*</code>. Create a suffix filter for the notification configuration to generate notifications only when the suffix includes .csv. Set the ARN of the Lambda function as the destination for the event notification</strong>"
      },
      {
        "answer": "",
        "explanation": "Both these options have the event type as <code>s3:*</code>, which means ALL S3 event types will trigger a notification for downstream processing, which is unnecessary. The given use case states that you need to process only when .csv files are uploaded (that is, you need to set up processing only for <code>s3:ObjectCreated:*</code> type of events that have a suffix of .csv). So, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon S3 event notification with event type as <code>s3:ObjectCreated:*</code>. Create a suffix filter for the notification configuration to generate notifications only when the suffix includes .csv. Set the destination as Amazon EventBridge and trigger the Lambda function from EventBridge</strong> - Unlike other destinations, you can either enable or disable events to be delivered to EventBridge for a bucket. If you enable delivery, all events are sent to EventBridge."
      },
      {
        "answer": "",
        "explanation": "Using Eventbridge to process S3 event notifications turns out to be a roundabout way of doing things for the given use case. There is no need to invoke the Lambda function via Eventbridge when you can configure Amazon S3 to directly trigger the Lambda function using the S3 event notifications. So, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-filtering.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html",
      "https://aws.amazon.com/blogs/aws/new-use-amazon-s3-event-notifications-with-amazon-eventbridge/"
    ]
  },
  {
    "id": 26,
    "question": "<p>A large SQL Server database on RDS contains historical transaction data. A process is needed to automate the following requirements: regularly identify data older than 3 months, and export the old data to an S3 bucket for long-term, cost-effective storage.</p>\n\n<p>Which of the following options can be used to automate this export from SQL Server to S3 and perform lifecycle management in Amazon S3 to build a solution with the least operational overhead? (Select two)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Write an AWS Lambda function that identifies and exports old data from RDS to S3. Trigger the Lambda function through Amazon CloudWatch RDS Events</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>For cost optimization, configure S3 lifecycle rules to transition archived data to Amazon S3 Glacier Deep Archive</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>For cost optimization, configure S3 lifecycle rules to transition archived data to Amazon S3 Intelligent-Tiering</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Database Migration Service (DMS) to set up a task that runs periodically to migrate data older than 3 months from your RDS instance to an S3 bucket</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Configure AWS Step Functions to create an automation workflow. The workflow will call DataSync to sync the database files to the Amazon S3 bucket</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use AWS Database Migration Service (DMS) to set up a task that runs periodically to migrate data older than 3 months from your RDS instance to an S3 bucket</strong></p>\n\n<p><strong>For cost optimization, configure S3 lifecycle rules to transition archived data to Amazon S3 Glacier Deep Archive</strong></p>\n\n<p>AWS Database Migration Service (AWS DMS) is a managed migration and replication service that helps you move your databases and analytics workloads to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.</p>\n\n<p>Use AWS Database Migration Service (DMS) to set up a task that runs periodically (e.g. weekly) to migrate data older than 3 months from your RDS instance to an S3 bucket. DMS supports homogeneous migrations between SQL Server and S3.</p>\n\n<p>For cost optimization, configure S3 lifecycle rules to transition archived data to Amazon S3 Glacier Deep Archive.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q15-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q15-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Write an AWS Lambda function that identifies and exports old data from RDS to S3. Trigger the Lambda function through Amazon CloudWatch RDS Events</strong> - An RDS event indicates a change in the Amazon RDS environment. RDS events cannot be triggered for data changes. Amazon RDS records events that relate to the following resources: DB instances, DB parameter groups, DB security groups, DB snapshots, RDS Proxy events, and Blue/green deployment events.</p>\n\n<p><strong>Configure AWS Step Functions to create an automation workflow. The workflow will call DataSync to sync the database files to the Amazon S3 bucket</strong> - This is unnecessarily complex. Also, the step function itself will need a trigger.</p>\n\n<p><strong>For cost optimization, configure S3 lifecycle rules to transition archived data to Amazon S3 Intelligent-Tiering</strong> - The S3 Intelligent-Tiering storage class is designed to optimize storage costs by automatically moving data to the most cost-effective access tier when access patterns change, without operational overhead or impact on performance. S3 Intelligent-Tiering is not suited for long-term archival of data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://repost.aws/questions/QUA-X-OBSIS36vgqSoS1srbw/efficiently-archiving-transactional-data-in-rds-mssql-to-s3\">https://repost.aws/questions/QUA-X-OBSIS36vgqSoS1srbw/efficiently-archiving-transactional-data-in-rds-mssql-to-s3</a></p>\n\n<p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Database Migration Service (DMS) to set up a task that runs periodically to migrate data older than 3 months from your RDS instance to an S3 bucket</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>For cost optimization, configure S3 lifecycle rules to transition archived data to Amazon S3 Glacier Deep Archive</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Database Migration Service (AWS DMS) is a managed migration and replication service that helps you move your databases and analytics workloads to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database."
      },
      {
        "answer": "",
        "explanation": "Use AWS Database Migration Service (DMS) to set up a task that runs periodically (e.g. weekly) to migrate data older than 3 months from your RDS instance to an S3 bucket. DMS supports homogeneous migrations between SQL Server and S3."
      },
      {
        "answer": "",
        "explanation": "For cost optimization, configure S3 lifecycle rules to transition archived data to Amazon S3 Glacier Deep Archive."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Write an AWS Lambda function that identifies and exports old data from RDS to S3. Trigger the Lambda function through Amazon CloudWatch RDS Events</strong> - An RDS event indicates a change in the Amazon RDS environment. RDS events cannot be triggered for data changes. Amazon RDS records events that relate to the following resources: DB instances, DB parameter groups, DB security groups, DB snapshots, RDS Proxy events, and Blue/green deployment events."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Step Functions to create an automation workflow. The workflow will call DataSync to sync the database files to the Amazon S3 bucket</strong> - This is unnecessarily complex. Also, the step function itself will need a trigger."
      },
      {
        "answer": "",
        "explanation": "<strong>For cost optimization, configure S3 lifecycle rules to transition archived data to Amazon S3 Intelligent-Tiering</strong> - The S3 Intelligent-Tiering storage class is designed to optimize storage costs by automatically moving data to the most cost-effective access tier when access patterns change, without operational overhead or impact on performance. S3 Intelligent-Tiering is not suited for long-term archival of data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html",
      "https://repost.aws/questions/QUA-X-OBSIS36vgqSoS1srbw/efficiently-archiving-transactional-data-in-rds-mssql-to-s3",
      "https://aws.amazon.com/dms/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html"
    ]
  },
  {
    "id": 27,
    "question": "<p>A company stores its user information in a MySQL database table called <code>user</code>. The <code>name</code> column has the users' names stored in <code>firstname lastname</code> format. Due to legacy reasons, a few users have the names stored in <code>lastname firstname</code> format. A data engineer has been tasked with developing a query that returns all records where the <code>name</code> column has values starting with <code>John</code> or <code>Doe</code>, on a case-insensitive basis.</p>\n\n<p>Which of the following queries represents the correct solution?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p><code>SELECT * FROM user WHERE name ~ '^(John|Doe)'</code></p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p><code>SELECT * FROM user WHERE name ~ * '$(John|Doe)'</code></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p><code>SELECT * FROM user WHERE name ~ '$(John|Doe)'</code></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p><code>SELECT * FROM user WHERE name ~ * '^(John|Doe)'</code></p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong><code>SELECT * FROM user WHERE name ~ * '^(John|Doe)'</code></strong></p>\n\n<p>In SQL, <code>~</code> is the regular expression operator. You can use <code>~*</code> to make the query case-insensitive. The <code>^</code> operator matches a pattern at the start of a string. For the given use case, you can use the <code>^</code> operator to find all records where the <code>name</code> column has values starting with <code>John</code> or <code>Doe</code> on a case-insensitive basis.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>SELECT * FROM user WHERE name ~ * '$(John|Doe)'</code></strong></p>\n\n<p><strong><code>SELECT * FROM user WHERE name ~ '$(John|Doe)'</code></strong></p>\n\n<p>The <code>$</code> operator matches a pattern at the end of a string. So, both these options are incorrect.</p>\n\n<p><strong><code>SELECT * FROM user WHERE name ~ '^(John|Doe)'</code></strong> - Without the <code>~ *</code> characters combination in the WHERE clause, the query would return the results on a case-sensitive basis, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://www.atlassian.com/data/sql/how-regex-works-in-sql\">https://www.atlassian.com/data/sql/how-regex-works-in-sql</a></p>\n\n<p><a href=\"https://dev.mysql.com/doc/refman/8.0/en/regexp.html\">https://dev.mysql.com/doc/refman/8.0/en/regexp.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong><code>SELECT * FROM user WHERE name ~ * '^(John|Doe)'</code></strong>"
      },
      {
        "answer": "",
        "explanation": "In SQL, <code>~</code> is the regular expression operator. You can use <code>~*</code> to make the query case-insensitive. The <code>^</code> operator matches a pattern at the start of a string. For the given use case, you can use the <code>^</code> operator to find all records where the <code>name</code> column has values starting with <code>John</code> or <code>Doe</code> on a case-insensitive basis."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong><code>SELECT * FROM user WHERE name ~ * '$(John|Doe)'</code></strong>"
      },
      {
        "answer": "",
        "explanation": "<strong><code>SELECT * FROM user WHERE name ~ '$(John|Doe)'</code></strong>"
      },
      {
        "answer": "",
        "explanation": "The <code>$</code> operator matches a pattern at the end of a string. So, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong><code>SELECT * FROM user WHERE name ~ '^(John|Doe)'</code></strong> - Without the <code>~ *</code> characters combination in the WHERE clause, the query would return the results on a case-sensitive basis, so this option is incorrect."
      }
    ],
    "references": [
      "https://www.atlassian.com/data/sql/how-regex-works-in-sql",
      "https://dev.mysql.com/doc/refman/8.0/en/regexp.html"
    ]
  },
  {
    "id": 28,
    "question": "<p>A data engineer routinely performs resource-intensive analytics once a month using Amazon Redshift, creating a new provisioned cluster each time. After completing the analytics-specific processes, the cluster is deleted, but not before the data is backed up to an Amazon S3 bucket. The data engineer is looking for a solution to conduct the monthly analytics that minimizes the need for manual infrastructure management.</p>\n\n<p>What solution would best meet these requirements with the lowest operational overhead?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use zero-ETL integrations to automatically process the resource-intensive workload</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon EventBridge Scheduler to start execution of a Step Functions state machine on a schedule. The Step Function will provision the Redshift cluster resources and run the analytics. Once the job is done, the data will be copied back to the S3 bucket and then the cluster will be decommissioned</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Purchase Redshift reserved node offerings to reduce the operational effort of infrastructure provisioning and maintenance</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Redshift Serverless to automatically process the resource-intensive analytics workload</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Redshift Serverless to automatically process the resource-intensive analytics workload</strong></p>\n\n<p>Amazon Redshift Serverless automatically provisions data warehouse capacity and intelligently scales the underlying resources. Amazon Redshift Serverless adjusts capacity in seconds to deliver consistently high performance and simplified operations for even the most demanding and volatile workloads.</p>\n\n<p>With Amazon Redshift Serverless, you can benefit from the following features:</p>\n\n<ol>\n<li><p>Access and analyze data without the need to set up, tune, and manage Amazon Redshift provisioned clusters.</p></li>\n<li><p>Use the superior Amazon Redshift SQL capabilities, industry-leading performance, and data-lake integration to seamlessly query across a data warehouse, a data lake, and operational data sources.</p></li>\n<li><p>Deliver consistently high performance and simplified operations for the most demanding and volatile workloads with intelligent and automatic scaling.</p></li>\n<li><p>Use workgroups and namespaces to organize compute resources and data with granular cost controls.</p></li>\n<li><p>Pay only when the data warehouse is in use.</p></li>\n</ol>\n\n<p>With Amazon Redshift Serverless, you use a console interface to reach a serverless data warehouse or APIs to build applications. Through the data warehouse, you can access your Amazon Redshift managed storage and your Amazon S3 data lake.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use zero-ETL integrations to automatically process the resource-intensive workload</strong> - Zero-ETL integration is a fully managed solution that makes transactional or operational data available in Amazon Redshift in near real-time. With this solution, you can configure an integration from your source to an Amazon Redshift data warehouse. You don't need to maintain an extract, transform, and load (ETL) pipeline. AWS takes care of the ETL for you by automating the creation and management of data replication from the data source to the Amazon Redshift cluster or Redshift Serverless namespace. However, ETL automation is not relevant to the given use case.</p>\n\n<p><strong>Purchase Redshift reserved node offerings to reduce the operational effort of infrastructure provisioning and maintenance</strong> - If you intend to keep your Amazon Redshift cluster running continuously for a prolonged period, you should consider purchasing reserved node offerings. These offerings provide significant savings over on-demand pricing, but they require you to reserve compute nodes and commit to paying for those nodes for either a one-year or three-year duration. Since the cluster will be decommissioned every month, this option is not useful.</p>\n\n<p><strong>Use Amazon EventBridge Scheduler to start execution of a Step Functions state machine on a schedule. The Step Function will provision the Redshift cluster resources and run the analytics. Once the job is done, the data will be copied back to the S3 bucket and then the cluster will be decommissioned</strong> - While these steps will automate the process, the solution is unnecessarily complex involving too many services. This option is not the best fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/zero-etl-using.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/zero-etl-using.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-whatis.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-whatis.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Redshift Serverless to automatically process the resource-intensive analytics workload</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift Serverless automatically provisions data warehouse capacity and intelligently scales the underlying resources. Amazon Redshift Serverless adjusts capacity in seconds to deliver consistently high performance and simplified operations for even the most demanding and volatile workloads."
      },
      {
        "answer": "",
        "explanation": "With Amazon Redshift Serverless, you can benefit from the following features:"
      },
      {},
      {
        "answer": "",
        "explanation": "With Amazon Redshift Serverless, you use a console interface to reach a serverless data warehouse or APIs to build applications. Through the data warehouse, you can access your Amazon Redshift managed storage and your Amazon S3 data lake."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use zero-ETL integrations to automatically process the resource-intensive workload</strong> - Zero-ETL integration is a fully managed solution that makes transactional or operational data available in Amazon Redshift in near real-time. With this solution, you can configure an integration from your source to an Amazon Redshift data warehouse. You don't need to maintain an extract, transform, and load (ETL) pipeline. AWS takes care of the ETL for you by automating the creation and management of data replication from the data source to the Amazon Redshift cluster or Redshift Serverless namespace. However, ETL automation is not relevant to the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Purchase Redshift reserved node offerings to reduce the operational effort of infrastructure provisioning and maintenance</strong> - If you intend to keep your Amazon Redshift cluster running continuously for a prolonged period, you should consider purchasing reserved node offerings. These offerings provide significant savings over on-demand pricing, but they require you to reserve compute nodes and commit to paying for those nodes for either a one-year or three-year duration. Since the cluster will be decommissioned every month, this option is not useful."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EventBridge Scheduler to start execution of a Step Functions state machine on a schedule. The Step Function will provision the Redshift cluster resources and run the analytics. Once the job is done, the data will be copied back to the S3 bucket and then the cluster will be decommissioned</strong> - While these steps will automate the process, the solution is unnecessarily complex involving too many services. This option is not the best fit for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/mgmt/zero-etl-using.html",
      "https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-whatis.html"
    ]
  },
  {
    "id": 29,
    "question": "<p>An HTTP application is deployed on an Auto Scaling Group, is accessible from an Application Load Balancer (ALB) that provides HTTPS termination and accesses a PostgreSQL database managed by Amazon RDS.</p>\n\n<p>How should you configure the security groups? (Select three)</p>",
    "corrects": [
      2,
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432</p>",
        "correct": true
      },
      {
        "id": 6,
        "answer": "<p>The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432</strong></p>\n\n<p><strong>The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80</strong></p>\n\n<p><strong>The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.</p>\n\n<p>The following are the characteristics of security group rules:\n1. By default, security groups allow all outbound traffic.\n2. Security group rules are always permissive; you can't create rules that deny access.\n3. Security groups are stateful</p>\n\n<p>PostgreSQL port = 5432\nHTTP port = 80\nHTTPS port = 443</p>\n\n<p>The traffic goes like this :\nThe client sends an HTTPS request to ALB on port 443. This is handled by the rule - \"The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443\"</p>\n\n<p>The Application Load Balancer then forwards the request to one of the Amazon EC2 instances. This is handled by the rule - \"The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80\"</p>\n\n<p>The Amazon EC2 instance further accesses the PostgreSQL database managed by Amazon RDS on port 5432. This is handled by the rule - \"The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432\"</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80</strong> - The client sends an HTTPS request to ALB on port 443 and not on port 80, so this is incorrect.</p>\n\n<p><strong>The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432</strong> - The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer and not from the security group of the Amazon RDS database, so this option is incorrect.</p>\n\n<p><strong>The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80</strong> - The Amazon EC2 instance further accesses the PostgreSQL database managed by Amazon RDS on port 5432 and not on port 80, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443</strong>"
      },
      {
        "answer": "",
        "explanation": "A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance."
      },
      {
        "answer": "",
        "explanation": "The following are the characteristics of security group rules:\n1. By default, security groups allow all outbound traffic.\n2. Security group rules are always permissive; you can't create rules that deny access.\n3. Security groups are stateful"
      },
      {
        "answer": "",
        "explanation": "PostgreSQL port = 5432\nHTTP port = 80\nHTTPS port = 443"
      },
      {
        "answer": "",
        "explanation": "The traffic goes like this :\nThe client sends an HTTPS request to ALB on port 443. This is handled by the rule - \"The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443\""
      },
      {
        "answer": "",
        "explanation": "The Application Load Balancer then forwards the request to one of the Amazon EC2 instances. This is handled by the rule - \"The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80\""
      },
      {
        "answer": "",
        "explanation": "The Amazon EC2 instance further accesses the PostgreSQL database managed by Amazon RDS on port 5432. This is handled by the rule - \"The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432\""
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80</strong> - The client sends an HTTPS request to ALB on port 443 and not on port 80, so this is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432</strong> - The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer and not from the security group of the Amazon RDS database, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80</strong> - The Amazon EC2 instance further accesses the PostgreSQL database managed by Amazon RDS on port 5432 and not on port 80, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html"
    ]
  },
  {
    "id": 30,
    "question": "<p>An AWS Glue job is scheduled to be run every Sunday. The Glue job copies data from certain folders in an S3 bucket to Redshift. To prevent reprocessing of old data, job bookmarks have been enabled on the AWS Glue job. However, the ETL job is reprocessing data that was already processed in an earlier run.</p>\n\n<p>What could be the underlying issue and how should it be fixed to stop reprocessing of data? (Select two)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Glue keeps track of job bookmarks by storing the metadata in Amazon S3 configured during job creation. Deleting the S3 bucket can result in job reprocessing</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>You have added a transformation context parameter to the DynamicFrame referenced within the job, which is causing the job to crash</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Job bookmarks do not work for Amazon S3 input sources</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>You have multiple concurrent jobs with job bookmarks, and the max concurrency isn't set to 1</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>The job.commit() object is missing</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>You have multiple concurrent jobs with job bookmarks, and the max concurrency isn't set to 1</strong> - Ensure that the maximum number of concurrent runs for the job is 1. When you have multiple concurrent jobs with job bookmarks and the maximum concurrency is set to 1, the job bookmark doesn't work correctly.</p>\n\n<p><strong>The job.commit() object is missing</strong> - Ensure that your job run script ends with the following commit: job.commit(). When you include this object, AWS Glue records the timestamp and path of the job run. If you run the job again with the same path, AWS Glue processes only the new files. If you don't include this object and job bookmarks are enabled, the job reprocesses the already processed files along with the new files and creates redundancy in the job's target data store.</p>\n\n<p>Troubleshooting reprocessing error in job bookmarks:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q33-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q33-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data\">https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Job bookmarks do not work for Amazon S3 input sources</strong> - This statement is incorrect. Job bookmarks can be configured even when the input source is Amazon S3.</p>\n\n<p><strong>You have added a transformation context parameter to the DynamicFrame referenced within the job, which is causing the job to crash</strong> - Transformation context is an optional parameter in the GlueContext class, but job bookmarks don't work if you don't include it. So this option is incorrect.</p>\n\n<p><strong>AWS Glue keeps track of job bookmarks by storing the metadata in Amazon S3 configured during job creation. Deleting the S3 bucket can result in job reprocessing</strong> - This statement is incorrect. AWS Glue keeps track of job bookmarks with the job itself. If you delete a job, the job bookmark is deleted.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data\">https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html\">https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You have multiple concurrent jobs with job bookmarks, and the max concurrency isn't set to 1</strong> - Ensure that the maximum number of concurrent runs for the job is 1. When you have multiple concurrent jobs with job bookmarks and the maximum concurrency is set to 1, the job bookmark doesn't work correctly."
      },
      {
        "answer": "",
        "explanation": "<strong>The job.commit() object is missing</strong> - Ensure that your job run script ends with the following commit: job.commit(). When you include this object, AWS Glue records the timestamp and path of the job run. If you run the job again with the same path, AWS Glue processes only the new files. If you don't include this object and job bookmarks are enabled, the job reprocesses the already processed files along with the new files and creates redundancy in the job's target data store."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q33-i1.jpg",
        "answer": "",
        "explanation": "Troubleshooting reprocessing error in job bookmarks:"
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Job bookmarks do not work for Amazon S3 input sources</strong> - This statement is incorrect. Job bookmarks can be configured even when the input source is Amazon S3."
      },
      {
        "answer": "",
        "explanation": "<strong>You have added a transformation context parameter to the DynamicFrame referenced within the job, which is causing the job to crash</strong> - Transformation context is an optional parameter in the GlueContext class, but job bookmarks don't work if you don't include it. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Glue keeps track of job bookmarks by storing the metadata in Amazon S3 configured during job creation. Deleting the S3 bucket can result in job reprocessing</strong> - This statement is incorrect. AWS Glue keeps track of job bookmarks with the job itself. If you delete a job, the job bookmark is deleted."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data",
      "https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html"
    ]
  },
  {
    "id": 31,
    "question": "<p>A company has created a data warehouse using Redshift that is used to analyze data from Amazon S3. From the usage patterns, the data engineering team has noticed that after 30 days, the data is rarely queried in Redshift and it's not \"hot data\" anymore. The team would like to preserve the SQL querying capability on the data and have the query execution start immediately. Also, the team wants to adopt a pricing model that allows the company to save the maximum amount of cost on Redshift.</p>\n\n<p>As an AWS Certified Data Engineer Associate, which of the following options would you recommend? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a smaller Redshift Cluster with the cold data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Move the data to S3 Standard IA after 30 days</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Move the data to S3 Glacier Deep Archive after 30 days</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Migrate the Redshift cluster's underlying storage class to Standard-IA</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Analyze the cold data with Athena</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Move the data to S3 Standard IA after 30 days</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.</p>\n\n<p><strong>Analyze the cold data with Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>Therefore, we should migrate the data to S3 Standard IA and use Athena to analyze the cold data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the Redshift cluster's underlying storage class to Standard-IA</strong> - Amazon Redshift is a fully managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications.</p>\n\n<p>Redshift's internal storage does not have \"tiers\" of storage classes like Amazon S3, so this option is also ruled out.</p>\n\n<p><strong>Create a smaller Redshift Cluster with the cold data</strong> - Creating a smaller cluster with the cold data would not decrease the storage cost of Redshift - which will only increase with time - as more data is added to the cluster. Therefore this option is ruled out.</p>\n\n<p><strong>Move the data to S3 Glacier Deep Archive after 30 days</strong> - Amazon S3 Glacier Deep Archive is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. Moving the data to the S3 Glacier Deep Archive will prevent us from being able to query it on an immediate basis.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Move the data to S3 Standard IA after 30 days</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days."
      },
      {
        "answer": "",
        "explanation": "<strong>Analyze the cold data with Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries."
      },
      {
        "answer": "",
        "explanation": "Therefore, we should migrate the data to S3 Standard IA and use Athena to analyze the cold data."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Migrate the Redshift cluster's underlying storage class to Standard-IA</strong> - Amazon Redshift is a fully managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications."
      },
      {
        "answer": "",
        "explanation": "Redshift's internal storage does not have \"tiers\" of storage classes like Amazon S3, so this option is also ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a smaller Redshift Cluster with the cold data</strong> - Creating a smaller cluster with the cold data would not decrease the storage cost of Redshift - which will only increase with time - as more data is added to the cluster. Therefore this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Move the data to S3 Glacier Deep Archive after 30 days</strong> - Amazon S3 Glacier Deep Archive is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. Moving the data to the S3 Glacier Deep Archive will prevent us from being able to query it on an immediate basis."
      }
    ],
    "references": [
      "https://aws.amazon.com/athena/",
      "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html"
    ]
  },
  {
    "id": 32,
    "question": "<p>An IT company is using AWS DMS to migrate its Amazon RDS for Oracle DB instance configured in a VPC in the us-east-1 Region to another VPC in the us-west-1 Region.</p>\n\n<p>Where would you place the DMS replication instance for the MOST optimal performance?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create the replication instance in the same Availability Zone and VPC as the target DB instance</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create the replication instance in the same Region and VPC as the target DB instance</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create the replication instance in the same Availability Zone and VPC as the source DB instance</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create the replication instance in the same Region and VPC as the source DB instance</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create the replication instance in the same Availability Zone and VPC as the target DB instance</strong></p>\n\n<p>AWS Database Migration Service (AWS DMS) is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. AWS DMS is a server in the AWS Cloud that runs replication software. You create a source and target connection to tell AWS DMS where to extract from and load to. Then you schedule a task that runs on this server to move your data. AWS DMS creates the tables and associated primary keys if they don't exist on the target.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q36-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q36-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html\">https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html</a><p></p>\n\n<p>While migrating an Amazon Relational Database Service (Amazon RDS) for Oracle source database to a different AWS Region, AWS suggests that you create the replication instance in the VPC of the target AWS Region. This is sufficient to get your solution working. However, to further optimize your solution, you should consider creating your replication instance in the same Availability Zone and VPC as the target DB instance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q36-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q36-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-amazon-rds-for-oracle-database-to-another-aws-account-and-aws-region-using-aws-dms-for-ongoing-replication.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-amazon-rds-for-oracle-database-to-another-aws-account-and-aws-region-using-aws-dms-for-ongoing-replication.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the replication instance in the same Region and VPC as the source DB instance</strong></p>\n\n<p><strong>Create the replication instance in the same Region and VPC as the target DB instance</strong></p>\n\n<p><strong>Create the replication instance in the same Availability Zone and VPC as the source DB instance</strong></p>\n\n<p>These three options contradict the details mentioned in the explanation above, hence these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html\">https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-amazon-rds-for-oracle-database-to-another-aws-account-and-aws-region-using-aws-dms-for-ongoing-replication.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-amazon-rds-for-oracle-database-to-another-aws-account-and-aws-region-using-aws-dms-for-ongoing-replication.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create the replication instance in the same Availability Zone and VPC as the target DB instance</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Database Migration Service (AWS DMS) is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. AWS DMS is a server in the AWS Cloud that runs replication software. You create a source and target connection to tell AWS DMS where to extract from and load to. Then you schedule a task that runs on this server to move your data. AWS DMS creates the tables and associated primary keys if they don't exist on the target."
      },
      {
        "link": "https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html"
      },
      {
        "answer": "",
        "explanation": "While migrating an Amazon Relational Database Service (Amazon RDS) for Oracle source database to a different AWS Region, AWS suggests that you create the replication instance in the VPC of the target AWS Region. This is sufficient to get your solution working. However, to further optimize your solution, you should consider creating your replication instance in the same Availability Zone and VPC as the target DB instance."
      },
      {
        "link": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-amazon-rds-for-oracle-database-to-another-aws-account-and-aws-region-using-aws-dms-for-ongoing-replication.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create the replication instance in the same Region and VPC as the source DB instance</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Create the replication instance in the same Region and VPC as the target DB instance</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Create the replication instance in the same Availability Zone and VPC as the source DB instance</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the details mentioned in the explanation above, hence these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html",
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-amazon-rds-for-oracle-database-to-another-aws-account-and-aws-region-using-aws-dms-for-ongoing-replication.html"
    ]
  },
  {
    "id": 33,
    "question": "<p>A financial services company wants a single log processing model for all the log files (consisting of system logs, application logs, database logs, etc) that can be processed in a serverless fashion and then durably stored for downstream analytics. The company wants to use an AWS-managed service that automatically scales to match the throughput of the log data and requires no ongoing administration to deliver the data to persistent storage.</p>\n\n<p>Which of the following AWS services would you recommend to solve this problem with the LEAST overhead?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Kinesis Data Firehose</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>AWS Lambda</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Kinesis Data Streams</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon EMR</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis Data Firehose</strong></p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. Therefore, this is the correct option.</p>\n\n<p>Please see this overview of how Kinesis Firehose works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. With Amazon Kinesis Data Streams, you can scale up to a sufficient number of shards (note, however, that you'll need to provision enough shards ahead of time). You could also automatically scale the capacity in response to varying data traffic using the on-demand mode. However, you still need to manually build the code to deliver the data to persistent storage, so this option is not the best fit.</p>\n\n<p><strong>Amazon EMR</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open-source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p>Using an Amazon EMR cluster would imply managing the underlying infrastructure, so it’s ruled out.</p>\n\n<p><strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for production-grade serverless log analytics.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Firehose</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. Therefore, this is the correct option."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png",
        "answer": "",
        "explanation": "Please see this overview of how Kinesis Firehose works:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-firehose/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. With Amazon Kinesis Data Streams, you can scale up to a sufficient number of shards (note, however, that you'll need to provision enough shards ahead of time). You could also automatically scale the capacity in response to varying data traffic using the on-demand mode. However, you still need to manually build the code to deliver the data to persistent storage, so this option is not the best fit."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EMR</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open-source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances."
      },
      {
        "answer": "",
        "explanation": "Using an Amazon EMR cluster would imply managing the underlying infrastructure, so it’s ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for production-grade serverless log analytics."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-firehose/"
    ]
  },
  {
    "id": 34,
    "question": "<p>An application hosted on Amazon EC2 contains sensitive personal information about all its customers and needs to be protected from all types of cyber-attacks. The company is considering using the AWS Web Application Firewall (AWS WAF) to handle this requirement.</p>\n\n<p>Can you identify the correct solution leveraging the capabilities of AWS WAF?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures</strong></p>\n\n<p>When you use AWS WAF with Amazon CloudFront, you can protect your applications running on any HTTP web server, whether it's a webserver that's running in Amazon Elastic Compute Cloud (Amazon EC2) or a web server that you manage privately. You can also configure Amazon CloudFront to require HTTPS between CloudFront and your own web server, as well as between viewers and Amazon CloudFront.</p>\n\n<p>AWS WAF is tightly integrated with Amazon CloudFront and the Application Load Balancer (ALB), services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn’t come at the expense of performance. Blocked requests are stopped before they reach your web servers. When you use AWS WAF on Application Load Balancer, your rules run in the region and can be used to protect internet-facing as well as internal load balancers.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too</strong> - This statement is wrong. You can configure AWS WAF on Application Load Balancers (ALB).</p>\n\n<p><strong>AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data</strong> - AWS WAF can be deployed on Amazon CloudFront, the Application Load Balancer (ALB), and Amazon API Gateway. It cannot be configured directly on an Amazon EC2 instance.</p>\n\n<p><strong>AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture</strong> - This statement is only partially correct. AWS WAF can also be deployed on Amazon CloudFront service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/waf/faqs/\">https://aws.amazon.com/waf/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures</strong>"
      },
      {
        "answer": "",
        "explanation": "When you use AWS WAF with Amazon CloudFront, you can protect your applications running on any HTTP web server, whether it's a webserver that's running in Amazon Elastic Compute Cloud (Amazon EC2) or a web server that you manage privately. You can also configure Amazon CloudFront to require HTTPS between CloudFront and your own web server, as well as between viewers and Amazon CloudFront."
      },
      {
        "answer": "",
        "explanation": "AWS WAF is tightly integrated with Amazon CloudFront and the Application Load Balancer (ALB), services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn’t come at the expense of performance. Blocked requests are stopped before they reach your web servers. When you use AWS WAF on Application Load Balancer, your rules run in the region and can be used to protect internet-facing as well as internal load balancers."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too</strong> - This statement is wrong. You can configure AWS WAF on Application Load Balancers (ALB)."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data</strong> - AWS WAF can be deployed on Amazon CloudFront, the Application Load Balancer (ALB), and Amazon API Gateway. It cannot be configured directly on an Amazon EC2 instance."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture</strong> - This statement is only partially correct. AWS WAF can also be deployed on Amazon CloudFront service."
      }
    ],
    "references": [
      "https://aws.amazon.com/waf/faqs/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html"
    ]
  },
  {
    "id": 35,
    "question": "<p>A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The data engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file.</p>\n\n<p>Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</strong></p>\n\n<p>Amazon S3 delivers strong read-after-write consistency automatically, without changes to performance or availability, without sacrificing regional isolation for applications, and at no additional cost.</p>\n\n<p>After a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. Amazon S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected.</p>\n\n<p>Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects.</p>\n\n<p>To summarize, all Amazon S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</strong></p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</strong></p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</strong></p>\n\n<p>These three options contradict the earlier details provided in the explanation.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 delivers strong read-after-write consistency automatically, without changes to performance or availability, without sacrificing regional isolation for applications, and at no additional cost."
      },
      {
        "answer": "",
        "explanation": "After a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. Amazon S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected."
      },
      {
        "answer": "",
        "explanation": "Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects."
      },
      {
        "answer": "",
        "explanation": "To summarize, all Amazon S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the earlier details provided in the explanation."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel",
      "https://aws.amazon.com/s3/faqs/"
    ]
  },
  {
    "id": 36,
    "question": "<p>A Silicon Valley based startup helps its users legally sign highly confidential contracts. To meet the compliance guidelines, the startup must ensure that the signed contracts are encrypted using the AES-256 algorithm via an encryption key that is generated as well as managed internally. The startup is now migrating to AWS Cloud and would like the data to be encrypted on AWS. The startup wants to continue using its existing encryption key generation as well as key management mechanism.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SSE-C</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>SSE-S3</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Client-Side Encryption</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SSE-KMS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>SSE-C</strong></p>\n\n<p>With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, as well as manages the decryption when you access your objects. With SSE-C, the startup can still generate and manage the encryption key but let AWS do the encryption. Therefore, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-KMS</strong> - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. But, you never get to know the actual key here.</p>\n\n<p><strong>SSE-S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However, this option does not provide the ability to audit trail the usage of the encryption keys.</p>\n\n<p><strong>Client-Side Encryption</strong> - Client-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options: Use an AWS KMS key stored in AWS Key Management Service (AWS KMS), or use a master key you store within your application. Since the customer wants to use AWS provided facility, this is not an option.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>SSE-C</strong>"
      },
      {
        "answer": "",
        "explanation": "With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, as well as manages the decryption when you access your objects. With SSE-C, the startup can still generate and manage the encryption key but let AWS do the encryption. Therefore, this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>SSE-KMS</strong> - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. But, you never get to know the actual key here."
      },
      {
        "answer": "",
        "explanation": "<strong>SSE-S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However, this option does not provide the ability to audit trail the usage of the encryption keys."
      },
      {
        "answer": "",
        "explanation": "<strong>Client-Side Encryption</strong> - Client-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options: Use an AWS KMS key stored in AWS Key Management Service (AWS KMS), or use a master key you store within your application. Since the customer wants to use AWS provided facility, this is not an option."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"
    ]
  },
  {
    "id": 37,
    "question": "<p>The HR department at a company wants to process employees data stored in Amazon S3 in the Microsoft Excel worksheet format. The data has the following column names: id, name, email, and phone. The department wants to create a single column to store these values in the following format:</p>\n\n<pre><code>{\n    \"id\": 1,\n    \"name\": \"John Doe\",\n    \"email\": \"johndoe@example.com\",\n    \"phone\": \"9876543210\"\n}\n</code></pre>\n\n<p>Which of the following options will meet this requirement with the LEAST coding effort?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Process the files using Amazon Athena and leverage the json_format function to create the new column</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Process the files using AWS Glue DataBrew and leverage the NEST_TO_ARRAY transformation to create the new column</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Process the files using Amazon Athena and leverage the json_parse function to create the new column</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Process the files using AWS Glue DataBrew and leverage the NEST_TO_MAP transformation to create the new column</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Process the files using AWS Glue DataBrew and leverage the NEST_TO_MAP transformation to create the new column</strong></p>\n\n<p>In DataBrew, a recipe step is an action that transforms your raw data into a form that is ready to be consumed by your data pipeline. A DataBrew function is a special kind of recipe step that performs a computation based on parameters.</p>\n\n<p>You can use the NEST_TO_MAP recipe step to convert user-selected columns into key-value pairs, each with a key representing the column name and a value representing the row value. The order of the selected column is not maintained while creating the resultant map.</p>\n\n<p>Example:</p>\n\n<pre><code>{\n    \"RecipeAction\": {\n        \"Operation\": \"NEST_TO_MAP\",\n        \"Parameters\": {\n            \"sourceColumns\": \"[\"age\",\"weight_kg\",\"height_cm\"]\",\n            \"targetColumn\": \"columnName\",\n            \"removeSourceColumns\": \"true\"\n        }\n    }\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Process the files using Amazon Athena and leverage the json_format function to create the new column</strong></p>\n\n<p><strong>Process the files using Amazon Athena and leverage the json_parse function to create the new column</strong></p>\n\n<p>The function json_format returns the JSON text serialized from the input JSON value.</p>\n\n<p>The function json_parse returns the JSON value deserialized from the input JSON text.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q1-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q1-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://prestodb.io/docs/current/functions/json.html\">https://prestodb.io/docs/current/functions/json.html</a><p></p>\n\n<p>Athena supports creating tables and querying data from CSV, TSV, custom-delimited, and JSON formats; data from Hadoop-related formats: ORC, Apache Avro, and Parquet; logs from Logstash, AWS CloudTrail logs, and Apache WebServer logs. Athena does not support querying data from files stored in the Microsoft Excel workbook format, so both these options are incorrect.</p>\n\n<p><strong>Process the files using AWS Glue DataBrew and leverage the NEST_TO_ARRAY transformation to create the new column</strong> - You can use the NEST_TO_ARRAY recipe step to convert user-selected columns into array values. The order of the selected columns is maintained while creating the resultant array. The different column data types are typecast to a common type that supports the data types of all columns. Since the output is an array and not a JSON, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/databrew/latest/dg/what-is.html\">https://docs.aws.amazon.com/databrew/latest/dg/what-is.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/what-is.html\">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/databrew/latest/dg/supported-data-file-sources.html\">https://docs.aws.amazon.com/databrew/latest/dg/supported-data-file-sources.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/supported-serdes.html\">https://docs.aws.amazon.com/athena/latest/ug/supported-serdes.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.NEST_TO_MAP.html\">https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.NEST_TO_MAP.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.NEST_TO_ARRAY.html\">https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.NEST_TO_ARRAY.html</a></p>\n\n<p><a href=\"https://prestodb.io/docs/current/functions/json.html\">https://prestodb.io/docs/current/functions/json.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Process the files using AWS Glue DataBrew and leverage the NEST_TO_MAP transformation to create the new column</strong>"
      },
      {
        "answer": "",
        "explanation": "In DataBrew, a recipe step is an action that transforms your raw data into a form that is ready to be consumed by your data pipeline. A DataBrew function is a special kind of recipe step that performs a computation based on parameters."
      },
      {
        "answer": "",
        "explanation": "You can use the NEST_TO_MAP recipe step to convert user-selected columns into key-value pairs, each with a key representing the column name and a value representing the row value. The order of the selected column is not maintained while creating the resultant map."
      },
      {
        "answer": "",
        "explanation": "Example:"
      },
      {}
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Process the files using Amazon Athena and leverage the json_format function to create the new column</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Process the files using Amazon Athena and leverage the json_parse function to create the new column</strong>"
      },
      {
        "answer": "",
        "explanation": "The function json_format returns the JSON text serialized from the input JSON value."
      },
      {
        "answer": "",
        "explanation": "The function json_parse returns the JSON value deserialized from the input JSON text."
      },
      {
        "link": "https://prestodb.io/docs/current/functions/json.html"
      },
      {
        "answer": "",
        "explanation": "Athena supports creating tables and querying data from CSV, TSV, custom-delimited, and JSON formats; data from Hadoop-related formats: ORC, Apache Avro, and Parquet; logs from Logstash, AWS CloudTrail logs, and Apache WebServer logs. Athena does not support querying data from files stored in the Microsoft Excel workbook format, so both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Process the files using AWS Glue DataBrew and leverage the NEST_TO_ARRAY transformation to create the new column</strong> - You can use the NEST_TO_ARRAY recipe step to convert user-selected columns into array values. The order of the selected columns is maintained while creating the resultant array. The different column data types are typecast to a common type that supports the data types of all columns. Since the output is an array and not a JSON, so this option is incorrect."
      }
    ],
    "references": [
      "https://prestodb.io/docs/current/functions/json.html",
      "https://docs.aws.amazon.com/databrew/latest/dg/what-is.html",
      "https://docs.aws.amazon.com/athena/latest/ug/what-is.html",
      "https://docs.aws.amazon.com/databrew/latest/dg/supported-data-file-sources.html",
      "https://docs.aws.amazon.com/athena/latest/ug/supported-serdes.html",
      "https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.NEST_TO_MAP.html",
      "https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.NEST_TO_ARRAY.html"
    ]
  },
  {
    "id": 38,
    "question": "<p>A company uses Amazon S3 buckets to store sensitive customer data that is business-critical. The data is kept segregated and well organized to ensure low latency. Recently, the data engineering team noticed that the lifecycle policies on the Amazon S3 buckets have not been applied optimally, resulting in higher costs.</p>\n\n<p>Can you recommend a solution to reduce storage costs on Amazon S3 while keeping the team's involvement to a minimum?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon S3 Glacier Deep Archive storage class to reduce the costs on Amazon S3 storage</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure Amazon EFS to provide a fast, cost-effective, and sharable storage service</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs</strong></p>\n\n<p>The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.</p>\n\n<p>For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in Amazon S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. There are no retrieval fees when using the Amazon S3 Intelligent-Tiering storage class, and no additional tiering fees when objects are moved between access tiers. It is the ideal storage class for long-lived data with access patterns that are unknown or unpredictable.</p>\n\n<p>Amazon S3 Storage Classes can be configured at the object level and a single bucket can contain objects stored in Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One Zone-IA. You can upload objects directly to Amazon S3 Intelligent-Tiering, or use S3 Lifecycle policies to transfer objects from Amazon S3 Standard and Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering. You can also archive objects from Amazon S3 Intelligent-Tiering to Amazon S3 Glacier.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon EFS to provide a fast, cost-effective, and sharable storage service</strong> - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS offers sharable service, unlike Amazon Elastic Block Storage (EBS) which cannot be shared by instances. Amazon EFS is costlier than storing data in Amazon S3. Also, Amazon EFS needs an Amazon EC2 instance or an AWS Direct Connect network connection. Hence, this is not the correct option.</p>\n\n<p><strong>Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage</strong> - Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other Amazon S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single AZ and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. This is not the right option, since the data stored is business-critical and cannot be risked by using Amazon S3 One Zone-IA.</p>\n\n<p><strong>Use Amazon S3 Glacier Deep Archive storage class to reduce the costs on Amazon S3 storage</strong> - Amazon S3 Glacier Deep Archive storage class has a first-byte latency of several hours, so this option is incorrect for the given use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs</strong>"
      },
      {
        "answer": "",
        "explanation": "The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access."
      },
      {
        "answer": "",
        "explanation": "For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in Amazon S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. There are no retrieval fees when using the Amazon S3 Intelligent-Tiering storage class, and no additional tiering fees when objects are moved between access tiers. It is the ideal storage class for long-lived data with access patterns that are unknown or unpredictable."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Storage Classes can be configured at the object level and a single bucket can contain objects stored in Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One Zone-IA. You can upload objects directly to Amazon S3 Intelligent-Tiering, or use S3 Lifecycle policies to transfer objects from Amazon S3 Standard and Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering. You can also archive objects from Amazon S3 Intelligent-Tiering to Amazon S3 Glacier."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon EFS to provide a fast, cost-effective, and sharable storage service</strong> - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS offers sharable service, unlike Amazon Elastic Block Storage (EBS) which cannot be shared by instances. Amazon EFS is costlier than storing data in Amazon S3. Also, Amazon EFS needs an Amazon EC2 instance or an AWS Direct Connect network connection. Hence, this is not the correct option."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage</strong> - Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other Amazon S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single AZ and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. This is not the right option, since the data stored is business-critical and cannot be risked by using Amazon S3 One Zone-IA."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Glacier Deep Archive storage class to reduce the costs on Amazon S3 storage</strong> - Amazon S3 Glacier Deep Archive storage class has a first-byte latency of several hours, so this option is incorrect for the given use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "id": 39,
    "question": "<p>A retail company wants to establish encrypted network connectivity between its on-premises data center and AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should also support encryption in transit.</p>\n\n<p>Which of the following solutions would you suggest to the company?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud</strong></p>\n\n<p>AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your on-premises network and Amazon VPC over the Internet. IPsec is a protocol suite for securing IP communications by authenticating and encrypting each IP packet in a data stream.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not encrypt your traffic that is in transit. To encrypt the data in transit that traverses AWS Direct Connect, you must use the transit encryption options for that service. As AWS Direct Connect does not support encrypted network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect.</p>\n\n<p><strong>Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud</strong> - AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and AWS. AWS DataSync eliminates or automatically handles many of these tasks, including scripting copy jobs, scheduling, and monitoring transfers, validating data, and optimizing network utilization. As AWS Data Sync cannot be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect.</p>\n\n<p><strong>Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud</strong> - AWS Secrets Manager helps you protect the secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. As AWS Secrets Manager cannot be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/internetwork-traffic-privacy.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/internetwork-traffic-privacy.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your on-premises network and Amazon VPC over the Internet. IPsec is a protocol suite for securing IP communications by authenticating and encrypting each IP packet in a data stream."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not encrypt your traffic that is in transit. To encrypt the data in transit that traverses AWS Direct Connect, you must use the transit encryption options for that service. As AWS Direct Connect does not support encrypted network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud</strong> - AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and AWS. AWS DataSync eliminates or automatically handles many of these tasks, including scripting copy jobs, scheduling, and monitoring transfers, validating data, and optimizing network utilization. As AWS Data Sync cannot be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud</strong> - AWS Secrets Manager helps you protect the secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. As AWS Secrets Manager cannot be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpn/latest/s2svpn/internetwork-traffic-privacy.html",
      "https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html"
    ]
  },
  {
    "id": 40,
    "question": "<p>As part of a workflow, the data engineering team at a company pushes data from Amazon Kinesis Data Firehose to Amazon Simple Storage Service (Amazon S3). However, the team has noticed that Kinesis Data Firehose is creating several small files in the Amazon S3 bucket, as opposed to a much lower expected number of files.</p>\n\n<p>Which of the following would you attribute as the most likely cause behind this issue?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Kinesis Data Firehose delivery stream has scaled</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Data delivery to destination is lagging behind the data being written to the delivery stream</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Compression is disabled on the Kinesis Data Firehose delivery stream</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A single delivery stream is configured to deliver data to multiple Amazon S3 buckets</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Kinesis Data Firehose delivery stream has scaled</strong></p>\n\n<p>If a limit increase was requested or Kinesis Data Firehose has automatically scaled, then the Data Firehose delivery stream can scale. By default, Kinesis Data Firehose automatically scales delivery streams up to a certain limit. Amazon Kinesis' automatic scaling behavior reduces the likelihood of throttling without requiring a limit increase.</p>\n\n<p>When Kinesis Data Firehose's delivery stream scales, it can cause an effect on the buffering hints?? of Data Firehose. The overall buffer size (SizeInMBs) of the delivery stream scales proportionally but inversely. For example, if the capacity of Kinesis Data Firehose increases by two times the original buffer size limit, the buffer size is halved. If Kinesis Data Firehose scales up to four times, the buffer size reduces to one-quarter of the overall buffer size.</p>\n\n<p>There is also a proportional number of parallel buffering within the Kinesis Data Firehose delivery stream, where data is delivered simultaneously from all these buffers. For example, the Kinesis Data Firehose can buffer the data and create a single file based on the buffer size limit. If Kinesis Data Firehose scales to double the buffer limit, then two separate channels will create the files within the same time interval. If Kinesis Data Firehose scales up to four times, there will be four different channels creating four files in S3 during the same time interval.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Data delivery to destination is lagging behind the data being written into the delivery stream</strong> - When data delivery to destination is lagging behind the data being written into the delivery stream, Firehose raises buffer size dynamically to catch up and make sure that all data is delivered to the destination. In these circumstances, the size of delivered S3 objects might be larger than the specified buffer size.</p>\n\n<p><strong>Compression is disabled on the Kinesis Data Firehose delivery stream</strong> - This statement is incorrect. Kinesis Data Firehose delivers smaller records than specified (in the BufferingHints API) if compression is enabled on your Kinesis Data Firehose delivery stream.</p>\n\n<p><strong>A single delivery stream is configured to deliver data to multiple Amazon S3 buckets</strong> - This statement is incorrect. A single delivery stream can only deliver data to only one Amazon S3 bucket.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-small-files-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-small-files-s3/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Kinesis Data Firehose delivery stream has scaled</strong>"
      },
      {
        "answer": "",
        "explanation": "If a limit increase was requested or Kinesis Data Firehose has automatically scaled, then the Data Firehose delivery stream can scale. By default, Kinesis Data Firehose automatically scales delivery streams up to a certain limit. Amazon Kinesis' automatic scaling behavior reduces the likelihood of throttling without requiring a limit increase."
      },
      {
        "answer": "",
        "explanation": "When Kinesis Data Firehose's delivery stream scales, it can cause an effect on the buffering hints?? of Data Firehose. The overall buffer size (SizeInMBs) of the delivery stream scales proportionally but inversely. For example, if the capacity of Kinesis Data Firehose increases by two times the original buffer size limit, the buffer size is halved. If Kinesis Data Firehose scales up to four times, the buffer size reduces to one-quarter of the overall buffer size."
      },
      {
        "answer": "",
        "explanation": "There is also a proportional number of parallel buffering within the Kinesis Data Firehose delivery stream, where data is delivered simultaneously from all these buffers. For example, the Kinesis Data Firehose can buffer the data and create a single file based on the buffer size limit. If Kinesis Data Firehose scales to double the buffer limit, then two separate channels will create the files within the same time interval. If Kinesis Data Firehose scales up to four times, there will be four different channels creating four files in S3 during the same time interval."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Data delivery to destination is lagging behind the data being written into the delivery stream</strong> - When data delivery to destination is lagging behind the data being written into the delivery stream, Firehose raises buffer size dynamically to catch up and make sure that all data is delivered to the destination. In these circumstances, the size of delivered S3 objects might be larger than the specified buffer size."
      },
      {
        "answer": "",
        "explanation": "<strong>Compression is disabled on the Kinesis Data Firehose delivery stream</strong> - This statement is incorrect. Kinesis Data Firehose delivers smaller records than specified (in the BufferingHints API) if compression is enabled on your Kinesis Data Firehose delivery stream."
      },
      {
        "answer": "",
        "explanation": "<strong>A single delivery stream is configured to deliver data to multiple Amazon S3 buckets</strong> - This statement is incorrect. A single delivery stream can only deliver data to only one Amazon S3 bucket."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-small-files-s3/"
    ]
  },
  {
    "id": 41,
    "question": "<p>An Extract, Transform, and Load (ETL) job needs to be implemented which will process data uploaded to an Amazon S3 bucket daily. The uploaded data is in the form of .csv files, each of which is around 100 MB in size.</p>\n\n<p>Which of the following represents the most cost-effective solution for the ETL job?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Run the transformations on the data using AWS Glue Data Studio</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Write an AWS Glue PySpark job. Use Apache Spark to transform the data</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure an AWS Glue Python shell job. Use the pre-loaded Pandas library to run transformations on the data</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Run the transformations on the data using AWS Glue DataBrew</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure an AWS Glue Python shell job. Use the pre-loaded Pandas library to run transformations on the data</strong></p>\n\n<p>AWS Glue ETL supports running plain non-distributed Python scripts as a shell script to run small to medium-sized generic tasks that are often part of an ETL workflow. For example, to submit SQL queries to services such as Amazon Redshift, Amazon Athena, or Amazon EMR, or run machine learning (ML) and scientific analyses.</p>\n\n<p>You can run Python shell jobs using one Data Processing Unit (DPU) or 0.0625 DPU (which is 1/16 DPU), allowing you to run cost-effective small to medium jobs that do not require Spark runtime.</p>\n\n<p>Compared to AWS Lambda, which has a strict 15-minute maximum timeout, AWS Glue Python Shell can be configured with a much longer timeout and higher memory, often required for data engineering jobs.</p>\n\n<p>Python shell jobs in AWS Glue come pre-loaded with libraries such as Boto3, NumPy, SciPy, Pandas, and others.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Write an AWS Glue PySpark job. Use Apache Spark to transform the data</strong> - The AWS Glue PySpark job can certainly be used for the ETL job, however, it is a costlier solution than just using the AWS Glue Python shell job (with Pandas library) for the given use case.</p>\n\n<p><strong>Run the transformations on the data using AWS Glue DataBrew</strong> - AWS Glue DataBrew is a visual data preparation tool that makes it easier for data analysts and data scientists to clean and normalize data to prepare it for analytics and machine learning (ML). AWS Glue DataBrew is not an ETL solution.</p>\n\n<p><strong>Run the transformations on the data using AWS Glue Data Studio</strong> - AWS Glue Studio is a graphical interface that makes it easy to create, run, and monitor data integration jobs in AWS Glue. You can visually compose data transformation workflows and seamlessly run them on the Apache Spark-based serverless ETL engine in AWS Glue. Glue Data Studio is not an ETL solution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/add-job-python.html\">https://docs.aws.amazon.com/glue/latest/dg/add-job-python.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-performant-data-pipeline/additional-considerations.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-performant-data-pipeline/additional-considerations.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure an AWS Glue Python shell job. Use the pre-loaded Pandas library to run transformations on the data</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Glue ETL supports running plain non-distributed Python scripts as a shell script to run small to medium-sized generic tasks that are often part of an ETL workflow. For example, to submit SQL queries to services such as Amazon Redshift, Amazon Athena, or Amazon EMR, or run machine learning (ML) and scientific analyses."
      },
      {
        "answer": "",
        "explanation": "You can run Python shell jobs using one Data Processing Unit (DPU) or 0.0625 DPU (which is 1/16 DPU), allowing you to run cost-effective small to medium jobs that do not require Spark runtime."
      },
      {
        "answer": "",
        "explanation": "Compared to AWS Lambda, which has a strict 15-minute maximum timeout, AWS Glue Python Shell can be configured with a much longer timeout and higher memory, often required for data engineering jobs."
      },
      {
        "answer": "",
        "explanation": "Python shell jobs in AWS Glue come pre-loaded with libraries such as Boto3, NumPy, SciPy, Pandas, and others."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Write an AWS Glue PySpark job. Use Apache Spark to transform the data</strong> - The AWS Glue PySpark job can certainly be used for the ETL job, however, it is a costlier solution than just using the AWS Glue Python shell job (with Pandas library) for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Run the transformations on the data using AWS Glue DataBrew</strong> - AWS Glue DataBrew is a visual data preparation tool that makes it easier for data analysts and data scientists to clean and normalize data to prepare it for analytics and machine learning (ML). AWS Glue DataBrew is not an ETL solution."
      },
      {
        "answer": "",
        "explanation": "<strong>Run the transformations on the data using AWS Glue Data Studio</strong> - AWS Glue Studio is a graphical interface that makes it easy to create, run, and monitor data integration jobs in AWS Glue. You can visually compose data transformation workflows and seamlessly run them on the Apache Spark-based serverless ETL engine in AWS Glue. Glue Data Studio is not an ETL solution."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/add-job-python.html",
      "https://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-performant-data-pipeline/additional-considerations.html"
    ]
  },
  {
    "id": 42,
    "question": "<p>A subscription streaming service delivers billions of hours of content from Amazon S3 to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline.</p>\n\n<p>Which of the following is the MOST cost-effective option to store this intermediary query data?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the intermediary query results in the S3 Standard-Infrequent Access storage class</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store the intermediary query results in the S3 Glacier Instant Retrieval storage class</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Store the intermediary query results in the S3 One Zone-Infrequent Access storage class</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the intermediary query results in the S3 Standard storage class</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the intermediary query results in the S3 Standard storage class</strong></p>\n\n<p>S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\nAs there is no minimum storage duration charge and no retrieval fee (remember that intermediary query results are heavily referenced by other parts of the analytics pipeline), this is the MOST cost-effective storage class amongst the given options.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q25-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q25-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the intermediary query results in the S3 Glacier Instant Retrieval storage class</strong> - S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives.</p>\n\n<p>The minimum storage duration charge is 90 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.</p>\n\n<p><strong>Store the intermediary query results in the S3 Standard-Infrequent Access storage class</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.</p>\n\n<p><strong>Store the intermediary query results in the S3 One Zone-Infrequent Access storage class</strong> - S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.</p>\n\n<p>To summarize again, S3 Standard-IA and S3 One Zone-IA have a minimum storage duration charge of 30 days (so instead of 24 hours, you end up paying for 30 days). S3 Standard-IA and S3 One Zone-IA also have retrieval charges (as the results are heavily referenced by other parts of the analytics pipeline, so the retrieval costs would be pretty high). Therefore, these storage classes are not cost-optimal for the given use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the intermediary query results in the S3 Standard storage class</strong>"
      },
      {
        "answer": "",
        "explanation": "S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\nAs there is no minimum storage duration charge and no retrieval fee (remember that intermediary query results are heavily referenced by other parts of the analytics pipeline), this is the MOST cost-effective storage class amongst the given options."
      },
      {
        "link": "https://aws.amazon.com/s3/storage-classes/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the intermediary query results in the S3 Glacier Instant Retrieval storage class</strong> - S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives."
      },
      {
        "answer": "",
        "explanation": "The minimum storage duration charge is 90 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Store the intermediary query results in the S3 Standard-Infrequent Access storage class</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Store the intermediary query results in the S3 One Zone-Infrequent Access storage class</strong> - S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct."
      },
      {
        "answer": "",
        "explanation": "To summarize again, S3 Standard-IA and S3 One Zone-IA have a minimum storage duration charge of 30 days (so instead of 24 hours, you end up paying for 30 days). S3 Standard-IA and S3 One Zone-IA also have retrieval charges (as the results are heavily referenced by other parts of the analytics pipeline, so the retrieval costs would be pretty high). Therefore, these storage classes are not cost-optimal for the given use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "id": 43,
    "question": "<p>A media company captures browsing metadata to contextually display relevant images, pages, and links to targeted users. A single page load can generate multiple events that need to be stored individually. Each page load must query the user's browsing history to provide targeted recommendations. The company expects over 1 million page visits per day. The startup now wants to add a caching layer to support high read volumes.</p>\n\n<p>Which of the following AWS services would you recommend as a caching layer for this use case? (Select two)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>ElastiCache</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Redshift</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>DynamoDB Accelerator (DAX)</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>RDS</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Elasticsearch</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. Therefore, this is a correct option.</p>\n\n<p>DAX Overview:\n<img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html</a><p></p>\n\n<p><strong>ElastiCache</strong> - Amazon ElastiCache for Memcached is an ideal front-end for data stores like Amazon RDS or Amazon DynamoDB, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. Therefore, this is also a correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>RDS</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. RDS cannot be used as a caching layer.</p>\n\n<p><strong>Elasticsearch</strong> - Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It cannot be used as a caching layer.</p>\n\n<p><strong>Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis. It cannot be used as a caching layer.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/faqs/\">https://aws.amazon.com/elasticache/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. Therefore, this is a correct option."
      },
      {
        "image": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png",
        "answer": "",
        "explanation": "DAX Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html"
      },
      {
        "answer": "",
        "explanation": "<strong>ElastiCache</strong> - Amazon ElastiCache for Memcached is an ideal front-end for data stores like Amazon RDS or Amazon DynamoDB, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. Therefore, this is also a correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>RDS</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. RDS cannot be used as a caching layer."
      },
      {
        "answer": "",
        "explanation": "<strong>Elasticsearch</strong> - Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It cannot be used as a caching layer."
      },
      {
        "answer": "",
        "explanation": "<strong>Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis. It cannot be used as a caching layer."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html",
      "https://aws.amazon.com/dynamodb/dax/",
      "https://aws.amazon.com/elasticache/faqs/"
    ]
  },
  {
    "id": 44,
    "question": "<p>A financial services company has recently migrated from on-premises infrastructure to AWS Cloud. The data engineering team wants to implement a solution that allows all resource configurations to be reviewed and make sure that they meet compliance guidelines. Also, the solution should be able to offer the capability to look into the resource configuration history across the application stack.</p>\n\n<p>Which of the following solutions would you recommend to the team?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</strong></p>\n\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”</p>\n\n<p>How AWS Config Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</strong> - AWS CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. You cannot use Amazon CloudWatch to maintain a history of resource configuration changes.</p>\n\n<p><strong>Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</strong> - With AWS CloudTrail, you can log, continuously monitor and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. AWS CloudTrail provides an event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You cannot use AWS CloudTrail to maintain a history of resource configuration changes.</p>\n\n<p><strong>Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</strong> - Using AWS Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. You cannot use AWS Systems Manager to maintain a history of resource configuration changes.</p>\n\n<p>Exam Alert:</p>\n\n<p>You may see scenario-based questions asking you to select one of Amazon CloudWatch vs AWS CloudTrail vs AWS Config. Just remember the following -</p>\n\n<p>Think resource performance monitoring, events, and alerts; think Amazon CloudWatch.</p>\n\n<p>Think account-specific activity and audit; think AWS CloudTrail.</p>\n\n<p>Think resource-specific history, audit, and compliance; think AWS Config.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/\">https://aws.amazon.com/cloudwatch/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”"
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png",
        "answer": "",
        "explanation": "How AWS Config Works:"
      },
      {
        "link": "https://aws.amazon.com/config/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</strong> - AWS CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. You cannot use Amazon CloudWatch to maintain a history of resource configuration changes."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</strong> - With AWS CloudTrail, you can log, continuously monitor and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. AWS CloudTrail provides an event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You cannot use AWS CloudTrail to maintain a history of resource configuration changes."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</strong> - Using AWS Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. You cannot use AWS Systems Manager to maintain a history of resource configuration changes."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "answer": "",
        "explanation": "You may see scenario-based questions asking you to select one of Amazon CloudWatch vs AWS CloudTrail vs AWS Config. Just remember the following -"
      },
      {
        "answer": "",
        "explanation": "Think resource performance monitoring, events, and alerts; think Amazon CloudWatch."
      },
      {
        "answer": "",
        "explanation": "Think account-specific activity and audit; think AWS CloudTrail."
      },
      {
        "answer": "",
        "explanation": "Think resource-specific history, audit, and compliance; think AWS Config."
      }
    ],
    "references": [
      "https://aws.amazon.com/config/",
      "https://aws.amazon.com/cloudwatch/",
      "https://aws.amazon.com/cloudtrail/",
      "https://aws.amazon.com/systems-manager/"
    ]
  },
  {
    "id": 45,
    "question": "<p>A data engineer is configuring an AWS Step Function for a banking system workflow. The workflow must process large amounts of data files in parallel and it should simultaneously apply transformations for each file.</p>\n\n<p>Which Step Functions state is the right fit for this requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Concurrent state</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Parallel state</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Map state</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Choice state</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Map state</strong></p>\n\n<p>You can use the Map state to run a set of workflow steps for each item in a dataset. The Map state's iterations run in parallel, which makes it possible to process a dataset quickly. Map states can use a variety of input types, including a JSON array, a list of Amazon S3 objects, or a CSV file.</p>\n\n<p>Step Functions provides two types of processing modes for using the Map state in your workflows: inline mode and distributed mode.</p>\n\n<p>By default, Map states run in inline mode. In inline mode, the Map state accepts only a JSON array as input. In this mode, the Map state supports up to 40 concurrent iterations.</p>\n\n<p>In distributed mode, the Map state allows high-concurrency processing. In distributed mode, the Map state processes the items in the dataset in iterations called child workflow executions. You can specify the number of child workflow executions that can run in parallel. Each child workflow execution has its own, separate execution history from that of the parent workflow. If you don't specify, Step Functions runs 10,000 parallel child workflow executions in parallel.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Parallel state</strong> - The Parallel state (\"Type\": \"Parallel\") can be used to add separate branches of execution in your state machine. A <code>Parallel</code> state causes AWS Step Functions to execute each branch, starting with the state named in that branch's <code>StartAt</code> field, as concurrently as possible, and wait until all branches terminate (reach a terminal state) before processing the <code>Parallel</code> state's <code>Next</code> field.</p>\n\n<p><strong>Choice state</strong> - A Choice state (\"Type\": \"Choice\") adds conditional logic to a state machine. This state contains an array of Choice Rules that determine which state the state machine transitions to next. You use a comparison operator in a Choice Rule to compare an input variable with a specific value. For example, using Choice Rules you can compare if an input variable is greater than or less than 100.</p>\n\n<p><strong>Concurrent state</strong> - This state does not exist. This is a made-up option that acts as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-map-state.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-map-state.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Map state</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use the Map state to run a set of workflow steps for each item in a dataset. The Map state's iterations run in parallel, which makes it possible to process a dataset quickly. Map states can use a variety of input types, including a JSON array, a list of Amazon S3 objects, or a CSV file."
      },
      {
        "answer": "",
        "explanation": "Step Functions provides two types of processing modes for using the Map state in your workflows: inline mode and distributed mode."
      },
      {
        "answer": "",
        "explanation": "By default, Map states run in inline mode. In inline mode, the Map state accepts only a JSON array as input. In this mode, the Map state supports up to 40 concurrent iterations."
      },
      {
        "answer": "",
        "explanation": "In distributed mode, the Map state allows high-concurrency processing. In distributed mode, the Map state processes the items in the dataset in iterations called child workflow executions. You can specify the number of child workflow executions that can run in parallel. Each child workflow execution has its own, separate execution history from that of the parent workflow. If you don't specify, Step Functions runs 10,000 parallel child workflow executions in parallel."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Parallel state</strong> - The Parallel state (\"Type\": \"Parallel\") can be used to add separate branches of execution in your state machine. A <code>Parallel</code> state causes AWS Step Functions to execute each branch, starting with the state named in that branch's <code>StartAt</code> field, as concurrently as possible, and wait until all branches terminate (reach a terminal state) before processing the <code>Parallel</code> state's <code>Next</code> field."
      },
      {
        "answer": "",
        "explanation": "<strong>Choice state</strong> - A Choice state (\"Type\": \"Choice\") adds conditional logic to a state machine. This state contains an array of Choice Rules that determine which state the state machine transitions to next. You use a comparison operator in a Choice Rule to compare an input variable with a specific value. For example, using Choice Rules you can compare if an input variable is greater than or less than 100."
      },
      {
        "answer": "",
        "explanation": "<strong>Concurrent state</strong> - This state does not exist. This is a made-up option that acts as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-map-state.html"
    ]
  },
  {
    "id": 46,
    "question": "<p>A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with Amazon S3 bucket settings being changed regularly.</p>\n\n<p>How can you figure out what's happening without restricting the rights of the users?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS CloudTrail to analyze the API calls made to Amazon S3</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Implement an IAM policy to forbid users from changing Amazon S3 bucket settings</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 access logs to analyze user access using Athena</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS CloudTrail to analyze the API calls made to Amazon S3</strong></p>\n\n<p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With AWS CloudTrail, you can log, continuously monitor and retain account activity related to actions across your AWS infrastructure. AWS CloudTrail provides the event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services.</p>\n\n<p>In general, to analyze any API calls made within an AWS account, AWS CloudTrail is used. You can record the actions that are taken by users, roles, or AWS services on Amazon S3 resources and maintain log records for auditing and compliance purposes. To do this, you can use server access logging, AWS CloudTrail logging, or a combination of both. AWS recommends that you use AWS CloudTrail for logging bucket and object-level actions for your Amazon S3 resources.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Implement an IAM policy to forbid users from changing Amazon S3 bucket settings</strong> - You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines its permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, AWS Organizations service control policy (SCP), access control list (ACL), and session policies.</p>\n\n<p>Implementing an IAM policy to forbid users would be disruptive and wouldn't go unnoticed.</p>\n\n<p><strong>Use Amazon S3 access logs to analyze user access using Athena</strong> - Amazon S3 server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill. AWS recommends that you use AWS CloudTrail for logging bucket and object-level actions for your Amazon S3 resources, as it provides more options to store, analyze, and act on the log information.</p>\n\n<p><strong>Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations</strong> - Amazon S3 supports MFA-protected API access, a feature that can enforce multi-factor authentication (MFA) for access to your Amazon S3 resources. Multi-factor authentication provides an extra level of security that you can apply to your AWS environment. It is a security feature that requires users to prove the physical possession of an MFA device by providing a valid MFA code. Changing the bucket policy to require MFA would not go unnoticed.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-7\">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-7</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS CloudTrail to analyze the API calls made to Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With AWS CloudTrail, you can log, continuously monitor and retain account activity related to actions across your AWS infrastructure. AWS CloudTrail provides the event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services."
      },
      {
        "answer": "",
        "explanation": "In general, to analyze any API calls made within an AWS account, AWS CloudTrail is used. You can record the actions that are taken by users, roles, or AWS services on Amazon S3 resources and maintain log records for auditing and compliance purposes. To do this, you can use server access logging, AWS CloudTrail logging, or a combination of both. AWS recommends that you use AWS CloudTrail for logging bucket and object-level actions for your Amazon S3 resources."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Implement an IAM policy to forbid users from changing Amazon S3 bucket settings</strong> - You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines its permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, AWS Organizations service control policy (SCP), access control list (ACL), and session policies."
      },
      {
        "answer": "",
        "explanation": "Implementing an IAM policy to forbid users would be disruptive and wouldn't go unnoticed."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 access logs to analyze user access using Athena</strong> - Amazon S3 server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill. AWS recommends that you use AWS CloudTrail for logging bucket and object-level actions for your Amazon S3 resources, as it provides more options to store, analyze, and act on the log information."
      },
      {
        "answer": "",
        "explanation": "<strong>Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations</strong> - Amazon S3 supports MFA-protected API access, a feature that can enforce multi-factor authentication (MFA) for access to your Amazon S3 resources. Multi-factor authentication provides an extra level of security that you can apply to your AWS environment. It is a security feature that requires users to prove the physical possession of an MFA device by providing a valid MFA code. Changing the bucket policy to require MFA would not go unnoticed."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html",
      "https://aws.amazon.com/cloudtrail/",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-7"
    ]
  },
  {
    "id": 47,
    "question": "<p>A company wants to store data from Athena CTAS (<code>CREATE TABLE AS SELECT</code>) query results in Amazon S3. A data engineer wants to understand the distinction between partitioning vs bucketing for storing data via such CTAS queries.</p>\n\n<p>As an AWS Certified Data Engineer Associate, which of the following options would you identify as the right fit for choosing bucketing vs partitioning? (Select two)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Bucketing CTAS query results works well when you bucket data by the column that has low cardinality and evenly distributed values</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and evenly distributed values</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Partitioning CTAS query results works well when the number of partitions you plan to have is limited and the partitioned columns have high cardinality</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Partitioning CTAS query results works well when the number of partitions you plan to have is limited and the partitioned columns have low cardinality</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and sparsely distributed values</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Partitioning CTAS query results works well when the number of partitions you plan to have is limited and the partitioned columns have low cardinality</strong></p>\n\n<p><strong>Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and evenly distributed values</strong></p>\n\n<p>By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but that is loaded only once per day, might partition by a data source identifier and date.</p>\n\n<p>When you run a CTAS query, Athena writes the results to a specified location in Amazon S3. If you specify partitions, it creates them and stores each partition in a separate partition folder in the same location. Having partitions in Amazon S3 helps with Athena query performance because this helps you run targeted queries for only specific partitions. As a best practice, you should partition data by those columns that have similar characteristics, such as records from the same department, and that can have a limited number of possible values (low cardinality), such as a limited number of distinct departments in an organization.</p>\n\n<p>Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and evenly distributed values. For example, columns storing timestamp data could potentially have a very large number of distinct values, and their data is evenly distributed across the data set. To choose the column by which to bucket the CTAS query results, use the column that has a high number of values (high cardinality) and whose data can be split for storage into many buckets that will have roughly the same amount of data. Columns that are sparsely populated with values are not good candidates for bucketing.</p>\n\n<p>You should also note that you can partition and use bucketing for storing the results of the same CTAS query. These techniques for writing data do not exclude each other. Typically, the columns you use for bucketing differ from those you use for partitioning.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q20-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q20-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/bucketing-vs-partitioning.html\">https://docs.aws.amazon.com/athena/latest/ug/bucketing-vs-partitioning.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Partitioning CTAS query results works well when the number of partitions you plan to have is limited and the partitioned columns have high cardinality</strong></p>\n\n<p><strong>Bucketing CTAS query results works well when you bucket data by the column that has low cardinality and evenly distributed values</strong></p>\n\n<p><strong>Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and sparsely distributed values</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/bucketing-vs-partitioning.html\">https://docs.aws.amazon.com/athena/latest/ug/bucketing-vs-partitioning.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Partitioning CTAS query results works well when the number of partitions you plan to have is limited and the partitioned columns have low cardinality</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and evenly distributed values</strong>"
      },
      {
        "answer": "",
        "explanation": "By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but that is loaded only once per day, might partition by a data source identifier and date."
      },
      {
        "answer": "",
        "explanation": "When you run a CTAS query, Athena writes the results to a specified location in Amazon S3. If you specify partitions, it creates them and stores each partition in a separate partition folder in the same location. Having partitions in Amazon S3 helps with Athena query performance because this helps you run targeted queries for only specific partitions. As a best practice, you should partition data by those columns that have similar characteristics, such as records from the same department, and that can have a limited number of possible values (low cardinality), such as a limited number of distinct departments in an organization."
      },
      {
        "answer": "",
        "explanation": "Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and evenly distributed values. For example, columns storing timestamp data could potentially have a very large number of distinct values, and their data is evenly distributed across the data set. To choose the column by which to bucket the CTAS query results, use the column that has a high number of values (high cardinality) and whose data can be split for storage into many buckets that will have roughly the same amount of data. Columns that are sparsely populated with values are not good candidates for bucketing."
      },
      {
        "answer": "",
        "explanation": "You should also note that you can partition and use bucketing for storing the results of the same CTAS query. These techniques for writing data do not exclude each other. Typically, the columns you use for bucketing differ from those you use for partitioning."
      },
      {
        "link": "https://docs.aws.amazon.com/athena/latest/ug/bucketing-vs-partitioning.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Partitioning CTAS query results works well when the number of partitions you plan to have is limited and the partitioned columns have high cardinality</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Bucketing CTAS query results works well when you bucket data by the column that has low cardinality and evenly distributed values</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and sparsely distributed values</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/bucketing-vs-partitioning.html",
      "https://docs.aws.amazon.com/athena/latest/ug/partitions.html"
    ]
  },
  {
    "id": 48,
    "question": "<p>The data engineering team at an e-commerce company is doing a root-cause analysis for a recent spike in CPU utilization for an RDS MySQL DB instance that caused the application to perform poorly. The standard metrics available in Amazon CloudWatch are not enough to guide the investigation. The company has hired you as an AWS Certified Data Engineer Associate to determine what caused the CPU spike.</p>\n\n<p>Which of the following steps would you recommend to provide more details about the underlying processes and queries resulting in an increase in the CPU load? (Select two)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Activate Enhanced Monitoring to view CPU utilization information for the RDS MySQL DB instance</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable RDS Performance Insights and review the appropriate dashboard to visualize the database load and filter the load by waits, SQL statements, hosts, or users</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Implement ElastiCache in front of the RDS DB instance to reduce the CPU load on the RDS instance</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Activate Amazon CloudWatch Events and review the event data that has the SQL statements behind the CPU spikes</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use Amazon Athena to analyze the SQL statements being run on the RDS instance</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p>Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon RDS. You can monitor network throughput, client connections, I/O for read, write, or metadata operations, and burst credit balances for your DB instances. AWS provides various tools that you can use to monitor Amazon RDS.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q35-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q35-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html</a><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q35-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q35-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html</a><p></p>\n\n<p><strong>Activate Enhanced Monitoring to view CPU utilization information for the RDS MySQL DB instance</strong></p>\n\n<p>You can use Enhanced Monitoring to capture metrics in real-time for the operating system (OS) that your DB instance runs on. Enhanced Monitoring metrics are useful for the given use case as it allows you to see how the different processes or threads on a DB instance use the CPU. By default, Enhanced Monitoring metrics are stored for 30 days in the CloudWatch Logs, which are different from typical CloudWatch metrics.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q35-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q35-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html</a><p></p>\n\n<p><strong>Enable RDS Performance Insights and review the appropriate dashboard to visualize the database load and filter the load by waits, SQL statements, hosts, or users</strong></p>\n\n<p>Performance Insights allows you to understand your database's performance and help you analyze any issues that affect it. With the Performance Insights dashboard, you can visualize the database load and filter the load by waits, SQL statements, hosts, or users.</p>\n\n<p>You can use Performance Insights for the given use case as it allows you to see the underlying SQL statements causing an increase in the database load.</p>\n\n<p>The central metric for Performance Insights is DB Load, which represents the average number of active sessions for the DB engine. The DB Load metric is collected every second. An active session is a connection that has submitted work to the DB engine and is waiting for a response. For example, if you submit a SQL query to the DB engine, the database session is active while the DB engine is processing the query.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q35-i4.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q35-i4.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Implement ElastiCache in front of the RDS DB instance to reduce the CPU load on the RDS instance</strong> - You can apply a band-aid on the underlying issue by using ElastiCache as it can potentially decrease the CPU load on the RDS DB instance, however, it will present no insights about the underlying processes and queries resulting in an increase in the CPU load.</p>\n\n<p><strong>Use Amazon Athena to analyze the SQL statements being run on the RDS instance</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>Recently Athena has introduced federated queries that can be used to quickly analyze records from different sources such as RDS MySQL. However, you still cannot use it to analyze the SQL statements being run on the RDS instance.</p>\n\n<p><strong>Activate Amazon CloudWatch Events and review the event data that has the SQL statements behind the CPU spikes</strong> - This is a made-up option as CloudWatch Events do not contain SQL statements in the event data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon RDS. You can monitor network throughput, client connections, I/O for read, write, or metadata operations, and burst credit balances for your DB instances. AWS provides various tools that you can use to monitor Amazon RDS."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Activate Enhanced Monitoring to view CPU utilization information for the RDS MySQL DB instance</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use Enhanced Monitoring to capture metrics in real-time for the operating system (OS) that your DB instance runs on. Enhanced Monitoring metrics are useful for the given use case as it allows you to see how the different processes or threads on a DB instance use the CPU. By default, Enhanced Monitoring metrics are stored for 30 days in the CloudWatch Logs, which are different from typical CloudWatch metrics."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Enable RDS Performance Insights and review the appropriate dashboard to visualize the database load and filter the load by waits, SQL statements, hosts, or users</strong>"
      },
      {
        "answer": "",
        "explanation": "Performance Insights allows you to understand your database's performance and help you analyze any issues that affect it. With the Performance Insights dashboard, you can visualize the database load and filter the load by waits, SQL statements, hosts, or users."
      },
      {
        "answer": "",
        "explanation": "You can use Performance Insights for the given use case as it allows you to see the underlying SQL statements causing an increase in the database load."
      },
      {
        "answer": "",
        "explanation": "The central metric for Performance Insights is DB Load, which represents the average number of active sessions for the DB engine. The DB Load metric is collected every second. An active session is a connection that has submitted work to the DB engine and is waiting for a response. For example, if you submit a SQL query to the DB engine, the database session is active while the DB engine is processing the query."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Implement ElastiCache in front of the RDS DB instance to reduce the CPU load on the RDS instance</strong> - You can apply a band-aid on the underlying issue by using ElastiCache as it can potentially decrease the CPU load on the RDS DB instance, however, it will present no insights about the underlying processes and queries resulting in an increase in the CPU load."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Athena to analyze the SQL statements being run on the RDS instance</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries."
      },
      {
        "answer": "",
        "explanation": "Recently Athena has introduced federated queries that can be used to quickly analyze records from different sources such as RDS MySQL. However, you still cannot use it to analyze the SQL statements being run on the RDS instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Activate Amazon CloudWatch Events and review the event data that has the SQL statements behind the CPU spikes</strong> - This is a made-up option as CloudWatch Events do not contain SQL statements in the event data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html"
    ]
  },
  {
    "id": 49,
    "question": "<p>A data engineer is tasked with using AWS services to ingest a dataset into an Amazon S3 data lake. Upon profiling the dataset manually, the engineer finds that it contains personally identifiable information (PII). The engineer needs to devise a method to both profile the dataset and mask the PII effectively.</p>\n\n<p>Which of the following options can be independently used to fulfill this requirement with the least amount of operational effort? (Select two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Utilize the Detect PII transform in AWS Glue Studio to identify the PII. Set up a rule in AWS Glue Data Quality to mask the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Utilize the Detect PII transform in AWS Glue Studio to identify and mask the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Run an AWS Glue DataBrew data profile job to identify and suggest potential PII columns present in the dataset. Execute a DataBrew job to apply the transformations to handle the sensitive columns in the entire dataset and store the masked data securely in Amazon S3</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Ingest the dataset into Amazon Kinesis Data Streams. Process the stream data using a Lambda function that masks the PII data before writing the output in Amazon S3</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Ingest the dataset into Amazon Kinesis Data Firehose. Leverage a data transformation Lambda function to mask the PII data in the delivery stream and write the transformed output in Amazon S3</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Utilize the Detect PII transform in AWS Glue Studio to identify and mask the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake</strong></p>\n\n<p>The Detect PII transform identifies Personal Identifiable Information (PII) in your data source. You choose the PII entity to identify, how you want the data to be scanned, and what to do with the PII entity that has been identified by the Detect PII transform.</p>\n\n<p>The Detect PII transform provides the ability to detect, mask, or remove entities that you define, or are pre-defined by AWS. This enables you to increase compliance and reduce liability. For example, you may want to ensure that no personally identifiable information exists in your data that can be read and want to mask social security numbers with a fixed string (such as xxx-xx-xxxx), phone numbers, or addresses. You can then use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q6-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q6-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html\">https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html</a><p></p>\n\n<p><strong>Run an AWS Glue DataBrew data profile job to identify and suggest potential PII columns present in the dataset. Execute a DataBrew job to apply the transformations to handle the sensitive columns in the entire dataset and store the masked data securely in Amazon S3</strong></p>\n\n<p>AWS Glue DataBrew, a visual data preparation tool, allows users to identify and handle sensitive data by applying advanced transformations like redaction, replacement, encryption, and decryption on their personally identifiable information (PII) data, and other types of data they deem sensitive. DataBrew has PII data handling transformations, which enable you to apply data masking, encryption, decryption, and other operations on your sensitive data.</p>\n\n<p>You can run a data profile job to identify and suggest potential PII columns present in a dataset. Then, you can target PII columns in a DataBrew project and apply various transformations to handle the sensitive columns existing in the dataset. Finally, run a DataBrew job to apply the transformations on the entire dataset and store the processed, masked, and encrypted data securely in Amazon S3.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q6-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q6-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html\">https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Ingest the dataset into Amazon Kinesis Data Firehose. Leverage a data transformation Lambda function to mask the PII data in the delivery stream and write the transformed output in Amazon S3</strong></p>\n\n<p><strong>Ingest the dataset into Amazon Kinesis Data Streams. Process the stream data using a Lambda function that masks the PII data before writing the output in Amazon S3</strong></p>\n\n<p>You will need to write custom code in the Lambda function to mask the PII data, which represents unnecessary operational effort for the given use case, so both these options are incorrect.</p>\n\n<p><strong>Utilize the Detect PII transform in AWS Glue Studio to identify the PII. Set up a rule in AWS Glue Data Quality to mask the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake</strong> - AWS Glue Data Quality is a feature of AWS Glue that reduces manual data quality effort by automatically measuring and monitoring the quality of data in data lakes and pipelines. In data lakes, it then automatically recommends data quality rules. You can modify these rules, add additional rules from built-in rule types, and configure actions to alert teams when quality issues occur.</p>\n\n<p>AWS Glue Data Quality currently supports 18 built-in rule types under four categories:</p>\n\n<p>Consistency rules check if data across different columns agrees by looking at column correlations.\nAccuracy rules check if record counts meet a set threshold and if columns are not empty, match certain patterns, have valid data types, and have valid values.\nIntegrity rules check if duplicates exist in a dataset.\nCompleteness rules check if data in your datasets do not have missing values.</p>\n\n<p>AWS Glue Data Quality does not have any pre-built rule to mask the PII. You could create a custom rule to mask the PII, but that involves additional operational effort, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/introducing-pii-data-identification-and-handling-using-aws-glue-databrew/\">https://aws.amazon.com/blogs/big-data/introducing-pii-data-identification-and-handling-using-aws-glue-databrew/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html\">https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/glue-data-quality.html\">https://docs.aws.amazon.com/glue/latest/dg/glue-data-quality.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Utilize the Detect PII transform in AWS Glue Studio to identify and mask the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake</strong>"
      },
      {
        "answer": "",
        "explanation": "The Detect PII transform identifies Personal Identifiable Information (PII) in your data source. You choose the PII entity to identify, how you want the data to be scanned, and what to do with the PII entity that has been identified by the Detect PII transform."
      },
      {
        "answer": "",
        "explanation": "The Detect PII transform provides the ability to detect, mask, or remove entities that you define, or are pre-defined by AWS. This enables you to increase compliance and reduce liability. For example, you may want to ensure that no personally identifiable information exists in your data that can be read and want to mask social security numbers with a fixed string (such as xxx-xx-xxxx), phone numbers, or addresses. You can then use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake."
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Run an AWS Glue DataBrew data profile job to identify and suggest potential PII columns present in the dataset. Execute a DataBrew job to apply the transformations to handle the sensitive columns in the entire dataset and store the masked data securely in Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Glue DataBrew, a visual data preparation tool, allows users to identify and handle sensitive data by applying advanced transformations like redaction, replacement, encryption, and decryption on their personally identifiable information (PII) data, and other types of data they deem sensitive. DataBrew has PII data handling transformations, which enable you to apply data masking, encryption, decryption, and other operations on your sensitive data."
      },
      {
        "answer": "",
        "explanation": "You can run a data profile job to identify and suggest potential PII columns present in a dataset. Then, you can target PII columns in a DataBrew project and apply various transformations to handle the sensitive columns existing in the dataset. Finally, run a DataBrew job to apply the transformations on the entire dataset and store the processed, masked, and encrypted data securely in Amazon S3."
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Ingest the dataset into Amazon Kinesis Data Firehose. Leverage a data transformation Lambda function to mask the PII data in the delivery stream and write the transformed output in Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Ingest the dataset into Amazon Kinesis Data Streams. Process the stream data using a Lambda function that masks the PII data before writing the output in Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "You will need to write custom code in the Lambda function to mask the PII data, which represents unnecessary operational effort for the given use case, so both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Utilize the Detect PII transform in AWS Glue Studio to identify the PII. Set up a rule in AWS Glue Data Quality to mask the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake</strong> - AWS Glue Data Quality is a feature of AWS Glue that reduces manual data quality effort by automatically measuring and monitoring the quality of data in data lakes and pipelines. In data lakes, it then automatically recommends data quality rules. You can modify these rules, add additional rules from built-in rule types, and configure actions to alert teams when quality issues occur."
      },
      {
        "answer": "",
        "explanation": "AWS Glue Data Quality currently supports 18 built-in rule types under four categories:"
      },
      {
        "answer": "",
        "explanation": "Consistency rules check if data across different columns agrees by looking at column correlations.\nAccuracy rules check if record counts meet a set threshold and if columns are not empty, match certain patterns, have valid data types, and have valid values.\nIntegrity rules check if duplicates exist in a dataset.\nCompleteness rules check if data in your datasets do not have missing values."
      },
      {
        "answer": "",
        "explanation": "AWS Glue Data Quality does not have any pre-built rule to mask the PII. You could create a custom rule to mask the PII, but that involves additional operational effort, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html",
      "https://aws.amazon.com/blogs/big-data/introducing-pii-data-identification-and-handling-using-aws-glue-databrew/",
      "https://docs.aws.amazon.com/glue/latest/dg/glue-data-quality.html"
    ]
  },
  {
    "id": 50,
    "question": "<p>A company uses AWS services such as Amazon Redshift and Amazon S3 as well as their on-premises SQL Server database to store the consumer data. The company also uses Salesforce as its SaaS application. The company wants to build a dashboard that will help the managers visualize the data points from all these systems.</p>\n\n<p>Which of the following represents a simple and easy way to build the dashboard in the least possible time?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the federated queries feature of Amazon Athena to join the different data sources and visualize the data using Amazon QuickSight</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Lake Formation to migrate the data sources into Amazon S3. Configure AWS Glue Data Catalog to connect the S3 data to Amazon Athena for further analysis and visualizations. Use a Glue Crawler to automate the process</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Lake Formation to migrate the data sources into Amazon S3. Use Amazon QuickSight to generate the visualizations needed for the dashboards</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure Amazon QuickSight to connect to the data sources and generate the visualizations needed for the dashboard</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon QuickSight to connect to the data sources and generate the visualizations needed for the dashboard</strong></p>\n\n<p>Amazon QuickSight is a very fast, easy-to-use, cloud-powered business analytics service that makes it easy for all employees within an organization to build visualizations, perform ad-hoc analysis, and quickly get business insights from their data, anytime, on any device.</p>\n\n<p>QuickSight can connect to AWS data sources including Amazon RDS, Amazon Aurora, Amazon Redshift, Amazon Athena, and Amazon S3. You can also upload Excel spreadsheets or flat files (CSV, TSV, CLF, and ELF), connect to on-premises databases like SQL Server, MySQL, and PostgreSQL, and import data from SaaS applications like Salesforce.</p>\n\n<p>QuickSight Key Features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q30-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q30-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/quicksight/\">https://aws.amazon.com/quicksight/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the federated queries feature of Amazon Athena to join the different data sources and visualize the data using Amazon QuickSight</strong> - Amazon Athena supports federated query, a feature that allows you to query data in sources other than Amazon Simple Storage Service (Amazon S3). You can use federated queries in Athena to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3.</p>\n\n<p>You can use different connectors to run federated queries with complex joins across different data sources with Athena and visualize the data with Amazon QuickSight. This option requires significant query development effort to join the data sources, so it's not the right fit for the given use case.</p>\n\n<p><strong>Use AWS Lake Formation to migrate the data sources into Amazon S3. Use Amazon QuickSight to generate the visualizations needed for the dashboards</strong></p>\n\n<p><strong>Use AWS Lake Formation to migrate the data sources into Amazon S3. Configure AWS Glue Data Catalog to connect the S3 data to Amazon Athena for further analysis and visualizations. Use a Glue Crawler to automate the process</strong></p>\n\n<p>Both these options require a lot of heavy lifting to migrate data from the different sources into S3. These options are unnecessarily complex and hence not the best fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/quicksight/resources/faqs/\">https://aws.amazon.com/quicksight/resources/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html\">https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon QuickSight to connect to the data sources and generate the visualizations needed for the dashboard</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon QuickSight is a very fast, easy-to-use, cloud-powered business analytics service that makes it easy for all employees within an organization to build visualizations, perform ad-hoc analysis, and quickly get business insights from their data, anytime, on any device."
      },
      {
        "answer": "",
        "explanation": "QuickSight can connect to AWS data sources including Amazon RDS, Amazon Aurora, Amazon Redshift, Amazon Athena, and Amazon S3. You can also upload Excel spreadsheets or flat files (CSV, TSV, CLF, and ELF), connect to on-premises databases like SQL Server, MySQL, and PostgreSQL, and import data from SaaS applications like Salesforce."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q30-i1.jpg",
        "answer": "",
        "explanation": "QuickSight Key Features:"
      },
      {
        "link": "https://aws.amazon.com/quicksight/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the federated queries feature of Amazon Athena to join the different data sources and visualize the data using Amazon QuickSight</strong> - Amazon Athena supports federated query, a feature that allows you to query data in sources other than Amazon Simple Storage Service (Amazon S3). You can use federated queries in Athena to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3."
      },
      {
        "answer": "",
        "explanation": "You can use different connectors to run federated queries with complex joins across different data sources with Athena and visualize the data with Amazon QuickSight. This option requires significant query development effort to join the data sources, so it's not the right fit for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Lake Formation to migrate the data sources into Amazon S3. Use Amazon QuickSight to generate the visualizations needed for the dashboards</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Lake Formation to migrate the data sources into Amazon S3. Configure AWS Glue Data Catalog to connect the S3 data to Amazon Athena for further analysis and visualizations. Use a Glue Crawler to automate the process</strong>"
      },
      {
        "answer": "",
        "explanation": "Both these options require a lot of heavy lifting to migrate data from the different sources into S3. These options are unnecessarily complex and hence not the best fit for the given use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/quicksight/",
      "https://aws.amazon.com/quicksight/resources/faqs/",
      "https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html"
    ]
  },
  {
    "id": 51,
    "question": "<p>A company needs to pull customer support data from Zendesk (a software-as-a-service product related to customer support) into an Amazon S3 bucket for analysis using Amazon Athena.</p>\n\n<p>Which AWS service or feature can address these requirements with the LEAST operational overhead?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Kinesis Data Streams</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon Managed Streaming for Apache Kafka (Amazon MSK)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Glue DataBrew</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon AppFlow</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon AppFlow</strong></p>\n\n<p>Amazon AppFlow is a fully managed integration service that enables you to securely transfer data between SaaS applications (such as Salesforce, Marketo, Slack, and ServiceNow) and AWS services (such as Amazon S3 and Amazon Redshift). With Amazon AppFlow, you can run data flows at nearly any scale and frequency (on a schedule, in response to a business event in real-time, or on demand). You can configure data transformations such as data masking and concatenation of fields, as well as validate and filter data (omitting records that don’t fit a criteria) to generate rich, ready-to-use data as part of the flow itself, without additional steps.</p>\n\n<p>Amazon AppFlow-based data ingestion pattern:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q17-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q17-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/patterns-for-ingesting-saas-data-into-aws-data-lakes/purpose-built-integration-service.html\">https://docs.aws.amazon.com/whitepapers/latest/patterns-for-ingesting-saas-data-into-aws-data-lakes/purpose-built-integration-service.html</a><p></p>\n\n<p>Because Amazon AppFlow can connect to many SaaS applications and is a low-code/no-code approach, this makes it very appealing for those who would want a quick and easy mechanism to ingest data from these SaaS applications. Some use cases are as follows:</p>\n\n<ol>\n<li><p>Create a copy of a Salesforce object (for example, opportunity, case, campaign) in Amazon S3.</p></li>\n<li><p>Send case tickets from Zendesk to Amazon S3.</p></li>\n<li><p>Hydrate an Amazon S3 data lake with transactional data from SAP S/4HANA enterprise resource planning (ERP).</p></li>\n<li><p>Send logs, metrics, and dashboards from Datadog to Amazon S3, to create monthly reports or perform other analyses automatically, instead of doing this manually.</p></li>\n<li><p>Send Marketo data, like new leads or email responses, in Amazon S3.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong> - You need to develop custom code to read the data from the SaaS application and write it into the respective AWS services by using Amazon Kinesis Data Streams. So, this option is incorrect.</p>\n\n<p><strong>AWS Glue DataBrew</strong> - AWS Glue DataBrew is a visual data preparation tool that makes it easier for data analysts and data scientists to clean and normalize data to prepare it for analytics and machine learning (ML). You can choose from over 250 prebuilt transformations to automate data preparation tasks, all without the need to write any code. You cannot use AWS Glue DataBrew to read the data from the SaaS application into any AWS service.</p>\n\n<p><strong>Amazon Managed Streaming for Apache Kafka (Amazon MSK)</strong> - Apache Kafka is a distributed data store optimized for ingesting and processing streaming data in real-time. You need to develop custom code to read the data from the SaaS application and write it into the respective AWS services by using Apache Kafka.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/patterns-for-ingesting-saas-data-into-aws-data-lakes/purpose-built-integration-service.html\">https://docs.aws.amazon.com/whitepapers/latest/patterns-for-ingesting-saas-data-into-aws-data-lakes/purpose-built-integration-service.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon AppFlow</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon AppFlow is a fully managed integration service that enables you to securely transfer data between SaaS applications (such as Salesforce, Marketo, Slack, and ServiceNow) and AWS services (such as Amazon S3 and Amazon Redshift). With Amazon AppFlow, you can run data flows at nearly any scale and frequency (on a schedule, in response to a business event in real-time, or on demand). You can configure data transformations such as data masking and concatenation of fields, as well as validate and filter data (omitting records that don’t fit a criteria) to generate rich, ready-to-use data as part of the flow itself, without additional steps."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q17-i1.jpg",
        "answer": "",
        "explanation": "Amazon AppFlow-based data ingestion pattern:"
      },
      {
        "link": "https://docs.aws.amazon.com/whitepapers/latest/patterns-for-ingesting-saas-data-into-aws-data-lakes/purpose-built-integration-service.html"
      },
      {
        "answer": "",
        "explanation": "Because Amazon AppFlow can connect to many SaaS applications and is a low-code/no-code approach, this makes it very appealing for those who would want a quick and easy mechanism to ingest data from these SaaS applications. Some use cases are as follows:"
      },
      {}
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Streams</strong> - You need to develop custom code to read the data from the SaaS application and write it into the respective AWS services by using Amazon Kinesis Data Streams. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Glue DataBrew</strong> - AWS Glue DataBrew is a visual data preparation tool that makes it easier for data analysts and data scientists to clean and normalize data to prepare it for analytics and machine learning (ML). You can choose from over 250 prebuilt transformations to automate data preparation tasks, all without the need to write any code. You cannot use AWS Glue DataBrew to read the data from the SaaS application into any AWS service."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Managed Streaming for Apache Kafka (Amazon MSK)</strong> - Apache Kafka is a distributed data store optimized for ingesting and processing streaming data in real-time. You need to develop custom code to read the data from the SaaS application and write it into the respective AWS services by using Apache Kafka."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/patterns-for-ingesting-saas-data-into-aws-data-lakes/purpose-built-integration-service.html"
    ]
  },
  {
    "id": 52,
    "question": "<p>An e-commerce company uses a two-tier architecture with application servers in the public subnet and an Amazon RDS MySQL DB in a private subnet. The data engineering team can use a bastion host in the public subnet to access the MySQL database and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the team noticed several \"could not connect to server: connection timed out\" error messages.</p>\n\n<p>Which of the following options represents the root cause for this issue?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The database user credentials (username and password) configured for the application are incorrect</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The database user credentials (username and password) configured for the application do not have the required privilege for the given database</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers</strong></p>\n\n<p>You should use security groups to control the inbound and outbound traffic for your database instance. For your application servers, create a security group with inbound rules that use the IP addresses of the client application as the source. This security group allows your client application to connect to your application servers. Then create a second security group for your database instance and create a new rule by specifying the security group that you created earlier as the source for this database-specific security group.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/images/AmazonRDS/latest/UserGuide/images/con-VPC-sec-grp.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/images/AmazonRDS/latest/UserGuide/images/con-VPC-sec-grp.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance</strong> - As mentioned in the explanation above, the application servers don't need inbound connections from the database instance, rather the database instance needs the correct inbound rule with the application servers' security group as the source.</p>\n\n<p><strong>The database user credentials (username and password) configured for the application are incorrect</strong></p>\n\n<p><strong>The database user credentials (username and password) configured for the application do not have the required privilege for the given database</strong></p>\n\n<p>These two options have been added as a distractor since the error mentions a \"connection timeout\" issue rather than an \"access denied\" error.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers</strong>"
      },
      {
        "answer": "",
        "explanation": "You should use security groups to control the inbound and outbound traffic for your database instance. For your application servers, create a security group with inbound rules that use the IP addresses of the client application as the source. This security group allows your client application to connect to your application servers. Then create a second security group for your database instance and create a new rule by specifying the security group that you created earlier as the source for this database-specific security group."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance</strong> - As mentioned in the explanation above, the application servers don't need inbound connections from the database instance, rather the database instance needs the correct inbound rule with the application servers' security group as the source."
      },
      {
        "answer": "",
        "explanation": "<strong>The database user credentials (username and password) configured for the application are incorrect</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>The database user credentials (username and password) configured for the application do not have the required privilege for the given database</strong>"
      },
      {
        "answer": "",
        "explanation": "These two options have been added as a distractor since the error mentions a \"connection timeout\" issue rather than an \"access denied\" error."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/"
    ]
  },
  {
    "id": 53,
    "question": "<p>A company runs on-demand Athena queries on a petabyte-scale dataset to support its custom dashboards. The data is updated once every day during non-business hours. The company has seen a sudden surge in access requests for the dashboard, thereby raising the cost of using Amazon Athena. The company is looking at optimizing this cost.</p>\n\n<p>Which of the following will address these requirements with the LEAST operational overhead?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Materialize frequently used queries to improve performance</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the query result reuse feature of Athena to reduce the query costs</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create Athena workgroup for each use case and attach appropriate tags. Monitor costs using CloudWatch alarms</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Migrate the data to columnar storage formats like Apache Parquet or ORC</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the query result reuse feature of Athena to reduce the query costs</strong></p>\n\n<p>AWS recommends using Query Result Reuse for every query where the source data doesn’t change frequently. Query Result Reuse allows results to be shared among users in a workgroup, as long as they have access to the tables and data. This means Query Result Reuse can benefit not only a single user but also other users in the workgroup who might be running the same queries. One example where this may be especially beneficial is when you have dashboards that are viewed by many users. The dashboard widgets run the same queries for all users and are therefore accelerated by Query Result Reuse, when enabled.</p>\n\n<p>Another example is if you have a dataset that is updated daily, and many users who all query the most recent data to create reports. Different people might run the same queries as part of their work; with Query Result Reuse, they can collectively avoid running the same query more than once, making everyone more productive and lowering overall costs by avoiding repeated scans of the same data.</p>\n\n<p>It's common for the same query to run multiple times within a short duration. You specify the maximum age of the results to be reused. If the same query was previously run within that time frame, Athena returns those results instead of running the query again.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Materialize frequently used queries to improve performance</strong> - 'Materializing' a query is a way of accelerating query performance by storing pre-computed complex query results (for example, aggregations and joins) for reuse in subsequent queries. This option is for improving the performance of complex queries that perform various complex operations. This option is not the best fit for the given use case, as it would not significantly reduce the cost of running the Athena queries.</p>\n\n<p><strong>Migrate the data to columnar storage formats like Apache Parquet or ORC</strong> - Apache Parquet and ORC are columnar storage formats that are optimized for fast retrieval of data and used in AWS analytical applications. Columnar formats could reduce some costs, but this option involves rewriting the existing data, so it has significant operational overhead.</p>\n\n<p><strong>Create an Athena workgroup for each use case and attach appropriate tags. Monitor costs using CloudWatch alarms</strong> - Use workgroups to separate users, teams, applications, or workloads, to set limits on the amount of data each query or the entire workgroup can process, and to track costs. You can also view query-related metrics in Amazon CloudWatch, control costs by configuring limits on the amount of data scanned, create thresholds, and trigger actions, such as Amazon SNS, when these thresholds are breached. While you could certainly monitor the costs, Athena workgroups by themselves would not reduce the costs for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html\">https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/reduce-cost-and-improve-query-performance-with-amazon-athena-query-result-reuse/\">https://aws.amazon.com/blogs/big-data/reduce-cost-and-improve-query-performance-with-amazon-athena-query-result-reuse/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the query result reuse feature of Athena to reduce the query costs</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS recommends using Query Result Reuse for every query where the source data doesn’t change frequently. Query Result Reuse allows results to be shared among users in a workgroup, as long as they have access to the tables and data. This means Query Result Reuse can benefit not only a single user but also other users in the workgroup who might be running the same queries. One example where this may be especially beneficial is when you have dashboards that are viewed by many users. The dashboard widgets run the same queries for all users and are therefore accelerated by Query Result Reuse, when enabled."
      },
      {
        "answer": "",
        "explanation": "Another example is if you have a dataset that is updated daily, and many users who all query the most recent data to create reports. Different people might run the same queries as part of their work; with Query Result Reuse, they can collectively avoid running the same query more than once, making everyone more productive and lowering overall costs by avoiding repeated scans of the same data."
      },
      {
        "answer": "",
        "explanation": "It's common for the same query to run multiple times within a short duration. You specify the maximum age of the results to be reused. If the same query was previously run within that time frame, Athena returns those results instead of running the query again."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Materialize frequently used queries to improve performance</strong> - 'Materializing' a query is a way of accelerating query performance by storing pre-computed complex query results (for example, aggregations and joins) for reuse in subsequent queries. This option is for improving the performance of complex queries that perform various complex operations. This option is not the best fit for the given use case, as it would not significantly reduce the cost of running the Athena queries."
      },
      {
        "answer": "",
        "explanation": "<strong>Migrate the data to columnar storage formats like Apache Parquet or ORC</strong> - Apache Parquet and ORC are columnar storage formats that are optimized for fast retrieval of data and used in AWS analytical applications. Columnar formats could reduce some costs, but this option involves rewriting the existing data, so it has significant operational overhead."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an Athena workgroup for each use case and attach appropriate tags. Monitor costs using CloudWatch alarms</strong> - Use workgroups to separate users, teams, applications, or workloads, to set limits on the amount of data each query or the entire workgroup can process, and to track costs. You can also view query-related metrics in Amazon CloudWatch, control costs by configuring limits on the amount of data scanned, create thresholds, and trigger actions, such as Amazon SNS, when these thresholds are breached. While you could certainly monitor the costs, Athena workgroups by themselves would not reduce the costs for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html",
      "https://aws.amazon.com/blogs/big-data/reduce-cost-and-improve-query-performance-with-amazon-athena-query-result-reuse/"
    ]
  },
  {
    "id": 54,
    "question": "<p>A company intends to organize its Amazon S3 storage used for a data lake by implementing partitioning that has the S3 object keys in the format: s3://bucket/prefix/year=2023/month=01/day=01. A data engineer is tasked with ensuring that the AWS Glue Data Catalog remains updated in sync with any new partitions added to the S3 bucket.</p>\n\n<p>What solution would best meet these requirements while minimizing latency?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure an AWS Glue crawler to run on a daily schedule</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Schedule an AWS Lambda function to run the AWS Glue CreatePartition API twice each day</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Update the existing code that writes data to Amazon S3, so that it also invokes the Boto3 AWS Glue create_partition API call</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Manually execute the MSCK REPAIR TABLE command from the AWS Glue console on demand</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Update the existing code that writes data to Amazon S3, so that it also invokes the Boto3 AWS Glue create_partition API call</strong></p>\n\n<p>Partitioning is an important technique for organizing datasets so they can be queried efficiently. It organizes data in a hierarchical directory structure based on the distinct values of one or more columns. For example, you might decide to partition your application logs in Amazon Simple Storage Service (Amazon S3) by date, broken down by year, month, and day. Files that correspond to a single day's worth of data are then placed under a prefix such as s3://my_bucket/logs/year=2018/month=01/day=23/.</p>\n\n<p>Partitioning means organizing data into directories (or \"prefixes\") on Amazon S3 based on a particular property of the data. Such properties are called partition keys. A common partition key is the date or some other unit of time such as the year or month. However, a dataset can be partitioned by more than one key. For example, data about product sales might be partitioned by date, product category, and market.</p>\n\n<p>For the given use case, you need to update the existing code that writes data to Amazon S3, so that it also invokes the Boto3 AWS Glue create_partition API call.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure an AWS Glue crawler to run on a daily schedule</strong> - A daily schedule to invoke the Glue Crawler invites severe latency into the solution. So, this option is incorrect.</p>\n\n<p><strong>Manually execute the MSCK REPAIR TABLE command from the AWS Glue console on demand</strong> - This option has the worst outcome in terms of latency, so this is incorrect.</p>\n\n<p><strong>Schedule an AWS Lambda function to run the AWS Glue CreatePartition API twice each day</strong> - A schedule that invokes a Lambda function twice a day run the AWS Glue CreatePartition API will have siginificant latency to sync any new partitions. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html\">https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html</a></p>\n\n<p><a href=\"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue/client/create_partition.html\">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue/client/create_partition.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/alter-table-add-partition.html\">https://docs.aws.amazon.com/athena/latest/ug/alter-table-add-partition.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/get-started-managing-partitions-for-amazon-s3-tables-backed-by-the-aws-glue-data-catalog/\">https://aws.amazon.com/blogs/big-data/get-started-managing-partitions-for-amazon-s3-tables-backed-by-the-aws-glue-data-catalog/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Update the existing code that writes data to Amazon S3, so that it also invokes the Boto3 AWS Glue create_partition API call</strong>"
      },
      {
        "answer": "",
        "explanation": "Partitioning is an important technique for organizing datasets so they can be queried efficiently. It organizes data in a hierarchical directory structure based on the distinct values of one or more columns. For example, you might decide to partition your application logs in Amazon Simple Storage Service (Amazon S3) by date, broken down by year, month, and day. Files that correspond to a single day's worth of data are then placed under a prefix such as s3://my_bucket/logs/year=2018/month=01/day=23/."
      },
      {
        "answer": "",
        "explanation": "Partitioning means organizing data into directories (or \"prefixes\") on Amazon S3 based on a particular property of the data. Such properties are called partition keys. A common partition key is the date or some other unit of time such as the year or month. However, a dataset can be partitioned by more than one key. For example, data about product sales might be partitioned by date, product category, and market."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you need to update the existing code that writes data to Amazon S3, so that it also invokes the Boto3 AWS Glue create_partition API call."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure an AWS Glue crawler to run on a daily schedule</strong> - A daily schedule to invoke the Glue Crawler invites severe latency into the solution. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Manually execute the MSCK REPAIR TABLE command from the AWS Glue console on demand</strong> - This option has the worst outcome in terms of latency, so this is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Schedule an AWS Lambda function to run the AWS Glue CreatePartition API twice each day</strong> - A schedule that invokes a Lambda function twice a day run the AWS Glue CreatePartition API will have siginificant latency to sync any new partitions. So, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html",
      "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue/client/create_partition.html",
      "https://docs.aws.amazon.com/athena/latest/ug/alter-table-add-partition.html",
      "https://aws.amazon.com/blogs/big-data/get-started-managing-partitions-for-amazon-s3-tables-backed-by-the-aws-glue-data-catalog/"
    ]
  },
  {
    "id": 55,
    "question": "<p>A company stores petabytes of data in hundreds of Amazon S3 buckets in the S3 Standard storage class. The data supports several analytics, visualization, and reporting-specific workflows. The data access patterns are unpredictable and variable. Some data is not accessed for months but needs to be available in milliseconds when accessed. The company wants to optimize the costs of storage.</p>\n\n<p>Which solution can address these requirements with the LEAST operational overhead?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon S3 Intelligent-Tiering with the default access tier</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Leverage S3 Storage Lens advanced metrics to determine when to move objects to more cost-optimized storage classes</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Opt for Amazon S3 Transfer Acceleration for milliseconds latencies</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon S3 Intelligent-Tiering with the Archive Instant Access tier</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 Intelligent-Tiering with the default access tier</strong></p>\n\n<p>The Amazon S3 Intelligent-Tiering storage class automatically stores objects in three access tiers. One tier is optimized for frequent access, one lower-cost tier is optimized for infrequent access, and another very low-cost tier is optimized for rarely accessed data. For a low monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects to the Infrequent Access tier when they haven't been accessed for 30 consecutive days. After 90 days of no access, the objects are moved to the Archive Instant Access tier without performance impact or operational overhead.</p>\n\n<p>S3 Intelligent-Tiering is the recommended storage class for data with unknown, changing, or unpredictable access patterns, independent of object size or retention period, such as data lakes, data analytics, and new applications.</p>\n\n<p>S3 Intelligent-Tiering access tiers:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q14-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q14-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage S3 Storage Lens advanced metrics to determine when to move objects to more cost-optimized storage classes</strong> - Amazon S3 Storage Lens provides organization-wide visibility into object storage usage and activity trends, as well as actionable recommendations to optimize costs and apply data protection best practices. Storage Lens offers an interactive dashboard containing a single view of your object storage usage and activity across tens or hundreds of accounts in your organization, with drill-downs to generate insights at multiple aggregation levels.</p>\n\n<p>S3 Storage Lens contains more than 60 metrics, grouped into free metrics and advanced metrics (available for an additional cost). Within free metrics, you receive metrics to analyze usage (based on a daily snapshot of your objects), which are organized into the categories of cost optimization, data protection, access management, performance, and events. Within advanced metrics, you receive metrics related to the activity (such as request counts), deeper cost optimization (such as S3 Lifecycle rule counts), additional data protection (such as S3 Replication rule counts), and detailed status codes (such as 403 authorization errors).</p>\n\n<p><em>Opt for Amazon S3 Transfer Acceleration for milliseconds latencies</em>* - Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Customers who have either web or mobile applications with widespread users or applications hosted far away from their S3 bucket can experience long and variable upload and download speeds over the Internet. This option acts as a distractor since S3 Transfer Acceleration will not reduce the cost of storage on Amazon S3, which is the main requirement for the given use case.</p>\n\n<p><strong>Use Amazon S3 Intelligent-Tiering with the Archive Instant Access tier</strong> - If an object is not accessed for 90 consecutive days, the object moves to the Archive Instant Access tier of Amazon S3 Intelligent-Tiering. The Archive Instant Access tier provides low latency and high-throughput performance. This is an automatic tier and one cannot opt for this tier manually.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Intelligent-Tiering with the default access tier</strong>"
      },
      {
        "answer": "",
        "explanation": "The Amazon S3 Intelligent-Tiering storage class automatically stores objects in three access tiers. One tier is optimized for frequent access, one lower-cost tier is optimized for infrequent access, and another very low-cost tier is optimized for rarely accessed data. For a low monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects to the Infrequent Access tier when they haven't been accessed for 30 consecutive days. After 90 days of no access, the objects are moved to the Archive Instant Access tier without performance impact or operational overhead."
      },
      {
        "answer": "",
        "explanation": "S3 Intelligent-Tiering is the recommended storage class for data with unknown, changing, or unpredictable access patterns, independent of object size or retention period, such as data lakes, data analytics, and new applications."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q14-i1.jpg",
        "answer": "",
        "explanation": "S3 Intelligent-Tiering access tiers:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage S3 Storage Lens advanced metrics to determine when to move objects to more cost-optimized storage classes</strong> - Amazon S3 Storage Lens provides organization-wide visibility into object storage usage and activity trends, as well as actionable recommendations to optimize costs and apply data protection best practices. Storage Lens offers an interactive dashboard containing a single view of your object storage usage and activity across tens or hundreds of accounts in your organization, with drill-downs to generate insights at multiple aggregation levels."
      },
      {
        "answer": "",
        "explanation": "S3 Storage Lens contains more than 60 metrics, grouped into free metrics and advanced metrics (available for an additional cost). Within free metrics, you receive metrics to analyze usage (based on a daily snapshot of your objects), which are organized into the categories of cost optimization, data protection, access management, performance, and events. Within advanced metrics, you receive metrics related to the activity (such as request counts), deeper cost optimization (such as S3 Lifecycle rule counts), additional data protection (such as S3 Replication rule counts), and detailed status codes (such as 403 authorization errors)."
      },
      {
        "answer": "",
        "explanation": "<em>Opt for Amazon S3 Transfer Acceleration for milliseconds latencies</em>* - Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Customers who have either web or mobile applications with widespread users or applications hosted far away from their S3 bucket can experience long and variable upload and download speeds over the Internet. This option acts as a distractor since S3 Transfer Acceleration will not reduce the cost of storage on Amazon S3, which is the main requirement for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Intelligent-Tiering with the Archive Instant Access tier</strong> - If an object is not accessed for 90 consecutive days, the object moves to the Archive Instant Access tier of Amazon S3 Intelligent-Tiering. The Archive Instant Access tier provides low latency and high-throughput performance. This is an automatic tier and one cannot opt for this tier manually."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html",
      "https://aws.amazon.com/s3/faqs/"
    ]
  },
  {
    "id": 56,
    "question": "<p>A company uses a two-tier architecture with application servers in the public subnet and an RDS-based database in a private subnet. The data engineering team is able to use a bastion host in the public subnet to log in to MySQL and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the team noticed several \"could not connect to server: connection timed out\" error messages.</p>\n\n<p>Which of the following would you identify as the root cause behind this issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The database user credentials (username and password) configured for the application do not have the required privilege for the given database</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The security group configuration for the DB instance does not have the correct rules to allow inbound connections from the application servers</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The security group configuration for the application servers does not have the correct rules to allow inbound connections from the DB instance</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The database user credentials (username and password) configured for the application are incorrect</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The security group configuration for the DB instance does not have the correct rules to allow inbound connections from the application servers</strong></p>\n\n<p>You should use security groups to control the inbound and outbound traffic for your DB instance. For your application servers, create a security group with inbound rules that use the IP addresses of the client application as the source. This security group allows your client application to connect to your application servers. Then create a second security group for your DB instance and create a new rule by specifying the security group that you created earlier as the source for this DB-specific security group.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q40-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q40-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html</a><p></p>\n\n<p>Please follow these troubleshooting guidelines for resolving RDS DB connection issues:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q40-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q40-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The security group configuration for the application servers does not have the correct rules to allow inbound connections from the DB instance</strong> - As mentioned in the explanation above, the application servers don't need inbound connections from the DB instance, rather the DB instance needs the correct inbound rule with application servers' security group as the source.</p>\n\n<p><strong>The database user credentials (username and password) configured for the application are incorrect</strong></p>\n\n<p><strong>The database user credentials (username and password) configured for the application do not have the required privilege for the given database</strong></p>\n\n<p>These two options have been added as a distractor since the error mentions a \"connection timeout\" issue rather than an \"access denied\" error.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The security group configuration for the DB instance does not have the correct rules to allow inbound connections from the application servers</strong>"
      },
      {
        "answer": "",
        "explanation": "You should use security groups to control the inbound and outbound traffic for your DB instance. For your application servers, create a security group with inbound rules that use the IP addresses of the client application as the source. This security group allows your client application to connect to your application servers. Then create a second security group for your DB instance and create a new rule by specifying the security group that you created earlier as the source for this DB-specific security group."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q40-i2.jpg",
        "answer": "",
        "explanation": "Please follow these troubleshooting guidelines for resolving RDS DB connection issues:"
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The security group configuration for the application servers does not have the correct rules to allow inbound connections from the DB instance</strong> - As mentioned in the explanation above, the application servers don't need inbound connections from the DB instance, rather the DB instance needs the correct inbound rule with application servers' security group as the source."
      },
      {
        "answer": "",
        "explanation": "<strong>The database user credentials (username and password) configured for the application are incorrect</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>The database user credentials (username and password) configured for the application do not have the required privilege for the given database</strong>"
      },
      {
        "answer": "",
        "explanation": "These two options have been added as a distractor since the error mentions a \"connection timeout\" issue rather than an \"access denied\" error."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/"
    ]
  },
  {
    "id": 57,
    "question": "<p>The data engineering team at a logistics company leverages AWS Cloud to process Internet of Things (IoT) sensor data from the field devices of the company. The team stores the sensor data in Amazon DynamoDB tables. To detect anomalous behaviors and respond quickly, all changes to the items stored in the DynamoDB tables must be logged in near real-time.</p>\n\n<p>As an AWS Certified Data Engineer Associate, which of the following solutions would you suggest to meet the requirements of the given use case so that it requires minimal custom development and infrastructure maintenance?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up DynamoDB Streams to capture and send updates to a Lambda function that outputs records to Kinesis Data Analytics (KDA) via Kinesis Data Streams (KDS). Detect and analyze anomalies in KDA and send notifications via SNS</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up DynamoDB Streams to capture and send updates to a Lambda function that outputs records directly to Kinesis Data Analytics (KDA). Detect and analyze anomalies in KDA and send notifications via SNS</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up CloudTrail to capture all API calls that update the DynamoDB tables. Leverage CloudTrail event filtering to analyze anomalous behaviors and send SNS notifications in case anomalies are detected</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure event patterns in CloudWatch Events to capture DynamoDB API call events and set up Lambda function as a target to analyze anomalous behavior. Send SNS notifications when anomalous behaviors are detected</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up DynamoDB Streams to capture and send updates to a Lambda function that outputs records to Kinesis Data Analytics (KDA) via Kinesis Data Streams (KDS). Detect and analyze anomalies in KDA and send notifications via SNS</strong></p>\n\n<p>A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table for up to 24 hours.</p>\n\n<p>Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table.</p>\n\n<p>DynamoDB Streams supports the following stream record views:</p>\n\n<p>KEYS_ONLY — Only the key attributes of the modified item\nNEW_IMAGE — The entire item, as it appears after it was modified\nOLD_IMAGE — The entire item, as it appears before it was modified\nNEW_AND_OLD_IMAGES — Both the new and the old images of the item</p>\n\n<p>You can process DynamoDB streams in multiple ways. The most common approaches use AWS Lambda or a standalone application that uses the Kinesis Client Library (KCL) with the DynamoDB Streams Kinesis Adapter. The KCL is a client-side library that provides an interface to process DynamoDB stream changes. If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.</p>\n\n<p>Reference architecture for DynamoDB streams design patterns:</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2017/06/26/DesignPatternReference.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2017/06/26/DesignPatternReference.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/\">https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/</a><p></p>\n\n<p>For the given use case, you can use a Lambda function to capture updates from DynamoDB Streams and send those records to KDA via KDS. You can then detect and analyze anomalies in KDA and send notifications via SNS.</p>\n\n<p>How KDS Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a><p></p>\n\n<p>How KDA Works:\n<img src=\"https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Kinesis-Data-Analytics-How-it-Works@2x-updated.7340988926f37d36097e2f9099483e7e67692deb.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Kinesis-Data-Analytics-How-it-Works@2x-updated.7340988926f37d36097e2f9099483e7e67692deb.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a><p></p>\n\n<p>It is important to note that Kinesis Data Analytics (KDA) only supports the following streaming sources for an application:</p>\n\n<p>A Kinesis data stream (KDS)</p>\n\n<p>A Kinesis Data Firehose (KDF) delivery stream</p>\n\n<p>Therefore, you cannot directly write the output of the records from a Lambda function to KDA, although you can certainly use a Lambda function to pre-process the incoming stream from either KDS or KDF.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up CloudTrail to capture all API calls that update the DynamoDB tables. Leverage CloudTrail event filtering to analyze anomalous behaviors and send SNS notifications in case anomalies are detected</strong> - You can use CloudTrail to capture API calls for DynamoDB as events. The calls captured include calls from the DynamoDB console and code calls to the DynamoDB API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for DynamoDB. The CloudTrail does not support the GetRecords API for DynamoDB Streams so you cannot use it to capture the actual records. Moreover, you cannot use CloudTrail event filtering to analyze anomalous behaviors as it is just a simple filtering mechanism based on certain event attributes such as Read Only, Event Source, Event Time, etc.</p>\n\n<p><strong>Configure event patterns in CloudWatch Events to capture DynamoDB API call events and set up Lambda function as a target to analyze anomalous behavior. Send SNS notifications when anomalous behaviors are detected</strong> - CloudWatch Events service does not offer event type for DynamoDB as it's dependent on CloudTrail to get the relevant API call information. As explained above, CloudTrail itself cannot capture the DynamoDB streams records as CloudTrail does not support the GetRecords API for DynamoDB Streams. Therefore this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q26-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q26-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p><strong>Set up DynamoDB Streams to capture and send updates to a Lambda function that outputs records directly to Kinesis Data Analytics (KDA). Detect and analyze anomalies in KDA and send notifications via SNS</strong> - As mentioned earlier, KDA only supports KDS and KDF as the streaming sources for an application, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/\">https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-input.html\">https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-input.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/logging-using-cloudtrail.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/logging-using-cloudtrail.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up DynamoDB Streams to capture and send updates to a Lambda function that outputs records to Kinesis Data Analytics (KDA) via Kinesis Data Streams (KDS). Detect and analyze anomalies in KDA and send notifications via SNS</strong>"
      },
      {
        "answer": "",
        "explanation": "A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table for up to 24 hours."
      },
      {
        "answer": "",
        "explanation": "Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table."
      },
      {
        "answer": "",
        "explanation": "DynamoDB Streams supports the following stream record views:"
      },
      {
        "answer": "",
        "explanation": "KEYS_ONLY — Only the key attributes of the modified item\nNEW_IMAGE — The entire item, as it appears after it was modified\nOLD_IMAGE — The entire item, as it appears before it was modified\nNEW_AND_OLD_IMAGES — Both the new and the old images of the item"
      },
      {
        "answer": "",
        "explanation": "You can process DynamoDB streams in multiple ways. The most common approaches use AWS Lambda or a standalone application that uses the Kinesis Client Library (KCL) with the DynamoDB Streams Kinesis Adapter. The KCL is a client-side library that provides an interface to process DynamoDB stream changes. If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records."
      },
      {
        "answer": "",
        "explanation": "Reference architecture for DynamoDB streams design patterns:"
      },
      {
        "link": "https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can use a Lambda function to capture updates from DynamoDB Streams and send those records to KDA via KDS. You can then detect and analyze anomalies in KDA and send notifications via SNS."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png",
        "answer": "",
        "explanation": "How KDS Works:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/"
      },
      {
        "image": "https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Kinesis-Data-Analytics-How-it-Works@2x-updated.7340988926f37d36097e2f9099483e7e67692deb.png",
        "answer": "",
        "explanation": "How KDA Works:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-analytics/"
      },
      {
        "answer": "",
        "explanation": "It is important to note that Kinesis Data Analytics (KDA) only supports the following streaming sources for an application:"
      },
      {
        "answer": "",
        "explanation": "A Kinesis data stream (KDS)"
      },
      {
        "answer": "",
        "explanation": "A Kinesis Data Firehose (KDF) delivery stream"
      },
      {
        "answer": "",
        "explanation": "Therefore, you cannot directly write the output of the records from a Lambda function to KDA, although you can certainly use a Lambda function to pre-process the incoming stream from either KDS or KDF."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up CloudTrail to capture all API calls that update the DynamoDB tables. Leverage CloudTrail event filtering to analyze anomalous behaviors and send SNS notifications in case anomalies are detected</strong> - You can use CloudTrail to capture API calls for DynamoDB as events. The calls captured include calls from the DynamoDB console and code calls to the DynamoDB API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for DynamoDB. The CloudTrail does not support the GetRecords API for DynamoDB Streams so you cannot use it to capture the actual records. Moreover, you cannot use CloudTrail event filtering to analyze anomalous behaviors as it is just a simple filtering mechanism based on certain event attributes such as Read Only, Event Source, Event Time, etc."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure event patterns in CloudWatch Events to capture DynamoDB API call events and set up Lambda function as a target to analyze anomalous behavior. Send SNS notifications when anomalous behaviors are detected</strong> - CloudWatch Events service does not offer event type for DynamoDB as it's dependent on CloudTrail to get the relevant API call information. As explained above, CloudTrail itself cannot capture the DynamoDB streams records as CloudTrail does not support the GetRecords API for DynamoDB Streams. Therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up DynamoDB Streams to capture and send updates to a Lambda function that outputs records directly to Kinesis Data Analytics (KDA). Detect and analyze anomalies in KDA and send notifications via SNS</strong> - As mentioned earlier, KDA only supports KDS and KDF as the streaming sources for an application, so this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/",
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/kinesis/data-analytics/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html",
      "https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-input.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/logging-using-cloudtrail.html"
    ]
  },
  {
    "id": 58,
    "question": "<p>A health care application processes the real-time health data of the patients into an analytics workflow. With a sharp increase in the number of users, the system has become slow and sometimes even unresponsive as it does not have a retry mechanism. The startup is looking at a scalable solution that has minimal implementation overhead.</p>\n\n<p>Which of the following would you recommend as a scalable alternative to the current solution?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda, or run analytics using Amazon Kinesis Data Analytics</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon API Gateway with the existing REST-based interface to create a high-performing architecture</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda, or run analytics using Amazon Kinesis Data Analytics</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service with support for a retry mechanism. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>KDS makes sure your streaming data is available to multiple real-time analytics applications, to Amazon S3, or AWS Lambda within 70 milliseconds of the data being collected. Amazon Kinesis data streams scale from megabytes to terabytes per hour and scale from thousands to millions of PUT records per second. You can dynamically adjust the throughput of your stream at any time based on the volume of your input data.</p>\n\n<p>How Data Streams Work:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q54-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q54-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2\">https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing</strong> - Amazon Simple Notification Service (Amazon SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication. Amazon SNS is a push mechanism that does not support robust retry mechanisms, as is needed in the current use case.</p>\n\n<p><strong>Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing</strong> - Amazon Simple Queue Service (Amazon SQS) is a messaging service that helps in decoupling systems and reducing the complexity of architecture. Amazon SQS can still work but Amazon Kinesis Data streams is custom-made for streaming real-time data.</p>\n\n<p><strong>Use Amazon API Gateway with the existing REST-based interface to create a high-performing architecture</strong> - Amazon API Gateway is not meant for processing real-time streaming data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2\">https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda, or run analytics using Amazon Kinesis Data Analytics</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service with support for a retry mechanism. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more."
      },
      {
        "answer": "",
        "explanation": "KDS makes sure your streaming data is available to multiple real-time analytics applications, to Amazon S3, or AWS Lambda within 70 milliseconds of the data being collected. Amazon Kinesis data streams scale from megabytes to terabytes per hour and scale from thousands to millions of PUT records per second. You can dynamically adjust the throughput of your stream at any time based on the volume of your input data."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q54-i1.jpg",
        "answer": "",
        "explanation": "How Data Streams Work:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/?nc=sn&loc=2&dn=2"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing</strong> - Amazon Simple Notification Service (Amazon SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication. Amazon SNS is a push mechanism that does not support robust retry mechanisms, as is needed in the current use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing</strong> - Amazon Simple Queue Service (Amazon SQS) is a messaging service that helps in decoupling systems and reducing the complexity of architecture. Amazon SQS can still work but Amazon Kinesis Data streams is custom-made for streaming real-time data."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon API Gateway with the existing REST-based interface to create a high-performing architecture</strong> - Amazon API Gateway is not meant for processing real-time streaming data."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2"
    ]
  },
  {
    "id": 59,
    "question": "<p>A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline.</p>\n\n<p>Which of the following is the MOST cost-effective strategy for storing this intermediary query data?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the intermediary query results in Amazon S3 Standard storage class</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the intermediary query results in Amazon S3 Standard storage class</strong></p>\n\n<p>Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\nAs there is no minimum storage duration charge and no retrieval fee (remember that intermediary query results are heavily referenced by other parts of the analytics pipeline), this is the MOST cost-effective storage class amongst the given options.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class</strong> - Amazon S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives.</p>\n\n<p>The minimum storage duration charge is 90 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.</p>\n\n<p><strong>Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class</strong> - Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.</p>\n\n<p><strong>Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class</strong> - Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.</p>\n\n<p>To summarize again, S3 Standard-IA and S3 One Zone-IA have a minimum storage duration charge of 30 days (so instead of 24 hours, you end up paying for 30 days). S3 Standard-IA and S3 One Zone-IA also have retrieval charges (as the results are heavily referenced by other parts of the analytics pipeline, so the retrieval costs would be pretty high). Therefore, these storage classes are not cost-optimal for the given use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the intermediary query results in Amazon S3 Standard storage class</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\nAs there is no minimum storage duration charge and no retrieval fee (remember that intermediary query results are heavily referenced by other parts of the analytics pipeline), this is the MOST cost-effective storage class amongst the given options."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class</strong> - Amazon S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives."
      },
      {
        "answer": "",
        "explanation": "The minimum storage duration charge is 90 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class</strong> - Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class</strong> - Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct."
      },
      {
        "answer": "",
        "explanation": "To summarize again, S3 Standard-IA and S3 One Zone-IA have a minimum storage duration charge of 30 days (so instead of 24 hours, you end up paying for 30 days). S3 Standard-IA and S3 One Zone-IA also have retrieval charges (as the results are heavily referenced by other parts of the analytics pipeline, so the retrieval costs would be pretty high). Therefore, these storage classes are not cost-optimal for the given use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "id": 60,
    "question": "<p>An e-commerce company tracks user clicks on its flagship website and performs analytics to provide near-real-time product recommendations. An Amazon EC2 instance receives data from the website and sends the data to an Amazon Aurora Database instance. Another Amazon EC2 instance continuously checks the changes in the database and executes SQL queries to provide recommendations. Now, the company wants a redesign to decouple and scale the infrastructure. The solution must ensure that data can be analyzed in real-time without any data loss even when the company sees huge traffic spikes.</p>\n\n<p>What would you recommend?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Autoscaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real-time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real-time</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real-time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real-time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3</strong></p>\n\n<p>You can use Amazon Kinesis Data Streams to build custom applications that process or analyze streaming data for specialized needs.\nAmazon Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams.</p>\n\n<p>For the given use case, you can use Amazon Kinesis Data Analytics to transform and analyze incoming streaming data from Kinesis Data Streams in real-time. Kinesis Data Analytics takes care of everything required to run streaming applications continuously, and scales automatically to match the volume and throughput of your incoming data. With Amazon Kinesis Data Analytics, there are no servers to manage, no minimum fee or setup cost, and you only pay for the resources your streaming applications consume.</p>\n\n<p>Amazon Kinesis Data Analytics:\n<img src=\"https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Amazon-Kinesis-Data-Analytics_HIW.82e3aa53a5c87db03c766218b3d51f1a110c60eb.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Amazon-Kinesis-Data-Analytics_HIW.82e3aa53a5c87db03c766218b3d51f1a110c60eb.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/\">https://aws.amazon.com/kinesis/</a><p></p>\n\n<p>Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms, and delivers streaming data to data lakes, data stores, and analytics services.</p>\n\n<p>For the given use case, post the real-time analysis, the output feed from Kinesis Data Analytics is output into Kinesis Data Firehose which dumps the data into Amazon S3 without any data loss.</p>\n\n<p>Amazon Kinesis Data Firehose:\n<img src=\"https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/\">https://aws.amazon.com/kinesis/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real-time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3</strong> - Amazon QuickSight cannot use Amazon Kinesis Data Streams as a source. In addition, Amazon QuickSight cannot be used for real-time streaming data analysis from its source. Therefore this option is incorrect.</p>\n\n<p><strong>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real-time</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena cannot be used to analyze data in real-time. Therefore this option is incorrect.</p>\n\n<p><strong>Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Autoscaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances</strong> - Even though using Amazon SQS with Amazon EC2 instances can decouple the architecture, however, performing real-time analytics using a third-party library on the Amazon EC2 instances is not the right fit for the given use case. The Amazon Kinesis family of services is the better fit for the given scenario as these services allow streaming data ingestion, real-time analysis, and reliable data delivery to the data sink.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/\">https://aws.amazon.com/kinesis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/quicksight/resources/faqs/\">https://aws.amazon.com/quicksight/resources/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real-time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use Amazon Kinesis Data Streams to build custom applications that process or analyze streaming data for specialized needs.\nAmazon Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can use Amazon Kinesis Data Analytics to transform and analyze incoming streaming data from Kinesis Data Streams in real-time. Kinesis Data Analytics takes care of everything required to run streaming applications continuously, and scales automatically to match the volume and throughput of your incoming data. With Amazon Kinesis Data Analytics, there are no servers to manage, no minimum fee or setup cost, and you only pay for the resources your streaming applications consume."
      },
      {
        "image": "https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Amazon-Kinesis-Data-Analytics_HIW.82e3aa53a5c87db03c766218b3d51f1a110c60eb.png",
        "answer": "",
        "explanation": "Amazon Kinesis Data Analytics:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms, and delivers streaming data to data lakes, data stores, and analytics services."
      },
      {
        "answer": "",
        "explanation": "For the given use case, post the real-time analysis, the output feed from Kinesis Data Analytics is output into Kinesis Data Firehose which dumps the data into Amazon S3 without any data loss."
      },
      {
        "image": "https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png",
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real-time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3</strong> - Amazon QuickSight cannot use Amazon Kinesis Data Streams as a source. In addition, Amazon QuickSight cannot be used for real-time streaming data analysis from its source. Therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real-time</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena cannot be used to analyze data in real-time. Therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Autoscaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances</strong> - Even though using Amazon SQS with Amazon EC2 instances can decouple the architecture, however, performing real-time analytics using a third-party library on the Amazon EC2 instances is not the right fit for the given use case. The Amazon Kinesis family of services is the better fit for the given scenario as these services allow streaming data ingestion, real-time analysis, and reliable data delivery to the data sink."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/",
      "https://aws.amazon.com/quicksight/resources/faqs/"
    ]
  },
  {
    "id": 61,
    "question": "<p>An Amazon Redshift cluster is used to store sensitive information of a business-critical application. The regulatory guidelines mandate tracking audit logs of the Redshift cluster. The business needs to store the audit logs securely by encrypting the logs at rest. The logs are to be stored for a year at least and audits need to be conducted on the audit logs on a monthly basis.</p>\n\n<p>Which of the following is a cost-effective solution that fulfills the requirement of storing the logs securely while having access to the logs for monthly audits?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable default encryption on the Amazon S3 bucket that uses Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Use Amazon Redshift Spectrum to query the data for monthly audits</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable default encryption on the Amazon S3 bucket that uses Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Copy the data into the Amazon Redshift cluster from Amazon S3 when data needs to be queried for monthly audits</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable encryption on the Amazon S3 bucket that uses Amazon Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS) for audit logging. Use Amazon Redshift Spectrum to query the data for monthly audits</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable encryption on the Amazon S3 bucket that uses Amazon Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS) for audit logging. Use Amazon QuickSight to query the data for monthly audits</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable default encryption on the Amazon S3 bucket that uses Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Use Amazon Redshift Spectrum to query the data for monthly audits</strong></p>\n\n<p>Audit logging is not turned on by default in Amazon Redshift. When you turn on logging on your cluster, Amazon Redshift creates and uploads logs to Amazon S3 that capture data from the time audit logging is enabled to the present time. Each logging update is a continuation of the information that was already logged.</p>\n\n<p>Audit logging to Amazon S3 is an optional, manual process. When you enable logging on your cluster, you are enabling logging to Amazon S3 only. Currently, you can only use Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging.</p>\n\n<p>Amazon Redshift Spectrum is a feature of Amazon Redshift that lets you run queries against your data lake in Amazon S3, with no data loading or ETL required. When you issue an SQL query, it goes to the Amazon Redshift endpoint, which generates and optimizes a query plan. Amazon Redshift determines what data is local and what is in Amazon S3, generates a plan to minimize the amount of S3 data that needs to be read, and requests Amazon Redshift Spectrum workers out of a shared resource pool to read and process data from S3.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q31-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q31-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable default encryption on the Amazon S3 bucket that uses Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Copy the data into the Amazon Redshift cluster from Amazon S3 when data needs to be queried for monthly audits</strong> - Copying data into the Redshift cluster for enabling monthly audits would turn out to be a costly solution. Using Redshift Spectrum is the right fit for the given scenario.</p>\n\n<p><strong>Enable encryption on the Amazon S3 bucket that uses Amazon Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS) for audit logging. Use Amazon QuickSight to query the data for monthly audits</strong></p>\n\n<p><strong>Enable encryption on the Amazon S3 bucket that uses Amazon Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS) for audit logging. Use Amazon Redshift Spectrum to query the data for monthly audits</strong></p>\n\n<p>Currently, you can only use Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Hence, both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable default encryption on the Amazon S3 bucket that uses Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Use Amazon Redshift Spectrum to query the data for monthly audits</strong>"
      },
      {
        "answer": "",
        "explanation": "Audit logging is not turned on by default in Amazon Redshift. When you turn on logging on your cluster, Amazon Redshift creates and uploads logs to Amazon S3 that capture data from the time audit logging is enabled to the present time. Each logging update is a continuation of the information that was already logged."
      },
      {
        "answer": "",
        "explanation": "Audit logging to Amazon S3 is an optional, manual process. When you enable logging on your cluster, you are enabling logging to Amazon S3 only. Currently, you can only use Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging."
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift Spectrum is a feature of Amazon Redshift that lets you run queries against your data lake in Amazon S3, with no data loading or ETL required. When you issue an SQL query, it goes to the Amazon Redshift endpoint, which generates and optimizes a query plan. Amazon Redshift determines what data is local and what is in Amazon S3, generates a plan to minimize the amount of S3 data that needs to be read, and requests Amazon Redshift Spectrum workers out of a shared resource pool to read and process data from S3."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable default encryption on the Amazon S3 bucket that uses Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Copy the data into the Amazon Redshift cluster from Amazon S3 when data needs to be queried for monthly audits</strong> - Copying data into the Redshift cluster for enabling monthly audits would turn out to be a costly solution. Using Redshift Spectrum is the right fit for the given scenario."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable encryption on the Amazon S3 bucket that uses Amazon Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS) for audit logging. Use Amazon QuickSight to query the data for monthly audits</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Enable encryption on the Amazon S3 bucket that uses Amazon Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS) for audit logging. Use Amazon Redshift Spectrum to query the data for monthly audits</strong>"
      },
      {
        "answer": "",
        "explanation": "Currently, you can only use Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Hence, both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/",
      "https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html"
    ]
  },
  {
    "id": 62,
    "question": "<p>The data engineering team at a retail company has set up a workflow to ingest the clickstream data into the raw zone of the S3 data lake. The team wants to run some SQL-based data sanity checks on the raw zone of the data lake.</p>\n\n<p>What AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Athena to run SQL-based analytics against S3 data</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Load the incremental raw zone data into RDS on an hourly basis and run the SQL-based sanity checks</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Load the incremental raw zone data into Redshift on an hourly basis and run the SQL-based sanity checks</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Load the incremental raw zone data into an EMR-based Spark Cluster on an hourly basis and use SparkSQL to run the SQL-based sanity checks</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Athena to run SQL-based analytics against S3 data</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>AWS Athena Benefits:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q21-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q21-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Load the incremental raw zone data into Redshift on an hourly basis and run the SQL-based sanity checks</strong> - Amazon Redshift is a fully managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis.\nAs the development team would have to maintain and monitor the Redshift cluster size and would require significant development time to set up the processes to consume the data periodically, so this option is ruled out.</p>\n\n<p><strong>Load the incremental raw zone data into an EMR-based Spark Cluster on an hourly basis and use SparkSQL to run the SQL-based sanity checks</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing an EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the correct solution for the given use case should require the least amount of development effort and ongoing maintenance.</p>\n\n<p><strong>Load the incremental raw zone data into RDS on an hourly basis and run the SQL-based sanity checks</strong> - Loading the incremental data into RDS implies data migration jobs will have to be written via a Lambda function or an EC2-based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance. Hence this option is not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Athena to run SQL-based analytics against S3 data</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q21-i1.jpg",
        "answer": "",
        "explanation": "AWS Athena Benefits:"
      },
      {
        "link": "https://aws.amazon.com/athena/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Load the incremental raw zone data into Redshift on an hourly basis and run the SQL-based sanity checks</strong> - Amazon Redshift is a fully managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis.\nAs the development team would have to maintain and monitor the Redshift cluster size and would require significant development time to set up the processes to consume the data periodically, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Load the incremental raw zone data into an EMR-based Spark Cluster on an hourly basis and use SparkSQL to run the SQL-based sanity checks</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing an EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the correct solution for the given use case should require the least amount of development effort and ongoing maintenance."
      },
      {
        "answer": "",
        "explanation": "<strong>Load the incremental raw zone data into RDS on an hourly basis and run the SQL-based sanity checks</strong> - Loading the incremental data into RDS implies data migration jobs will have to be written via a Lambda function or an EC2-based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance. Hence this option is not correct."
      }
    ],
    "references": [
      "https://aws.amazon.com/athena/"
    ]
  },
  {
    "id": 63,
    "question": "<p>An e-commerce company stores all transaction data in Amazon RDS in the us-east-1 Region. The transformed transaction data is also kept in the us-east-1 Region in Amazon Redshift. The data engineering team wants to improve the user experience by developing a business intelligence (BI) dashboard that highlights the sales trends over the last year. A team in India configured Amazon QuickSight in ap-south-1 Region during development. The team is experiencing connectivity issues between Amazon QuickSight in ap-south-1 Region and Amazon Redshift in us-east-1 Region.</p>\n\n<p>Which of the following solutions would you recommend to address this requirement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure a new security group for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in ap-south-1</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up a VPC endpoint from the Amazon QuickSight VPC in ap-south-1 Region to the Amazon Redshift VPC in us-east-1 Region, so QuickSight can privately access data from Redshift</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure a new Network Access Control List (NACL) for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in ap-south-1</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a daily cross-Region snapshot for Redshift and set the destination Region as ap-south-1. Restore the Amazon Redshift Cluster from the snapshot ap-south-1 Region and connect the  QuickSight dashboard in ap-south-1 to this redshift cluster</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a new security group for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in ap-south-1</strong></p>\n\n<p>Amazon QuickSight is a cloud-powered business analytics service that allows users to build visualizations, perform ad-hoc analysis, and quickly get business insights from their data.</p>\n\n<p>Amazon QuickSight is built with \"SPICE\" – a Super-fast, Parallel, In-memory Calculation Engine. Built from the ground up for the cloud, SPICE uses a combination of columnar storage, in-memory technologies enabled through the latest hardware innovations, and machine code generation to run interactive queries on large datasets and get rapid responses.</p>\n\n<p>Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale.</p>\n\n<p>For Amazon QuickSight to connect to an Amazon Redshift instance, you must create a new security group for that instance. This security group contains an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in that AWS Region. You can configure such a security group irrespective of whether the Redshift cluster has been created in a VPC or not.</p>\n\n<p>Manually enabling access to an Amazon Redshift cluster in a VPC:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q19-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q19-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\">https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html</a><p></p>\n\n<p>Manually enabling access to an Amazon Redshift cluster that is not in a VPC:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q19-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q19-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\">https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a new Network Access Control List (NACL) for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in ap-south-1</strong> - A Network Access Control List (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. NACLs are associated with subnets, which are in turn created within VPCs. You should note that a Redshift cluster, as well as a QuickSight dashboard, can be created outside of a VPC. So this option is incorrect for the given use case.</p>\n\n<p><strong>Set up a VPC endpoint from the Amazon QuickSight VPC in ap-south-1 Region to the Amazon Redshift VPC in us-east-1 Region, so QuickSight can privately access data from Redshift</strong> - Only the QuickSight Enterprise edition can be integrated with the Amazon VPC service. A VPC endpoint enables connections between a virtual private cloud (VPC) and supported services, without requiring that you use an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Therefore, you control the specific API endpoints, sites, and services that are reachable from your VPC. VPC endpoints can only be used to access resources in the same Region as the endpoint. Since the QuickSight dashboard and Redshift cluster are in separate Regions, you cannot use VPC endpoint for the given scenario.</p>\n\n<p><strong>Set up a daily cross-Region snapshot for Redshift and set the destination Region as ap-south-1. Restore the Amazon Redshift Cluster from the snapshot ap-south-1 Region and connect the  QuickSight dashboard in ap-south-1 to this redshift cluster</strong> - You cannot set a direct cross-Region snapshot in Redshift. You need to configure an automated snapshot in the same Region as the Redshift cluster and then you can configure Amazon Redshift to automatically copy snapshots (automated or manual) for a cluster to another AWS Region. When a snapshot is created in the cluster's primary AWS Region, it's copied to a secondary AWS Region. The two AWS Regions are known respectively as the source AWS Region and destination AWS Region. In addition, you should note that using a snapshot copy can make the data available in the other Region which can be used for the QuickSight dashboard, however, this would turn out to be an expensive solution. So this option is not the best fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\">https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/regions.html\">https://docs.aws.amazon.com/quicksight/latest/user/regions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/welcome.html\">https://docs.aws.amazon.com/quicksight/latest/user/welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/regions.html\">https://docs.aws.amazon.com/quicksight/latest/user/regions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure a new security group for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in ap-south-1</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon QuickSight is a cloud-powered business analytics service that allows users to build visualizations, perform ad-hoc analysis, and quickly get business insights from their data."
      },
      {
        "answer": "",
        "explanation": "Amazon QuickSight is built with \"SPICE\" – a Super-fast, Parallel, In-memory Calculation Engine. Built from the ground up for the cloud, SPICE uses a combination of columnar storage, in-memory technologies enabled through the latest hardware innovations, and machine code generation to run interactive queries on large datasets and get rapid responses."
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale."
      },
      {
        "answer": "",
        "explanation": "For Amazon QuickSight to connect to an Amazon Redshift instance, you must create a new security group for that instance. This security group contains an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in that AWS Region. You can configure such a security group irrespective of whether the Redshift cluster has been created in a VPC or not."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q19-i1.jpg",
        "answer": "",
        "explanation": "Manually enabling access to an Amazon Redshift cluster in a VPC:"
      },
      {
        "link": "https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q19-i2.jpg",
        "answer": "",
        "explanation": "Manually enabling access to an Amazon Redshift cluster that is not in a VPC:"
      },
      {
        "link": "https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure a new Network Access Control List (NACL) for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in ap-south-1</strong> - A Network Access Control List (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. NACLs are associated with subnets, which are in turn created within VPCs. You should note that a Redshift cluster, as well as a QuickSight dashboard, can be created outside of a VPC. So this option is incorrect for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up a VPC endpoint from the Amazon QuickSight VPC in ap-south-1 Region to the Amazon Redshift VPC in us-east-1 Region, so QuickSight can privately access data from Redshift</strong> - Only the QuickSight Enterprise edition can be integrated with the Amazon VPC service. A VPC endpoint enables connections between a virtual private cloud (VPC) and supported services, without requiring that you use an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Therefore, you control the specific API endpoints, sites, and services that are reachable from your VPC. VPC endpoints can only be used to access resources in the same Region as the endpoint. Since the QuickSight dashboard and Redshift cluster are in separate Regions, you cannot use VPC endpoint for the given scenario."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up a daily cross-Region snapshot for Redshift and set the destination Region as ap-south-1. Restore the Amazon Redshift Cluster from the snapshot ap-south-1 Region and connect the  QuickSight dashboard in ap-south-1 to this redshift cluster</strong> - You cannot set a direct cross-Region snapshot in Redshift. You need to configure an automated snapshot in the same Region as the Redshift cluster and then you can configure Amazon Redshift to automatically copy snapshots (automated or manual) for a cluster to another AWS Region. When a snapshot is created in the cluster's primary AWS Region, it's copied to a secondary AWS Region. The two AWS Regions are known respectively as the source AWS Region and destination AWS Region. In addition, you should note that using a snapshot copy can make the data available in the other Region which can be used for the QuickSight dashboard, however, this would turn out to be an expensive solution. So this option is not the best fit for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html",
      "https://docs.aws.amazon.com/quicksight/latest/user/regions.html",
      "https://docs.aws.amazon.com/quicksight/latest/user/welcome.html"
    ]
  },
  {
    "id": 64,
    "question": "<p>A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects.</p>\n\n<p>What are your recommendations to address these guidelines? (Select two) ?</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Change the configuration on the Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable versioning on the Amazon S3 bucket</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Establish a process to get managerial approval for deleting Amazon S3 objects</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Enable versioning on the Amazon S3 bucket</strong></p>\n\n<p>Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket.\nVersioning-enabled buckets enable you to recover objects from accidental deletion or overwrite.</p>\n\n<p>For example:</p>\n\n<p>If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version.\nIf you delete an object, instead of removing it permanently, Amazon S3 inserts a delete marker, which becomes the current object version. You can always restore the previous version. Hence, this is the correct option.</p>\n\n<p>Versioning Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q43-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q43-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a><p></p>\n\n<p><strong>Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket</strong></p>\n\n<p>To provide additional protection, multi-factor authentication (MFA) delete can be enabled. MFA delete requires secondary authentication to take place before objects can be permanently deleted from an Amazon S3 bucket. Hence, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager</strong> - Sending an event trigger after object deletion does not meet the objective of preventing object deletion by mistake because the object has already been deleted. So, this option is incorrect.</p>\n\n<p><strong>Establish a process to get managerial approval for deleting Amazon S3 objects</strong> - This option for getting managerial approval is just a distractor.</p>\n\n<p><strong>Change the configuration on the Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object</strong> - There is no provision to set up Amazon S3 configuration to ask for additional confirmation before deleting an object. This option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable versioning on the Amazon S3 bucket</strong>"
      },
      {
        "answer": "",
        "explanation": "Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket.\nVersioning-enabled buckets enable you to recover objects from accidental deletion or overwrite."
      },
      {
        "answer": "",
        "explanation": "For example:"
      },
      {
        "answer": "",
        "explanation": "If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version.\nIf you delete an object, instead of removing it permanently, Amazon S3 inserts a delete marker, which becomes the current object version. You can always restore the previous version. Hence, this is the correct option."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt2-q43-i1.jpg",
        "answer": "",
        "explanation": "Versioning Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket</strong>"
      },
      {
        "answer": "",
        "explanation": "To provide additional protection, multi-factor authentication (MFA) delete can be enabled. MFA delete requires secondary authentication to take place before objects can be permanently deleted from an Amazon S3 bucket. Hence, this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager</strong> - Sending an event trigger after object deletion does not meet the objective of preventing object deletion by mistake because the object has already been deleted. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Establish a process to get managerial approval for deleting Amazon S3 objects</strong> - This option for getting managerial approval is just a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the configuration on the Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object</strong> - There is no provision to set up Amazon S3 configuration to ask for additional confirmation before deleting an object. This option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html"
    ]
  },
  {
    "id": 65,
    "question": "<p>A financial services company is planning to establish a data mesh architecture that facilitates centralized data governance, analysis, and access control. The company has opted to utilize AWS Glue for managing data catalogs and conducting extract, transform, and load (ETL) operations.</p>\n\n<p>What combination of AWS services would be suitable to implement this data mesh effectively? (Select two)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage AWS Lake Formation to centrally govern, secure, and globally share data</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Leverage AWS Data Exchange to globally share data and control access</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Choose Amazon S3 as the data storage service and leverage Amazon Athena for data analysis</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Leverage AWS DataSync to globally share data and control access</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Leverage AWS Glue DataBrew integration with AWS Glue Studio to orchestrate data sharing and control access</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Choose Amazon S3 as the data storage service and leverage Amazon Athena for data analysis</strong></p>\n\n<p><strong>Leverage AWS Lake Formation to centrally govern, secure, and globally share data</strong></p>\n\n<p>A data mesh architecture empowers business units (organized into domains) to have high ownership and autonomy for the technologies they use while providing technology that enforces data security policies both within and between domains through data sharing. Data consumers request access to these data products, which are approved by producer owners within a framework that provides decentralized governance, but centralized monitoring and auditing of the data sharing process.</p>\n\n<p>AWS Lake Formation helps you centrally govern, secure, and globally share data for analytics and machine learning. With Lake Formation, you can manage fine-grained access control for your data lake data on Amazon Simple Storage Service (Amazon S3) and its metadata in the AWS Glue Data Catalog.</p>\n\n<p>Lake Formation tag-based access control (TBAC) solves the role explosion problem by allowing data stewards to create LF-tags (based on their business needs) that are attached to resources. You can create policies on a smaller number of logical tags instead of specifying policies on named resources. LF-tags enable you to categorize and explore data based on taxonomies, which reduces policy complexity and scales permissions management. You can create and manage policies with tens of logical tags instead of thousands of resources.</p>\n\n<p>Lake Formation TBAC decouples policy creation from resource creation, which helps data stewards manage permissions on many databases, tables, and columns by removing the need to update policies every time a new resource is added to the data lake. Finally, TBAC allows you to create policies even before the resources come into existence. All you have to do is tag the resource with the right LF-tag to make sure existing policies manage it.</p>\n\n<p>Here is a deep-dive into data mesh implementation using Lake Formation with a case study:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/build-a-modern-data-architecture-and-data-mesh-pattern-at-scale-using-aws-lake-formation-tag-based-access-control/\">https://aws.amazon.com/blogs/big-data/build-a-modern-data-architecture-and-data-mesh-pattern-at-scale-using-aws-lake-formation-tag-based-access-control/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage AWS DataSync to globally share data and control access</strong> - AWS DataSync is an online data movement and discovery service that simplifies and accelerates data migrations to AWS as well as moving data to and from on-premises storage, edge locations, other cloud providers, and AWS Storage services. This option acts as a distractor.</p>\n\n<p><strong>Leverage AWS Data Exchange to globally share data and control access</strong> - AWS Data Exchange is a service that makes it easy for customers to find, subscribe to, and use third-party data in the AWS Cloud. This option acts as a distractor.</p>\n\n<p><strong>Leverage AWS Glue DataBrew integration with AWS Glue Studio to orchestrate data sharing and control access</strong> - AWS Glue DataBrew is a fully managed visual data preparation service for cleaning, normalizing, and transforming data. AWS Glue DataBrew is integrated with AWS Glue Studio, so you can orchestrate DataBrew recipes within your AWS Glue ETL jobs and workflows. You cannot use these services to implement a data mesh.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/what-is/data-mesh/\">https://aws.amazon.com/what-is/data-mesh/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/build-a-modern-data-architecture-and-data-mesh-pattern-at-scale-using-aws-lake-formation-tag-based-access-control/\">https://aws.amazon.com/blogs/big-data/build-a-modern-data-architecture-and-data-mesh-pattern-at-scale-using-aws-lake-formation-tag-based-access-control/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Choose Amazon S3 as the data storage service and leverage Amazon Athena for data analysis</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Lake Formation to centrally govern, secure, and globally share data</strong>"
      },
      {
        "answer": "",
        "explanation": "A data mesh architecture empowers business units (organized into domains) to have high ownership and autonomy for the technologies they use while providing technology that enforces data security policies both within and between domains through data sharing. Data consumers request access to these data products, which are approved by producer owners within a framework that provides decentralized governance, but centralized monitoring and auditing of the data sharing process."
      },
      {
        "answer": "",
        "explanation": "AWS Lake Formation helps you centrally govern, secure, and globally share data for analytics and machine learning. With Lake Formation, you can manage fine-grained access control for your data lake data on Amazon Simple Storage Service (Amazon S3) and its metadata in the AWS Glue Data Catalog."
      },
      {
        "answer": "",
        "explanation": "Lake Formation tag-based access control (TBAC) solves the role explosion problem by allowing data stewards to create LF-tags (based on their business needs) that are attached to resources. You can create policies on a smaller number of logical tags instead of specifying policies on named resources. LF-tags enable you to categorize and explore data based on taxonomies, which reduces policy complexity and scales permissions management. You can create and manage policies with tens of logical tags instead of thousands of resources."
      },
      {
        "answer": "",
        "explanation": "Lake Formation TBAC decouples policy creation from resource creation, which helps data stewards manage permissions on many databases, tables, and columns by removing the need to update policies every time a new resource is added to the data lake. Finally, TBAC allows you to create policies even before the resources come into existence. All you have to do is tag the resource with the right LF-tag to make sure existing policies manage it."
      },
      {
        "answer": "",
        "explanation": "Here is a deep-dive into data mesh implementation using Lake Formation with a case study:"
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/build-a-modern-data-architecture-and-data-mesh-pattern-at-scale-using-aws-lake-formation-tag-based-access-control/",
        "answer": "",
        "explanation": "<a href=\"https://aws.amazon.com/blogs/big-data/build-a-modern-data-architecture-and-data-mesh-pattern-at-scale-using-aws-lake-formation-tag-based-access-control/\">https://aws.amazon.com/blogs/big-data/build-a-modern-data-architecture-and-data-mesh-pattern-at-scale-using-aws-lake-formation-tag-based-access-control/</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS DataSync to globally share data and control access</strong> - AWS DataSync is an online data movement and discovery service that simplifies and accelerates data migrations to AWS as well as moving data to and from on-premises storage, edge locations, other cloud providers, and AWS Storage services. This option acts as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Data Exchange to globally share data and control access</strong> - AWS Data Exchange is a service that makes it easy for customers to find, subscribe to, and use third-party data in the AWS Cloud. This option acts as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Glue DataBrew integration with AWS Glue Studio to orchestrate data sharing and control access</strong> - AWS Glue DataBrew is a fully managed visual data preparation service for cleaning, normalizing, and transforming data. AWS Glue DataBrew is integrated with AWS Glue Studio, so you can orchestrate DataBrew recipes within your AWS Glue ETL jobs and workflows. You cannot use these services to implement a data mesh."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/build-a-modern-data-architecture-and-data-mesh-pattern-at-scale-using-aws-lake-formation-tag-based-access-control/",
      "https://aws.amazon.com/what-is/data-mesh/"
    ]
  }
]