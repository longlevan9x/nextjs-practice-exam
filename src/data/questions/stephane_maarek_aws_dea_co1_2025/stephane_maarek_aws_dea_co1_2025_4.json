[
  {
    "id": 1,
    "question": "<p>A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams.</p>\n\n<p>Which of the following would you suggest as the fastest possible way of building a solution for this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams</strong></p>\n\n<p>You can achieve this by using AWS Database Migration Service (AWS DMS). AWS DMS enables you to seamlessly migrate data from supported sources to relational databases, data warehouses, streaming platforms, and other data stores in the AWS cloud.</p>\n\n<p>The given requirement needs the functionality to be implemented in the least possible time. You can use AWS DMS for such data-processing requirements. AWS DMS lets you expand the existing application to stream data from Amazon S3 into Amazon Kinesis Data Streams for real-time analytics without writing and maintaining new code. AWS DMS supports specifying Amazon S3 as the source and streaming services like Kinesis and Amazon Managed Streaming of Kafka (Amazon MSK) as the target. AWS DMS allows migration of full and change data capture (CDC) files to these services. AWS DMS performs this task out of box without any complex configuration or code development. You can also configure an AWS DMS replication instance to scale up or down depending on the workload.</p>\n\n<p>AWS DMS supports Amazon S3 as the source and Kinesis as the target, so data stored in an S3 bucket is streamed to Kinesis. Several consumers, such as AWS Lambda, Amazon Kinesis Data Firehose, Amazon Kinesis Data Analytics, and the Kinesis Consumer Library (KCL), can consume the data concurrently to perform real-time analytics on the dataset. Each AWS service in this architecture can scale independently as needed.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q49-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q49-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/\">https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams</strong> - You will need to enable AWS Cloudtrail trail to use object-level actions as a trigger for Amazon EventBridge events. Also, using AWS Lambda functions would require significant custom development to write the data into Amazon Kinesis Data Streams, so this option is not the right fit.</p>\n\n<p><strong>Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams</strong> - Using AWS Lambda functions would require significant custom development to write the data into Amazon Kinesis Data Streams, so this option is not the right fit.</p>\n\n<p><strong>Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams</strong> - Amazon S3 cannot directly write data into Amazon SNS, although it can certainly use Amazon S3 event notifications to send an event to Amazon SNS. Also, Amazon SNS cannot directly send messages to Amazon Kinesis Data Streams. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/\">https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams</strong>"
      },
      {
        "answer": "",
        "explanation": "You can achieve this by using AWS Database Migration Service (AWS DMS). AWS DMS enables you to seamlessly migrate data from supported sources to relational databases, data warehouses, streaming platforms, and other data stores in the AWS cloud."
      },
      {
        "answer": "",
        "explanation": "The given requirement needs the functionality to be implemented in the least possible time. You can use AWS DMS for such data-processing requirements. AWS DMS lets you expand the existing application to stream data from Amazon S3 into Amazon Kinesis Data Streams for real-time analytics without writing and maintaining new code. AWS DMS supports specifying Amazon S3 as the source and streaming services like Kinesis and Amazon Managed Streaming of Kafka (Amazon MSK) as the target. AWS DMS allows migration of full and change data capture (CDC) files to these services. AWS DMS performs this task out of box without any complex configuration or code development. You can also configure an AWS DMS replication instance to scale up or down depending on the workload."
      },
      {
        "answer": "",
        "explanation": "AWS DMS supports Amazon S3 as the source and Kinesis as the target, so data stored in an S3 bucket is streamed to Kinesis. Several consumers, such as AWS Lambda, Amazon Kinesis Data Firehose, Amazon Kinesis Data Analytics, and the Kinesis Consumer Library (KCL), can consume the data concurrently to perform real-time analytics on the dataset. Each AWS service in this architecture can scale independently as needed."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams</strong> - You will need to enable AWS Cloudtrail trail to use object-level actions as a trigger for Amazon EventBridge events. Also, using AWS Lambda functions would require significant custom development to write the data into Amazon Kinesis Data Streams, so this option is not the right fit."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams</strong> - Using AWS Lambda functions would require significant custom development to write the data into Amazon Kinesis Data Streams, so this option is not the right fit."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams</strong> - Amazon S3 cannot directly write data into Amazon SNS, although it can certainly use Amazon S3 event notifications to send an event to Amazon SNS. Also, Amazon SNS cannot directly send messages to Amazon Kinesis Data Streams. So this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/"
    ]
  },
  {
    "id": 2,
    "question": "<p>A company uses Amazon S3 as its data lake solution, where it stores all of its data including personally identifiable information (PII). This data is used by multiple teams and user groups. As a data engineer, you have been tasked to ensure that user teams and groups can access only the PII that they require.</p>\n\n<p>How will you implement a solution for this requirement with the LEAST operational effort?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon QuickSight to restrict access to a dataset by configuring row-level security (RLS) on it based on the PII needs of each team. To do this, you create a query or file that has one column named <code>UserName</code> or <code>GroupName</code>and then add one column to the query or file for each field that you want to grant or restrict access to</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon S3 Access Points to create unique access control policies for each access point to easily control access to shared datasets. Create different Access Points to different user groups with different PII access requirements</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up AWS Lake Formation and create data filters based on the access permissions needed to each user group. Grant the data filter permissions to different IAM roles. Assign the IAM roles to users based on PII access needs. Use Athena to query the data</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create IAM roles that have different levels of granular access for different PII requirements. Assign the IAM roles to IAM groups. Use an identity-based policy to row and column level access to the IAM groups</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS Lake Formation and create data filters based on the access permissions needed for each user group. Grant the data filter permissions to different IAM roles. Assign the IAM roles to users as needed. Use Athena to query the data</strong></p>\n\n<p>You can implement column-level, row-level, and cell-level security by creating data filters. You select a data filter when you grant the SELECT Lake Formation permission on tables. If your table contains nested column structures, you can define a data filter by including or excluding the child columns and define row-level filter expressions on nested attributes.</p>\n\n<p>You can grant the SELECT, DESCRIBE, and DROP Lake Formation permissions on data filters to principals.</p>\n\n<p>Creating data filter from the Lake Formation console:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q13-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q13-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/data-filters-about.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/data-filters-about.html</a><p></p>\n\n<p>Granting data filter permissions:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q13-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q13-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/granting-filter-perms.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/granting-filter-perms.html</a><p></p>\n\n<p>AWS Lake Formation allows you to define and enforce database, table, and column-level access policies when using Athena queries to read data stored in Amazon S3. Lake Formation provides an authorization and governance layer on data stored in Amazon S3.</p>\n\n<p>Lake Formation permissions apply when using Athena to query source data from Amazon S3 locations that are registered with Lake Formation. Lake Formation permissions also apply when you create databases and tables that point to registered Amazon S3 data locations. To use Athena with data registered using Lake Formation, Athena must be configured to use the AWS Glue Data Catalog.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon QuickSight to restrict access to a dataset by configuring row-level security (RLS) on it based on the PII needs of each team. To do this, you create a query or file that has one column named <code>UserName</code> or <code>GroupName</code>and then add one column to the query or file for each field that you want to grant or restrict access to</strong> - Using row-level security (RLS) with user-based rules to restrict access to a dataset is only available for Enterprise edition.</p>\n\n<p><strong>Use Amazon S3 Access Points to create unique access control policies for each access point to easily control access to shared datasets. Create different Access Points for different user groups with different PII access requirements</strong> - Amazon S3 Access Points, a feature of S3, simplify data access for any AWS service or customer application that stores data in S3. With S3 Access Points, customers can create unique access control policies for each access point to easily control access to shared datasets. Fine-grained row-level and column-level access control are not possible with S3 Access Points.</p>\n\n<p><strong>Create IAM roles that have different levels of granular access for different PII requirements. Assign the IAM roles to IAM groups. Use an identity-based policy to row and column level access to the IAM groups</strong> - Identity-based IAM policies cannot provide the fine-grained access needed for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/data-filters-about.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/data-filters-about.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/security-athena-lake-formation.html\">https://docs.aws.amazon.com/athena/latest/ug/security-athena-lake-formation.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up AWS Lake Formation and create data filters based on the access permissions needed for each user group. Grant the data filter permissions to different IAM roles. Assign the IAM roles to users as needed. Use Athena to query the data</strong>"
      },
      {
        "answer": "",
        "explanation": "You can implement column-level, row-level, and cell-level security by creating data filters. You select a data filter when you grant the SELECT Lake Formation permission on tables. If your table contains nested column structures, you can define a data filter by including or excluding the child columns and define row-level filter expressions on nested attributes."
      },
      {
        "answer": "",
        "explanation": "You can grant the SELECT, DESCRIBE, and DROP Lake Formation permissions on data filters to principals."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q13-i1.jpg",
        "answer": "",
        "explanation": "Creating data filter from the Lake Formation console:"
      },
      {
        "link": "https://docs.aws.amazon.com/lake-formation/latest/dg/data-filters-about.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q13-i2.jpg",
        "answer": "",
        "explanation": "Granting data filter permissions:"
      },
      {
        "link": "https://docs.aws.amazon.com/lake-formation/latest/dg/granting-filter-perms.html"
      },
      {
        "answer": "",
        "explanation": "AWS Lake Formation allows you to define and enforce database, table, and column-level access policies when using Athena queries to read data stored in Amazon S3. Lake Formation provides an authorization and governance layer on data stored in Amazon S3."
      },
      {
        "answer": "",
        "explanation": "Lake Formation permissions apply when using Athena to query source data from Amazon S3 locations that are registered with Lake Formation. Lake Formation permissions also apply when you create databases and tables that point to registered Amazon S3 data locations. To use Athena with data registered using Lake Formation, Athena must be configured to use the AWS Glue Data Catalog."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon QuickSight to restrict access to a dataset by configuring row-level security (RLS) on it based on the PII needs of each team. To do this, you create a query or file that has one column named <code>UserName</code> or <code>GroupName</code>and then add one column to the query or file for each field that you want to grant or restrict access to</strong> - Using row-level security (RLS) with user-based rules to restrict access to a dataset is only available for Enterprise edition."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Access Points to create unique access control policies for each access point to easily control access to shared datasets. Create different Access Points for different user groups with different PII access requirements</strong> - Amazon S3 Access Points, a feature of S3, simplify data access for any AWS service or customer application that stores data in S3. With S3 Access Points, customers can create unique access control policies for each access point to easily control access to shared datasets. Fine-grained row-level and column-level access control are not possible with S3 Access Points."
      },
      {
        "answer": "",
        "explanation": "<strong>Create IAM roles that have different levels of granular access for different PII requirements. Assign the IAM roles to IAM groups. Use an identity-based policy to row and column level access to the IAM groups</strong> - Identity-based IAM policies cannot provide the fine-grained access needed for this use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lake-formation/latest/dg/data-filters-about.html",
      "https://docs.aws.amazon.com/lake-formation/latest/dg/granting-filter-perms.html",
      "https://docs.aws.amazon.com/athena/latest/ug/security-athena-lake-formation.html"
    ]
  },
  {
    "id": 3,
    "question": "<p>An analytics company wants to improve the performance of its big data processing workflows running on Amazon Elastic File System (Amazon EFS). Which of the following performance modes should be used for Amazon EFS to address this requirement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Max I/O</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Bursting Throughput</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Provisioned Throughput</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>General Purpose</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Max I/O</strong></p>\n\n<p>How Amazon EFS Works:\n<img src=\"https://d1.awsstatic.com/product-page-diagram_Amazon-EFS-Feb%202021.1936d75b4b5b81b192aada931a1170054e7c196c.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/product-page-diagram_Amazon-EFS-Feb%202021.1936d75b4b5b81b192aada931a1170054e7c196c.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a><p></p>\n\n<p>Max I/O performance mode is used to scale to higher levels of aggregate throughput and operations per second. This scaling is done with a tradeoff of slightly higher latencies for file metadata operations. Highly parallelized applications and workloads, such as big data analysis, media processing, and genomic analysis, can benefit from this mode.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q47-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q47-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provisioned Throughput</strong></p>\n\n<p><strong>Bursting Throughput</strong></p>\n\n<p>These two options have been added as distractors as these refer to the throughput mode of Amazon EFS and not the performance mode. There are two throughput modes to choose from for your file system, Bursting Throughput and Provisioned Throughput. With Bursting Throughput mode, throughput on Amazon EFS scales as the size of your file system in the standard storage class grows. With Provisioned Throughput mode, you can instantly provision the throughput of your file system (in MiB/s) independent of the amount of data stored.</p>\n\n<p><strong>General Purpose</strong> - General Purpose performance mode is ideal for latency-sensitive use cases, like web serving environments, content management systems, home directories, and general file serving. If you don't choose a performance mode when you create your file system, Amazon EFS selects the General Purpose mode for you by default.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Max I/O</strong>"
      },
      {
        "image": "https://d1.awsstatic.com/product-page-diagram_Amazon-EFS-Feb%202021.1936d75b4b5b81b192aada931a1170054e7c196c.png",
        "answer": "",
        "explanation": "How Amazon EFS Works:"
      },
      {
        "link": "https://aws.amazon.com/efs/"
      },
      {
        "answer": "",
        "explanation": "Max I/O performance mode is used to scale to higher levels of aggregate throughput and operations per second. This scaling is done with a tradeoff of slightly higher latencies for file metadata operations. Highly parallelized applications and workloads, such as big data analysis, media processing, and genomic analysis, can benefit from this mode."
      },
      {
        "link": "https://docs.aws.amazon.com/efs/latest/ug/performance.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Provisioned Throughput</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Bursting Throughput</strong>"
      },
      {
        "answer": "",
        "explanation": "These two options have been added as distractors as these refer to the throughput mode of Amazon EFS and not the performance mode. There are two throughput modes to choose from for your file system, Bursting Throughput and Provisioned Throughput. With Bursting Throughput mode, throughput on Amazon EFS scales as the size of your file system in the standard storage class grows. With Provisioned Throughput mode, you can instantly provision the throughput of your file system (in MiB/s) independent of the amount of data stored."
      },
      {
        "answer": "",
        "explanation": "<strong>General Purpose</strong> - General Purpose performance mode is ideal for latency-sensitive use cases, like web serving environments, content management systems, home directories, and general file serving. If you don't choose a performance mode when you create your file system, Amazon EFS selects the General Purpose mode for you by default."
      }
    ],
    "references": [
      "https://aws.amazon.com/efs/",
      "https://docs.aws.amazon.com/efs/latest/ug/performance.html"
    ]
  },
  {
    "id": 4,
    "question": "<p>A financial services company has developed its flagship application on AWS Cloud with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption key.</p>\n\n<p>Which of the following Amazon S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Server-Side Encryption with Amazon S3 managed keys (SSE-S3)</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Server-Side Encryption with Customer-Provided Keys (SSE-C)</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong></p>\n\n<p>You have the following options for protecting data at rest in Amazon S3:</p>\n\n<p>Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p>\n\n<p>Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>For the given use case, the company wants to manage the encryption keys via its custom application and let Amazon S3 manage the encryption, therefore you must use Server-Side Encryption with Customer-Provided Keys (SSE-C).</p>\n\n<p>Server Side Encryption on Amazon S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q45-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q45-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Server-Side Encryption with Amazon S3 managed keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect.</p>\n\n<p><strong>Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)</strong> - Unless you specify otherwise, buckets use SSE-S3 by default to encrypt objects. However, you can choose to configure buckets to use server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) instead. Amazon S3 uses server-side encryption with AWS KMS (SSE-KMS) to encrypt your S3 object data. Also, when SSE-KMS is requested for the object, the S3 checksum as part of the object's metadata, is stored in encrypted form.</p>\n\n<p><strong>Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3</strong> - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong>"
      },
      {
        "answer": "",
        "explanation": "You have the following options for protecting data at rest in Amazon S3:"
      },
      {
        "answer": "",
        "explanation": "Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects."
      },
      {
        "answer": "",
        "explanation": "Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools."
      },
      {
        "answer": "",
        "explanation": "For the given use case, the company wants to manage the encryption keys via its custom application and let Amazon S3 manage the encryption, therefore you must use Server-Side Encryption with Customer-Provided Keys (SSE-C)."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q45-i1.jpg",
        "answer": "",
        "explanation": "Server Side Encryption on Amazon S3:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Server-Side Encryption with Amazon S3 managed keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)</strong> - Unless you specify otherwise, buckets use SSE-S3 by default to encrypt objects. However, you can choose to configure buckets to use server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) instead. Amazon S3 uses server-side encryption with AWS KMS (SSE-KMS) to encrypt your S3 object data. Also, when SSE-KMS is requested for the object, the S3 checksum as part of the object's metadata, is stored in encrypted form."
      },
      {
        "answer": "",
        "explanation": "<strong>Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3</strong> - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"
    ]
  },
  {
    "id": 5,
    "question": "<p>A financial services firm is modernizing its message queuing system by migrating from self-managed message-oriented middleware systems to Amazon SQS. The firm is using SQS to migrate several applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The data engineering team at the firm expects a peak rate of about 2,400 transactions per second to be processed via SQS. The messages must be processed in the order they are received.</p>\n\n<p>Which of the following options can be used to implement this system most cost-effectively?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon SQS standard queue to process the messages</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon SQS FIFO queue in batch mode of 12 transactions per operation to process the transactions at the peak rate</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon SQS FIFO queue in batch mode of 4 transactions per operation to process the transactions at the peak rate</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon SQS FIFO queue in batch mode of 8 transactions per operation to process the transactions at the peak rate</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon SQS FIFO queue in batch mode of 8 transactions per operation to process the transactions at the peak rate</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues - Standard queues vs FIFO queues.</p>\n\n<p>For FIFO queues, the order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out). On the other hand, the standard SQS queues offer best-effort ordering. This means that occasionally, messages might be delivered in an order different from which they were sent.</p>\n\n<p>By default, FIFO queues support up to 300 transactions (API calls) per second (300 send, receive, or delete operations per second). When you batch 10 transactions per operation (maximum), FIFO queues can support up to 3,000 (300<em>10) transactions per second. Therefore, you need to process 8 transactions per operation so that the FIFO queue can support up to 2,400 (300</em>8) transactions per second, which satisfies the peak rate constraint.</p>\n\n<p>FIFO Queues Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q25-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q25-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/about-aws/whats-new/2016/11/amazon-sqs-introduces-fifo-queues-with-exactly-once-processing-and-lower-prices-for-standard-queues/\">https://aws.amazon.com/about-aws/whats-new/2016/11/amazon-sqs-introduces-fifo-queues-with-exactly-once-processing-and-lower-prices-for-standard-queues/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon SQS standard queue to process the messages</strong> - As messages need to be processed in order, therefore standard queues are ruled out.</p>\n\n<p><strong>Use Amazon SQS FIFO queue in batch mode of 12 transactions per operation to process the transactions at the peak rate</strong> - This option has been added as a distractor, as SQS FIFO only supports a maximum of 10 transactions per operation in batch mode.</p>\n\n<p><strong>Use Amazon SQS FIFO queue in batch mode of 4 transactions per operation to process the transactions at the peak rate</strong> - As mentioned earlier in the explanation, you need to use FIFO queues in batch mode and process 8 transactions per operation, so that the FIFO queue can support up to 2,400 transactions per second. With 4 transactions per operation, you can only support up to 1,200 transactions per second.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS FIFO queue in batch mode of 8 transactions per operation to process the transactions at the peak rate</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues - Standard queues vs FIFO queues."
      },
      {
        "answer": "",
        "explanation": "For FIFO queues, the order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out). On the other hand, the standard SQS queues offer best-effort ordering. This means that occasionally, messages might be delivered in an order different from which they were sent."
      },
      {
        "answer": "",
        "explanation": "By default, FIFO queues support up to 300 transactions (API calls) per second (300 send, receive, or delete operations per second). When you batch 10 transactions per operation (maximum), FIFO queues can support up to 3,000 (300<em>10) transactions per second. Therefore, you need to process 8 transactions per operation so that the FIFO queue can support up to 2,400 (300</em>8) transactions per second, which satisfies the peak rate constraint."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q25-i1.jpg",
        "answer": "",
        "explanation": "FIFO Queues Overview:"
      },
      {
        "link": "https://aws.amazon.com/about-aws/whats-new/2016/11/amazon-sqs-introduces-fifo-queues-with-exactly-once-processing-and-lower-prices-for-standard-queues/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS standard queue to process the messages</strong> - As messages need to be processed in order, therefore standard queues are ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS FIFO queue in batch mode of 12 transactions per operation to process the transactions at the peak rate</strong> - This option has been added as a distractor, as SQS FIFO only supports a maximum of 10 transactions per operation in batch mode."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS FIFO queue in batch mode of 4 transactions per operation to process the transactions at the peak rate</strong> - As mentioned earlier in the explanation, you need to use FIFO queues in batch mode and process 8 transactions per operation, so that the FIFO queue can support up to 2,400 transactions per second. With 4 transactions per operation, you can only support up to 1,200 transactions per second."
      }
    ],
    "references": [
      "https://aws.amazon.com/about-aws/whats-new/2016/11/amazon-sqs-introduces-fifo-queues-with-exactly-once-processing-and-lower-prices-for-standard-queues/",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/sqs/features/"
    ]
  },
  {
    "id": 6,
    "question": "<p>The data engineering team at a company wants to create a daily big data analysis job leveraging Spark for analyzing online/offline sales and customer loyalty data to create customized reports on a client-by-client basis. The big data analysis job needs to read the data from Amazon S3 and output it back to Amazon S3.</p>\n\n<p>Which technology do you recommend to run the Big Data analysis job? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Glue</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Batch</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Athena</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Amazon EMR</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Amazon EMR</strong></p>\n\n<p>Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. EMR is used for launching Hadoop / Spark clusters. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p><strong>AWS Glue</strong></p>\n\n<p>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. AWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. AWS Glue jobs extract data, transform it, and load the resulting data back to S3, data stores in a VPC, or on-premises JDBC data stores as a target.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications.</p>\n\n<p><strong>Amazon Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p><strong>AWS Batch</strong> - AWS Batch can be used to plan, schedule, and execute your batch computing workloads on Amazon EC2 Instances. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/emr/\">https://aws.amazon.com/emr/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/\">https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EMR</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. EMR is used for launching Hadoop / Spark clusters. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Glue</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. AWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. AWS Glue jobs extract data, transform it, and load the resulting data back to S3, data stores in a VPC, or on-premises JDBC data stores as a target."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Batch</strong> - AWS Batch can be used to plan, schedule, and execute your batch computing workloads on Amazon EC2 Instances. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted."
      }
    ],
    "references": [
      "https://aws.amazon.com/emr/",
      "https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/"
    ]
  },
  {
    "id": 7,
    "question": "<p>A company runs a real-time data processing application that uses Kinesis Client Library (KCL) to help consume and process data from the real-time data streams. The development team has raised a query on the viability of using the same DynamoDB table for different KCL applications.</p>\n\n<p>Which of the following are correct statements for KCL while consuming Kinesis Data Streams? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Multiple KCL applications can share a DynamoDB table if Amazon Amazon Simple Storage Service (Amazon S3) is configured to save transit data and metadata</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>You can only use DynamoDB for checkpointing KCL</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon Relational Database Service (Amazon RDS) can also be used for checkpointing KCL</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Multiple KCL applications can share a DynamoDB table</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Each KCL application must use its own DynamoDB table</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Each KCL application must use its own DynamoDB table</strong></p>\n\n<p>Users can't use different KCL applications with the same DynamoDB table for the following reasons:</p>\n\n<ol>\n<li><p>Scan operations are used to obtain leases from a DynamoDB table. Therefore, if a table contains leases of different KCL applications, each application could receive a lease that isn't related to the application itself.</p></li>\n<li><p>Shard IDs in streams are used as primary keys in DynamoDB tables during checkpointing. When different KCL applications use the same DynamoDB table and the same shard IDs are used in the streams, inconsistencies in checkpoints can occur.</p></li>\n</ol>\n\n<p><strong>You can only use DynamoDB for checkpointing KCL</strong> - Users can only use DynamoDB as a checkpointing table for the KCL. A DynamoDB table is required as a checkpointing table for the KCL because the KCL behavior and implementation are interconnected with DynamoDB in the following ways:</p>\n\n<ol>\n<li><p>The KCL includes <code>ShardSyncTask.java</code>, which guarantees that shard leases in a stream are included in the DynamoDB table. This check is conducted periodically in the KCL.</p></li>\n<li><p>The KCL includes <code>DynamoDBLeaseTaker.java</code> and <code>DynamoDBLeaseRenewer.java</code>, which are components that manage and update leases in the KCL. <code>DynamoDBLeaseTaker.java</code> and <code>DynamoDBLeaseRenewer.java</code> work with <code>DynamoDBLeaseRefresher.java</code> to make frequent API requests to DynamoDB.</p></li>\n<li><p>When the KCL makes checkpoints, requests from <code>DynamoDBCheckpointer.java</code> and <code>DynamoDBLeaseCoordinator.java</code> are made to DynamoDB.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Multiple KCL applications can share a DynamoDB table</strong></p>\n\n<p><strong>Amazon Relational Database Service (Amazon RDS) can also be used for checkpointing KCL</strong></p>\n\n<p><strong>Multiple KCL applications can share a DynamoDB table if Amazon Amazon Simple Storage Service (Amazon S3) is configured to save transit data and metadata</strong></p>\n\n<p>These three options are incorrect. Each KCL application must use its own DynamoDB table and only DynamoDB can be used for checkpointing KCL.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-kcl-apps-dynamodb-table/\">https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-kcl-apps-dynamodb-table/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Each KCL application must use its own DynamoDB table</strong>"
      },
      {
        "answer": "",
        "explanation": "Users can't use different KCL applications with the same DynamoDB table for the following reasons:"
      },
      {},
      {
        "answer": "",
        "explanation": "<strong>You can only use DynamoDB for checkpointing KCL</strong> - Users can only use DynamoDB as a checkpointing table for the KCL. A DynamoDB table is required as a checkpointing table for the KCL because the KCL behavior and implementation are interconnected with DynamoDB in the following ways:"
      },
      {}
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Multiple KCL applications can share a DynamoDB table</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Relational Database Service (Amazon RDS) can also be used for checkpointing KCL</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Multiple KCL applications can share a DynamoDB table if Amazon Amazon Simple Storage Service (Amazon S3) is configured to save transit data and metadata</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options are incorrect. Each KCL application must use its own DynamoDB table and only DynamoDB can be used for checkpointing KCL."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-kcl-apps-dynamodb-table/"
    ]
  },
  {
    "id": 8,
    "question": "<p>A company has multiple Amazon EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on Amazon S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in an Amazon DynamoDB table.</p>\n\n<p>How would you go about providing private access to these AWS resources which are not part of this custom VPC?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Control (OAC) for Amazon S3 and then connect to the S3 service using the private IP address</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</strong></p>\n\n<p>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p>\n\n<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p>\n\n<p>There are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.</p>\n\n<p>A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:</p>\n\n<p>Amazon S3</p>\n\n<p>Amazon DynamoDB</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC</strong></p>\n\n<p><strong>Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC</strong></p>\n\n<p>Amazon DynamoDB supports AWS PrivateLink. With AWS PrivateLink, you can simplify private network connectivity between virtual private clouds (VPCs), DynamoDB, and your on-premises data centers using interface VPC endpoints and private IP addresses. So, Amazon DynamoDB supports both interface endpoints as well as gateway endpoints. However, to use the interface endpoints, you need to connect to the given services using the private IP address, instead of creating an entry as a target in the route table of the custom VPC. Therefore, both these options are incorrect.</p>\n\n<p><strong>Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Control (OAC) for Amazon S3 and then connect to the S3 service using the private IP address</strong> - Origin Access Control (OAC) is used within the context of Amazon CloudFront. OAC enables CloudFront customers to easily secure their S3 origins by permitting only designated CloudFront distributions to access their S3 buckets. You cannot use OAC to facilitate access to Amazon S3 from a VPC.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2024/03/amazon-dynamodb-aws-privatelink/\">https://aws.amazon.com/about-aws/whats-new/2024/03/amazon-dynamodb-aws-privatelink/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</strong>"
      },
      {
        "answer": "",
        "explanation": "Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic."
      },
      {
        "answer": "",
        "explanation": "A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network."
      },
      {
        "answer": "",
        "explanation": "There are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service."
      },
      {
        "answer": "",
        "explanation": "A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:"
      },
      {
        "answer": "",
        "explanation": "Amazon S3"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB supports AWS PrivateLink. With AWS PrivateLink, you can simplify private network connectivity between virtual private clouds (VPCs), DynamoDB, and your on-premises data centers using interface VPC endpoints and private IP addresses. So, Amazon DynamoDB supports both interface endpoints as well as gateway endpoints. However, to use the interface endpoints, you need to connect to the given services using the private IP address, instead of creating an entry as a target in the route table of the custom VPC. Therefore, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Control (OAC) for Amazon S3 and then connect to the S3 service using the private IP address</strong> - Origin Access Control (OAC) is used within the context of Amazon CloudFront. OAC enables CloudFront customers to easily secure their S3 origins by permitting only designated CloudFront distributions to access their S3 buckets. You cannot use OAC to facilitate access to Amazon S3 from a VPC."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://aws.amazon.com/about-aws/whats-new/2024/03/amazon-dynamodb-aws-privatelink/"
    ]
  },
  {
    "id": 9,
    "question": "<p>A company is looking to develop real-time analytics capabilities and plans to utilize Amazon Kinesis Data Streams and Amazon Redshift to handle streaming data at rates of several gigabytes per second. The company aims to achieve near real-time insights using the existing business intelligence (BI) and analytics tools.</p>\n\n<p>What solution would fulfill these requirements with minimal operational overhead?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up Amazon Redshift streaming ingestion by configuring an external schema that maps to the streaming data source in Amazon Kinesis Data Streams. Create a materialized view that references the external schema. Configure the materialized view to auto-refresh</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up streaming data ingestion in Amazon Kinesis Data Streams and create materialized views directly on top of the stream by using SQL queries. Configure the materialized view to auto-refresh</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up streaming data ingestion in Amazon Kinesis Data Firehose, write the delivery streams data into Amazon S3 and load it into the Redshift cluster using the COPY command on a real-time basis</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up streaming data ingestion in Amazon Kinesis Data Streams, stage the streams data into Amazon S3 and load it into the Redshift cluster using the COPY command on a real-time basis</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up Amazon Redshift streaming ingestion by configuring an external schema that maps to the streaming data source in Amazon Kinesis Data Streams. Create a materialized view that references the external schema. Configure the materialized view to auto-refresh</strong></p>\n\n<p>Setting up Amazon Redshift streaming ingestion involves creating an external schema that maps to the streaming data source and creating a materialized view that references the external schema. Amazon Redshift streaming ingestion supports Kinesis Data Streams as a source. As such, you must have a Kinesis Data Streams source available before configuring streaming ingestion.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q3-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q3-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/\">https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/</a><p></p>\n\n<p>Amazon Redshift streaming ingestion uses a materialized view, which is updated directly from the stream when REFRESH is run. The materialized view maps to the stream data source. You can perform filtering and aggregations on the stream data as part of the materialized view definition. Your streaming ingestion materialized view (the base materialized view) can reference only one stream, but you can create additional materialized views that join with the base materialized view and with other materialized views or tables. The materialized view is set to auto-refresh and will be refreshed as data keeps arriving in the stream.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q3-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q3-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/\">https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/</a><p></p>\n\n<p>For the given use case, you can leverage Amazon Redshift streaming ingestion for Amazon Kinesis Data Streams, which enables you to ingest data directly from the Kinesis data stream without having to stage the data in Amazon Simple Storage Service (Amazon S3). Streaming ingestion allows you to achieve low latency in the order of seconds while ingesting hundreds of megabytes of data into your Amazon Redshift cluster.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up streaming data ingestion in Amazon Kinesis Data Streams, stage the streams data into Amazon S3 and load it into the Redshift cluster using the COPY command on a real-time basis</strong> - Amazon Redshift streaming ingestion allows you to connect to Kinesis Data Streams directly, without the latency and complexity associated with staging the data in Amazon S3 and loading it into the cluster. However, this option proposes staging the streaming data from Kinesis Data Streams into Amazon S3 and then loading it to the Redshift cluster, which represents an unnecessary operational overhead, so this option is incorrect.</p>\n\n<p><strong>Set up streaming data ingestion in Amazon Kinesis Data Firehose, write the delivery streams data into Amazon S3 and load it into the Redshift cluster using the COPY command on a real-time basis</strong> - As mentioned above, writing the delivery streams data from Kinesis Data Firehose into Amazon S3 and then loading to the Redshift cluster represents an unnecessary operational overhead, so this option is incorrect. You should also note that Kinesis Data Firehose can only perform near real-time processing and NOT real-time data streams processing.</p>\n\n<p><strong>Set up streaming data ingestion in Amazon Kinesis Data Streams and create materialized views directly on top of the stream by using SQL queries. Configure the materialized view to auto-refresh</strong> - This option acts as a distractor. You cannot create materialized views directly on top of a stream in Amazon Kinesis Data Streams.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/\">https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion-getting-started.html\">https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion-getting-started.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon Redshift streaming ingestion by configuring an external schema that maps to the streaming data source in Amazon Kinesis Data Streams. Create a materialized view that references the external schema. Configure the materialized view to auto-refresh</strong>"
      },
      {
        "answer": "",
        "explanation": "Setting up Amazon Redshift streaming ingestion involves creating an external schema that maps to the streaming data source and creating a materialized view that references the external schema. Amazon Redshift streaming ingestion supports Kinesis Data Streams as a source. As such, you must have a Kinesis Data Streams source available before configuring streaming ingestion."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/"
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift streaming ingestion uses a materialized view, which is updated directly from the stream when REFRESH is run. The materialized view maps to the stream data source. You can perform filtering and aggregations on the stream data as part of the materialized view definition. Your streaming ingestion materialized view (the base materialized view) can reference only one stream, but you can create additional materialized views that join with the base materialized view and with other materialized views or tables. The materialized view is set to auto-refresh and will be refreshed as data keeps arriving in the stream."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can leverage Amazon Redshift streaming ingestion for Amazon Kinesis Data Streams, which enables you to ingest data directly from the Kinesis data stream without having to stage the data in Amazon Simple Storage Service (Amazon S3). Streaming ingestion allows you to achieve low latency in the order of seconds while ingesting hundreds of megabytes of data into your Amazon Redshift cluster."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up streaming data ingestion in Amazon Kinesis Data Streams, stage the streams data into Amazon S3 and load it into the Redshift cluster using the COPY command on a real-time basis</strong> - Amazon Redshift streaming ingestion allows you to connect to Kinesis Data Streams directly, without the latency and complexity associated with staging the data in Amazon S3 and loading it into the cluster. However, this option proposes staging the streaming data from Kinesis Data Streams into Amazon S3 and then loading it to the Redshift cluster, which represents an unnecessary operational overhead, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up streaming data ingestion in Amazon Kinesis Data Firehose, write the delivery streams data into Amazon S3 and load it into the Redshift cluster using the COPY command on a real-time basis</strong> - As mentioned above, writing the delivery streams data from Kinesis Data Firehose into Amazon S3 and then loading to the Redshift cluster represents an unnecessary operational overhead, so this option is incorrect. You should also note that Kinesis Data Firehose can only perform near real-time processing and NOT real-time data streams processing."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up streaming data ingestion in Amazon Kinesis Data Streams and create materialized views directly on top of the stream by using SQL queries. Configure the materialized view to auto-refresh</strong> - This option acts as a distractor. You cannot create materialized views directly on top of a stream in Amazon Kinesis Data Streams."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/",
      "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion-getting-started.html"
    ]
  },
  {
    "id": 10,
    "question": "<p>During a code review it was discovered that the credentials to access an Amazon Redshift cluster were hard-coded into an AWS Glue job script. As a data engineer, you have been tasked with remediating the security vulnerability in the script and suggesting a way to securely access the needed credentials by the AWS Glue job.</p>\n\n<p>Which options would you recommend for this remediation task? (Select two)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an IAM role with necessary permissions to access the Secrets Manager and attach the role to the Redshift cluster</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an IAM role with necessary permissions to access the Secrets Manager and attach the role to the AWS Glue job</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Store the credentials in the AWS Glue job parameters that will be accessed dynamically when the job is started</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the credentials in AWS Secrets Manager</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create a configuration file on Amazon S3 and access the file using the AWS Glue job</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the credentials in AWS Secrets Manager</strong></p>\n\n<p><strong>Create an IAM role with the necessary permissions to access the Secrets Manager and attach the role to the AWS Glue job</strong></p>\n\n<p>AWS recommends that you use AWS Secrets Manager to supply connection credentials for your data store. Using Secrets Manager this way lets AWS Glue access your secret at runtime for ETL jobs and crawler runs, and helps keep your credentials secure.</p>\n\n<p>To use Secrets Manager with AWS Glue, you must attach an IAM role with the necessary permissions to the AWS Glue job.</p>\n\n<p>Creating secret for AWS Glue:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q8-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q8-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/connection-properties-secrets-manager.html\">https://docs.aws.amazon.com/glue/latest/dg/connection-properties-secrets-manager.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a configuration file on Amazon S3 and access the file using the AWS Glue job</strong> - The configuration file can be read by any other user or program and hence is not secure enough to store Redshift credentials.</p>\n\n<p><strong>Create an IAM role with necessary permissions to access the Secrets Manager and attach the role to the Redshift cluster</strong> - Attaching an IAM role to Redshift will not provide the necessary access to the AWS Glue job.</p>\n\n<p><strong>Store the credentials in the AWS Glue job parameters that will be accessed dynamically when the job is started</strong> - When creating an AWS Glue job, you can provide additional configuration information through the Argument fields (Job Parameters in the console). When writing AWS Glue scripts, you may want to access job parameter values to alter the behavior of your code. AWS Glue has predefined argument names that it recognizes and can be used to set up the script environment for your jobs and job runs. You should never store passwords or other access credentials in AWS Glue job parameters, as these can be compromised.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/connection-properties-secrets-manager.html\">https://docs.aws.amazon.com/glue/latest/dg/connection-properties-secrets-manager.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the credentials in AWS Secrets Manager</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Create an IAM role with the necessary permissions to access the Secrets Manager and attach the role to the AWS Glue job</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS recommends that you use AWS Secrets Manager to supply connection credentials for your data store. Using Secrets Manager this way lets AWS Glue access your secret at runtime for ETL jobs and crawler runs, and helps keep your credentials secure."
      },
      {
        "answer": "",
        "explanation": "To use Secrets Manager with AWS Glue, you must attach an IAM role with the necessary permissions to the AWS Glue job."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q8-i1.jpg",
        "answer": "",
        "explanation": "Creating secret for AWS Glue:"
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/connection-properties-secrets-manager.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a configuration file on Amazon S3 and access the file using the AWS Glue job</strong> - The configuration file can be read by any other user or program and hence is not secure enough to store Redshift credentials."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an IAM role with necessary permissions to access the Secrets Manager and attach the role to the Redshift cluster</strong> - Attaching an IAM role to Redshift will not provide the necessary access to the AWS Glue job."
      },
      {
        "answer": "",
        "explanation": "<strong>Store the credentials in the AWS Glue job parameters that will be accessed dynamically when the job is started</strong> - When creating an AWS Glue job, you can provide additional configuration information through the Argument fields (Job Parameters in the console). When writing AWS Glue scripts, you may want to access job parameter values to alter the behavior of your code. AWS Glue has predefined argument names that it recognizes and can be used to set up the script environment for your jobs and job runs. You should never store passwords or other access credentials in AWS Glue job parameters, as these can be compromised."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/connection-properties-secrets-manager.html"
    ]
  },
  {
    "id": 11,
    "question": "<p>A legacy application is built using a tightly-coupled monolithic architecture. Due to a sharp increase in the number of users, the application performance has degraded. The company now wants to decouple the architecture and adopt AWS microservices architecture. Some of these microservices need to handle fast-running processes whereas other microservices need to handle slower processes.</p>\n\n<p>Which of these options would you identify as the right way of connecting these microservices?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add Amazon EventBridge to decouple the complex architecture</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones</strong></p>\n\n<p>Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p>\n\n<p>Use Amazon SQS to transmit any volume of data, at any level of throughput, without losing messages or requiring other services to be available. Amazon SQS lets you decouple application components so that they run and fail independently, increasing the overall fault tolerance of the system. Multiple copies of every message are stored redundantly across multiple availability zones so that they are available whenever needed. Being able to store the messages and replay them is a very important feature in decoupling the system architecture, as is needed in the current use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones</strong> - Amazon SNS follows the \"publish-subscribe\" (pub-sub) messaging paradigm, with notifications being delivered to clients using a \"push\" mechanism. This is an important difference between Amazon SNS and Amazon SQS. Whereas Amazon SQS is a polling mechanism, that gives applications the chance to poll at their own comfort, the push mechanism assumes the other applications are present. For the current requirement, we need messages to be stored till they are processed by the downstream applications. Hence, Amazon SQS is the right choice.</p>\n\n<p><strong>Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones</strong> - Amazon Kinesis Data Streams are used for streaming real-time high-volume data. Amazon Kinesis is a publish-subscribe model, used when publisher applications need to publish the same data to different consumers in parallel. Amazon SQS is the right fit for the current use case.</p>\n\n<p><strong>Add Amazon EventBridge to decouple the complex architecture</strong> - This event-based service is extremely useful for connecting non-AWS SaaS (Software as a Service) services to AWS services. With Amazon Eventbridge, the downstream application would need to immediately process the events whenever they arrive, thereby making it a tightly coupled scenario. Hence, this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available."
      },
      {
        "answer": "",
        "explanation": "Use Amazon SQS to transmit any volume of data, at any level of throughput, without losing messages or requiring other services to be available. Amazon SQS lets you decouple application components so that they run and fail independently, increasing the overall fault tolerance of the system. Multiple copies of every message are stored redundantly across multiple availability zones so that they are available whenever needed. Being able to store the messages and replay them is a very important feature in decoupling the system architecture, as is needed in the current use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones</strong> - Amazon SNS follows the \"publish-subscribe\" (pub-sub) messaging paradigm, with notifications being delivered to clients using a \"push\" mechanism. This is an important difference between Amazon SNS and Amazon SQS. Whereas Amazon SQS is a polling mechanism, that gives applications the chance to poll at their own comfort, the push mechanism assumes the other applications are present. For the current requirement, we need messages to be stored till they are processed by the downstream applications. Hence, Amazon SQS is the right choice."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones</strong> - Amazon Kinesis Data Streams are used for streaming real-time high-volume data. Amazon Kinesis is a publish-subscribe model, used when publisher applications need to publish the same data to different consumers in parallel. Amazon SQS is the right fit for the current use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Add Amazon EventBridge to decouple the complex architecture</strong> - This event-based service is extremely useful for connecting non-AWS SaaS (Software as a Service) services to AWS services. With Amazon Eventbridge, the downstream application would need to immediately process the events whenever they arrive, thereby making it a tightly coupled scenario. Hence, this option is not correct."
      }
    ],
    "references": [
      "https://aws.amazon.com/sqs/"
    ]
  },
  {
    "id": 12,
    "question": "<p>A company is transitioning a legacy application to a data lake on Amazon S3. During the course of the review, a data engineer discovered duplicates in the legacy data. What is the most efficient way for the data engineer to eliminate these duplicates from the legacy application data with minimal operational effort?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Develop a custom extract, transform, and load (ETL) job in Python using AWS Lambda. Set up the Python dedupe library as a Lambda layer and then leverage the dedupe library to perform data deduplication</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Develop an AWS Glue extract, transform, and load (ETL) job. Convert the Glue DynamicFrame into an Apache Spark DataFrame and leverage the DataFrame.dropDuplicates method for data deduplication</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Develop an AWS Glue extract, transform, and load (ETL) job. Import the Python Pandas library and leverage the DataFrame.dropDuplicates method for data deduplication</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Develop an AWS Glue extract, transform, and load (ETL) job and leverage the FindMatches machine learning (ML) transform for data deduplication</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Develop an AWS Glue extract, transform, and load (ETL) job and leverage the FindMatches machine learning (ML) transform for data deduplication</strong></p>\n\n<p>You can use AWS Glue to orchestrate your ETL (extract, transform, and load) jobs to build data warehouses and data lakes and generate output streams. AWS Glue calls API operations to transform your data, create runtime logs, store your job logic, and create notifications to help you monitor your job runs.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q1-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q1-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a><p></p>\n\n<p>You can create machine learning transforms to cleanse your data. You can call these transforms from your ETL script. Your data passes from transform to transform in a data structure called a DynamicFrame, which is an extension to an Apache Spark SQL DataFrame. The DynamicFrame contains your data, and you reference its schema to process your data. The FindMatches transform enables you to identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly. This will not require writing any code or knowing how machine learning works. You teach this machine learning transform by labeling example datasets to indicate which rows match. The machine learning transform learns which rows should be matched the more you teach it with example labeled data.</p>\n\n<p>This option is the best fit for the given use case as it involves minimal operational effort to eliminate duplicates since it leverages a built-in ML transform that is tailored for deduplication.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Develop a custom extract, transform, and load (ETL) job in Python using AWS Lambda. Set up the Python dedupe library as a Lambda layer and then leverage the dedupe library to perform data deduplication</strong> - This option involves additional overhead of configuring the requisite libraries using an AWS Lambda function layers, so this option is incorrect.</p>\n\n<p><strong>Develop an AWS Glue extract, transform, and load (ETL) job. Import the Python Pandas library and leverage the DataFrame.dropDuplicates method for data deduplication</strong> - This option involves additional overhead of importing the requisite libraries and developing custom code for data deduplication, so this option is incorrect.</p>\n\n<p><strong>Develop an AWS Glue extract, transform, and load (ETL) job. Convert the Glue DynamicFrame into an Apache Spark DataFrame and leverage the DataFrame.dropDuplicates method for data deduplication</strong> - This option involves additional overhead of converting the data into an Apache Spark DataFrame and developing custom code for data deduplication, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-intro-tutorial.html\">https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-intro-tutorial.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/machine-learning.html\">https://docs.aws.amazon.com/glue/latest/dg/machine-learning.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Develop an AWS Glue extract, transform, and load (ETL) job and leverage the FindMatches machine learning (ML) transform for data deduplication</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use AWS Glue to orchestrate your ETL (extract, transform, and load) jobs to build data warehouses and data lakes and generate output streams. AWS Glue calls API operations to transform your data, create runtime logs, store your job logic, and create notifications to help you monitor your job runs."
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html"
      },
      {
        "answer": "",
        "explanation": "You can create machine learning transforms to cleanse your data. You can call these transforms from your ETL script. Your data passes from transform to transform in a data structure called a DynamicFrame, which is an extension to an Apache Spark SQL DataFrame. The DynamicFrame contains your data, and you reference its schema to process your data. The FindMatches transform enables you to identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly. This will not require writing any code or knowing how machine learning works. You teach this machine learning transform by labeling example datasets to indicate which rows match. The machine learning transform learns which rows should be matched the more you teach it with example labeled data."
      },
      {
        "answer": "",
        "explanation": "This option is the best fit for the given use case as it involves minimal operational effort to eliminate duplicates since it leverages a built-in ML transform that is tailored for deduplication."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Develop a custom extract, transform, and load (ETL) job in Python using AWS Lambda. Set up the Python dedupe library as a Lambda layer and then leverage the dedupe library to perform data deduplication</strong> - This option involves additional overhead of configuring the requisite libraries using an AWS Lambda function layers, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Develop an AWS Glue extract, transform, and load (ETL) job. Import the Python Pandas library and leverage the DataFrame.dropDuplicates method for data deduplication</strong> - This option involves additional overhead of importing the requisite libraries and developing custom code for data deduplication, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Develop an AWS Glue extract, transform, and load (ETL) job. Convert the Glue DynamicFrame into an Apache Spark DataFrame and leverage the DataFrame.dropDuplicates method for data deduplication</strong> - This option involves additional overhead of converting the data into an Apache Spark DataFrame and developing custom code for data deduplication, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html",
      "https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-intro-tutorial.html",
      "https://docs.aws.amazon.com/glue/latest/dg/machine-learning.html"
    ]
  },
  {
    "id": 13,
    "question": "<p>The technology team at a Wall Street trading firm uses DynamoDB to facilitate high-frequency trading where multiple trades can try and update an item at the same time.</p>\n\n<p>As a data engineer, which of the following actions would you recommend to make sure that only the last updated value of any item is used in the application?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use ConsistentRead = true while doing PutItem operation for any item</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use ConsistentRead = false while doing PutItem operation for any item</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use ConsistentRead = true while doing UpdateItem operation for any item</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use ConsistentRead = true while doing GetItem operation for any item</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use ConsistentRead = true while doing GetItem operation for any item</strong></p>\n\n<p>DynamoDB supports eventually consistent and strongly consistent reads.</p>\n\n<p>Eventually Consistent Reads</p>\n\n<p>When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation. The response might include some stale data. If you repeat your read request after a short time, the response should return the latest data.</p>\n\n<p>Strongly Consistent Reads</p>\n\n<p>When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful.</p>\n\n<p>DynamoDB uses eventually consistent reads by default. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. As per the given use-case, to make sure that only the last updated value of any item is used in the application, you should use strongly consistent reads by setting ConsistentRead = true for GetItem operation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q39-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q39-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use ConsistentRead = true while doing UpdateItem operation for any item</strong></p>\n\n<p><strong>Use ConsistentRead = true while doing PutItem operation for any item</strong></p>\n\n<p><strong>Use ConsistentRead = false while doing PutItem operation for any item</strong></p>\n\n<p>As mentioned in the explanation above, strongly consistent reads apply only while using the read operations (such as GetItem, Query, and Scan). So these three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use ConsistentRead = true while doing GetItem operation for any item</strong>"
      },
      {
        "answer": "",
        "explanation": "DynamoDB supports eventually consistent and strongly consistent reads."
      },
      {
        "answer": "",
        "explanation": "Eventually Consistent Reads"
      },
      {
        "answer": "",
        "explanation": "When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation. The response might include some stale data. If you repeat your read request after a short time, the response should return the latest data."
      },
      {
        "answer": "",
        "explanation": "Strongly Consistent Reads"
      },
      {
        "answer": "",
        "explanation": "When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful."
      },
      {
        "answer": "",
        "explanation": "DynamoDB uses eventually consistent reads by default. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. As per the given use-case, to make sure that only the last updated value of any item is used in the application, you should use strongly consistent reads by setting ConsistentRead = true for GetItem operation."
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use ConsistentRead = true while doing UpdateItem operation for any item</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Use ConsistentRead = true while doing PutItem operation for any item</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Use ConsistentRead = false while doing PutItem operation for any item</strong>"
      },
      {
        "answer": "",
        "explanation": "As mentioned in the explanation above, strongly consistent reads apply only while using the read operations (such as GetItem, Query, and Scan). So these three options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html"
    ]
  },
  {
    "id": 14,
    "question": "<p>The data engineering team at a company has set up a workflow to ingest the clickstream data into the raw zone of the S3 data lake. The team wants to run some SQL based data sanity checks on the raw zone of the data lake. The company has hired you as an AWS Certified Data Engineer Associate to build a serverless solution that involves the least amount of development effort. The solution should be cost-effective and easy to maintain.</p>\n\n<p>Which of the following solutions would you build for this use-case?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Load the incremental raw zone data into RDS on an hourly basis and run the SQL based sanity checks</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Load the incremental raw zone data into Redshift on an hourly basis and run the SQL based sanity checks</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Athena to run SQL based analytics on the data in Amazon S3</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Load the incremental raw zone data into DynamoDB on an hourly basis and run the SQL based sanity checks</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Athena to run SQL based analytics on the data in Amazon S3</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>AWS Athena Benefits:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q35-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q35-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Load the incremental raw zone data into Redshift on an hourly basis and run the SQL based sanity checks</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis.</p>\n\n<p>As the team would have to maintain and monitor the Redshift cluster size and would require significant time to set up the processes to consume the data periodically, so this option is ruled out.</p>\n\n<p><strong>Load the incremental raw zone data into DynamoDB on an hourly basis and run the SQL based sanity checks</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>Loading the incremental data into DynamoDB implies data migration jobs will have to be written via a Lambda function or an EC2 based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance.</p>\n\n<p><strong>Load the incremental raw zone data into RDS on an hourly basis and run the SQL based sanity checks</strong> - Loading the incremental data into RDS implies data migration jobs will have to be written via a Lambda function or an EC2 based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance. Hence this option is not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Athena to run SQL based analytics on the data in Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q35-i1.jpg",
        "answer": "",
        "explanation": "AWS Athena Benefits:"
      },
      {
        "link": "https://aws.amazon.com/athena/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Load the incremental raw zone data into Redshift on an hourly basis and run the SQL based sanity checks</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis."
      },
      {
        "answer": "",
        "explanation": "As the team would have to maintain and monitor the Redshift cluster size and would require significant time to set up the processes to consume the data periodically, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Load the incremental raw zone data into DynamoDB on an hourly basis and run the SQL based sanity checks</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications."
      },
      {
        "answer": "",
        "explanation": "Loading the incremental data into DynamoDB implies data migration jobs will have to be written via a Lambda function or an EC2 based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance."
      },
      {
        "answer": "",
        "explanation": "<strong>Load the incremental raw zone data into RDS on an hourly basis and run the SQL based sanity checks</strong> - Loading the incremental data into RDS implies data migration jobs will have to be written via a Lambda function or an EC2 based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance. Hence this option is not correct."
      }
    ],
    "references": [
      "https://aws.amazon.com/athena/"
    ]
  },
  {
    "id": 15,
    "question": "<p>A media company has recently migrated their technology infrastructure to AWS Cloud. The data engineering team is centralizing database access credentials to align with IAM based authentication.</p>\n\n<p>Which of the following AWS database engines can be configured with IAM Database Authentication? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>RDS PostgreSQL</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>RDS SQL Server</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>RDS Db2</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>RDS Oracle</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>RDS MySQL</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p>You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication.  With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM.</p>\n\n<p><strong>RDS MySQL</strong> - IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS.</p>\n\n<p><strong>RDS PostgreSQL</strong> - IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>RDS Oracle</strong></p>\n\n<p><strong>RDS SQL Server</strong></p>\n\n<p>These two options contradict the details in the explanation above, so these are incorrect.</p>\n\n<p><strong>RDS Db2</strong> - This option has been added as a distractor. Db2 is a family of data management products, including database servers, developed by IBM. RDS does not support Db2 database engine.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication.  With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM."
      },
      {
        "answer": "",
        "explanation": "<strong>RDS MySQL</strong> - IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS."
      },
      {
        "answer": "",
        "explanation": "<strong>RDS PostgreSQL</strong> - IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>RDS Oracle</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>RDS SQL Server</strong>"
      },
      {
        "answer": "",
        "explanation": "These two options contradict the details in the explanation above, so these are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>RDS Db2</strong> - This option has been added as a distractor. Db2 is a family of data management products, including database servers, developed by IBM. RDS does not support Db2 database engine."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html"
    ]
  },
  {
    "id": 16,
    "question": "<p>A trading firm collects daily stock trading data from exchanges and stores it in a data warehouse. The data engineering team at the firm needs a solution that streams data directly into the data repository but should also allow SQL-based data modifications when needed. The solution should facilitate complex analytical queries that execute in the fastest possible time. The solution should also offer a business intelligence dashboard that highlights any stock price anomalies.</p>\n\n<p>Which of the following solutions represents the best fit for the given scenario?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up Amazon Kinesis Data Streams to stream data to Amazon S3. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Athena as a data source</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up Amazon Kinesis Data Firehose to stream data to Amazon S3. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Athena as a data source</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up Amazon Kinesis Data Firehose to stream data to Amazon Redshift. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Redshift as a data source</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up Amazon Kinesis Data Streams to stream data to Amazon Redshift. Create a business intelligence dashboard by using Amazon Athena that has Amazon Redshift as a data source</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up Amazon Kinesis Data Firehose to stream data to Amazon Redshift. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Redshift as a data source</strong></p>\n\n<p>Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms, and delivers streaming data to data lakes, data stores, and analytics services. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards.</p>\n\n<p>Kinesis Data Firehose Overview:\n<img src=\"https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a><p></p>\n\n<p>Amazon Kinesis Data Streams is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. Amazon Kinesis Data Streams is integrated with a number of AWS services, including Amazon Kinesis Data Firehose for near real-time transformation.</p>\n\n<p>Key Concepts for Kinesis Data Streams:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q18-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q18-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q18-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q18-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/getting-started/\">https://aws.amazon.com/kinesis/data-streams/getting-started/</a><p></p>\n\n<p>For the given use case, you can use Kinesis Data Firehose to stream data to Amazon Redshift. For a Redshift destination, streaming data is delivered to an S3 bucket first. Kinesis Data Firehose then issues an Amazon Redshift COPY command to load data from your S3 bucket to your Amazon Redshift cluster. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket. Once the data is in Redshift, you can use Quicksight to create a business intelligence dashboard that has Redshift as the data source.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-rs.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-rs.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up Amazon Kinesis Data Streams to stream data to Amazon Redshift. Create a business intelligence dashboard by using Amazon Athena that has Amazon Redshift as a data source</strong></p>\n\n<p>You can use streaming ingestion to provide low-latency, high-speed ingestion of streaming data from Amazon Kinesis Data Streams and Amazon Managed Streaming for Apache Kafka into an Amazon Redshift provisioned or Amazon Redshift Serverless materialized view. It lowers the time it takes to access data and it reduces storage cost. You can configure streaming ingestion for your Amazon Redshift cluster or for Amazon Redshift Serverless and create a materialized view, using SQL statements. You cannot use Amazon Athena to create a business intelligence dashboard, so this option is incorrect.</p>\n\n<p><strong>Set up Amazon Kinesis Data Firehose to stream data to Amazon S3. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Athena as a data source</strong></p>\n\n<p><strong>Set up Amazon Kinesis Data Streams to stream data to Amazon S3. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Athena as a data source</strong></p>\n\n<p>Storing the data in S3 and then querying the data via Athena would not facilitate the execution of complex queries in the fastest possible time since Redshift has much better performance than Athena for complex analytical queries. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/getting-started/\">https://aws.amazon.com/kinesis/data-streams/getting-started/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/quicksight-redshift-private-connection/\">https://aws.amazon.com/premiumsupport/knowledge-center/quicksight-redshift-private-connection/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\">https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon Kinesis Data Firehose to stream data to Amazon Redshift. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Redshift as a data source</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms, and delivers streaming data to data lakes, data stores, and analytics services. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards."
      },
      {
        "image": "https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png",
        "answer": "",
        "explanation": "Kinesis Data Firehose Overview:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-firehose/"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. Amazon Kinesis Data Streams is integrated with a number of AWS services, including Amazon Kinesis Data Firehose for near real-time transformation."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q18-i1.jpg",
        "answer": "",
        "explanation": "Key Concepts for Kinesis Data Streams:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/getting-started/"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can use Kinesis Data Firehose to stream data to Amazon Redshift. For a Redshift destination, streaming data is delivered to an S3 bucket first. Kinesis Data Firehose then issues an Amazon Redshift COPY command to load data from your S3 bucket to your Amazon Redshift cluster. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket. Once the data is in Redshift, you can use Quicksight to create a business intelligence dashboard that has Redshift as the data source."
      },
      {
        "link": "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon Kinesis Data Streams to stream data to Amazon Redshift. Create a business intelligence dashboard by using Amazon Athena that has Amazon Redshift as a data source</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use streaming ingestion to provide low-latency, high-speed ingestion of streaming data from Amazon Kinesis Data Streams and Amazon Managed Streaming for Apache Kafka into an Amazon Redshift provisioned or Amazon Redshift Serverless materialized view. It lowers the time it takes to access data and it reduces storage cost. You can configure streaming ingestion for your Amazon Redshift cluster or for Amazon Redshift Serverless and create a materialized view, using SQL statements. You cannot use Amazon Athena to create a business intelligence dashboard, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon Kinesis Data Firehose to stream data to Amazon S3. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Athena as a data source</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon Kinesis Data Streams to stream data to Amazon S3. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Athena as a data source</strong>"
      },
      {
        "answer": "",
        "explanation": "Storing the data in S3 and then querying the data via Athena would not facilitate the execution of complex queries in the fastest possible time since Redshift has much better performance than Athena for complex analytical queries. So both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://aws.amazon.com/kinesis/data-streams/getting-started/",
      "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/quicksight-redshift-private-connection/",
      "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html"
    ]
  },
  {
    "id": 17,
    "question": "<p>A government healthcare agency receives multiple compressed (gzip) CSV files containing data about contagious diseases for the past month aggregated from all government-managed hospitals. The files are about ~300 GB and are stored in Amazon Glacier Deep Archive. As per the government guidelines, the agency needs to query a portion of this data to prepare a report every year.</p>\n\n<p>Which of the following is the MOST cost-effective way to query this data?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Load the data into Amazon S3 from Glacier Deep Archive and query the required data with Amazon S3 Select</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Load the data into Amazon S3 from Glacier Deep Archive and query the required data with Amazon Athena</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Load the data to Amazon S3 and query it with Amazon Redshift Spectrum</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage Amazon S3 Select to query data from Glacier Deep Archive directly</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Load the data into Amazon S3 from Glacier Deep Archive and query the required data with Amazon S3 Select</strong> - S3 Select is an Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple SQL expressions without having to retrieve the entire object. S3 Select simplifies and improves the performance of scanning and filtering the contents of objects into a smaller, targeted dataset by up to 400%. With S3 Select, you can also perform operational investigations on log files in Amazon S3 without the need to operate or manage a compute cluster.</p>\n\n<p>You can use S3 Select to retrieve a subset of data using SQL clauses, like SELECT and WHERE, from objects stored in CSV, JSON, or Apache Parquet format. It also works with objects that are compressed with GZIP or BZIP2 (for CSV and JSON objects only) and server-side encrypted objects.</p>\n\n<p>You can use S3 Select with AWS Lambda to build serverless applications that use S3 Select to efficiently and easily retrieve data from Amazon S3 instead of retrieving and processing entire objects. You can also use S3 Select with Big Data frameworks, such as Presto, Apache Hive, and Apache Spark to scan and filter the data in Amazon S3.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q27-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q27-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/aws/s3-glacier-select/\">https://aws.amazon.com/blogs/aws/s3-glacier-select/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon S3 Select to query data from Glacier Deep Archive directly</strong> - You cannot use Amazon S3 Select to query data from Glacier Deep Archive directly. Glacier Deep Archive has a first-byte latency of hours, so you must migrate the data into Amazon S3 to query the data.</p>\n\n<p><strong>Load the data to Amazon S3 and query it with Amazon Redshift Spectrum</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to run very fast against large datasets. This is a doable solution but is not cost-effective because of the costs involved for Redshift.</p>\n\n<p><strong>Load the data into Amazon S3 from Glacier Deep Archive and query the required data with Amazon Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL queries. Athena is serverless, so there is no infrastructure to set up or manage, and you can start analyzing data immediately. You don’t even need to load your data into Athena, it works directly with data stored in any S3 storage class. Though viable, this option is costlier than S3 Select.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/storage/querying-data-without-servers-or-databases-using-amazon-s3-select/\">https://aws.amazon.com/blogs/storage/querying-data-without-servers-or-databases-using-amazon-s3-select/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/s3-glacier-select/\">https://aws.amazon.com/blogs/aws/s3-glacier-select/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Load the data into Amazon S3 from Glacier Deep Archive and query the required data with Amazon S3 Select</strong> - S3 Select is an Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple SQL expressions without having to retrieve the entire object. S3 Select simplifies and improves the performance of scanning and filtering the contents of objects into a smaller, targeted dataset by up to 400%. With S3 Select, you can also perform operational investigations on log files in Amazon S3 without the need to operate or manage a compute cluster."
      },
      {
        "answer": "",
        "explanation": "You can use S3 Select to retrieve a subset of data using SQL clauses, like SELECT and WHERE, from objects stored in CSV, JSON, or Apache Parquet format. It also works with objects that are compressed with GZIP or BZIP2 (for CSV and JSON objects only) and server-side encrypted objects."
      },
      {
        "answer": "",
        "explanation": "You can use S3 Select with AWS Lambda to build serverless applications that use S3 Select to efficiently and easily retrieve data from Amazon S3 instead of retrieving and processing entire objects. You can also use S3 Select with Big Data frameworks, such as Presto, Apache Hive, and Apache Spark to scan and filter the data in Amazon S3."
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/s3-glacier-select/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon S3 Select to query data from Glacier Deep Archive directly</strong> - You cannot use Amazon S3 Select to query data from Glacier Deep Archive directly. Glacier Deep Archive has a first-byte latency of hours, so you must migrate the data into Amazon S3 to query the data."
      },
      {
        "answer": "",
        "explanation": "<strong>Load the data to Amazon S3 and query it with Amazon Redshift Spectrum</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to run very fast against large datasets. This is a doable solution but is not cost-effective because of the costs involved for Redshift."
      },
      {
        "answer": "",
        "explanation": "<strong>Load the data into Amazon S3 from Glacier Deep Archive and query the required data with Amazon Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL queries. Athena is serverless, so there is no infrastructure to set up or manage, and you can start analyzing data immediately. You don’t even need to load your data into Athena, it works directly with data stored in any S3 storage class. Though viable, this option is costlier than S3 Select."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/s3-glacier-select/",
      "https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html",
      "https://aws.amazon.com/blogs/storage/querying-data-without-servers-or-databases-using-amazon-s3-select/"
    ]
  },
  {
    "id": 18,
    "question": "<p>The data engineering team at an e-commerce company is looking at setting up a data lake on Amazon S3 that uses a columnar storage format that is optimized for fast retrieval of data. The team wants to ensure that the underlying data storage format also supports complex data types.</p>\n\n<p>Which of the following data file formats would you recommend for the given use case?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>XML</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Avro</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Parquet</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>ORC</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>ORC</strong></p>\n\n<p>Apache Parquet and ORC are columnar storage formats that are optimized for fast retrieval of data and used in AWS analytical applications.</p>\n\n<p>Columnar storage formats have the following characteristics that make them suitable for use with Athena:</p>\n\n<p>Compression by column, with compression algorithm selected for the column data type to save storage space in Amazon S3 and reduce disk space and I/O during query processing.</p>\n\n<p>Predicate pushdown in Parquet and ORC enables Athena queries to fetch only the needed blocks, improving query performance. When an Athena query obtains specific column values from your data, it uses statistics from data block predicates, such as max/min values, to determine whether to read or skip the block.</p>\n\n<p>Splitting of data in Parquet and ORC allows Athena to split the reading of data to multiple readers and increase parallelism during its query processing.</p>\n\n<p>ORC (Optimized Row Columnar) format also provides an efficient way to store Hive data. ORC files are often smaller than Parquet files, and ORC indexes can make querying faster. In addition, ORC supports complex types such as structs, maps, and lists.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q2-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q2-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html\">https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Parquet</strong> - Apache Parquet provides efficient data compression and encoding schemes and is ideal for running complex queries and processing large amounts of data. As mentioned earlier, for complex data types, ORC might be a better choice as it supports a wider range of complex data types.</p>\n\n<p><strong>Avro</strong> - Avro is an open-source object container file format. It features row-based storage. Avro stores data definition in JSON so data can be easily read and interpreted. It uses the JSON file format for defining the data types, and protocols and serializes the data in a compact binary format, making for efficient, resource-sparing storage. A great feature of Avro is Schema evolution which supports data schemas that undergo changes over a period. It deals with schema changes such as missing fields, added fields, and changed fields.</p>\n\n<p><strong>XML</strong> - The XML format does not represent a columnar storage format that is optimized for fast retrieval of data. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html\">https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/serde-about.html\">https://docs.aws.amazon.com/athena/latest/ug/serde-about.html</a></p>\n\n<p><a href=\"https://bryteflow.com/how-to-choose-between-parquet-orc-and-avro/\">https://bryteflow.com/how-to-choose-between-parquet-orc-and-avro/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>ORC</strong>"
      },
      {
        "answer": "",
        "explanation": "Apache Parquet and ORC are columnar storage formats that are optimized for fast retrieval of data and used in AWS analytical applications."
      },
      {
        "answer": "",
        "explanation": "Columnar storage formats have the following characteristics that make them suitable for use with Athena:"
      },
      {
        "answer": "",
        "explanation": "Compression by column, with compression algorithm selected for the column data type to save storage space in Amazon S3 and reduce disk space and I/O during query processing."
      },
      {
        "answer": "",
        "explanation": "Predicate pushdown in Parquet and ORC enables Athena queries to fetch only the needed blocks, improving query performance. When an Athena query obtains specific column values from your data, it uses statistics from data block predicates, such as max/min values, to determine whether to read or skip the block."
      },
      {
        "answer": "",
        "explanation": "Splitting of data in Parquet and ORC allows Athena to split the reading of data to multiple readers and increase parallelism during its query processing."
      },
      {
        "answer": "",
        "explanation": "ORC (Optimized Row Columnar) format also provides an efficient way to store Hive data. ORC files are often smaller than Parquet files, and ORC indexes can make querying faster. In addition, ORC supports complex types such as structs, maps, and lists."
      },
      {
        "link": "https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Parquet</strong> - Apache Parquet provides efficient data compression and encoding schemes and is ideal for running complex queries and processing large amounts of data. As mentioned earlier, for complex data types, ORC might be a better choice as it supports a wider range of complex data types."
      },
      {
        "answer": "",
        "explanation": "<strong>Avro</strong> - Avro is an open-source object container file format. It features row-based storage. Avro stores data definition in JSON so data can be easily read and interpreted. It uses the JSON file format for defining the data types, and protocols and serializes the data in a compact binary format, making for efficient, resource-sparing storage. A great feature of Avro is Schema evolution which supports data schemas that undergo changes over a period. It deals with schema changes such as missing fields, added fields, and changed fields."
      },
      {
        "answer": "",
        "explanation": "<strong>XML</strong> - The XML format does not represent a columnar storage format that is optimized for fast retrieval of data. So, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html",
      "https://docs.aws.amazon.com/athena/latest/ug/serde-about.html",
      "https://bryteflow.com/how-to-choose-between-parquet-orc-and-avro/"
    ]
  },
  {
    "id": 19,
    "question": "<p>An application needs the output of SELECT query from Amazon Athena to be stored in an Amazon S3 bucket in Apache Parquet format. The data engineer has been advised against creating any additional tables in Athena.</p>\n\n<p>How can this be achieved with the LEAST possible effort?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the CREATE TABLE AS SELECT (CTAS) and INSERT INTO statements in Athena</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the SELECT command in Athena</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the CREATE TABLE AS SELECT (CTAS) query in Athena</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the UNLOAD statement in Athena</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the UNLOAD statement in Athena</strong></p>\n\n<p>Amazon Athena automatically stores query results and metadata information for each query that runs in a query result location that you can specify in Amazon S3.</p>\n\n<p>Writes query results from a SELECT statement to the specified data format. Supported formats for UNLOAD include Apache Parquet, ORC, Apache Avro, and JSON. CSV is the only output format supported by the Athena SELECT command, but you can use the UNLOAD command, which supports a variety of output formats, to enclose your SELECT query and rewrite its output to one of the formats that UNLOAD supports.</p>\n\n<p>The UNLOAD statement is useful when you want to output the results of a SELECT query in a non-CSV format but do not require the associated table. For example, a downstream application might require the results of a SELECT query to be in JSON format, and Parquet or ORC might provide a performance advantage over CSV if you intend to use the results of the SELECT query for additional analysis.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the CREATE TABLE AS SELECT (CTAS) query in Athena</strong> - A CREATE TABLE AS SELECT (CTAS) query creates a new table in Athena from the results of a SELECT statement from another query.</p>\n\n<p><strong>Use the SELECT command in Athena</strong> - CSV is the only output format supported by the Athena SELECT command, so this option is incorrect.</p>\n\n<p><strong>Use the CREATE TABLE AS SELECT (CTAS) and INSERT INTO statements in Athena</strong> - You can use Create Table as Select (CTAS) and INSERT INTO statements in Athena to extract, transform, and load (ETL) data into Amazon S3 for data processing. A new table will still be created in Athena.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/unload.html\">https://docs.aws.amazon.com/athena/latest/ug/unload.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/querying.html\">https://docs.aws.amazon.com/athena/latest/ug/querying.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the UNLOAD statement in Athena</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Athena automatically stores query results and metadata information for each query that runs in a query result location that you can specify in Amazon S3."
      },
      {
        "answer": "",
        "explanation": "Writes query results from a SELECT statement to the specified data format. Supported formats for UNLOAD include Apache Parquet, ORC, Apache Avro, and JSON. CSV is the only output format supported by the Athena SELECT command, but you can use the UNLOAD command, which supports a variety of output formats, to enclose your SELECT query and rewrite its output to one of the formats that UNLOAD supports."
      },
      {
        "answer": "",
        "explanation": "The UNLOAD statement is useful when you want to output the results of a SELECT query in a non-CSV format but do not require the associated table. For example, a downstream application might require the results of a SELECT query to be in JSON format, and Parquet or ORC might provide a performance advantage over CSV if you intend to use the results of the SELECT query for additional analysis."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the CREATE TABLE AS SELECT (CTAS) query in Athena</strong> - A CREATE TABLE AS SELECT (CTAS) query creates a new table in Athena from the results of a SELECT statement from another query."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the SELECT command in Athena</strong> - CSV is the only output format supported by the Athena SELECT command, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the CREATE TABLE AS SELECT (CTAS) and INSERT INTO statements in Athena</strong> - You can use Create Table as Select (CTAS) and INSERT INTO statements in Athena to extract, transform, and load (ETL) data into Amazon S3 for data processing. A new table will still be created in Athena."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/unload.html",
      "https://docs.aws.amazon.com/athena/latest/ug/querying.html"
    ]
  },
  {
    "id": 20,
    "question": "<p>The data engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching.</p>\n\n<p>Which of the following steps would you have in the migration checklist? (Select three)</p>",
    "corrects": [
      2,
      3,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Convert the existing standard queue into a FIFO (First-In-First-Out) queue</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue</p>",
        "correct": false
      },
      {
        "id": 6,
        "answer": "<p>Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue</strong></p>\n\n<p><strong>Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix</strong></p>\n\n<p><strong>Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware, and empowers developers to focus on differentiating work. Using Amazon SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p>\n\n<p>Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>By default, FIFO queues support up to 3,000 messages per second with batching, or up to 300 messages per second (300 send, receive, or delete operations per second) without batching. Therefore, using batching you can meet a throughput requirement of up to 3,000 messages per second.</p>\n\n<p>The name of a FIFO queue must end with the .fifo suffix. The suffix counts towards the 80-character queue name limit. To determine whether a queue is FIFO, you can check whether the queue name ends with the suffix.</p>\n\n<p>If you have an existing application that uses standard queues and you want to take advantage of the ordering or exactly-once processing features of FIFO queues, you need to configure the queue and your application correctly. You can't convert an existing standard queue into a FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Convert the existing standard queue into a FIFO (First-In-First-Out) queue</strong> - You cannot convert a standard queue into a FIFO queue.</p>\n\n<p><strong>Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue</strong> - The name of a FIFO queue must end with the .fifo suffix.</p>\n\n<p><strong>Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second</strong> - By default, FIFO queues support up to 3,000 messages per second with batching.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware, and empowers developers to focus on differentiating work. Using Amazon SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available."
      },
      {
        "answer": "",
        "explanation": "Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent."
      },
      {
        "answer": "",
        "explanation": "By default, FIFO queues support up to 3,000 messages per second with batching, or up to 300 messages per second (300 send, receive, or delete operations per second) without batching. Therefore, using batching you can meet a throughput requirement of up to 3,000 messages per second."
      },
      {
        "answer": "",
        "explanation": "The name of a FIFO queue must end with the .fifo suffix. The suffix counts towards the 80-character queue name limit. To determine whether a queue is FIFO, you can check whether the queue name ends with the suffix."
      },
      {
        "answer": "",
        "explanation": "If you have an existing application that uses standard queues and you want to take advantage of the ordering or exactly-once processing features of FIFO queues, you need to configure the queue and your application correctly. You can't convert an existing standard queue into a FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Convert the existing standard queue into a FIFO (First-In-First-Out) queue</strong> - You cannot convert a standard queue into a FIFO queue."
      },
      {
        "answer": "",
        "explanation": "<strong>Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue</strong> - The name of a FIFO queue must end with the .fifo suffix."
      },
      {
        "answer": "",
        "explanation": "<strong>Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second</strong> - By default, FIFO queues support up to 3,000 messages per second with batching."
      }
    ],
    "references": [
      "https://aws.amazon.com/sqs/faqs/",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"
    ]
  },
  {
    "id": 21,
    "question": "<p>An Internet-of-Things (IoT) company is planning on distributing a master sensor in people's homes to measure the key metrics from its smart devices. In order to provide adjustment commands for these devices, the company would like to have a streaming system that supports ordered data based on the sensor's key and also sustains high throughput messages (thousands of messages per second).</p>\n\n<p>Which of the following AWS services would you recommend for this use-case?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Simple Queue Service (Amazon SQS)</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Lambda</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Kinesis Data Streams</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon Simple Notification Service (Amazon SNS)</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service.  KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits by increasing the number of shards within a data stream.</p>\n\n<p>However, there are certain limits you should keep in mind while using Amazon Kinesis Data Streams:</p>\n\n<p>A Kinesis data stream stores records from 24 hours by default, up to 8760 hours (365 days).</p>\n\n<p>The maximum size of a data blob (the data payload before Base64-encoding) within one record is 1 megabyte (MB).\nEach shard can support up to 1000 PUT records per second.</p>\n\n<p>Kinesis is the right answer, since by providing a partition key in your message, you can guarantee ordered messages for a specific sensor, even if your stream is sharded.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Queue Service (Amazon SQS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using Amazon SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Kinesis is better for streaming data since queues aren't meant for real-time streaming of data.</p>\n\n<p><strong>Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS cannot be used for data streaming. Therefore this option is not the best fit for the given use-case.</p>\n\n<p><strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for production-grade serverless log analytics. Lambda isn't meant to retain data either. Therefore this option is not the best fit for the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Streams</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service.  KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits by increasing the number of shards within a data stream."
      },
      {
        "answer": "",
        "explanation": "However, there are certain limits you should keep in mind while using Amazon Kinesis Data Streams:"
      },
      {
        "answer": "",
        "explanation": "A Kinesis data stream stores records from 24 hours by default, up to 8760 hours (365 days)."
      },
      {
        "answer": "",
        "explanation": "The maximum size of a data blob (the data payload before Base64-encoding) within one record is 1 megabyte (MB).\nEach shard can support up to 1000 PUT records per second."
      },
      {
        "answer": "",
        "explanation": "Kinesis is the right answer, since by providing a partition key in your message, you can guarantee ordered messages for a specific sensor, even if your stream is sharded."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (Amazon SQS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using Amazon SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Kinesis is better for streaming data since queues aren't meant for real-time streaming of data."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS cannot be used for data streaming. Therefore this option is not the best fit for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for production-grade serverless log analytics. Lambda isn't meant to retain data either. Therefore this option is not the best fit for the given use-case."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 22,
    "question": "<p>A digital media company needs to manage uploads of around 1 terabyte each from an application being used by a partner company.</p>\n\n<p>How will you handle the upload of these files to Amazon S3?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use multi-part upload feature of Amazon S3</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Direct Connect to provide extra bandwidth</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 Versioning</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Snowball Edge Storage Optimized device</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use multi-part upload feature of Amazon S3</strong></p>\n\n<p>Multi-part upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object.</p>\n\n<p>AWS recommends that you use multi-part uploading in the following ways:\n1. If you're uploading large objects over a stable high-bandwidth network, use multi-part uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance.\n2. If you're uploading over a spotty network, use multi-part uploading to increase resiliency to network errors by avoiding upload restarts. When using multi-part uploading, you need to retry uploading only parts that are interrupted during the upload. You don't need to restart uploading your object from the beginning.</p>\n\n<p>In general, when your object size reaches 100 megabytes, you should consider using multipart uploads instead of uploading the object in a single operation. If the file is greater than 5 gigabytes in size, you must use a multi-part upload to upload that file to Amazon S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon S3 Versioning</strong> - Amazon S3 Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures. When you enable versioning for a bucket, if Amazon S3 receives multiple write requests for the same object simultaneously, it stores all of the objects. If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version.</p>\n\n<p><strong>Use AWS Direct Connect to provide extra bandwidth</strong> - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your data center, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.</p>\n\n<p>AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. This dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon Virtual Private Cloud (VPC) using private IP space, while maintaining network separation between the public and private environments. Virtual interfaces can be reconfigured at any time to meet your changing needs. This is a physical connection that takes at least a month to set up.</p>\n\n<p><strong>Use AWS Snowball Edge Storage Optimized device</strong> - AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 terabytes of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 gigabytes of network connectivity to address large scale data transfer and pre-processing use cases. AWS Snowball is meant to accelerate moving offline data or remote storage to the cloud. It is not the right fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use multi-part upload feature of Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "Multi-part upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object."
      },
      {
        "answer": "",
        "explanation": "AWS recommends that you use multi-part uploading in the following ways:\n1. If you're uploading large objects over a stable high-bandwidth network, use multi-part uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance.\n2. If you're uploading over a spotty network, use multi-part uploading to increase resiliency to network errors by avoiding upload restarts. When using multi-part uploading, you need to retry uploading only parts that are interrupted during the upload. You don't need to restart uploading your object from the beginning."
      },
      {
        "answer": "",
        "explanation": "In general, when your object size reaches 100 megabytes, you should consider using multipart uploads instead of uploading the object in a single operation. If the file is greater than 5 gigabytes in size, you must use a multi-part upload to upload that file to Amazon S3."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Versioning</strong> - Amazon S3 Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures. When you enable versioning for a bucket, if Amazon S3 receives multiple write requests for the same object simultaneously, it stores all of the objects. If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Direct Connect to provide extra bandwidth</strong> - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your data center, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections."
      },
      {
        "answer": "",
        "explanation": "AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. This dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon Virtual Private Cloud (VPC) using private IP space, while maintaining network separation between the public and private environments. Virtual interfaces can be reconfigured at any time to meet your changing needs. This is a physical connection that takes at least a month to set up."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Snowball Edge Storage Optimized device</strong> - AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 terabytes of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 gigabytes of network connectivity to address large scale data transfer and pre-processing use cases. AWS Snowball is meant to accelerate moving offline data or remote storage to the cloud. It is not the right fit for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html"
    ]
  },
  {
    "id": 23,
    "question": "<p>A US-based healthcare startup manages an interactive diagnostic tool for COVID-19 related assessments. The users are required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail on the usage of the encryption key.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use client-side encryption with client-provided keys and then upload the encrypted user data to Amazon S3</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3</strong></p>\n\n<p>AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created.\nSSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Therefore SSE-KMS is the correct solution for this use-case.</p>\n\n<p>Server Side Encryption in S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q43-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q43-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However, this option does not provide the ability to audit trail the usage of the encryption keys.</p>\n\n<p><strong>Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. However, this option is ruled out as the startup does not want to provide the encryption keys.</p>\n\n<p><strong>Use client-side encryption with client-provided keys and then upload the encrypted user data to Amazon S3</strong> - Using client-side encryption with client-provided keys is ruled out as the startup does not want to provide the encryption keys.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created.\nSSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Therefore SSE-KMS is the correct solution for this use-case."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q43-i1.jpg",
        "answer": "",
        "explanation": "Server Side Encryption in S3:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However, this option does not provide the ability to audit trail the usage of the encryption keys."
      },
      {
        "answer": "",
        "explanation": "<strong>Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. However, this option is ruled out as the startup does not want to provide the encryption keys."
      },
      {
        "answer": "",
        "explanation": "<strong>Use client-side encryption with client-provided keys and then upload the encrypted user data to Amazon S3</strong> - Using client-side encryption with client-provided keys is ruled out as the startup does not want to provide the encryption keys."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html"
    ]
  },
  {
    "id": 24,
    "question": "<p>A company has developed Athena SQL queries for Extract, Transform, and Load (ETL) tasks by using Create Table As Select (CTAS). The company now needs to replace SQL with Apache Spark for its new analytics solution.</p>\n\n<p>Which configuration changes are needed to begin using Apache Spark on Amazon Athena ?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a Spark-enabled Athena workgroup</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Activate Apache Spark in the underlying Amazon Athena Engine version</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Update Athena query settings to enable Apache Spark</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Athena Query Editor to access Apache Spark from Athena</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a Spark-enabled Athena workgroup</strong></p>\n\n<p>Amazon Athena makes it easy to interactively run data analytics and exploration using Apache Spark without the need to plan for, configure, or manage resources. Running Apache Spark applications on Athena means submitting Spark code for processing and receiving the results directly without the need for additional configuration. You can use the simplified notebook experience in the Amazon Athena console to develop Apache Spark applications using Python or Athena notebook APIs. Apache Spark on Amazon Athena is serverless and provides automatic, on-demand scaling that delivers instant-on compute to meet changing data volumes and processing requirements.</p>\n\n<p>To get started with Apache Spark on Amazon Athena, you must first create a Spark-enabled Athena workgroup. After you switch to the workgroup, you can create a notebook or open an existing notebook.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update Athena query settings to enable Apache Spark</strong> - This option acts as a distractor. There is no such thing as Athena query settings that can be used to enable Apache Spark.</p>\n\n<p><strong>Activate Apache Spark in the underlying Amazon Athena Engine version</strong> - Athena occasionally releases a new engine version to provide improved performance, functionality, and code fixes. Engine versioning is configured per workgroup. You can use workgroups to control which query engine your queries use and whether to let Athena automatically upgrade your workgroups. There is no such option to directly activate Apache Spark in the underlying Amazon Athena Engine version.</p>\n\n<p><strong>Use Athena Query Editor to access Apache Spark from Athena</strong> - Athena query editor is used to run queries. In fact, after the initial setup, Athena opens in the query editor when accessed from the Athena console. This is no specific configuration to use Apache Spark on Amazon Athena query editor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html\">https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark.html\">https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/engine-versions.html\">https://docs.aws.amazon.com/athena/latest/ug/engine-versions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a Spark-enabled Athena workgroup</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Athena makes it easy to interactively run data analytics and exploration using Apache Spark without the need to plan for, configure, or manage resources. Running Apache Spark applications on Athena means submitting Spark code for processing and receiving the results directly without the need for additional configuration. You can use the simplified notebook experience in the Amazon Athena console to develop Apache Spark applications using Python or Athena notebook APIs. Apache Spark on Amazon Athena is serverless and provides automatic, on-demand scaling that delivers instant-on compute to meet changing data volumes and processing requirements."
      },
      {
        "answer": "",
        "explanation": "To get started with Apache Spark on Amazon Athena, you must first create a Spark-enabled Athena workgroup. After you switch to the workgroup, you can create a notebook or open an existing notebook."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Update Athena query settings to enable Apache Spark</strong> - This option acts as a distractor. There is no such thing as Athena query settings that can be used to enable Apache Spark."
      },
      {
        "answer": "",
        "explanation": "<strong>Activate Apache Spark in the underlying Amazon Athena Engine version</strong> - Athena occasionally releases a new engine version to provide improved performance, functionality, and code fixes. Engine versioning is configured per workgroup. You can use workgroups to control which query engine your queries use and whether to let Athena automatically upgrade your workgroups. There is no such option to directly activate Apache Spark in the underlying Amazon Athena Engine version."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Athena Query Editor to access Apache Spark from Athena</strong> - Athena query editor is used to run queries. In fact, after the initial setup, Athena opens in the query editor when accessed from the Athena console. This is no specific configuration to use Apache Spark on Amazon Athena query editor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html",
      "https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark.html",
      "https://docs.aws.amazon.com/athena/latest/ug/engine-versions.html"
    ]
  },
  {
    "id": 25,
    "question": "<p>A company wants to store business-critical data on Amazon Elastic Block Store (Amazon EBS) volumes which provide persistent storage independent of Amazon EC2 instances. During a test run, the data engineering team found that on terminating an Amazon EC2 instance, the attached Amazon EBS volume was also lost, which was contrary to their assumptions.</p>\n\n<p>How would you explain this issue?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume</strong></p>\n\n<p>Amazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale.</p>\n\n<p>When you launch an instance, the root device volume contains the image used to boot the instance. You can choose between AMIs backed by Amazon EC2 instance store and AMIs backed by Amazon EBS.</p>\n\n<p>By default, the root volume for an AMI backed by Amazon EBS is deleted when the instance terminates. You can change the default behavior to ensure that the volume persists after the instance terminates. Non-root EBS volumes remain available even after you terminate an instance to which the volumes were attached. Therefore, this option is correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume</strong></p>\n\n<p><strong>The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume</strong></p>\n\n<p>Amazon EBS volumes do not need to back up the data on Amazon S3 or Amazon EFS filesystem. Both these options are added as distractors.</p>\n\n<p><strong>On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated</strong> - As mentioned earlier, non-root Amazon EBS volumes remain available even after you terminate an instance to which the volumes were attached. Hence this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale."
      },
      {
        "answer": "",
        "explanation": "When you launch an instance, the root device volume contains the image used to boot the instance. You can choose between AMIs backed by Amazon EC2 instance store and AMIs backed by Amazon EBS."
      },
      {
        "answer": "",
        "explanation": "By default, the root volume for an AMI backed by Amazon EBS is deleted when the instance terminates. You can change the default behavior to ensure that the volume persists after the instance terminates. Non-root EBS volumes remain available even after you terminate an instance to which the volumes were attached. Therefore, this option is correct."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon EBS volumes do not need to back up the data on Amazon S3 or Amazon EFS filesystem. Both these options are added as distractors."
      },
      {
        "answer": "",
        "explanation": "<strong>On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated</strong> - As mentioned earlier, non-root Amazon EBS volumes remain available even after you terminate an instance to which the volumes were attached. Hence this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html"
    ]
  },
  {
    "id": 26,
    "question": "<p>A consulting firm uses Amazon Athena to analyze data in Amazon S3 using SQL. A rapid expansion plan has resulted in the company doubling its data engineers in a year resulting in high usage costs for Athena. Preliminary investigation suggests that most of the day-to-day queries run for only a few seconds fetching limited data. The firm wants to define different thresholds on hourly or daily aggregates on data scanned by the queries.</p>\n\n<p>As an AWS Certified Data Engineer Associate, how will you configure a solution for this requirement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure multiple per-workgroup limits by utilizing the workgroup-wide data usage control limit on Athena</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure a single per-workgroup limit by configuring all the mentioned thresholds into a single limit configuration</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure multiple per-query limits by utilizing the per-query cost control limit feature on Athena</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a single per-query limit by configuring all the mentioned thresholds into a single limit configuration</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure multiple per-workgroup limits by utilizing the workgroup-wide data usage control limit on Athena</strong> - Athena allows you to set two types of cost controls: per-query limit and per-workgroup limit. For each workgroup, you can set only one per-query limit and multiple per-workgroup limits.</p>\n\n<p>The workgroup-wide data usage control limit specifies the total amount of data scanned for all queries that run in this workgroup during the specified time period. You can create multiple limits per workgroup. The workgroup-wide query limit allows you to set multiple thresholds on hourly or daily aggregates on data scanned by queries running in the workgroup.</p>\n\n<p>If the aggregate amount of data scanned exceeds the threshold, you can push a notification to an Amazon SNS topic. You can also create an alarm and an action on any metric that Athena publishes from the CloudWatch console.</p>\n\n<p>Data Usage Control Limits in Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q31-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q31-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html#configure-control-limit-per-query\">https://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html#configure-control-limit-per-query</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure multiple per-query limits by utilizing the per-query cost control limit feature on Athena</strong> - The per-query control limit specifies the total amount of data scanned per query. If any query that runs in the workgroup exceeds the limit, it is canceled. You can create only one per-query control limit in a workgroup and it applies to each query that runs in it.</p>\n\n<p><strong>Configure a single per-query limit by configuring all the mentioned thresholds into a single limit configuration</strong> - This is an invalid statement, given only as a distractor.</p>\n\n<p><strong>Configure a single per-workgroup limit by configuring all the mentioned thresholds into a single limit configuration</strong> - As discussed above, multiple per-workgroup limits have to be defined to cater to hourly and daily aggregate thresholds of data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html#configure-control-limit-per-query\">https://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html#configure-control-limit-per-query</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure multiple per-workgroup limits by utilizing the workgroup-wide data usage control limit on Athena</strong> - Athena allows you to set two types of cost controls: per-query limit and per-workgroup limit. For each workgroup, you can set only one per-query limit and multiple per-workgroup limits."
      },
      {
        "answer": "",
        "explanation": "The workgroup-wide data usage control limit specifies the total amount of data scanned for all queries that run in this workgroup during the specified time period. You can create multiple limits per workgroup. The workgroup-wide query limit allows you to set multiple thresholds on hourly or daily aggregates on data scanned by queries running in the workgroup."
      },
      {
        "answer": "",
        "explanation": "If the aggregate amount of data scanned exceeds the threshold, you can push a notification to an Amazon SNS topic. You can also create an alarm and an action on any metric that Athena publishes from the CloudWatch console."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q31-i1.jpg",
        "answer": "",
        "explanation": "Data Usage Control Limits in Athena:"
      },
      {
        "link": "https://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html#configure-control-limit-per-query"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure multiple per-query limits by utilizing the per-query cost control limit feature on Athena</strong> - The per-query control limit specifies the total amount of data scanned per query. If any query that runs in the workgroup exceeds the limit, it is canceled. You can create only one per-query control limit in a workgroup and it applies to each query that runs in it."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a single per-query limit by configuring all the mentioned thresholds into a single limit configuration</strong> - This is an invalid statement, given only as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a single per-workgroup limit by configuring all the mentioned thresholds into a single limit configuration</strong> - As discussed above, multiple per-workgroup limits have to be defined to cater to hourly and daily aggregate thresholds of data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html#configure-control-limit-per-query"
    ]
  },
  {
    "id": 27,
    "question": "<p>The data engineering team at a company is working on the Disaster Recovery (DR) plans for a Redshift cluster deployed in the us-east-1 Region. The existing cluster is encrypted via AWS KMS and the team wants to copy the Redshift snapshots to another Region to meet the DR requirements.</p>\n\n<p>Which of the following solutions would you suggest to address the given use-case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region replication in the source Region</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region snapshots in the source Region</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create an IAM role in the destination Region with access to the KMS key in the source Region. Create a snapshot copy grant in the destination Region for this KMS key in the source Region. Configure Redshift cross-Region snapshots in the source Region</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a snapshot copy grant in the source Region for a KMS key in the source Region. Configure Redshift cross-Region snapshots in the destination Region</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region snapshots in the source Region</strong></p>\n\n<p>To copy snapshots for AWS KMS–encrypted clusters to another AWS Region, you need to create a grant for Redshift to use a KMS customer master key (CMK) in the destination AWS Region. Then choose that grant when you enable copying of snapshots in the source AWS Region. You cannot use a KMS key from the source Region as AWS KMS keys are specific to an AWS Region.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q37-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q37-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a snapshot copy grant in the source Region for a KMS key in the source Region. Configure Redshift cross-Region snapshots in the destination Region</strong> - As described above, you need to configure the Redshift cross-Region snapshot in the source Region and not the destination Region. Also the snapshot copy grant must be set up in the destination Region for a KMS key in the destination Region.</p>\n\n<p><strong>Create an IAM role in the destination Region with access to the KMS key in the source Region. Create a snapshot copy grant in the destination Region for this KMS key in the source Region. Configure Redshift cross-Region snapshots in the source Region</strong> - This has been added as a distractor as AWS KMS keys are specific to an AWS Region. You cannot create a snapshot copy grant in the destination Region for a KMS key in the source Region.</p>\n\n<p><strong>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region replication in the source Region</strong> - This has been added as a distractor as there is no such thing as cross-Region replication for Redshift. The concept of cross-Region replication (CRR) applies to Amazon S3.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region snapshots in the source Region</strong>"
      },
      {
        "answer": "",
        "explanation": "To copy snapshots for AWS KMS–encrypted clusters to another AWS Region, you need to create a grant for Redshift to use a KMS customer master key (CMK) in the destination AWS Region. Then choose that grant when you enable copying of snapshots in the source AWS Region. You cannot use a KMS key from the source Region as AWS KMS keys are specific to an AWS Region."
      },
      {
        "link": "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a snapshot copy grant in the source Region for a KMS key in the source Region. Configure Redshift cross-Region snapshots in the destination Region</strong> - As described above, you need to configure the Redshift cross-Region snapshot in the source Region and not the destination Region. Also the snapshot copy grant must be set up in the destination Region for a KMS key in the destination Region."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an IAM role in the destination Region with access to the KMS key in the source Region. Create a snapshot copy grant in the destination Region for this KMS key in the source Region. Configure Redshift cross-Region snapshots in the source Region</strong> - This has been added as a distractor as AWS KMS keys are specific to an AWS Region. You cannot create a snapshot copy grant in the destination Region for a KMS key in the source Region."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region replication in the source Region</strong> - This has been added as a distractor as there is no such thing as cross-Region replication for Redshift. The concept of cross-Region replication (CRR) applies to Amazon S3."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant"
    ]
  },
  {
    "id": 28,
    "question": "<p>A company stores its data in a single Amazon S3 bucket that is updated daily. An AWS Glue job processes this data which is consumed by Amazon QuickSight to generate an interactive dashboard. Of late, the dashboard queries have been getting slower and an initial inspection has revealed that AWS Glue jobs are running longer than expected.</p>\n\n<p>As a data engineer, which of the following actions will you take to improve the performance of the AWS Glue job? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Partition the data in the S3 bucket based on year, month, and day</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Setup AWS Glue Streaming to efficiently handle streaming data in near real-time</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Check the IAM permissions on the service role created for the AWS Glue job</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Cross-job data access could be the reason for slow-running queries. Use a separate AWS Glue Spark cluster</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Configure larger Glue job workers by changing the instance type of the workers</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Partition the data in the S3 bucket based on year, month, and day</strong></p>\n\n<p>Partition columns are usually designed by a common query pattern in your use case. For example, a common practice is to partition the data based on year, month, and day because many queries tend to run time series analyses in typical use cases. This often leads to a multi-level partitioning scheme. Data is organized in a hierarchical directory structure based on the distinct values of one or more columns.</p>\n\n<p>Overview of partitions for Amazon S3 bucket:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q7-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q7-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/get-started-managing-partitions-for-amazon-s3-tables-backed-by-the-aws-glue-data-catalog\">https://aws.amazon.com/blogs/big-data/get-started-managing-partitions-for-amazon-s3-tables-backed-by-the-aws-glue-data-catalog</a><p></p>\n\n<p><strong>Configure larger Glue job workers by changing the instance type of the workers</strong></p>\n\n<p>You can manually change the instance type of your AWS Glue workers to use workers with more cores, memory, and storage. Larger worker types make it possible for you to vertically scale and run intensive data integration jobs, such as memory-intensive data transforms, skewed aggregations, and entity-detection checks involving petabytes of data.</p>\n\n<p>Scaling up also assists in cases where the Spark driver needs larger capacity—for instance, because the job query plan is quite large. Using larger workers can also reduce the total number of workers needed, which increases performance by reducing shuffles in intensive operations such as a join.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Cross-job data access could be the reason for slow-running queries. Use a separate AWS Glue Spark cluster</strong> - Cross-job data access is possible if you have two AWS Glue Spark jobs in a single AWS Account, each running in a separate AWS Glue Spark cluster. This option acts as a distractor, as the given use case talks about a single Glue job.</p>\n\n<p><strong>Setup AWS Glue Streaming to efficiently handle streaming data in near real-time</strong> - Glue Streaming is for efficiently handling streaming data in near real-time. The given scenario revolves around slow queries, therefore the use case of streaming is not relevant here.</p>\n\n<p><strong>Check the IAM permissions on the service role created for the AWS Glue job</strong> - IAM permissions are irrelevant here since access is not an issue, rather performance is.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/get-started-managing-partitions-for-amazon-s3-tables-backed-by-the-aws-glue-data-catalog\">https://aws.amazon.com/blogs/big-data/get-started-managing-partitions-for-amazon-s3-tables-backed-by-the-aws-glue-data-catalog</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/tuning-aws-glue-for-apache-spark/scale-cluster-capacity.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/tuning-aws-glue-for-apache-spark/scale-cluster-capacity.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Partition the data in the S3 bucket based on year, month, and day</strong>"
      },
      {
        "answer": "",
        "explanation": "Partition columns are usually designed by a common query pattern in your use case. For example, a common practice is to partition the data based on year, month, and day because many queries tend to run time series analyses in typical use cases. This often leads to a multi-level partitioning scheme. Data is organized in a hierarchical directory structure based on the distinct values of one or more columns."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q7-i1.jpg",
        "answer": "",
        "explanation": "Overview of partitions for Amazon S3 bucket:"
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/get-started-managing-partitions-for-amazon-s3-tables-backed-by-the-aws-glue-data-catalog"
      },
      {
        "answer": "",
        "explanation": "<strong>Configure larger Glue job workers by changing the instance type of the workers</strong>"
      },
      {
        "answer": "",
        "explanation": "You can manually change the instance type of your AWS Glue workers to use workers with more cores, memory, and storage. Larger worker types make it possible for you to vertically scale and run intensive data integration jobs, such as memory-intensive data transforms, skewed aggregations, and entity-detection checks involving petabytes of data."
      },
      {
        "answer": "",
        "explanation": "Scaling up also assists in cases where the Spark driver needs larger capacity—for instance, because the job query plan is quite large. Using larger workers can also reduce the total number of workers needed, which increases performance by reducing shuffles in intensive operations such as a join."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Cross-job data access could be the reason for slow-running queries. Use a separate AWS Glue Spark cluster</strong> - Cross-job data access is possible if you have two AWS Glue Spark jobs in a single AWS Account, each running in a separate AWS Glue Spark cluster. This option acts as a distractor, as the given use case talks about a single Glue job."
      },
      {
        "answer": "",
        "explanation": "<strong>Setup AWS Glue Streaming to efficiently handle streaming data in near real-time</strong> - Glue Streaming is for efficiently handling streaming data in near real-time. The given scenario revolves around slow queries, therefore the use case of streaming is not relevant here."
      },
      {
        "answer": "",
        "explanation": "<strong>Check the IAM permissions on the service role created for the AWS Glue job</strong> - IAM permissions are irrelevant here since access is not an issue, rather performance is."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/get-started-managing-partitions-for-amazon-s3-tables-backed-by-the-aws-glue-data-catalog",
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/tuning-aws-glue-for-apache-spark/scale-cluster-capacity.html"
    ]
  },
  {
    "id": 29,
    "question": "<p>A financial services company bills its clients based on per unit of clickstream data provided to the clients. As the company operates in a regulated industry, it needs to have the same ordered clickstream data available for auditing within a window of 7 days.</p>\n\n<p>Which of the following AWS services provides the ability to run the billing process and auditing process on the given clickstream data in the same order?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Kinesis Data Analytics</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon Simple Queue Service (SQS)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Kinesis Data Streams</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon Kinesis Data Firehose</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\nAmazon Kinesis Data Streams is recommended when you need the ability to consume records in the same order a few hours later.</p>\n\n<p>For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for a maximum of 365 days, you can easily run the audit application up to 7 days behind the billing application.</p>\n\n<p>KDS provides the ability to consume records in the same order a few hours later\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q52-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q52-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Amazon Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.</p>\n\n<p><strong>Amazon Kinesis Data Analytics</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: set up your streaming data sources, write your queries or streaming applications, and set up your destination for processed data. As Amazon Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.</p>\n\n<p><strong>Amazon Simple Queue Service (SQS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO (First-In-First-Out) queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For Amazon SQS, you cannot have the same message being consumed by multiple consumers in the same order a few hours later, therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Streams</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\nAmazon Kinesis Data Streams is recommended when you need the ability to consume records in the same order a few hours later."
      },
      {
        "answer": "",
        "explanation": "For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for a maximum of 365 days, you can easily run the audit application up to 7 days behind the billing application."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q52-i1.jpg",
        "answer": "",
        "explanation": "KDS provides the ability to consume records in the same order a few hours later"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/faqs/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Amazon Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Analytics</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: set up your streaming data sources, write your queries or streaming applications, and set up your destination for processed data. As Amazon Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (SQS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO (First-In-First-Out) queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For Amazon SQS, you cannot have the same message being consumed by multiple consumers in the same order a few hours later, therefore this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/faqs/",
      "https://aws.amazon.com/kinesis/data-firehose/faqs/",
      "https://aws.amazon.com/kinesis/data-analytics/faqs/"
    ]
  },
  {
    "id": 30,
    "question": "<p>A data engineer at an IT company just upgraded an Amazon EC2 instance type from t2.nano (0.5G of RAM, 1 vCPU) to u-12tb1.metal (12.3 TB of RAM, 448 vCPUs). How would you categorize this upgrade?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>This is a scale up example of vertical scaling</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>This is an example of high availability</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>This is a scale up example of horizontal scaling</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>This is a scale out example of vertical scaling</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>This is a scale up example of vertical scaling</strong></p>\n\n<p>Vertical scaling means increasing the size of the instance. For example, your application runs on a t2.micro. Scaling up that application vertically means running it on a larger instance such as t2.large. Scaling down that application vertically means running it on a smaller instance such as t2.nano. Scalability is very common for non-distributed systems, such as a database. There’s usually a limit to how much you can vertically scale (hardware limit).\nIn this case, as the instance type was upgraded from t2.nano to u-12tb1.metal, this is a scale up example of vertical scaling.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>This is a scale up example of horizontal scaling</strong> - Horizontal scaling means increasing the number of instances/systems for your application. When you increase the number of instances, it's called scale out whereas if you decrease the number of instances, it's called scale-in. Scale up is used in conjunction with vertical scaling and not with horizontal scaling. Hence this is incorrect.</p>\n\n<p><strong>This is a scale out example of vertical scaling</strong> - Scale out is used in conjunction with horizontal scaling and not with vertical scaling. Hence this is incorrect.</p>\n\n<p><strong>This is an example of high availability</strong> - High availability means running your application/system in at least 2 data centers (== Availability Zones). The goal of high availability is to survive a data center loss. An example of High Availability is when you run instances for the same application across multi AZ. This option has been added as a distractor.</p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>This is a scale up example of vertical scaling</strong>"
      },
      {
        "answer": "",
        "explanation": "Vertical scaling means increasing the size of the instance. For example, your application runs on a t2.micro. Scaling up that application vertically means running it on a larger instance such as t2.large. Scaling down that application vertically means running it on a smaller instance such as t2.nano. Scalability is very common for non-distributed systems, such as a database. There’s usually a limit to how much you can vertically scale (hardware limit).\nIn this case, as the instance type was upgraded from t2.nano to u-12tb1.metal, this is a scale up example of vertical scaling."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>This is a scale up example of horizontal scaling</strong> - Horizontal scaling means increasing the number of instances/systems for your application. When you increase the number of instances, it's called scale out whereas if you decrease the number of instances, it's called scale-in. Scale up is used in conjunction with vertical scaling and not with horizontal scaling. Hence this is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>This is a scale out example of vertical scaling</strong> - Scale out is used in conjunction with horizontal scaling and not with vertical scaling. Hence this is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>This is an example of high availability</strong> - High availability means running your application/system in at least 2 data centers (== Availability Zones). The goal of high availability is to survive a data center loss. An example of High Availability is when you run instances for the same application across multi AZ. This option has been added as a distractor."
      }
    ],
    "references": []
  },
  {
    "id": 31,
    "question": "<p>A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 terabytes of storage with the maximum possible I/O performance for processing certain video files on its servers. The company also needs close to 450 terabytes of very durable storage for storing media content and almost double of it, i.e. 900 terabytes for archival of legacy data.</p>\n\n<p>Which set of services will you recommend to meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier Deep Archive for archival storage</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier Deep Archive for archival storage</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon EC2 instance store for maximum performance, AWS DataSync for durable data storage and Amazon S3 Glacier Deep Archive for archival storage</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier Deep Archive for archival storage</strong></p>\n\n<p>An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. An instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.</p>\n\n<p>You can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance.</p>\n\n<p>Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures.</p>\n\n<p>Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, Amazon S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.</p>\n\n<p>Amazon S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions. To keep costs low yet suitable for varying needs, Amazon S3 Glacier provides three retrieval options that range from a few minutes to hours. You can upload objects directly to Amazon S3 Glacier, or use S3 Lifecycle policies to transfer data between any of the Amazon S3 Storage Classes for active data (S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA) and S3 Glacier.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage</strong> - Amazon EC2 instance store volumes provide the best I/O performance for low latency requirements, so Amazon S3 standard storage is not the right fit for I/O performance requirements.</p>\n\n<p>The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead.</p>\n\n<p>Amazon S3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice a year. It is designed for customers — particularly those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors — that retain data sets for 7-10 years or longer to meet regulatory compliance requirements.</p>\n\n<p><strong>Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier Deep Archive for archival storage</strong> - Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances. Amazon EBS volumes are particularly well-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage. For high I/O performance, instance store volumes are a better option.</p>\n\n<p><strong>Amazon EC2 instance store for maximum performance, AWS DataSync for durable data storage and Amazon S3 Glacier Deep Archive for archival storage</strong> -AWS DataSync is an online data movement and discovery service that simplifies and accelerates data migrations to AWS as well as moving data to and from on-premises storage, edge locations, other cloud providers, and AWS Storage services. AWS DataSync cannot be used for durable data storage.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier Deep Archive for archival storage</strong>"
      },
      {
        "answer": "",
        "explanation": "An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. An instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers."
      },
      {
        "answer": "",
        "explanation": "You can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance."
      },
      {
        "answer": "",
        "explanation": "Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, Amazon S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions. To keep costs low yet suitable for varying needs, Amazon S3 Glacier provides three retrieval options that range from a few minutes to hours. You can upload objects directly to Amazon S3 Glacier, or use S3 Lifecycle policies to transfer data between any of the Amazon S3 Storage Classes for active data (S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA) and S3 Glacier."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage</strong> - Amazon EC2 instance store volumes provide the best I/O performance for low latency requirements, so Amazon S3 standard storage is not the right fit for I/O performance requirements."
      },
      {
        "answer": "",
        "explanation": "The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice a year. It is designed for customers — particularly those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors — that retain data sets for 7-10 years or longer to meet regulatory compliance requirements."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier Deep Archive for archival storage</strong> - Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances. Amazon EBS volumes are particularly well-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage. For high I/O performance, instance store volumes are a better option."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 instance store for maximum performance, AWS DataSync for durable data storage and Amazon S3 Glacier Deep Archive for archival storage</strong> -AWS DataSync is an online data movement and discovery service that simplifies and accelerates data migrations to AWS as well as moving data to and from on-premises storage, edge locations, other cloud providers, and AWS Storage services. AWS DataSync cannot be used for durable data storage."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "id": 32,
    "question": "<p>A company maintains three environments for different stages of testing. AWS Glue jobs run in these environments to perform certain tasks every day. Although these tasks need to run every day, there is no specific start and end time mandated for them. The company is looking at options to reduce the cost of these jobs.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Glue Spark shuffle plugin to reduce the cost of storage</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure AWS Glue job with FLEX job execution class</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Opt for the Spot Instance type in Glue job properties</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure AWS Glue job with STANDARD job execution class</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure AWS Glue job with FLEX job execution class</strong></p>\n\n<p>When you configure a job using AWS Studio or the API you may specify a standard or flexible job execution class. The flexible execution class is appropriate for non-urgent jobs such as pre-production jobs, testing, and one-time data loads. Flexible job runs are supported for jobs using AWS Glue version 3.0 or later and G.1X or G.2X worker types. Flex job runs are billed based on the number of workers running at any point in time. A number of workers may be added or removed for a flexible job run.</p>\n\n<p>Flex allows you to optimize your costs on your non-urgent or non-time sensitive data integration workloads such as testing, and one-time data loads. With Flex, AWS Glue jobs run on spare compute capacity instead of dedicated hardware. The start and run times of jobs using Flex can vary because spare compute resources aren’t readily available and can be reclaimed during the run of a job.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q10-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q10-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/rds/performance-insights/\">https://aws.amazon.com/rds/performance-insights/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Glue Spark shuffle plugin to reduce the cost of storage</strong> - Shuffling is an important step in a Spark job where data is rearranged between partitions. This has no relevance to the given use case.</p>\n\n<p><strong>Configure AWS Glue job with STANDARD job execution class</strong> - The standard execution class is ideal for time-sensitive workloads that require fast job startup and dedicated resources. This execution class does not offer any cost benefits.</p>\n\n<p><strong>Opt for the Spot Instance type in Glue job properties</strong> - This option has been added as a distractor. AWS Glue provides a SERVERLESS environment to extract, transform, and load (ETL) data from AWS data sources to a target, so you can't opt for a specific type of EC2 instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/introducing-aws-glue-flex-jobs-cost-savings-on-etl-workloads/\">https://aws.amazon.com/blogs/big-data/introducing-aws-glue-flex-jobs-cost-savings-on-etl-workloads/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/add-job.html\">https://docs.aws.amazon.com/glue/latest/dg/add-job.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-shuffle-manager.html\">https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-shuffle-manager.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Glue job with FLEX job execution class</strong>"
      },
      {
        "answer": "",
        "explanation": "When you configure a job using AWS Studio or the API you may specify a standard or flexible job execution class. The flexible execution class is appropriate for non-urgent jobs such as pre-production jobs, testing, and one-time data loads. Flexible job runs are supported for jobs using AWS Glue version 3.0 or later and G.1X or G.2X worker types. Flex job runs are billed based on the number of workers running at any point in time. A number of workers may be added or removed for a flexible job run."
      },
      {
        "answer": "",
        "explanation": "Flex allows you to optimize your costs on your non-urgent or non-time sensitive data integration workloads such as testing, and one-time data loads. With Flex, AWS Glue jobs run on spare compute capacity instead of dedicated hardware. The start and run times of jobs using Flex can vary because spare compute resources aren’t readily available and can be reclaimed during the run of a job."
      },
      {
        "link": "https://aws.amazon.com/rds/performance-insights/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue Spark shuffle plugin to reduce the cost of storage</strong> - Shuffling is an important step in a Spark job where data is rearranged between partitions. This has no relevance to the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Glue job with STANDARD job execution class</strong> - The standard execution class is ideal for time-sensitive workloads that require fast job startup and dedicated resources. This execution class does not offer any cost benefits."
      },
      {
        "answer": "",
        "explanation": "<strong>Opt for the Spot Instance type in Glue job properties</strong> - This option has been added as a distractor. AWS Glue provides a SERVERLESS environment to extract, transform, and load (ETL) data from AWS data sources to a target, so you can't opt for a specific type of EC2 instance."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/performance-insights/",
      "https://aws.amazon.com/blogs/big-data/introducing-aws-glue-flex-jobs-cost-savings-on-etl-workloads/",
      "https://docs.aws.amazon.com/glue/latest/dg/add-job.html",
      "https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-shuffle-manager.html"
    ]
  },
  {
    "id": 33,
    "question": "<p>An e-commerce application runs on a single EC2 instance and processes one Kinesis data stream that has four shards. The instance has one KCL worker configured on it. As part of application scaling, another EC2 instance has been added to this configuration.</p>\n\n<p>What is the outcome of this change?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>When the KCL worker starts up on the second instance, it load-balances with the first instance, and each instance will now process two shards</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>When the KCL worker starts up on the second instance, it can randomly pick between one to four shards depending on the load experienced by the second instance</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>When the KCL worker starts up on the second instance, it load-balances with the first instance, and each instance will now process four shards</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>When you use the KCL, you should ensure that the number of instances exceeds the number of shards for better performance and scaling abilities, so the data processing will be paused</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>When the KCL worker starts up on the second instance, it load-balances with the first instance, and each instance will now process two shards</strong> - Resharding enables you to increase or decrease the number of shards in a stream to adapt to changes in the rate of data flowing through the stream. Resharding is typically performed by an administrative application that monitors shard data-handling metrics. Although the KCL itself doesn't initiate resharding operations, it is designed to adapt to changes in the number of shards that result from resharding.</p>\n\n<p>KCL tracks the shards in the stream using an Amazon DynamoDB table. When new shards are created as a result of resharding, the KCL discovers the new shards and populates new rows in the table. The workers automatically discover the new shards and create processors to handle the data from them. The KCL also distributes the shards in the stream across all the available workers and record processors.</p>\n\n<p>The following example illustrates how the KCL helps you handle scaling and resharding:</p>\n\n<ol>\n<li><p>For example, if your application is running on one EC2 instance, and is processing one Kinesis data stream that has four shards. This one instance has one KCL worker and four record processors (one record processor for every shard). These four record processors run in parallel within the same process.</p></li>\n<li><p>Next, if you scale the application to use another instance, you have two instances processing one stream that has four shards. When the KCL worker starts up on the second instance, it load-balances with the first instance, so that each instance now processes two shards.</p></li>\n<li><p>If you then decide to split the four shards into five shards. The KCL again coordinates the processing across instances: one instance processes three shards, and the other processes two shards. Similar coordination occurs when you merge shards.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>When the KCL worker starts up on the second instance, it load-balances with the first instance, and each instance will now process four shards</strong> - As mentioned in the explanation above, each instance will process two shards for the given use case.</p>\n\n<p><strong>When the KCL worker starts up on the second instance, it can randomly pick between one to four shards depending on the load experienced by the second instance</strong> - As mentioned in the explanation above, each instance will process two shards for the given use case.</p>\n\n<p><strong>When you use the KCL, you should ensure that the number of instances exceeds the number of shards for better performance and scaling abilities, so the data processing will be paused</strong> - This option has been added as a distractor. Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>When the KCL worker starts up on the second instance, it load-balances with the first instance, and each instance will now process two shards</strong> - Resharding enables you to increase or decrease the number of shards in a stream to adapt to changes in the rate of data flowing through the stream. Resharding is typically performed by an administrative application that monitors shard data-handling metrics. Although the KCL itself doesn't initiate resharding operations, it is designed to adapt to changes in the number of shards that result from resharding."
      },
      {
        "answer": "",
        "explanation": "KCL tracks the shards in the stream using an Amazon DynamoDB table. When new shards are created as a result of resharding, the KCL discovers the new shards and populates new rows in the table. The workers automatically discover the new shards and create processors to handle the data from them. The KCL also distributes the shards in the stream across all the available workers and record processors."
      },
      {
        "answer": "",
        "explanation": "The following example illustrates how the KCL helps you handle scaling and resharding:"
      },
      {}
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>When the KCL worker starts up on the second instance, it load-balances with the first instance, and each instance will now process four shards</strong> - As mentioned in the explanation above, each instance will process two shards for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>When the KCL worker starts up on the second instance, it can randomly pick between one to four shards depending on the load experienced by the second instance</strong> - As mentioned in the explanation above, each instance will process two shards for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>When you use the KCL, you should ensure that the number of instances exceeds the number of shards for better performance and scaling abilities, so the data processing will be paused</strong> - This option has been added as a distractor. Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html"
    ]
  },
  {
    "id": 34,
    "question": "<p>A company needs to set up a data pipeline that includes an AWS Lambda function and an AWS Glue job. You have been hired as an AWS Certified Data Engineer Associate to build a solution that has the least management overhead and is fully integrated with AWS services.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage Amazon Managed Workflows for Apache Airflow to set up an Apache Airflow workflow that is deployed on an Amazon Elastic Container Service (Amazon ECS) cluster. The workflow definition should have the first task as the Lambda function and the second task as the AWS Glue job</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an AWS Step Functions state machine to run the Lambda function and then the AWS Glue job</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up an AWS Glue workflow to run the Lambda function and then the AWS Glue job</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage Amazon Managed Workflows for Apache Airflow to set up an Apache Airflow workflow that is deployed on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The workflow definition should have the first task as the Lambda function and the second task as the AWS Glue job</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an AWS Step Functions state machine to run the Lambda function and then the AWS Glue job</strong></p>\n\n<p>AWS Step Functions is a serverless orchestration service that lets you integrate with AWS Lambda functions and other AWS services to build business-critical applications. Step Functions is based on state machines and tasks. In Step Functions, a workflow is called a state machine, which is a series of event-driven steps. Each step in a workflow is called a state. A Task state represents a unit of work that another AWS service, such as AWS Lambda, performs. A Task state can call any AWS service or API.</p>\n\n<p>With Step Functions' built-in controls, you examine the state of each step in your workflow to make sure that your application runs in order and as expected. Depending on your use case, you can have Step Functions call AWS services, such as Lambda, to perform tasks. You can create workflows that process and publish machine learning models. You can have Step Functions control AWS services, such as AWS Glue, to create extract, transform, and load (ETL) workflows.</p>\n\n<p>AWS Step Functions state machine represents a simple way to orchestrate a workflow on AWS, as it has minimal management overhead.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon Managed Workflows for Apache Airflow to set up an Apache Airflow workflow that is deployed on an Amazon Elastic Container Service (Amazon ECS) cluster. The workflow definition should have the first task as the Lambda function and the second task as the AWS Glue job</strong></p>\n\n<p><strong>Leverage Amazon Managed Workflows for Apache Airflow to set up an Apache Airflow workflow that is deployed on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The workflow definition should have the first task as the Lambda function and the second task as the AWS Glue job</strong></p>\n\n<p>Amazon Managed Workflows for Apache Airflow is a managed orchestration service for Apache Airflow that you can use to set up and operate data pipelines in the cloud at scale. Apache Airflow is an open-source tool used to programmatically author, schedule, and monitor sequences of processes and tasks referred to as workflows.</p>\n\n<p>Amazon MWAA offers support for using containers to scale the worker fleet on demand and reduce scheduler outages using Amazon ECS on AWS Fargate. Operators that invoke tasks on Amazon ECS containers, and Kubernetes operators that create and run pods on a Kubernetes cluster are supported.</p>\n\n<p>Using Amazon Managed Workflows for Apache Airflow to set up an Apache Airflow workflow that is deployed on an Amazon ECS cluster or Amazon EKS cluster represents a solution with a very high management overhead. So, both these options are incorrect.</p>\n\n<p><strong>Set up an AWS Glue workflow to run the Lambda function and then the AWS Glue job</strong> - AWS Glue workflow is only for orchestrating complex ETL operations using AWS Glue, so you cannot have a Lambda function as a component of the workflow. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\">https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/workflows_overview.html\">https://docs.aws.amazon.com/glue/latest/dg/workflows_overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html\">https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an AWS Step Functions state machine to run the Lambda function and then the AWS Glue job</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Step Functions is a serverless orchestration service that lets you integrate with AWS Lambda functions and other AWS services to build business-critical applications. Step Functions is based on state machines and tasks. In Step Functions, a workflow is called a state machine, which is a series of event-driven steps. Each step in a workflow is called a state. A Task state represents a unit of work that another AWS service, such as AWS Lambda, performs. A Task state can call any AWS service or API."
      },
      {
        "answer": "",
        "explanation": "With Step Functions' built-in controls, you examine the state of each step in your workflow to make sure that your application runs in order and as expected. Depending on your use case, you can have Step Functions call AWS services, such as Lambda, to perform tasks. You can create workflows that process and publish machine learning models. You can have Step Functions control AWS services, such as AWS Glue, to create extract, transform, and load (ETL) workflows."
      },
      {
        "answer": "",
        "explanation": "AWS Step Functions state machine represents a simple way to orchestrate a workflow on AWS, as it has minimal management overhead."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Managed Workflows for Apache Airflow to set up an Apache Airflow workflow that is deployed on an Amazon Elastic Container Service (Amazon ECS) cluster. The workflow definition should have the first task as the Lambda function and the second task as the AWS Glue job</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Managed Workflows for Apache Airflow to set up an Apache Airflow workflow that is deployed on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The workflow definition should have the first task as the Lambda function and the second task as the AWS Glue job</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Managed Workflows for Apache Airflow is a managed orchestration service for Apache Airflow that you can use to set up and operate data pipelines in the cloud at scale. Apache Airflow is an open-source tool used to programmatically author, schedule, and monitor sequences of processes and tasks referred to as workflows."
      },
      {
        "answer": "",
        "explanation": "Amazon MWAA offers support for using containers to scale the worker fleet on demand and reduce scheduler outages using Amazon ECS on AWS Fargate. Operators that invoke tasks on Amazon ECS containers, and Kubernetes operators that create and run pods on a Kubernetes cluster are supported."
      },
      {
        "answer": "",
        "explanation": "Using Amazon Managed Workflows for Apache Airflow to set up an Apache Airflow workflow that is deployed on an Amazon ECS cluster or Amazon EKS cluster represents a solution with a very high management overhead. So, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an AWS Glue workflow to run the Lambda function and then the AWS Glue job</strong> - AWS Glue workflow is only for orchestrating complex ETL operations using AWS Glue, so you cannot have a Lambda function as a component of the workflow. So, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html",
      "https://docs.aws.amazon.com/glue/latest/dg/workflows_overview.html",
      "https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html"
    ]
  },
  {
    "id": 35,
    "question": "<p>A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company.</p>\n\n<p>Which configuration should be used to meet this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the security group of the Amazon EC2 instances to enforce geo-restriction</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Geo Restriction feature of Amazon CloudFront that is deployed in an Amazon Virtual Private Cloud (Amazon VPC)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure the security group of the Application Load Balancer to enforce geo restriction</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer that is deployed in an Amazon Virtual Private Cloud (Amazon VPC)</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS Web Application Firewall (AWS WAF) is a web application firewall service that lets you monitor web requests and protect your web applications from malicious requests. Use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses. You can also use AWS WAF preconfigured protections to block common attacks like SQL injection or cross-site scripting.</p>\n\n<p><strong>Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer that is deployed in an Amazon Virtual Private Cloud (Amazon VPC)</strong></p>\n\n<p>You can use AWS WAF with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL). Geographic (Geo) Match Conditions in AWS WAF allow you to use AWS WAF to restrict application access based on the geographic location of your viewers. With geo match conditions you can choose the countries from which AWS WAF should allow access.</p>\n\n<p>Geo-match conditions are important for many customers. For example, legal and licensing requirements restrict some customers from delivering their applications outside certain countries. These customers can configure a whitelist that allows only viewers in those countries. Other customers need to prevent the downloading of their encrypted software by users in certain countries. These customers can configure a blacklist so that end-users from those countries are blocked from downloading their software.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Geo Restriction feature of Amazon CloudFront that is deployed in an Amazon Virtual Private Cloud (Amazon VPC)</strong> - The Geo Restriction feature of Amazon CloudFront helps in restricting traffic based on the user's geographic location. But, CloudFront works from edge locations and doesn't belong to a VPC. Hence, this option itself is incorrect and given only as a distractor.</p>\n\n<p><strong>Configure the security group of the Application Load Balancer to enforce geo restriction</strong></p>\n\n<p><strong>Configure the security group of the Amazon EC2 instances to enforce geo-restriction</strong></p>\n\n<p>Security Groups cannot be used to enforce geo restrictions. So, both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/\">https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/\">https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/\">https://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS Web Application Firewall (AWS WAF) is a web application firewall service that lets you monitor web requests and protect your web applications from malicious requests. Use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses. You can also use AWS WAF preconfigured protections to block common attacks like SQL injection or cross-site scripting."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer that is deployed in an Amazon Virtual Private Cloud (Amazon VPC)</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use AWS WAF with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL). Geographic (Geo) Match Conditions in AWS WAF allow you to use AWS WAF to restrict application access based on the geographic location of your viewers. With geo match conditions you can choose the countries from which AWS WAF should allow access."
      },
      {
        "answer": "",
        "explanation": "Geo-match conditions are important for many customers. For example, legal and licensing requirements restrict some customers from delivering their applications outside certain countries. These customers can configure a whitelist that allows only viewers in those countries. Other customers need to prevent the downloading of their encrypted software by users in certain countries. These customers can configure a blacklist so that end-users from those countries are blocked from downloading their software."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Geo Restriction feature of Amazon CloudFront that is deployed in an Amazon Virtual Private Cloud (Amazon VPC)</strong> - The Geo Restriction feature of Amazon CloudFront helps in restricting traffic based on the user's geographic location. But, CloudFront works from edge locations and doesn't belong to a VPC. Hence, this option itself is incorrect and given only as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure the security group of the Application Load Balancer to enforce geo restriction</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Configure the security group of the Amazon EC2 instances to enforce geo-restriction</strong>"
      },
      {
        "answer": "",
        "explanation": "Security Groups cannot be used to enforce geo restrictions. So, both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/",
      "https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/",
      "https://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/"
    ]
  },
  {
    "id": 36,
    "question": "<p>You are establishing a monitoring solution for desktop systems, that will be sending telemetry data into AWS every 1 minute. Data for each system must be processed in order, independently, and you would like to scale the number of consumers to be possibly equal to the number of desktop systems that are being monitored.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Kinesis Data Streams, and send the telemetry data with a Partition ID that uses the value of the Desktop ID</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>We, therefore, need to use an SQS FIFO queue. If we don't specify a GroupID, then all the messages are in absolute order, but we can only have 1 consumer at most.\nTo allow for multiple consumers to read data for each Desktop application, and to scale the number of consumers, we should use the \"Group ID\" attribute. So this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is</strong> - This is incorrect because if we send the telemetry data as is then we will not be able to scale the number of consumers to be equal to the number of desktop systems. In this case, each message will have its consumer. So we should use the \"Group ID\" attribute so that multiple consumers can read data for each Desktop application.</p>\n\n<p><strong>Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is</strong> - An Amazon SQS standard queue has no ordering capability so that's ruled out.</p>\n\n<p><strong>Use Amazon Kinesis Data Streams, and send the telemetry data with a Partition ID that uses the value of the Desktop ID</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>Kinesis Data Streams would work and would give us the data for each desktop application within shards, but we can only have as many consumers as shards in Kinesis (which is in practice, much less than the number of producers).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/\">https://aws.amazon.com/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent."
      },
      {
        "answer": "",
        "explanation": "We, therefore, need to use an SQS FIFO queue. If we don't specify a GroupID, then all the messages are in absolute order, but we can only have 1 consumer at most.\nTo allow for multiple consumers to read data for each Desktop application, and to scale the number of consumers, we should use the \"Group ID\" attribute. So this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is</strong> - This is incorrect because if we send the telemetry data as is then we will not be able to scale the number of consumers to be equal to the number of desktop systems. In this case, each message will have its consumer. So we should use the \"Group ID\" attribute so that multiple consumers can read data for each Desktop application."
      },
      {
        "answer": "",
        "explanation": "<strong>Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is</strong> - An Amazon SQS standard queue has no ordering capability so that's ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Streams, and send the telemetry data with a Partition ID that uses the value of the Desktop ID</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events."
      },
      {
        "answer": "",
        "explanation": "Kinesis Data Streams would work and would give us the data for each desktop application within shards, but we can only have as many consumers as shards in Kinesis (which is in practice, much less than the number of producers)."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/",
      "https://aws.amazon.com/sqs/faqs/",
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 37,
    "question": "<p>A global CRM company has recently migrated its technology infrastructure from its on-premises data center to AWS Cloud. The data engineering team has provisioned an RDS PostgreSQL DB cluster for the company's flagship CRM application. An analytics workload also runs on the same database which publishes near real-time reports for the senior management of the company. When the analytics workload runs, it slows down the CRM application as well, resulting in a bad user experience.</p>\n\n<p>Which of the following would you recommend as the MOST cost-optimal solution to fix this issue?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a Read Replica in the same Region as the Master database and point the analytics workload there</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable Multi-AZ for the RDS database and run the analytics workload on the standby database</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>For Disaster Recovery purposes, create a Read Replica in another Region as the Master database and point the analytics workload there</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Migrate the analytics application to AWS Lambda</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a Read Replica in the same Region as the Master database and point the analytics workload there</strong></p>\n\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance. Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.</p>\n\n<p>Creating a Read Replica within the same Region is the correct answer. As we want to minimize the costs, we need to launch the Read Replica in the same Region, because we have to pay for inter-Region data transfer, whereas the transfer of data within a single Region is free.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q36-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q36-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a><p></p>\n\n<p>Comparison for Multi-AZ vs Read Replica for RDS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q36-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q36-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Multi-AZ for the RDS database and run the analytics workload on the standby database</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region.</p>\n\n<p>Enabling Multi-AZ helps make our database highly-available, but the standby database is not accessible and cannot be used for reads or writes. It's just a database that will become primary when the other database encounters a failure. So this option is not correct.</p>\n\n<p><strong>Migrate the analytics application to AWS Lambda</strong>- AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>Running the application on AWS Lambda will not help, as it will still run against the main database and slow down the application.</p>\n\n<p><strong>For Disaster Recovery purposes, create a Read Replica in another Region as the Master database and point the analytics workload there</strong> - This option is not correct because we have to pay for inter-Region data transfer for the Read Replica, whereas the transfer of data within a single Region is free. Disaster Recovery is not within the ambit of the requirements mentioned for the given use case. The correct solution needs to optimize on cost.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a Read Replica in the same Region as the Master database and point the analytics workload there</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance. Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region."
      },
      {
        "answer": "",
        "explanation": "Creating a Read Replica within the same Region is the correct answer. As we want to minimize the costs, we need to launch the Read Replica in the same Region, because we have to pay for inter-Region data transfer, whereas the transfer of data within a single Region is free."
      },
      {
        "link": "https://aws.amazon.com/rds/faqs/"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q36-i2.jpg",
        "answer": "",
        "explanation": "Comparison for Multi-AZ vs Read Replica for RDS:"
      },
      {
        "link": "https://aws.amazon.com/rds/features/multi-az/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable Multi-AZ for the RDS database and run the analytics workload on the standby database</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region."
      },
      {
        "answer": "",
        "explanation": "Enabling Multi-AZ helps make our database highly-available, but the standby database is not accessible and cannot be used for reads or writes. It's just a database that will become primary when the other database encounters a failure. So this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Migrate the analytics application to AWS Lambda</strong>- AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume."
      },
      {
        "answer": "",
        "explanation": "Running the application on AWS Lambda will not help, as it will still run against the main database and slow down the application."
      },
      {
        "answer": "",
        "explanation": "<strong>For Disaster Recovery purposes, create a Read Replica in another Region as the Master database and point the analytics workload there</strong> - This option is not correct because we have to pay for inter-Region data transfer for the Read Replica, whereas the transfer of data within a single Region is free. Disaster Recovery is not within the ambit of the requirements mentioned for the given use case. The correct solution needs to optimize on cost."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/faqs/",
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://aws.amazon.com/rds/features/read-replicas/"
    ]
  },
  {
    "id": 38,
    "question": "<p>An online gaming company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on Amazon EC2 instances running under an Application Load Balancer with AWS Web Application Firewall (AWS WAF).</p>\n\n<p>Which of the following solutions can be combined to address the given use case? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS WAF geo match statement listing the countries that you want to block</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Application Load Balancer geo match statement listing the countries that you want to block</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use AWS WAF geo match statement listing the countries that you want to block</strong></p>\n\n<p><strong>Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns and rules that filter out specific traffic patterns you define.</p>\n\n<p>You can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts your web servers or origin servers running on Amazon EC2, or Amazon API Gateway for your APIs.</p>\n\n<p>AWS WAF - How it Works?:\n<img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a><p></p>\n\n<p>To block specific countries, you can create an AWS WAF geo match statement listing the countries that you want to block and to allow traffic from IPs of the remote development team, you can create a WAF IP set statement that specifies the IP addresses that you want to allow through. You can combine the two rules as shown below:</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances</strong> - A network access control list (network ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. A network access control list (network ACL) does not have the capability to block traffic based on geographic match conditions.</p>\n\n<p><strong>Use Application Load Balancer geo match statement listing the countries that you want to block</strong></p>\n\n<p><strong>Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through</strong></p>\n\n<p>An Application Load Balancer operates at the request level (layer 7), routing traffic to targets – Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at the delivery of modern application architectures, including microservices and container-based applications.</p>\n\n<p>An Application Load Balancer cannot block or allow traffic based on geographic match conditions or IP based conditions. Both these options have been added as distractors.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/\">https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS WAF geo match statement listing the countries that you want to block</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns and rules that filter out specific traffic patterns you define."
      },
      {
        "answer": "",
        "explanation": "You can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts your web servers or origin servers running on Amazon EC2, or Amazon API Gateway for your APIs."
      },
      {
        "image": "https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png",
        "answer": "",
        "explanation": "AWS WAF - How it Works?:"
      },
      {
        "link": "https://aws.amazon.com/waf/"
      },
      {
        "answer": "",
        "explanation": "To block specific countries, you can create an AWS WAF geo match statement listing the countries that you want to block and to allow traffic from IPs of the remote development team, you can create a WAF IP set statement that specifies the IP addresses that you want to allow through. You can combine the two rules as shown below:"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances</strong> - A network access control list (network ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. A network access control list (network ACL) does not have the capability to block traffic based on geographic match conditions."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Application Load Balancer geo match statement listing the countries that you want to block</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through</strong>"
      },
      {
        "answer": "",
        "explanation": "An Application Load Balancer operates at the request level (layer 7), routing traffic to targets – Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at the delivery of modern application architectures, including microservices and container-based applications."
      },
      {
        "answer": "",
        "explanation": "An Application Load Balancer cannot block or allow traffic based on geographic match conditions or IP based conditions. Both these options have been added as distractors."
      }
    ],
    "references": [
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html",
      "https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/"
    ]
  },
  {
    "id": 39,
    "question": "<p>A research agency stores and manages the global seismological data for the last 100 years. The data has a velocity of 1GB per minute. You would like to store the data with only the most relevant attributes to build a predictive model for earthquakes.</p>\n\n<p>Which of the following solutions would you use to build the most cost-effective solution that requires the LEAST infrastructure maintenance?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Ingest the data in Kinesis Data Streams and use an intermediary Lambda function to filter and transform the incoming stream before the output is written to S3</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Ingest the data in Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to S3</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Ingest the data in Kinesis Data Firehose and use an intermediary Lambda function to filter and transform the incoming stream before the output is written to S3</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Ingest the data in a Spark Streaming Cluster on EMR use Spark Streaming transformations before writing to S3</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Ingest the data in Kinesis Data Firehose and use an intermediary Lambda function to filter and transform the incoming stream before the output is written to S3</strong></p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</p>\n\n<p>Kinesis Data Firehose Overview\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a><p></p>\n\n<p>The correct option is to ingest the data in Kinesis Data Firehose and use a Lambda function to filter and transform the incoming data before the output is written to S3. This way you only need to store a sliced version of the data with only the relevant data attributes required for your model. Also, it should be noted that this solution is entirely serverless and requires no infrastructure maintenance.</p>\n\n<p>Kinesis Data Firehose to S3:\n<img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-s3.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-s3.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Ingest the data in Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to S3</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: set up your streaming data sources, write your queries or streaming applications, and set up your destination for processed data. Kinesis Data Analytics cannot directly ingest data from the source as it ingests data either from Kinesis Data Streams or Kinesis Data Firehose, so this option is ruled out.</p>\n\n<p><strong>Ingest the data in Kinesis Data Streams and use an intermediary Lambda function to filter and transform the incoming stream before the output is written to S3</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. Amazon Kinesis Data Streams is integrated with many AWS services, including Amazon Kinesis Data Firehose for near real-time transformation.</p>\n\n<p>Kinesis Data Streams cannot directly write the output to S3. In addition, KDS does not offer a plug-and-play integration with an intermediary Lambda function like Firehose does. You will need to do a lot of custom coding to get the Lambda function to process the incoming stream and then reliably dump the transformed output to S3. So this option is incorrect.</p>\n\n<p><strong>Ingest the data in a Spark Streaming Cluster on EMR use Spark Streaming transformations before writing to S3</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing an EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the correct solution for the given use case should require the least amount of infrastructure maintenance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Ingest the data in Kinesis Data Firehose and use an intermediary Lambda function to filter and transform the incoming stream before the output is written to S3</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png",
        "answer": "",
        "explanation": "Kinesis Data Firehose Overview"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-firehose/"
      },
      {
        "answer": "",
        "explanation": "The correct option is to ingest the data in Kinesis Data Firehose and use a Lambda function to filter and transform the incoming data before the output is written to S3. This way you only need to store a sliced version of the data with only the relevant data attributes required for your model. Also, it should be noted that this solution is entirely serverless and requires no infrastructure maintenance."
      },
      {
        "image": "https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-s3.png",
        "answer": "",
        "explanation": "Kinesis Data Firehose to S3:"
      },
      {
        "link": "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Ingest the data in Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to S3</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: set up your streaming data sources, write your queries or streaming applications, and set up your destination for processed data. Kinesis Data Analytics cannot directly ingest data from the source as it ingests data either from Kinesis Data Streams or Kinesis Data Firehose, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Ingest the data in Kinesis Data Streams and use an intermediary Lambda function to filter and transform the incoming stream before the output is written to S3</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. Amazon Kinesis Data Streams is integrated with many AWS services, including Amazon Kinesis Data Firehose for near real-time transformation."
      },
      {
        "answer": "",
        "explanation": "Kinesis Data Streams cannot directly write the output to S3. In addition, KDS does not offer a plug-and-play integration with an intermediary Lambda function like Firehose does. You will need to do a lot of custom coding to get the Lambda function to process the incoming stream and then reliably dump the transformed output to S3. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Ingest the data in a Spark Streaming Cluster on EMR use Spark Streaming transformations before writing to S3</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing an EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the correct solution for the given use case should require the least amount of infrastructure maintenance."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-firehose/",
      "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
    ]
  },
  {
    "id": 40,
    "question": "<p>The data engineering team at a company is building an Opensearch-based index for all the existing files in S3. To build this index, it only needs to read the first 250 bytes of each object in S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, adding up to 50TB of data.</p>\n\n<p>Which of the following solutions can be used to build this index MOST efficiently? (Select two)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the Database Migration Service to load the entire data from S3 to Opensearch and then Opensearch will automatically build the index</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an application that will traverse the S3 bucket, read the entire files one by one, extract the first 250 bytes, and store that information in Opensearch</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Opensearch</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use the Opensearch Import feature to load the entire data from S3 to Opensearch and then Opensearch will automatically build the index</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Create an application that will use the S3 Select ScanRange parameter to get the first 250 bytes and store that information in Opensearch</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Opensearch</strong></p>\n\n<p>Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.</p>\n\n<p>Amazon Opensearch Service is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. With Amazon Opensearch, you get direct access to the Elasticsearch APIs; existing code and applications work seamlessly with the service.</p>\n\n<p>Using the Range HTTP header in a GET Object request, you can fetch a byte-range from an object, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. This helps you achieve higher aggregate throughput versus a single whole-object request. Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted.</p>\n\n<p>A byte-range request is a perfect way to get the beginning of a file and ensure that we remain efficient during the scan of our S3 bucket. You can then store the relevant information in the form of a JSON document in ElasticSearch.</p>\n\n<p><strong>Create an application that will use the S3 Select ScanRange parameter to get the first 250 bytes and store that information in ElasticSearch</strong></p>\n\n<p>With Amazon S3 Select, you can scan a subset of an object by specifying a range of bytes to query using the ScanRange parameter. This capability lets you parallelize scanning the whole object by splitting the work into separate Amazon S3 Select requests for a series of non-overlapping scan ranges. Use the Amazon S3 Select ScanRange parameter and Start at (Byte) and End at (Byte). You can then store the relevant information in the form of a JSON document in ElasticSearch.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q19-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q19-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the Opensearch Import feature to load the entire data from S3 to Opensearch and then Opensearch will automatically build the index</strong> - This option has been added as a distractor as there is no Opensearch Import feature to load data from S3.</p>\n\n<p><strong>Create an application that will traverse the S3 bucket, read the entire files one by one, extract the first 250 bytes, and store that information in Opensearch</strong> - If you build an application that loads all the files from S3, that would work, but you would read 50TB of data and that may be very expensive and slow. So this option is incorrect.</p>\n\n<p><strong>Use the Database Migration Service to load the entire data from S3 to Opensearch and then Opensearch will automatically build the index</strong> - Although you could use Database Migration Service to load the entire data from S3 to Opensearch, you would read 50TB of data and that may be very expensive and slow. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/opensearch-service/\">https://aws.amazon.com/opensearch-service/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Opensearch</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance."
      },
      {
        "answer": "",
        "explanation": "Amazon Opensearch Service is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. With Amazon Opensearch, you get direct access to the Elasticsearch APIs; existing code and applications work seamlessly with the service."
      },
      {
        "answer": "",
        "explanation": "Using the Range HTTP header in a GET Object request, you can fetch a byte-range from an object, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. This helps you achieve higher aggregate throughput versus a single whole-object request. Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted."
      },
      {
        "answer": "",
        "explanation": "A byte-range request is a perfect way to get the beginning of a file and ensure that we remain efficient during the scan of our S3 bucket. You can then store the relevant information in the form of a JSON document in ElasticSearch."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an application that will use the S3 Select ScanRange parameter to get the first 250 bytes and store that information in ElasticSearch</strong>"
      },
      {
        "answer": "",
        "explanation": "With Amazon S3 Select, you can scan a subset of an object by specifying a range of bytes to query using the ScanRange parameter. This capability lets you parallelize scanning the whole object by splitting the work into separate Amazon S3 Select requests for a series of non-overlapping scan ranges. Use the Amazon S3 Select ScanRange parameter and Start at (Byte) and End at (Byte). You can then store the relevant information in the form of a JSON document in ElasticSearch."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the Opensearch Import feature to load the entire data from S3 to Opensearch and then Opensearch will automatically build the index</strong> - This option has been added as a distractor as there is no Opensearch Import feature to load data from S3."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an application that will traverse the S3 bucket, read the entire files one by one, extract the first 250 bytes, and store that information in Opensearch</strong> - If you build an application that loads all the files from S3, that would work, but you would read 50TB of data and that may be very expensive and slow. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the Database Migration Service to load the entire data from S3 to Opensearch and then Opensearch will automatically build the index</strong> - Although you could use Database Migration Service to load the entire data from S3 to Opensearch, you would read 50TB of data and that may be very expensive and slow. So this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range",
      "https://aws.amazon.com/opensearch-service/"
    ]
  },
  {
    "id": 41,
    "question": "<p>A company needs to upgrade its Amazon Elastic Block Store - General Purpose SSD storage from gp2 to gp3. The change should not interrupt the services running on the Amazon EC2 instance.</p>\n\n<p>Which solution will meet the given requirements with the LEAST operational overhead ?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Choose <code>Modify Volume</code> from the Amazon EC2 console and change the <code>Volume Type</code> to gp3. Also modify the <code>Size</code>, <code>IOPS</code>, and <code>Throughput</code> parameters. To migrate to gp3, you need to increase the volume size</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>You need to stop the instance, apply the modifications, and then restart the instance. Downtime cannot fully be avoided unless a failover instance is maintained</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>You need to hibernate the instance, apply the modifications, and then restart the instance. Downtime cannot fully be avoided unless a failover instance is maintained</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Choose <code>Modify Volume</code> from the Amazon EC2 console and change the <code>Volume Type</code> to gp3. Also modify <code>Size</code>, <code>IOPS</code> and <code>Throughput</code> parameters</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Choose <code>Modify Volume</code> from the Amazon EC2 console and change the <code>Volume Type</code> to gp3. Also modify <code>Size</code>, <code>IOPS</code>, and <code>Throughput</code> parameters</strong></p>\n\n<p>Amazon EBS Elastic Volumes enable you to modify your volume type from gp2 to gp3 without detaching volumes or restarting instances (requirements for modification), which means that there are no interruptions to your applications during modification.</p>\n\n<p>To modify an Amazon EBS volume using the AWS Management Console:</p>\n\n<p>How to migrate from gp2 to gp3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q16-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q16-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/\">https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You need to stop the instance, apply the modifications, and then restart the instance. Downtime cannot fully be avoided unless a failover instance is maintained</strong> - To modify an EBS volume from gp2 to gp3, you do not need to stop the instance.</p>\n\n<p><strong>You need to hibernate the instance, apply the modifications, and then restart the instance. Downtime cannot fully be avoided unless a failover instance is maintained</strong> - When you hibernate an instance, Amazon EC2 signals the operating system to perform hibernation (suspend-to-disk). Hibernation saves the contents from the instance memory (RAM) to your Amazon Elastic Block Store (Amazon EBS) root volume. Amazon EC2 persists the instance's EBS root volume and any attached EBS data volumes. To modify an EBS volume from gp2 to gp3, you do not need to hibernate the instance.</p>\n\n<p><strong>Choose <code>Modify Volume</code> from the Amazon EC2 console and change the <code>Volume Type</code>to gp3. Also modify the <code>Size</code>, <code>IOPS</code>, and <code>Throughput</code> parameters. To migrate to gp3, you need to increase the volume size</strong> - To migrate to gp3, you do not need to increase the volume size.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/\">https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/ebs/latest/userguide/modify-volume-requirements.html\">https://docs.aws.amazon.com/ebs/latest/userguide/modify-volume-requirements.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Choose <code>Modify Volume</code> from the Amazon EC2 console and change the <code>Volume Type</code> to gp3. Also modify <code>Size</code>, <code>IOPS</code>, and <code>Throughput</code> parameters</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon EBS Elastic Volumes enable you to modify your volume type from gp2 to gp3 without detaching volumes or restarting instances (requirements for modification), which means that there are no interruptions to your applications during modification."
      },
      {
        "answer": "",
        "explanation": "To modify an Amazon EBS volume using the AWS Management Console:"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q16-i1.jpg",
        "answer": "",
        "explanation": "How to migrate from gp2 to gp3:"
      },
      {
        "link": "https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You need to stop the instance, apply the modifications, and then restart the instance. Downtime cannot fully be avoided unless a failover instance is maintained</strong> - To modify an EBS volume from gp2 to gp3, you do not need to stop the instance."
      },
      {
        "answer": "",
        "explanation": "<strong>You need to hibernate the instance, apply the modifications, and then restart the instance. Downtime cannot fully be avoided unless a failover instance is maintained</strong> - When you hibernate an instance, Amazon EC2 signals the operating system to perform hibernation (suspend-to-disk). Hibernation saves the contents from the instance memory (RAM) to your Amazon Elastic Block Store (Amazon EBS) root volume. Amazon EC2 persists the instance's EBS root volume and any attached EBS data volumes. To modify an EBS volume from gp2 to gp3, you do not need to hibernate the instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Choose <code>Modify Volume</code> from the Amazon EC2 console and change the <code>Volume Type</code>to gp3. Also modify the <code>Size</code>, <code>IOPS</code>, and <code>Throughput</code> parameters. To migrate to gp3, you need to increase the volume size</strong> - To migrate to gp3, you do not need to increase the volume size."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/",
      "https://docs.aws.amazon.com/ebs/latest/userguide/modify-volume-requirements.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html"
    ]
  },
  {
    "id": 42,
    "question": "<p>A Silicon Valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon Simple Storage Service (Amazon S3). The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access.</p>\n\n<p>Which of the following solutions would you recommend?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon S3 Glacier Deep Archive to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 Glacier Deep Archive to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls</strong></p>\n\n<p>Amazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>An Amazon S3 Glacier vault is a container for storing archives. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. Amazon S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual Amazon S3 Glacier vaults with a vault lock policy. You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the policy from future edits. Therefore, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon S3 Glacier Deep Archive to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls</strong> - You can use lifecycle policy to define actions you want Amazon S3 to take during an object's lifetime. For example, use a lifecycle policy to transition objects to another storage class, archive them, or delete them after a specified period. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p><strong>Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls</strong>- Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p><strong>Use Amazon S3 Glacier Deep Archive to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements."
      },
      {
        "answer": "",
        "explanation": "An Amazon S3 Glacier vault is a container for storing archives. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. Amazon S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual Amazon S3 Glacier vaults with a vault lock policy. You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the policy from future edits. Therefore, this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Glacier Deep Archive to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls</strong> - You can use lifecycle policy to define actions you want Amazon S3 to take during an object's lifetime. For example, use a lifecycle policy to transition objects to another storage class, archive them, or delete them after a specified period. It cannot be used to enforce compliance controls. Therefore, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls</strong>- Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Glacier Deep Archive to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html",
      "https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html"
    ]
  },
  {
    "id": 43,
    "question": "<p>An Internet-of-Things (IoT) company needs a solution that can collect near real-time data from all its devices/sensors and store them in nested JSON format. The solution must offer data persistence and support the capability to query the data with a maximum latency of 10 milliseconds.</p>\n\n<p>As a data engineer, how will you implement an optimal solution such that it has the LEAST operational overhead?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Kinesis Data Streams to capture the sensor data. Define an AWS Lambda function to process the data and write to the DynamoDB table. Store the data in Amazon DynamoDB for querying</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Data Firehose to capture the sensor data. Directly store the data in Amazon DynamoDB for querying</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure Amazon Simple Queue Service (Amazon SQS) to capture the real-time sensor data. Define an AWS Lambda function to poll the SQS queues and process the data. Store the data in Amazon DynamoDB for querying</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the fully managed Apache Kafka cluster to capture the sensor data in near real-time. Store the data in Amazon S3 for querying</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to capture the sensor data. Define an AWS Lambda function to process the data and write to the DynamoDB table. Store the data in Amazon DynamoDB for querying</strong></p>\n\n<p>Amazon Kinesis Data Streams is a serverless streaming data service that makes it easy to capture, process, and store data streams at any scale. Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams.</p>\n\n<p>Amazon Kinesis Data Streams is the right choice for capturing the data with the least operational overhead. AWS Lambda function can be defined to process the data and write it to Amazon DynamoDB. Amazon DynamoDB is a serverless, NoSQL, fully managed database service with single-digit millisecond response times at any scale, enabling you to develop and run modern applications while only paying for what you use. DynamoDB supports nested JSON objects.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the fully managed Apache Kafka cluster to capture the sensor data in near real-time. Store the data in Amazon S3 for querying</strong> - Apache Kafka is a distributed data store optimized for ingesting and processing streaming data in real time. Streaming data can be defined as data that is continuously generated by thousands of data sources, which typically send the data records simultaneously. For the given use case, Kafka will not be as operationally efficient as Kinesis Data Streams. Also, storing nested JSON data in Amazon S3 will not support the requirement of a maximum allowed latency of 10 milliseconds.</p>\n\n<p><strong>Use Amazon Data Firehose to capture the sensor data. Directly store the data in Amazon DynamoDB for querying</strong> - Amazon Data Firehose cannot directly write to Amazon DynamoDB. Firehose currently supports Amazon S3, Amazon Redshift, Amazon OpenSearch Service, Splunk, Datadog, NewRelic, Dynatrace, Sumo Logic, LogicMonitor, MongoDB, and HTTP End Point as destinations.</p>\n\n<p><strong>Configure Amazon Simple Queue Service (Amazon SQS) to capture the real-time sensor data. Define an AWS Lambda function to poll the SQS queues and process the data. Store the data in Amazon DynamoDB for querying</strong> - Amazon SQS offers a reliable, highly scalable hosted queue for storing messages as they travel between applications or microservices. It moves data between distributed application components and helps you decouple these components. Amazon SQS cannot be used to capture and process real-time data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Streams to capture the sensor data. Define an AWS Lambda function to process the data and write to the DynamoDB table. Store the data in Amazon DynamoDB for querying</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams is a serverless streaming data service that makes it easy to capture, process, and store data streams at any scale. Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams is the right choice for capturing the data with the least operational overhead. AWS Lambda function can be defined to process the data and write it to Amazon DynamoDB. Amazon DynamoDB is a serverless, NoSQL, fully managed database service with single-digit millisecond response times at any scale, enabling you to develop and run modern applications while only paying for what you use. DynamoDB supports nested JSON objects."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the fully managed Apache Kafka cluster to capture the sensor data in near real-time. Store the data in Amazon S3 for querying</strong> - Apache Kafka is a distributed data store optimized for ingesting and processing streaming data in real time. Streaming data can be defined as data that is continuously generated by thousands of data sources, which typically send the data records simultaneously. For the given use case, Kafka will not be as operationally efficient as Kinesis Data Streams. Also, storing nested JSON data in Amazon S3 will not support the requirement of a maximum allowed latency of 10 milliseconds."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Data Firehose to capture the sensor data. Directly store the data in Amazon DynamoDB for querying</strong> - Amazon Data Firehose cannot directly write to Amazon DynamoDB. Firehose currently supports Amazon S3, Amazon Redshift, Amazon OpenSearch Service, Splunk, Datadog, NewRelic, Dynatrace, Sumo Logic, LogicMonitor, MongoDB, and HTTP End Point as destinations."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon Simple Queue Service (Amazon SQS) to capture the real-time sensor data. Define an AWS Lambda function to poll the SQS queues and process the data. Store the data in Amazon DynamoDB for querying</strong> - Amazon SQS offers a reliable, highly scalable hosted queue for storing messages as they travel between applications or microservices. It moves data between distributed application components and helps you decouple these components. Amazon SQS cannot be used to capture and process real-time data."
      }
    ],
    "references": [
      "https://aws.amazon.com/dynamodb/",
      "https://aws.amazon.com/kinesis/data-streams/faqs/",
      "https://aws.amazon.com/sqs/faqs/"
    ]
  },
  {
    "id": 44,
    "question": "<p>A data engineer has created a Data Migration task using an AWS Database Migration Service (AWS DMS). The task seems to migrate all the tables with data but does not migrate the tables that have no data in them.</p>\n\n<p>What is the root cause behind this behavior?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>This is the expected behavior of AWS DMS. Empty tables are not migrated</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>There are inadequate resources allocated to the AWS DMS replication instance</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>You must include the schema and the tables in the table mapping of your DMS task</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Tables that have secondary indexes are not migrated</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>This is the expected behavior of DMS. Empty tables are not migrated</strong></p>\n\n<p>AWS Database Migration Service (AWS DMS) is a managed migration and replication service that helps you move your databases and analytics workloads to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.</p>\n\n<p>AWS DMS does not migrate empty tables. As a workaround, dummy data can be fed into the empty tables before the migration task to help migrate all the tables.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>There are inadequate resources allocated to the AWS DMS replication instance</strong> - A migration task runs slowly if enough resources are not allocated. Tables will not be dropped altogether for lack of resources.</p>\n\n<p><strong>You must include the schema and the tables in the table mapping of your DMS task</strong> - This option acts as a distractor.</p>\n\n<p><strong>Tables that have secondary indexes are not migrated</strong> - DMS doesn't create secondary indexes, non-primary key constraints, or data defaults of the table. Tables will however be migrated if they are not empty.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://repost.aws/questions/QUY-hYAEpWS6ixjllKK_y5kg/dms-task-tables-with-no-data-is-not-getting-migrated\">https://repost.aws/questions/QUY-hYAEpWS6ixjllKK_y5kg/dms-task-tables-with-no-data-is-not-getting-migrated</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Troubleshooting.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Troubleshooting.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>This is the expected behavior of DMS. Empty tables are not migrated</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Database Migration Service (AWS DMS) is a managed migration and replication service that helps you move your databases and analytics workloads to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database."
      },
      {
        "answer": "",
        "explanation": "AWS DMS does not migrate empty tables. As a workaround, dummy data can be fed into the empty tables before the migration task to help migrate all the tables."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>There are inadequate resources allocated to the AWS DMS replication instance</strong> - A migration task runs slowly if enough resources are not allocated. Tables will not be dropped altogether for lack of resources."
      },
      {
        "answer": "",
        "explanation": "<strong>You must include the schema and the tables in the table mapping of your DMS task</strong> - This option acts as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Tables that have secondary indexes are not migrated</strong> - DMS doesn't create secondary indexes, non-primary key constraints, or data defaults of the table. Tables will however be migrated if they are not empty."
      }
    ],
    "references": [
      "https://repost.aws/questions/QUY-hYAEpWS6ixjllKK_y5kg/dms-task-tables-with-no-data-is-not-getting-migrated",
      "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Troubleshooting.html"
    ]
  },
  {
    "id": 45,
    "question": "<p>A company utilizes an Amazon S3 bucket to store datasets accessed by various applications, including a financial services application that generates datasets containing personally identifiable information (PII). There is also an internal application that does not need access to this PII. To adhere to regulations, the company must avoid unnecessary sharing of PII. A data engineer is tasked with finding a solution that can dynamically redact PII depending on the specific needs of each application accessing the dataset.</p>\n\n<p>What solution can achieve this with minimal operational overhead?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an S3 Access Point to read data from the S3 bucket. Leverage the S3 Access Points policy to dynamically redact PII based on the needs of each application that accesses the data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an S3 gateway endpoint to read data from the S3 bucket. Leverage the S3 bucket policy to control access for each application that accesses the data only via the gateway endpoint</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up an S3 Object Lambda endpoint. Use the S3 Object Lambda endpoint to read data from the S3 bucket. Implement redaction logic within an S3 Object Lambda function to dynamically redact PII based on the needs of each application that accesses the data</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up an S3 interface endpoint to read data from the S3 bucket. Leverage the S3 bucket policy to control access for each application that accesses the data only via the interface endpoint</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an S3 Object Lambda endpoint. Use the S3 Object Lambda endpoint to read data from the S3 bucket. Implement redaction logic within an S3 Object Lambda function to dynamically redact PII based on the needs of each application that accesses the data</strong></p>\n\n<p>Use Amazon S3 Object Lambda Access Points for personally identifiable information (PII) to configure how documents are retrieved from your Amazon S3 bucket. You can control access to documents that contain PII and redact PII from documents. Amazon S3 Object Lambda Access Points use AWS Lambda functions to automatically transform the output of a standard Amazon S3 GET request. When you create an Amazon S3 Object Lambda Access Point for PII, documents are processed using Amazon Comprehend Lambda functions to control access of documents that contain PII and redact PII from documents.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q4-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q4-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/comprehend/latest/dg/using-access-points.html\">https://docs.aws.amazon.com/comprehend/latest/dg/using-access-points.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an S3 Access Point to read data from the S3 bucket. Leverage the S3 Access Points policy to dynamically redact PII based on the needs of each application that accesses the data</strong> - Amazon S3 Access Points simplify data access for any AWS service or customer application that stores data in S3. With S3 Access Points, customers can create unique access control policies for each access point to easily control access to shared datasets.</p>\n\n<p>Each S3 Access Point is configured with an access policy specific to a use case or application. For example, you can create an access point for your S3 bucket that grants access to groups of users or applications for your data lake. An Access Point can support a single user or application, or groups of users or applications within and across accounts, allowing separate management of each access point.</p>\n\n<p>You cannot use an S3 Access Points policy to dynamically redact PII based on the needs of each application that accesses the data.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q4-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q4-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/s3/features/access-points/\">https://aws.amazon.com/s3/features/access-points/</a><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q4-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q4-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/s3/features/access-points/\">https://aws.amazon.com/s3/features/access-points/</a><p></p>\n\n<p><strong>Set up an S3 interface endpoint to read data from the S3 bucket. Leverage the S3 bucket policy to dynamically redact PII for each application by allowing access only via the interface endpoint</strong> - With AWS PrivateLink for Amazon S3, you can provision interface VPC endpoints (interface endpoints) in your virtual private cloud (VPC). These endpoints are directly accessible from applications that are on premises over VPN and AWS Direct Connect, or in a different AWS Region over VPC peering. You cannot use an S3 bucket policy to dynamically redact PII by allowing access only via the interface endpoint. This option acts as a distractor.</p>\n\n<p><strong>Set up an S3 gateway endpoint to read data from the S3 bucket. Leverage the S3 bucket policy to dynamically redact PII for each application by allowing access only via the gateway endpoint</strong> - You can access Amazon S3 from your VPC using gateway VPC endpoints. After you create the gateway endpoint, you can add it as a target in your route table for traffic destined from your VPC to Amazon S3. With a gateway endpoint, you can access Amazon S3 from your VPC, without requiring an internet gateway or NAT device for your VPC, and with no additional cost. However, gateway endpoints do not allow access from on-premises networks, from peered VPCs in other AWS Regions, or through a transit gateway. You cannot use an S3 bucket policy to dynamically redact PII by allowing access only via the gateway endpoint. This option acts as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/comprehend/latest/dg/using-access-points.html\">https://docs.aws.amazon.com/comprehend/latest/dg/using-access-points.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/features/access-points/\">https://aws.amazon.com/s3/features/access-points/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an S3 Object Lambda endpoint. Use the S3 Object Lambda endpoint to read data from the S3 bucket. Implement redaction logic within an S3 Object Lambda function to dynamically redact PII based on the needs of each application that accesses the data</strong>"
      },
      {
        "answer": "",
        "explanation": "Use Amazon S3 Object Lambda Access Points for personally identifiable information (PII) to configure how documents are retrieved from your Amazon S3 bucket. You can control access to documents that contain PII and redact PII from documents. Amazon S3 Object Lambda Access Points use AWS Lambda functions to automatically transform the output of a standard Amazon S3 GET request. When you create an Amazon S3 Object Lambda Access Point for PII, documents are processed using Amazon Comprehend Lambda functions to control access of documents that contain PII and redact PII from documents."
      },
      {
        "link": "https://docs.aws.amazon.com/comprehend/latest/dg/using-access-points.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an S3 Access Point to read data from the S3 bucket. Leverage the S3 Access Points policy to dynamically redact PII based on the needs of each application that accesses the data</strong> - Amazon S3 Access Points simplify data access for any AWS service or customer application that stores data in S3. With S3 Access Points, customers can create unique access control policies for each access point to easily control access to shared datasets."
      },
      {
        "answer": "",
        "explanation": "Each S3 Access Point is configured with an access policy specific to a use case or application. For example, you can create an access point for your S3 bucket that grants access to groups of users or applications for your data lake. An Access Point can support a single user or application, or groups of users or applications within and across accounts, allowing separate management of each access point."
      },
      {
        "answer": "",
        "explanation": "You cannot use an S3 Access Points policy to dynamically redact PII based on the needs of each application that accesses the data."
      },
      {
        "link": "https://aws.amazon.com/s3/features/access-points/"
      },
      {
        "link": "https://aws.amazon.com/s3/features/access-points/"
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an S3 interface endpoint to read data from the S3 bucket. Leverage the S3 bucket policy to dynamically redact PII for each application by allowing access only via the interface endpoint</strong> - With AWS PrivateLink for Amazon S3, you can provision interface VPC endpoints (interface endpoints) in your virtual private cloud (VPC). These endpoints are directly accessible from applications that are on premises over VPN and AWS Direct Connect, or in a different AWS Region over VPC peering. You cannot use an S3 bucket policy to dynamically redact PII by allowing access only via the interface endpoint. This option acts as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an S3 gateway endpoint to read data from the S3 bucket. Leverage the S3 bucket policy to dynamically redact PII for each application by allowing access only via the gateway endpoint</strong> - You can access Amazon S3 from your VPC using gateway VPC endpoints. After you create the gateway endpoint, you can add it as a target in your route table for traffic destined from your VPC to Amazon S3. With a gateway endpoint, you can access Amazon S3 from your VPC, without requiring an internet gateway or NAT device for your VPC, and with no additional cost. However, gateway endpoints do not allow access from on-premises networks, from peered VPCs in other AWS Regions, or through a transit gateway. You cannot use an S3 bucket policy to dynamically redact PII by allowing access only via the gateway endpoint. This option acts as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/comprehend/latest/dg/using-access-points.html",
      "https://aws.amazon.com/s3/features/access-points/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html",
      "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
    ]
  },
  {
    "id": 46,
    "question": "<p>A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Data Engineer Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution.</p>\n\n<p>Which of the following will you recommend to meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB</strong></p>\n\n<p>To help ingest real-time data or streaming data at large scales, you can use Amazon Kinesis Data Streams (KDS). KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. The data collected is available in milliseconds, enabling real-time analytics. KDS provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications.</p>\n\n<p>AWS Lambda integrates natively with Kinesis Data Streams. The polling, checkpointing, and error-handling complexities are abstracted when you use this native integration. The processed data can then be configured to be saved in Amazon DynamoDB.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database</strong></p>\n\n<p><strong>Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB</strong></p>\n\n<p><strong>Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance</strong></p>\n\n<p>These three options use Amazon EC2 instances as part of the solution architecture. The use case seeks to minimize the management overhead required to maintain the solution. However, Amazon EC2 instances involve several maintenance activities such as managing the guest operating system and software deployed to the guest operating system, including updates and security patches, etc. Hence these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/\">https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB</strong>"
      },
      {
        "answer": "",
        "explanation": "To help ingest real-time data or streaming data at large scales, you can use Amazon Kinesis Data Streams (KDS). KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. The data collected is available in milliseconds, enabling real-time analytics. KDS provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications."
      },
      {
        "answer": "",
        "explanation": "AWS Lambda integrates natively with Kinesis Data Streams. The polling, checkpointing, and error-handling complexities are abstracted when you use this native integration. The processed data can then be configured to be saved in Amazon DynamoDB."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options use Amazon EC2 instances as part of the solution architecture. The use case seeks to minimize the management overhead required to maintain the solution. However, Amazon EC2 instances involve several maintenance activities such as managing the guest operating system and software deployed to the guest operating system, including updates and security patches, etc. Hence these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/"
    ]
  },
  {
    "id": 47,
    "question": "<p>A company maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the company has transitioned from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all the data to an Amazon S3 bucket, the company is now looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data.</p>\n\n<p>How will you implement this requirement without adding the overhead of splitting the data into logical groups?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong></p>\n\n<p>Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. When you use server-side encryption with Amazon S3 managed keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates.</p>\n\n<p>By default, Amazon S3 applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. All objects uploaded to Amazon S3 are automatically encrypted at no additional cost and with no impact on performance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong> - Server-side encryption with Amazon S3 managed keys (SSE-S3) is the easiest way to implement the given requirement, as there is no additional overhead of splitting data. Multiple S3 buckets are redundant for this requirement.</p>\n\n<p><strong>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data</strong> - Server-side encryption is the encryption of data at its destination by the application or service that receives it. The requirement is about server-side encryption and not about client-side encryption, hence this choice is incorrect.</p>\n\n<p><strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket</strong> - An encryption context is a set of key-value pairs that contain additional contextual information about the data. When an encryption context is specified for an encryption operation, Amazon S3 must specify the same encryption context for the decryption operation. The encryption context offers another level of security for the encryption key. However, it is not useful for generating unique keys.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong>"
      },
      {
        "answer": "",
        "explanation": "Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. When you use server-side encryption with Amazon S3 managed keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates."
      },
      {
        "answer": "",
        "explanation": "By default, Amazon S3 applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. All objects uploaded to Amazon S3 are automatically encrypted at no additional cost and with no impact on performance."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong> - Server-side encryption with Amazon S3 managed keys (SSE-S3) is the easiest way to implement the given requirement, as there is no additional overhead of splitting data. Multiple S3 buckets are redundant for this requirement."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data</strong> - Server-side encryption is the encryption of data at its destination by the application or service that receives it. The requirement is about server-side encryption and not about client-side encryption, hence this choice is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket</strong> - An encryption context is a set of key-value pairs that contain additional contextual information about the data. When an encryption context is specified for an encryption operation, Amazon S3 must specify the same encryption context for the decryption operation. The encryption context offers another level of security for the encryption key. However, it is not useful for generating unique keys."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html"
    ]
  },
  {
    "id": 48,
    "question": "<p>An e-commerce company wants to develop a click analytics dashboard to see near-real-time user click patterns. The clicks are currently ingested from various devices through Amazon Kinesis Data Streams. The dashboard must be refreshed automatically every ten seconds to display the most updated data. The company is looking for an easy-to-implement solution that can be put into production as soon as possible.</p>\n\n<p>Which solution would you recommend for the given requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Managed Streaming for Apache Kafka (MSK) to read the data in near-real-time. Develop a custom application for the dashboard by using D3.js</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Kinesis Data Firehose to push data into Amazon S3. Use Amazon QuickSight to build the dashboards from S3 data</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Kinesis Data Firehose to push the data into an Amazon OpenSearch Service. Visualize the data by using OpenSearch (Kibana) dashboards</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Glue streaming ETL to store the data into Amazon S3. Use S3 Analytics for analyzing the data</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Kinesis Data Firehose to push the data into an Amazon OpenSearch Service. Visualize the data by using OpenSearch (Kibana) dashboards</strong></p>\n\n<p>You can load streaming data into your Amazon OpenSearch Service domain from many different sources. Some sources, like Amazon Kinesis Data Firehose and Amazon CloudWatch Logs, have built-in support for OpenSearch Service. Others, like Amazon S3, Amazon Kinesis Data Streams, and Amazon DynamoDB, use AWS Lambda functions as event handlers.</p>\n\n<p>Amazon Kinesis Data Firehose manages all underlying infrastructure, storage, networking, and configuration needed to capture and load your data into Amazon S3, Amazon Redshift, or Amazon OpenSearch Service. OpenSearch (Kibana) dashboards support auto-refresh of data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Glue streaming ETL to store the data into Amazon S3. Use S3 Analytics for analyzing the data</strong> - By using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. S3 Analytics helps monitor S3 storage patterns and is not useful for running real-time analytics on data stored on S3.</p>\n\n<p><strong>Use Amazon Managed Streaming for Apache Kafka (MSK) to read the data in near-real-time. Develop a custom application for the dashboard by using D3.js</strong> - While Apache Kafka has several advantages, it is not easy to set up and takes more development time. Moreover, custom dashboard development by using D3.js will take additional time. Hence, this is not an option for the given use case.</p>\n\n<p><strong>Use Amazon Kinesis Data Firehose to push data into Amazon S3. Use Amazon QuickSight to build the dashboards from S3 data</strong> - Amazon QuickSight is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights to the people who you work with, wherever they are. Amazon QuickSight connects to your data in the cloud and combines data from many different sources. QuickSight has options to refresh data on a Daily, Weekly, or Monthly basis. For the enterprise edition only, you have an option to choose Hourly. Hence, QuickSight is an incorrect option since the expected refresh rate cannot be met.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://www.amazonaws.cn/en/kinesis/data-firehose/faqs/\">https://www.amazonaws.cn/en/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/refreshing-imported-data.html\">https://docs.aws.amazon.com/quicksight/latest/user/refreshing-imported-data.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Firehose to push the data into an Amazon OpenSearch Service. Visualize the data by using OpenSearch (Kibana) dashboards</strong>"
      },
      {
        "answer": "",
        "explanation": "You can load streaming data into your Amazon OpenSearch Service domain from many different sources. Some sources, like Amazon Kinesis Data Firehose and Amazon CloudWatch Logs, have built-in support for OpenSearch Service. Others, like Amazon S3, Amazon Kinesis Data Streams, and Amazon DynamoDB, use AWS Lambda functions as event handlers."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose manages all underlying infrastructure, storage, networking, and configuration needed to capture and load your data into Amazon S3, Amazon Redshift, or Amazon OpenSearch Service. OpenSearch (Kibana) dashboards support auto-refresh of data."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue streaming ETL to store the data into Amazon S3. Use S3 Analytics for analyzing the data</strong> - By using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. S3 Analytics helps monitor S3 storage patterns and is not useful for running real-time analytics on data stored on S3."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Managed Streaming for Apache Kafka (MSK) to read the data in near-real-time. Develop a custom application for the dashboard by using D3.js</strong> - While Apache Kafka has several advantages, it is not easy to set up and takes more development time. Moreover, custom dashboard development by using D3.js will take additional time. Hence, this is not an option for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Firehose to push data into Amazon S3. Use Amazon QuickSight to build the dashboards from S3 data</strong> - Amazon QuickSight is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights to the people who you work with, wherever they are. Amazon QuickSight connects to your data in the cloud and combines data from many different sources. QuickSight has options to refresh data on a Daily, Weekly, or Monthly basis. For the enterprise edition only, you have an option to choose Hourly. Hence, QuickSight is an incorrect option since the expected refresh rate cannot be met."
      }
    ],
    "references": [
      "https://www.amazonaws.cn/en/kinesis/data-firehose/faqs/",
      "https://docs.aws.amazon.com/quicksight/latest/user/refreshing-imported-data.html"
    ]
  },
  {
    "id": 49,
    "question": "<p>A data-storage service uses Amazon S3 under the hood to power its storage offerings which allow the customers to upload and view the files immediately. Currently, all the customer files are uploaded directly under a single S3 bucket. The data analytics team has started seeing scalability issues where customer file uploads are failing during peak access hours with more than 5000 requests per second.</p>\n\n<p>Which of the following represents the MOST resource-efficient and cost-optimal way of resolving this issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Change the application architecture to create a new S3 bucket for each day's data and then upload the daily files directly under that day's bucket</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Change the application architecture to create customer-specific custom prefixes within the single bucket and then upload the daily files into those prefixed locations</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Change the application architecture to create a new S3 bucket for each customer and then upload each customer's files directly under the respective buckets</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Change the application architecture to use the S3 Glacier Deep Archive storage class</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Change the application architecture to create customer-specific custom prefixes within the single bucket and then upload the daily files into those prefixed locations</strong></p>\n\n<p>Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket.</p>\n\n<p>There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second. Please see this example for more clarity on prefixes:\nif you have a file f1 stored in an S3 object path like so <code>s3://your_bucket_name/folder1/sub_folder_1/f1</code>, then <code>/folder1/sub_folder_1/</code> becomes the prefix for file f1.</p>\n\n<p>Some data lake applications on Amazon S3 scan millions or billions of objects for queries that run over petabytes of data. These data lake applications achieve single-instance transfer rates that maximize the network interface used for their Amazon EC2 instance, which can be up to 100 Gb/s on a single instance. These applications then aggregate throughput across multiple instances to get multiple terabits per second. Therefore creating customer-specific custom prefixes within the single bucket and then uploading the daily files into those prefixed locations is the BEST solution for the given constraints.</p>\n\n<p>Optimizing Amazon S3 Performance:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q23-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q23-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the application architecture to create a new S3 bucket for each customer and then upload each customer's files directly under the respective buckets</strong> - Creating a new S3 bucket for each new customer is an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some customers may use the service sparingly but the bucket name is locked for them forever. Moreover, this is really not required as we can use S3 prefixes to improve the performance.</p>\n\n<p><strong>Change the application architecture to create a new S3 bucket for each day's data and then upload the daily files directly under that day's bucket</strong> - Creating a new S3 bucket for each new day's data is also an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some of the bucket names may not be available for daily data processing. Moreover, this is really not required as we can use S3 prefixes to improve the performance.</p>\n\n<p><strong>Change the application architecture to use the S3 Glacier Deep Archive storage class</strong> - Glacier Deep Archive is designed to provide durable and secure long-term storage for large amounts of data at a price that is competitive with off-premises tape archival services. Data is stored across 3 or more AWS Availability Zones and can be retrieved in 12 hours or less. This option is a distractor because Glacier Deep Archive would not allow customers to retrieve the objects immediately.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Change the application architecture to create customer-specific custom prefixes within the single bucket and then upload the daily files into those prefixed locations</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket."
      },
      {
        "answer": "",
        "explanation": "There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second. Please see this example for more clarity on prefixes:\nif you have a file f1 stored in an S3 object path like so <code>s3://your_bucket_name/folder1/sub_folder_1/f1</code>, then <code>/folder1/sub_folder_1/</code> becomes the prefix for file f1."
      },
      {
        "answer": "",
        "explanation": "Some data lake applications on Amazon S3 scan millions or billions of objects for queries that run over petabytes of data. These data lake applications achieve single-instance transfer rates that maximize the network interface used for their Amazon EC2 instance, which can be up to 100 Gb/s on a single instance. These applications then aggregate throughput across multiple instances to get multiple terabits per second. Therefore creating customer-specific custom prefixes within the single bucket and then uploading the daily files into those prefixed locations is the BEST solution for the given constraints."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q23-i1.jpg",
        "answer": "",
        "explanation": "Optimizing Amazon S3 Performance:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Change the application architecture to create a new S3 bucket for each customer and then upload each customer's files directly under the respective buckets</strong> - Creating a new S3 bucket for each new customer is an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some customers may use the service sparingly but the bucket name is locked for them forever. Moreover, this is really not required as we can use S3 prefixes to improve the performance."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the application architecture to create a new S3 bucket for each day's data and then upload the daily files directly under that day's bucket</strong> - Creating a new S3 bucket for each new day's data is also an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some of the bucket names may not be available for daily data processing. Moreover, this is really not required as we can use S3 prefixes to improve the performance."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the application architecture to use the S3 Glacier Deep Archive storage class</strong> - Glacier Deep Archive is designed to provide durable and secure long-term storage for large amounts of data at a price that is competitive with off-premises tape archival services. Data is stored across 3 or more AWS Availability Zones and can be retrieved in 12 hours or less. This option is a distractor because Glacier Deep Archive would not allow customers to retrieve the objects immediately."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html"
    ]
  },
  {
    "id": 50,
    "question": "<p>A global media company uses a fleet of Amazon EC2 instances (behind an Application Load Balancer) to power its video streaming application. To improve the performance of the application, the data engineering team has also created an Amazon CloudFront distribution with the Application Load Balancer as the custom origin. The security team at the company has noticed a spike in the number and types of SQL injection and cross-site scripting attack vectors on the application.</p>\n\n<p>Which of the following solutions would you recommend as the MOST effective in countering these malicious attacks?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use security groups with Amazon CloudFront distribution</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Config with CloudFront distribution</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Route 53 with Amazon CloudFront distribution</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.</p>\n\n<p>How AWS WAF Works:\n<img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a><p></p>\n\n<p>A web access control list (web ACL) gives you fine-grained control over the web requests that your Amazon CloudFront distribution, Amazon API Gateway API, or Application Load Balancer responds to.</p>\n\n<p>When you create a web ACL, you can specify one or more Amazon CloudFront distributions that you want AWS WAF to inspect. AWS WAF starts to allow, block, or count web requests for those distributions based on the conditions that you identify in the web ACL. Therefore, combining AWS WAF with Amazon CloudFront can prevent SQL injection and cross-site scripting attacks. So this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Route 53 with Amazon CloudFront distribution</strong> - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. You cannot use Route 53 to prevent SQL injection and cross-site scripting attacks. So this option is incorrect.</p>\n\n<p><strong>Use security groups with Amazon CloudFront distribution</strong> - A security group controls the traffic that is allowed to reach and leave the resources that it is associated with. For example, after you associate a security group with an EC2 instance, it controls the inbound and outbound traffic for the instance.</p>\n\n<p>When you create a VPC, it comes with a default security group. You can create additional security groups for a VPC, each with its own inbound and outbound rules. You can specify the source, port range, and protocol for each inbound rule. You can specify the destination, port range, and protocol for each outbound rule. You cannot use security groups to prevent SQL injection and cross-site scripting attacks. So this option is incorrect.</p>\n\n<p><strong>Use AWS Config with CloudFront distribution</strong> - AWS Config helps with auditing and recording compliance of your AWS resources. You cannot use AWS Config to prevent SQL injection and cross-site scripting attacks. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/waf/features/\">https://aws.amazon.com/waf/features/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html\">https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define."
      },
      {
        "image": "https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png",
        "answer": "",
        "explanation": "How AWS WAF Works:"
      },
      {
        "link": "https://aws.amazon.com/waf/"
      },
      {
        "answer": "",
        "explanation": "A web access control list (web ACL) gives you fine-grained control over the web requests that your Amazon CloudFront distribution, Amazon API Gateway API, or Application Load Balancer responds to."
      },
      {
        "answer": "",
        "explanation": "When you create a web ACL, you can specify one or more Amazon CloudFront distributions that you want AWS WAF to inspect. AWS WAF starts to allow, block, or count web requests for those distributions based on the conditions that you identify in the web ACL. Therefore, combining AWS WAF with Amazon CloudFront can prevent SQL injection and cross-site scripting attacks. So this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Route 53 with Amazon CloudFront distribution</strong> - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. You cannot use Route 53 to prevent SQL injection and cross-site scripting attacks. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use security groups with Amazon CloudFront distribution</strong> - A security group controls the traffic that is allowed to reach and leave the resources that it is associated with. For example, after you associate a security group with an EC2 instance, it controls the inbound and outbound traffic for the instance."
      },
      {
        "answer": "",
        "explanation": "When you create a VPC, it comes with a default security group. You can create additional security groups for a VPC, each with its own inbound and outbound rules. You can specify the source, port range, and protocol for each inbound rule. You can specify the destination, port range, and protocol for each outbound rule. You cannot use security groups to prevent SQL injection and cross-site scripting attacks. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Config with CloudFront distribution</strong> - AWS Config helps with auditing and recording compliance of your AWS resources. You cannot use AWS Config to prevent SQL injection and cross-site scripting attacks. So this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/waf/",
      "https://aws.amazon.com/waf/features/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html",
      "https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html"
    ]
  },
  {
    "id": 51,
    "question": "<p>An IT company has built a custom data warehousing solution for a large shipping company by using Amazon Redshift. The solution helps the shipping company to analyze the international/domestic cargo transportation details and operational records for the ships. As part of the cost optimizations, the shipping company now wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last year. However, the data engineers at multiple divisions of the shipping company want to retain the ability to cross-reference this historical data along with the daily reports. The shipping company wants to develop a solution with the LEAST amount of effort and MINIMUM cost.</p>\n\n<p>Which option would you recommend for this requirement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Redshift Spectrum to create Redshift cluster tables pointing to the underlying historical data in S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up access to the historical data via Athena. The analytics team can run historical data queries on Athena and continue the daily reporting on Redshift. In case the reports need to be cross-referenced, the analytics team needs to export these in flat files and then do further analysis</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the Redshift COPY command to load the S3-based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Glue ETL job to load the S3-based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Redshift Spectrum to create Redshift cluster tables pointing to the underlying historical data in S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift</strong></p>\n\n<p>Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis.</p>\n\n<p>Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables.</p>\n\n<p>Amazon Redshift Spectrum resides on dedicated Amazon Redshift servers that are independent of your cluster. The Redshift Spectrum pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to the Redshift Spectrum layer. Thus, Redshift Spectrum queries use less of your cluster's processing capacity than other queries.</p>\n\n<p>Redshift Spectrum Overview\n<img src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2017/07/18/redshift_spectrum-1.gif\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2017/07/18/redshift_spectrum-1.gif\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Setup access to the historical data via Athena. The analytics team can run historical data queries on Athena and continue the daily reporting on Redshift. In case the reports need to be cross-referenced, the analytics team needs to export these in flat files and then do further analysis</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>Providing access to historical data via Athena would mean that historical data reconciliation would become difficult as the daily report would still be produced via Redshift. Such a setup is cumbersome to maintain on a day-to-day basis. Hence the option to use Athena is ruled out.</p>\n\n<p><strong>Use the Redshift COPY command to load the S3-based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</strong></p>\n\n<p><strong>Use Glue ETL job to load the S3-based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</strong></p>\n\n<p>Loading historical data into Redshift via COPY command or via Glue ETL job would be cost-heavy for a one-time ad-hoc process. The same result can be achieved more cost-efficiently by using Redshift Spectrum. Therefore both these options to load historical data into Redshift are also incorrect for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/\">https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Redshift Spectrum to create Redshift cluster tables pointing to the underlying historical data in S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis."
      },
      {
        "answer": "",
        "explanation": "Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables."
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift Spectrum resides on dedicated Amazon Redshift servers that are independent of your cluster. The Redshift Spectrum pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to the Redshift Spectrum layer. Thus, Redshift Spectrum queries use less of your cluster's processing capacity than other queries."
      },
      {
        "image": "https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2017/07/18/redshift_spectrum-1.gif",
        "answer": "",
        "explanation": "Redshift Spectrum Overview"
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Setup access to the historical data via Athena. The analytics team can run historical data queries on Athena and continue the daily reporting on Redshift. In case the reports need to be cross-referenced, the analytics team needs to export these in flat files and then do further analysis</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries."
      },
      {
        "answer": "",
        "explanation": "Providing access to historical data via Athena would mean that historical data reconciliation would become difficult as the daily report would still be produced via Redshift. Such a setup is cumbersome to maintain on a day-to-day basis. Hence the option to use Athena is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the Redshift COPY command to load the S3-based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Use Glue ETL job to load the S3-based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</strong>"
      },
      {
        "answer": "",
        "explanation": "Loading historical data into Redshift via COPY command or via Glue ETL job would be cost-heavy for a one-time ad-hoc process. The same result can be achieved more cost-efficiently by using Redshift Spectrum. Therefore both these options to load historical data into Redshift are also incorrect for the given use-case."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/",
      "https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/"
    ]
  },
  {
    "id": 52,
    "question": "<p>A company is using a fleet of Amazon EC2 instances to ingest Internet-of-Things (IoT) data from various data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is restarted, the in-flight data is lost. The data engineering team at the company wants to store as well as query the ingested data in near-real-time.</p>\n\n<p>Which of the following solutions provides near-real-time data querying that is scalable with minimal data loss?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Capture data in Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Capture data in an Amazon EBS volume and then publish this data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Capture data in Amazon Kinesis Data Streams. Use Amazon Kinesis Data Analytics to query and analyze this streaming data in real-time</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Capture data in an Amazon EC2 instance store and then publish this data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Capture data in Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data</strong></p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New Relic, MongoDB, and Splunk.</p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to capture, transform, and load streaming data into Redshift for near real-time analytics. It is also an auto-scaling solution as there is no need to provision any shards like Kinesis Data Streams.</p>\n\n<p>Amazon Redshift allows you to run complex analytic queries against petabytes of structured data, using sophisticated query optimization, columnar storage on high-performance local disks, and massively parallel query execution. Most results come back in seconds.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Capture data in an Amazon EC2 instance store and then publish this data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data</strong> - Instance store is a temporary storage available on Amazon EC2 instances. The in-flight data (that is, data arriving from the source) being processed by a specific Amazon EC2 instance will be lost in case that instance is restarted. Hence, this cannot be the option for the given use case.</p>\n\n<p><strong>Capture data in an Amazon EBS volume and then publish this data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data</strong> - Amazon EBS volumes cannot be used to store high volume data. Amazon EBS can be used to store cache data if a database is hosted on an Amazon EC2 instance. However, Amazon EBS cannot be used in place of a database. Amazon ElastiCache is a caching service. It is not relevant to the given use case.</p>\n\n<p><strong>Capture data in Amazon Kinesis Data Streams. Use Amazon Kinesis Data Analytics to query and analyze this streaming data in real-time</strong> - For Amazon Kinesis Data Streams, you have to manually allocate the shards for scaling the data ingestion process. Amazon Kinesis Data Streams (KDS) and Amazon Kinesis Data Analytics are for real-time processing of data and cannot provide long-term storage of data unlike a database or a data warehouse. So, this option is not right for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/redshift/features/\">https://aws.amazon.com/redshift/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Capture data in Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New Relic, MongoDB, and Splunk."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is the easiest way to capture, transform, and load streaming data into Redshift for near real-time analytics. It is also an auto-scaling solution as there is no need to provision any shards like Kinesis Data Streams."
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift allows you to run complex analytic queries against petabytes of structured data, using sophisticated query optimization, columnar storage on high-performance local disks, and massively parallel query execution. Most results come back in seconds."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Capture data in an Amazon EC2 instance store and then publish this data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data</strong> - Instance store is a temporary storage available on Amazon EC2 instances. The in-flight data (that is, data arriving from the source) being processed by a specific Amazon EC2 instance will be lost in case that instance is restarted. Hence, this cannot be the option for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Capture data in an Amazon EBS volume and then publish this data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data</strong> - Amazon EBS volumes cannot be used to store high volume data. Amazon EBS can be used to store cache data if a database is hosted on an Amazon EC2 instance. However, Amazon EBS cannot be used in place of a database. Amazon ElastiCache is a caching service. It is not relevant to the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Capture data in Amazon Kinesis Data Streams. Use Amazon Kinesis Data Analytics to query and analyze this streaming data in real-time</strong> - For Amazon Kinesis Data Streams, you have to manually allocate the shards for scaling the data ingestion process. Amazon Kinesis Data Streams (KDS) and Amazon Kinesis Data Analytics are for real-time processing of data and cannot provide long-term storage of data unlike a database or a data warehouse. So, this option is not right for the current use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/redshift/features/",
      "https://aws.amazon.com/kinesis/data-firehose/faqs/",
      "https://aws.amazon.com/kinesis/data-analytics/faqs/",
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 53,
    "question": "<p>A healthcare company uses an Amazon Redshift database cluster to store sensitive user data. The regulatory guidelines mandate logging so that any database authentication attempts as well as the connections/disconnections are recorded. Also, the logs must include a record of each query executed against the database along with the database user who executed that query.</p>\n\n<p>Which of the following options represents the best solution for these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable audit trail for Amazon Redshift on Amazon CloudTrail</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable audit metrics on Amazon CloudWatch</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable audit logging for Amazon Redshift</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Enable and download audit reports from AWS Config</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable audit logging for Amazon Redshift</strong></p>\n\n<p>Amazon Redshift logs information about connections and user activities in your database. These logs help you monitor the database for security and troubleshooting purposes, a process called database auditing. The logs are stored in Amazon S3 buckets.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q20-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q20-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html</a><p></p>\n\n<p>Audit logging is not turned on by default in Amazon Redshift. When you turn on logging on your cluster, Amazon Redshift creates and uploads logs to Amazon S3 that capture data from the time audit logging is enabled to the present time. Each logging update is a continuation of the information that was already logged.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable audit trail for Amazon Redshift on Amazon CloudTrail</strong> - Amazon Redshift is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon Redshift. CloudTrail captures all API calls for Amazon Redshift as events. These include calls from the Amazon Redshift console and from code calls to the Amazon Redshift API operations. For example, you can have a CloudTrail log entry for a CreateCluster call or a DeleteCluster call. CloudTrail cannot capture details such as database authentication attempts, any connections/disconnections to the database, or capturing the queries executed by Redshift users. So this option is incorrect.</p>\n\n<p><strong>Enable and download audit reports from AWS Config</strong> - AWS Config helps with auditing and recording compliance of your AWS resources. You cannot use AWS Config for capturing Redshift details such as database authentication attempts, any connections/disconnections to the database or capturing the queries executed by Redshift users. So this option is incorrect.</p>\n\n<p><strong>Enable audit metrics on Amazon CloudWatch</strong> - You can use CloudWatch to monitor the performance of your Redshift cluster and not for audit purposes.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/metrics-listing.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/metrics-listing.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable audit logging for Amazon Redshift</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift logs information about connections and user activities in your database. These logs help you monitor the database for security and troubleshooting purposes, a process called database auditing. The logs are stored in Amazon S3 buckets."
      },
      {
        "link": "https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html"
      },
      {
        "answer": "",
        "explanation": "Audit logging is not turned on by default in Amazon Redshift. When you turn on logging on your cluster, Amazon Redshift creates and uploads logs to Amazon S3 that capture data from the time audit logging is enabled to the present time. Each logging update is a continuation of the information that was already logged."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable audit trail for Amazon Redshift on Amazon CloudTrail</strong> - Amazon Redshift is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon Redshift. CloudTrail captures all API calls for Amazon Redshift as events. These include calls from the Amazon Redshift console and from code calls to the Amazon Redshift API operations. For example, you can have a CloudTrail log entry for a CreateCluster call or a DeleteCluster call. CloudTrail cannot capture details such as database authentication attempts, any connections/disconnections to the database, or capturing the queries executed by Redshift users. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable and download audit reports from AWS Config</strong> - AWS Config helps with auditing and recording compliance of your AWS resources. You cannot use AWS Config for capturing Redshift details such as database authentication attempts, any connections/disconnections to the database or capturing the queries executed by Redshift users. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable audit metrics on Amazon CloudWatch</strong> - You can use CloudWatch to monitor the performance of your Redshift cluster and not for audit purposes."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html",
      "https://docs.aws.amazon.com/redshift/latest/mgmt/metrics-listing.html"
    ]
  },
  {
    "id": 54,
    "question": "<p>A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket.</p>\n\n<p>Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use multipart uploads for faster file uploads into the destination Amazon S3 bucket</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket</strong></p>\n\n<p>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Amazon S3TA takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p>\n\n<p><strong>Use multipart uploads for faster file uploads into the destination Amazon S3 bucket</strong></p>\n\n<p>Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput, therefore it facilitates faster file uploads.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3</strong> - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations.\nDirect connect takes significant time (several months) to be provisioned and is an overkill for the given use case.</p>\n\n<p><strong>Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet.\nVPN Connections are a good solution if you have low to modest bandwidth requirements and can tolerate the inherent variability in Internet-based connectivity. Site-to-site VPN will not help in accelerating the file transfer speeds into S3 for the given use case.</p>\n\n<p><strong>Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket</strong> - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. AWS Global Accelerator will not help in accelerating the file transfer speeds into S3 for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Amazon S3TA takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path."
      },
      {
        "answer": "",
        "explanation": "<strong>Use multipart uploads for faster file uploads into the destination Amazon S3 bucket</strong>"
      },
      {
        "answer": "",
        "explanation": "Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput, therefore it facilitates faster file uploads."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3</strong> - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations.\nDirect connect takes significant time (several months) to be provisioned and is an overkill for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet.\nVPN Connections are a good solution if you have low to modest bandwidth requirements and can tolerate the inherent variability in Internet-based connectivity. Site-to-site VPN will not help in accelerating the file transfer speeds into S3 for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket</strong> - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. AWS Global Accelerator will not help in accelerating the file transfer speeds into S3 for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html"
    ]
  },
  {
    "id": 55,
    "question": "<p>A niche social media application allows users to connect with sports athletes. As a data engineer, you've designed the architecture of the application to be fully serverless using Amazon API Gateway and AWS Lambda. The backend uses an Amazon DynamoDB table. Some of the star athletes using the application are highly popular, and therefore Amazon DynamoDB has increased the read capacity units (RCUs). Still, the application is experiencing a hot partition problem.</p>\n\n<p>What can you do to improve the performance of Amazon DynamoDB and eliminate the hot partition problem without a lot of application refactoring?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon DynamoDB Global Tables</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon DynamoDB Streams</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon DynamoDB DAX</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon ElastiCache</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon DynamoDB DAX</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.</p>\n\n<p>DAX will be transparent and won't require application refactoring, and will cache the \"hotkeys\". Therefore, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon DynamoDB Global Tables</strong> - Amazon DynamoDB Global Tables builds upon DynamoDB’s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions. But Global Tables cannot address the hotkey issue.</p>\n\n<p><strong>Use Amazon DynamoDB Streams</strong> - Amazon DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. DynamoDB Streams cannot address the hotkey issue.</p>\n\n<p><strong>Use Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. ElastiCache could also be a solution, but it will require a lot of refactoring work on the AWS Lambda side.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon DynamoDB DAX</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications."
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management."
      },
      {
        "answer": "",
        "explanation": "DAX will be transparent and won't require application refactoring, and will cache the \"hotkeys\". Therefore, this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon DynamoDB Global Tables</strong> - Amazon DynamoDB Global Tables builds upon DynamoDB’s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions. But Global Tables cannot address the hotkey issue."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon DynamoDB Streams</strong> - Amazon DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. DynamoDB Streams cannot address the hotkey issue."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. ElastiCache could also be a solution, but it will require a lot of refactoring work on the AWS Lambda side."
      }
    ],
    "references": [
      "https://aws.amazon.com/dynamodb/dax/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"
    ]
  },
  {
    "id": 56,
    "question": "<p>An investment firm uses AWS Cloud for its IT infrastructure. The firm runs several investment-related risk-simulation applications and develops complex algorithms to simulate multiple scenarios to build a financial roadmap for its customers. The firm stores customers' financial data on Amazon S3. The data engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access.</p>\n\n<p>Which of the following solutions would you suggest for this use case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use S3 Glacier to store the sensitive archived data and then use an S3 lifecycle policy to enforce compliance controls</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use S3 Glacier vault to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use S3 Glacier to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls</strong></p>\n\n<p>Amazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>An S3 Glacier vault is a container for storing archives. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy. You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the policy from future edits. Therefore, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use S3 Glacier to store the sensitive archived data and then use an S3 lifecycle policy to enforce compliance controls</strong> - You can use lifecycle policy to define actions you want Amazon S3 to take during an object's lifetime. For example, use a lifecycle policy to transition objects to another storage class, archive them, or delete them after a specified period. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p><strong>Use S3 Glacier vault to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p><strong>Use S3 Glacier to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements."
      },
      {
        "answer": "",
        "explanation": "An S3 Glacier vault is a container for storing archives. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy. You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the policy from future edits. Therefore, this is the correct option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use S3 Glacier to store the sensitive archived data and then use an S3 lifecycle policy to enforce compliance controls</strong> - You can use lifecycle policy to define actions you want Amazon S3 to take during an object's lifetime. For example, use a lifecycle policy to transition objects to another storage class, archive them, or delete them after a specified period. It cannot be used to enforce compliance controls. Therefore, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use S3 Glacier vault to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use S3 Glacier to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html",
      "https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html"
    ]
  },
  {
    "id": 57,
    "question": "<p>A company uses an Amazon DynamoDB table in provisioned capacity mode to store data for an application that experiences predictable demand. Activity spikes sharply every Monday morning, while weekends see very low usage. The company needs a solution that ensures consistent application performance during these peak periods in a cost-effective manner.</p>\n\n<p>What is the best solution to meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Update the capacity mode from provisioned to on-demand so that the table's capacity automatically adjusts to the traffic variations</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add Global Secondary Index as well as Local Secondary Index to the DynamoDB table to ensure maximum performance</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Implement AWS Application Auto Scaling to adjust provisioned capacity to meet the maximum demand seen in the last 12 months</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Implement AWS Application Auto Scaling to adjust provisioned capacity according to usage patterns. Set higher capacity during anticipated peak times and reduce it during periods of low activity</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement AWS Application Auto Scaling to adjust provisioned capacity according to usage patterns. Set higher capacity during anticipated peak times and reduce it during periods of low activity</strong></p>\n\n<p>With provisioned capacity, you pay for the provision of read and write capacity units for your DynamoDB tables. Whereas with DynamoDB on-demand you pay per request for the data reads and writes that your application performs on your tables.</p>\n\n<p>With provisioned capacity, you can also use auto-scaling to automatically adjust your table’s capacity based on the specified utilization rate to ensure application performance, and potentially reduce costs. To configure auto-scaling in DynamoDB, set the minimum and maximum levels of read and write capacity in addition to the target utilization percentage.</p>\n\n<p>Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you don't pay for unused provisioned capacity.</p>\n\n<p>With Application Auto Scaling, you create a scaling policy for a table or a global secondary index. The scaling policy specifies whether you want to scale read capacity or write capacity (or both), and the minimum and maximum provisioned capacity unit settings for the table or index. The scaling policy also contains a target utilization—the percentage of consumed provisioned throughput at a point in time. Application Auto Scaling uses a target tracking algorithm to adjust the provisioned throughput of the table (or index) upward or downward in response to actual workloads so that the actual capacity utilization remains at or near your target utilization.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q9-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q9-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the capacity mode from provisioned to on-demand so that the table's capacity automatically adjusts to the traffic variations</strong> - With on-demand capacity mode, DynamoDB charges you for the data reads and writes your application performs on your tables. You do not need to specify how much read and write throughput you expect your application to perform because DynamoDB instantly accommodates your workloads as they ramp up or down. With provisioned capacity mode, you specify the number of reads and writes per second that you expect your application to require, and you are billed based on that. Furthermore, if you can forecast your capacity requirements you can also reserve a portion of DynamoDB provisioned capacity and optimize your costs even further.</p>\n\n<p>It is important to note that DynamoDB auto scaling modifies provisioned throughput settings only when the actual workload stays elevated or depressed for a sustained period of several minutes. This applies to scaling up or down the provisioned capacity of a DynamoDB table. In the case that you have an occasional usage spike auto scaling might not be able to react in time. This sometimes can be mitigated from DynamoDB burst capacity where DynamoDB reserves a portion of the unused provisioned capacity for later bursts of throughput. The burst capacity is limited though and these extra capacity units can be consumed quickly.</p>\n\n<p>This means that provisioned capacity is best for you if you have relatively predictable application traffic (the given use case mentions that the application experiences predictable demand), run applications whose traffic is consistent, and ramps up or down gradually.</p>\n\n<p><strong>Implement AWS Application Auto Scaling to adjust provisioned capacity to meet the maximum demand seen in the last 12 months</strong> - There is no need to have the Application Auto Scaling to adjust provisioned capacity to meet the maximum demand seen in the last 12 months, as it is a wasteful use of resources and also a much costlier solution.</p>\n\n<p><strong>Add Global Secondary Index as well as Local Secondary Index to the DynamoDB table to ensure maximum performance</strong> - This option has been added as a distractor.</p>\n\n<p>Amazon DynamoDB supports two types of secondary indexes:</p>\n\n<p>Global secondary index (GSI) — An index with a partition key and a sort key that can be different from those on the base table.</p>\n\n<p>Local secondary index (LSI)—An index that has the same partition key as the base table, but a different sort key.</p>\n\n<p>Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. GSI and LSI can improve performance by improving data access.</p>\n\n<p>However, the use case needs a solution that can adjust to variations in traffic on a predictable basis. GSI and LSI cannot address this requirement.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.Console.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.Console.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html\">https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Implement AWS Application Auto Scaling to adjust provisioned capacity according to usage patterns. Set higher capacity during anticipated peak times and reduce it during periods of low activity</strong>"
      },
      {
        "answer": "",
        "explanation": "With provisioned capacity, you pay for the provision of read and write capacity units for your DynamoDB tables. Whereas with DynamoDB on-demand you pay per request for the data reads and writes that your application performs on your tables."
      },
      {
        "answer": "",
        "explanation": "With provisioned capacity, you can also use auto-scaling to automatically adjust your table’s capacity based on the specified utilization rate to ensure application performance, and potentially reduce costs. To configure auto-scaling in DynamoDB, set the minimum and maximum levels of read and write capacity in addition to the target utilization percentage."
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you don't pay for unused provisioned capacity."
      },
      {
        "answer": "",
        "explanation": "With Application Auto Scaling, you create a scaling policy for a table or a global secondary index. The scaling policy specifies whether you want to scale read capacity or write capacity (or both), and the minimum and maximum provisioned capacity unit settings for the table or index. The scaling policy also contains a target utilization—the percentage of consumed provisioned throughput at a point in time. Application Auto Scaling uses a target tracking algorithm to adjust the provisioned throughput of the table (or index) upward or downward in response to actual workloads so that the actual capacity utilization remains at or near your target utilization."
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Update the capacity mode from provisioned to on-demand so that the table's capacity automatically adjusts to the traffic variations</strong> - With on-demand capacity mode, DynamoDB charges you for the data reads and writes your application performs on your tables. You do not need to specify how much read and write throughput you expect your application to perform because DynamoDB instantly accommodates your workloads as they ramp up or down. With provisioned capacity mode, you specify the number of reads and writes per second that you expect your application to require, and you are billed based on that. Furthermore, if you can forecast your capacity requirements you can also reserve a portion of DynamoDB provisioned capacity and optimize your costs even further."
      },
      {
        "answer": "",
        "explanation": "It is important to note that DynamoDB auto scaling modifies provisioned throughput settings only when the actual workload stays elevated or depressed for a sustained period of several minutes. This applies to scaling up or down the provisioned capacity of a DynamoDB table. In the case that you have an occasional usage spike auto scaling might not be able to react in time. This sometimes can be mitigated from DynamoDB burst capacity where DynamoDB reserves a portion of the unused provisioned capacity for later bursts of throughput. The burst capacity is limited though and these extra capacity units can be consumed quickly."
      },
      {
        "answer": "",
        "explanation": "This means that provisioned capacity is best for you if you have relatively predictable application traffic (the given use case mentions that the application experiences predictable demand), run applications whose traffic is consistent, and ramps up or down gradually."
      },
      {
        "answer": "",
        "explanation": "<strong>Implement AWS Application Auto Scaling to adjust provisioned capacity to meet the maximum demand seen in the last 12 months</strong> - There is no need to have the Application Auto Scaling to adjust provisioned capacity to meet the maximum demand seen in the last 12 months, as it is a wasteful use of resources and also a much costlier solution."
      },
      {
        "answer": "",
        "explanation": "<strong>Add Global Secondary Index as well as Local Secondary Index to the DynamoDB table to ensure maximum performance</strong> - This option has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB supports two types of secondary indexes:"
      },
      {
        "answer": "",
        "explanation": "Global secondary index (GSI) — An index with a partition key and a sort key that can be different from those on the base table."
      },
      {
        "answer": "",
        "explanation": "Local secondary index (LSI)—An index that has the same partition key as the base table, but a different sort key."
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. GSI and LSI can improve performance by improving data access."
      },
      {
        "answer": "",
        "explanation": "However, the use case needs a solution that can adjust to variations in traffic on a predictable basis. GSI and LSI cannot address this requirement."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.Console.html",
      "https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html"
    ]
  },
  {
    "id": 58,
    "question": "<p>A company has established an ETL (extract, transform, and load) data pipeline using AWS Glue. A data engineer is tasked with crawling a table from Microsoft SQL Server and must manage the pipeline to extract, transform, and load the crawled data into an Amazon S3 bucket.</p>\n\n<p>Which AWS service or feature would facilitate these tasks most cost-effectively?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Glue DataBrew</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Glue workflows</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS Glue Studio</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Step Functions</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Glue workflows</strong></p>\n\n<p>Extract, transform, and load (ETL) orchestration is a common mechanism for building big data pipelines. Orchestration for parallel ETL processing requires the use of multiple tools to perform a variety of operations. AWS Glue workflows provide a visual and programmatic tool to author data pipelines by combining AWS Glue crawlers for schema discovery and AWS Glue Spark and Python shell jobs to transform the data. A workflow consists of one of more task nodes arranged as a graph. Relationships can be defined and parameters passed between task nodes to enable you to build pipelines of varying complexity.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q6-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q6-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/orchestrate-an-etl-pipeline-using-aws-glue-workflows-triggers-and-crawlers-with-custom-classifiers/\">https://aws.amazon.com/blogs/big-data/orchestrate-an-etl-pipeline-using-aws-glue-workflows-triggers-and-crawlers-with-custom-classifiers/</a><p></p>\n\n<p>Using AWS Glue workflows, you can design a complex multi-job, multi-crawler ETL process that AWS Glue can run and track as a single entity. After you create a workflow and specify the jobs, crawlers, and triggers in the workflow, you can run the workflow on demand or on a schedule.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Step Functions</strong> - AWS Step Functions is a generic serverless orchestration service offered by AWS. The given use case requires a feature that facilitates complex multi-job, multi-crawler ETL processes that AWS Glue can run and track as a single entity. Therefore, AWS Glue workflows is a much better fit for the given requirement.</p>\n\n<p><strong>AWS Glue Studio</strong> - AWS Glue Studio is a graphical interface that makes it easy to create, run, and monitor data integration jobs in AWS Glue. You can visually compose data transformation workflows and seamlessly run them on the Apache Spark-based serverless ETL engine in AWS Glue.</p>\n\n<p><strong>AWS Glue DataBrew</strong> - AWS Glue DataBrew is a visual data preparation tool that makes it easier for data analysts and data scientists to clean and normalize data to prepare it for analytics and machine learning (ML). You can choose from over 250 prebuilt transformations to automate data preparation tasks, all without the need to write any code.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/orchestrate-an-etl-pipeline-using-aws-glue-workflows-triggers-and-crawlers-with-custom-classifiers/\">https://aws.amazon.com/blogs/big-data/orchestrate-an-etl-pipeline-using-aws-glue-workflows-triggers-and-crawlers-with-custom-classifiers/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/orchestrate-using-workflows.html\">https://docs.aws.amazon.com/glue/latest/dg/orchestrate-using-workflows.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html\">https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/glue/features/databrew/\">https://aws.amazon.com/glue/features/databrew/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Glue workflows</strong>"
      },
      {
        "answer": "",
        "explanation": "Extract, transform, and load (ETL) orchestration is a common mechanism for building big data pipelines. Orchestration for parallel ETL processing requires the use of multiple tools to perform a variety of operations. AWS Glue workflows provide a visual and programmatic tool to author data pipelines by combining AWS Glue crawlers for schema discovery and AWS Glue Spark and Python shell jobs to transform the data. A workflow consists of one of more task nodes arranged as a graph. Relationships can be defined and parameters passed between task nodes to enable you to build pipelines of varying complexity."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/orchestrate-an-etl-pipeline-using-aws-glue-workflows-triggers-and-crawlers-with-custom-classifiers/"
      },
      {
        "answer": "",
        "explanation": "Using AWS Glue workflows, you can design a complex multi-job, multi-crawler ETL process that AWS Glue can run and track as a single entity. After you create a workflow and specify the jobs, crawlers, and triggers in the workflow, you can run the workflow on demand or on a schedule."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Step Functions</strong> - AWS Step Functions is a generic serverless orchestration service offered by AWS. The given use case requires a feature that facilitates complex multi-job, multi-crawler ETL processes that AWS Glue can run and track as a single entity. Therefore, AWS Glue workflows is a much better fit for the given requirement."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Glue Studio</strong> - AWS Glue Studio is a graphical interface that makes it easy to create, run, and monitor data integration jobs in AWS Glue. You can visually compose data transformation workflows and seamlessly run them on the Apache Spark-based serverless ETL engine in AWS Glue."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Glue DataBrew</strong> - AWS Glue DataBrew is a visual data preparation tool that makes it easier for data analysts and data scientists to clean and normalize data to prepare it for analytics and machine learning (ML). You can choose from over 250 prebuilt transformations to automate data preparation tasks, all without the need to write any code."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/orchestrate-an-etl-pipeline-using-aws-glue-workflows-triggers-and-crawlers-with-custom-classifiers/",
      "https://docs.aws.amazon.com/glue/latest/dg/orchestrate-using-workflows.html",
      "https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html",
      "https://aws.amazon.com/glue/features/databrew/"
    ]
  },
  {
    "id": 59,
    "question": "<p>An IT company is revamping its ETL process and wants to transfer data from Amazon S3 to an Amazon Redshift cluster. The company wants to load the data in bulk onto Amazon Redshift using the best possible high-performance solution.</p>\n\n<p>As an AWS Certified Data Engineer Associate, which of the following steps would you recommend for this requirement? (Select two)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>When loading multiple files into a single table, use a single S3DistCp command</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>When loading multiple files into a single table, use multiple COPY commands</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Leverage temporary staging tables during the data loading process</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Redshift Spectrum to upload data from multiple files in Amazon S3 into a single Amazon Redshift table</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>When loading multiple files into a single table, use a single COPY command</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Processing",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>When loading multiple files into a single table, use a single COPY command</strong></p>\n\n<p>Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale. Amazon Redshift is an MPP (massively parallel processing) database, where all the compute nodes divide and parallelize the work of ingesting data. Each node is further subdivided into slices, with each slice having one or more dedicated cores, equally dividing the processing capacity. When you load data into Amazon Redshift, you should aim to have each slice do an equal amount of work. When splitting your data files, ensure that they are of approximately equal size – between 1 MB and 1 GB after compression. The number of files should be a multiple of the number of slices in your cluster.</p>\n\n<p>When loading multiple files into a single table, use a single COPY command for the table, rather than multiple COPY commands. Amazon Redshift automatically parallelizes the data ingestion. Using a single COPY command to bulk load data into a table ensures optimal use of cluster resources and the quickest possible throughput.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q24-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q24-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/\">https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/</a><p></p>\n\n<p><strong>Leverage temporary staging tables during the data loading process</strong></p>\n\n<p>When you are doing bulk ETL for Redshift, AWS recommends that you use temporary staging tables to hold the data for transformation. These tables are automatically dropped after the ETL session is complete. This allows efficient and fast transfer of these bulk datasets into Amazon Redshift.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When loading multiple files into a single table, use a single S3DistCp command</strong> - Using S3DistCp, you can efficiently copy large amounts of data from Amazon S3 into HDFS where it can be processed by subsequent steps in your Amazon EMR cluster. S3DistCp cannot be used with a Redshift cluster, so this option is incorrect.</p>\n\n<p><strong>When loading multiple files into a single table, use multiple COPY commands</strong> - As mentioned in the explanation above, when loading multiple files into a single table, use a single COPY command for the table.</p>\n\n<p><strong>Use Amazon Redshift Spectrum to upload data from multiple files in Amazon S3 into a single Amazon Redshift table</strong> - Amazon Redshift Spectrum enables you to run Amazon Redshift SQL queries on data that is stored in Amazon S3. With Redshift Spectrum, data is not uploaded into Redshift tables. Instead, the native Amazon Redshift cluster makes the invocation to Amazon Redshift Spectrum when the SQL query requests data from an external table stored in Amazon S3. You can handle multiple requests in parallel by using Amazon Redshift Spectrum on external tables to scan, filter, aggregate, and return rows from Amazon S3 into the Amazon Redshift cluster. All these operations are performed outside of Amazon Redshift, which reduces the computational load on the Amazon Redshift cluster and improves concurrency.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/\">https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/\">https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>When loading multiple files into a single table, use a single COPY command</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale. Amazon Redshift is an MPP (massively parallel processing) database, where all the compute nodes divide and parallelize the work of ingesting data. Each node is further subdivided into slices, with each slice having one or more dedicated cores, equally dividing the processing capacity. When you load data into Amazon Redshift, you should aim to have each slice do an equal amount of work. When splitting your data files, ensure that they are of approximately equal size – between 1 MB and 1 GB after compression. The number of files should be a multiple of the number of slices in your cluster."
      },
      {
        "answer": "",
        "explanation": "When loading multiple files into a single table, use a single COPY command for the table, rather than multiple COPY commands. Amazon Redshift automatically parallelizes the data ingestion. Using a single COPY command to bulk load data into a table ensures optimal use of cluster resources and the quickest possible throughput."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/"
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage temporary staging tables during the data loading process</strong>"
      },
      {
        "answer": "",
        "explanation": "When you are doing bulk ETL for Redshift, AWS recommends that you use temporary staging tables to hold the data for transformation. These tables are automatically dropped after the ETL session is complete. This allows efficient and fast transfer of these bulk datasets into Amazon Redshift."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>When loading multiple files into a single table, use a single S3DistCp command</strong> - Using S3DistCp, you can efficiently copy large amounts of data from Amazon S3 into HDFS where it can be processed by subsequent steps in your Amazon EMR cluster. S3DistCp cannot be used with a Redshift cluster, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>When loading multiple files into a single table, use multiple COPY commands</strong> - As mentioned in the explanation above, when loading multiple files into a single table, use a single COPY command for the table."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Redshift Spectrum to upload data from multiple files in Amazon S3 into a single Amazon Redshift table</strong> - Amazon Redshift Spectrum enables you to run Amazon Redshift SQL queries on data that is stored in Amazon S3. With Redshift Spectrum, data is not uploaded into Redshift tables. Instead, the native Amazon Redshift cluster makes the invocation to Amazon Redshift Spectrum when the SQL query requests data from an external table stored in Amazon S3. You can handle multiple requests in parallel by using Amazon Redshift Spectrum on external tables to scan, filter, aggregate, and return rows from Amazon S3 into the Amazon Redshift cluster. All these operations are performed outside of Amazon Redshift, which reduces the computational load on the Amazon Redshift cluster and improves concurrency."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/",
      "https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/"
    ]
  },
  {
    "id": 60,
    "question": "<p>An IT company has recently migrated to AWS and the data engineering team is configuring security groups for the two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group.</p>\n\n<p>Which of the following would you identify as an INVALID option for setting up such a configuration?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>You can use an Internet Gateway ID as the custom source for the inbound rule</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>You can use a security group as the custom source for the inbound rule</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>You can use an IP address as the custom source for the inbound rule</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>You can use an Internet Gateway ID as the custom source for the inbound rule</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, you can use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group.</p>\n\n<p>List of allowed source or destination for security group rules:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q55-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q55-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a><p></p>\n\n<p>Therefore, you cannot use an Internet Gateway ID as the custom source for the inbound rule.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can use a security group as the custom source for the inbound rule</strong></p>\n\n<p><strong>You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule</strong></p>\n\n<p><strong>You can use an IP address as the custom source for the inbound rule</strong></p>\n\n<p>As described in the list of allowed sources or destinations for security group rules, the above options are supported.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You can use an Internet Gateway ID as the custom source for the inbound rule</strong>"
      },
      {
        "answer": "",
        "explanation": "A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, you can use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q55-i1.jpg",
        "answer": "",
        "explanation": "List of allowed source or destination for security group rules:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html"
      },
      {
        "answer": "",
        "explanation": "Therefore, you cannot use an Internet Gateway ID as the custom source for the inbound rule."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You can use a security group as the custom source for the inbound rule</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>You can use an IP address as the custom source for the inbound rule</strong>"
      },
      {
        "answer": "",
        "explanation": "As described in the list of allowed sources or destinations for security group rules, the above options are supported."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html"
    ]
  },
  {
    "id": 61,
    "question": "<p>A company runs an analytics workload with heavy reads and writes through the workload lifecycle. The data analytics team at the company is interested in using Amazon S3 as the data lake to support this workload. The team has hired you to advise them on the S3 data consistency model.</p>\n\n<p>Which of the following statements would you identify as correct?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon S3 is strongly consistent for all GET and PUT operations and eventually consistent for LIST operations</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon S3 is strongly consistent for all GET operations and eventually consistent for PUT and LIST operations</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon S3 is strongly consistent for all GET, PUT and LIST operations and eventually consistent for operations that need metadata information</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon S3 is strongly consistent for all GET, PUT and LIST operations</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon S3 is strongly consistent for all GET, PUT and LIST operations</strong> - After a successful write of a new object, or an overwrite or delete of an existing object, any subsequent read request immediately receives the latest version of the object. S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected.</p>\n\n<p>For all existing and new objects, and in all regions, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are now strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket.</p>\n\n<p>This improvement is great for data lakes, but other types of applications will also benefit. Because S3 now has strong consistency, migration of on-premises workloads and storage to AWS should now be easier than ever before.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 is strongly consistent for all GET operations and eventually consistent for PUT and LIST operations</strong></p>\n\n<p><strong>Amazon S3 is strongly consistent for all GET and PUT operations and eventually consistent for LIST operations</strong></p>\n\n<p>As mentioned in the explanation above, Amazon S3 is strongly consistent for all GET, PUT and LIST operations, so these two options are incorrect</p>\n\n<p><strong>Amazon S3 is strongly consistent for all GET, PUT and LIST operations and eventually consistent for operations that need metadata information</strong> - For all existing and new objects, and in all regions, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are now strongly consistent. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/consistency/\">https://aws.amazon.com/s3/consistency/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/\">https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 is strongly consistent for all GET, PUT and LIST operations</strong> - After a successful write of a new object, or an overwrite or delete of an existing object, any subsequent read request immediately receives the latest version of the object. S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected."
      },
      {
        "answer": "",
        "explanation": "For all existing and new objects, and in all regions, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are now strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket."
      },
      {
        "answer": "",
        "explanation": "This improvement is great for data lakes, but other types of applications will also benefit. Because S3 now has strong consistency, migration of on-premises workloads and storage to AWS should now be easier than ever before."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 is strongly consistent for all GET operations and eventually consistent for PUT and LIST operations</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 is strongly consistent for all GET and PUT operations and eventually consistent for LIST operations</strong>"
      },
      {
        "answer": "",
        "explanation": "As mentioned in the explanation above, Amazon S3 is strongly consistent for all GET, PUT and LIST operations, so these two options are incorrect"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 is strongly consistent for all GET, PUT and LIST operations and eventually consistent for operations that need metadata information</strong> - For all existing and new objects, and in all regions, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are now strongly consistent. So this option is not correct."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/consistency/",
      "https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/"
    ]
  },
  {
    "id": 62,
    "question": "<p>A data analyst is setting up Amazon QuickSight to create dashboards for the senior management. The data warehousing service is handled by the Amazon Redshift clusters, hosted in a public subnet of a VPC. Currently, data is fetched using SQL tools. When trying to launch QuickSight for the first time, QuickSight is failing with an error indicating that the connection to the data source is failing.</p>\n\n<p>What configuration changes are needed to connect QuickSight to the Redshift clusters?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a QuickSight admin user for creating the dataset</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add the QuickSight IP address range of the AWS Region to Amazon Redshift security group</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create an IAM role for QuickSight to access Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Grant the SELECT permissions on Amazon Redshift system tables</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Add the QuickSight IP address range of the AWS Region to the Amazon Redshift security group</strong></p>\n\n<p>For Amazon QuickSight to connect to an Amazon Redshift instance, you must create a new security group for that instance. This security group contains an inbound rule authorizing access from the appropriate IP address range for the Amazon QuickSight servers in that AWS Region. To create and assign a security group for an Amazon Redshift cluster, you must have AWS credentials that permit access to that cluster.</p>\n\n<p>If you activate Amazon QuickSight in multiple AWS Regions, you can create inbound rules for each Amazon QuickSight endpoint CIDR. Doing this allows Amazon QuickSight to have access to the Amazon RDS DB instance from any AWS Region defined in the inbound rules.</p>\n\n<p>An Amazon QuickSight user or administrator who uses Amazon QuickSight in multiple AWS Regions is treated as a single user. In other words, even if you are using Amazon QuickSight in every AWS Region, both your Amazon QuickSight account and your users are global.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Grant the SELECT permissions on Amazon Redshift system tables</strong> - When you connect to a data source that requires a user name, the user name must have SELECT permissions on some system tables. These permissions allow Amazon QuickSight to do things such as discover table schemas and estimate table size. So this option is not relevant to the given use case.</p>\n\n<p><strong>Use a QuickSight admin user for creating the dataset</strong> - This statement is incorrect and just used as a distractor.</p>\n\n<p><strong>Create an IAM role for QuickSight to access Amazon Redshift</strong> - In case of an incorrect IAM role being assigned to QuickSight, you would see an UnauthorizedOperation error or AccessDenied error. The given use case mentions an error where the connection has timed out, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\">https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Add the QuickSight IP address range of the AWS Region to the Amazon Redshift security group</strong>"
      },
      {
        "answer": "",
        "explanation": "For Amazon QuickSight to connect to an Amazon Redshift instance, you must create a new security group for that instance. This security group contains an inbound rule authorizing access from the appropriate IP address range for the Amazon QuickSight servers in that AWS Region. To create and assign a security group for an Amazon Redshift cluster, you must have AWS credentials that permit access to that cluster."
      },
      {
        "answer": "",
        "explanation": "If you activate Amazon QuickSight in multiple AWS Regions, you can create inbound rules for each Amazon QuickSight endpoint CIDR. Doing this allows Amazon QuickSight to have access to the Amazon RDS DB instance from any AWS Region defined in the inbound rules."
      },
      {
        "answer": "",
        "explanation": "An Amazon QuickSight user or administrator who uses Amazon QuickSight in multiple AWS Regions is treated as a single user. In other words, even if you are using Amazon QuickSight in every AWS Region, both your Amazon QuickSight account and your users are global."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Grant the SELECT permissions on Amazon Redshift system tables</strong> - When you connect to a data source that requires a user name, the user name must have SELECT permissions on some system tables. These permissions allow Amazon QuickSight to do things such as discover table schemas and estimate table size. So this option is not relevant to the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use a QuickSight admin user for creating the dataset</strong> - This statement is incorrect and just used as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an IAM role for QuickSight to access Amazon Redshift</strong> - In case of an incorrect IAM role being assigned to QuickSight, you would see an UnauthorizedOperation error or AccessDenied error. The given use case mentions an error where the connection has timed out, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html"
    ]
  },
  {
    "id": 63,
    "question": "<p>A data engineer needs to ascertain the outcome of S3 Lifecycle configurations on an Amazon S3 bucket. The data engineer realized that multiple rules have been defined on the same S3 bucket and an object can become eligible for multiple S3 Lifecycle actions.</p>\n\n<p>What is the order of preference that Amazon S3 uses for objects that fall under multiple S3 Lifecycle rules? (Select two)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>S3 objects that fall under multiple lifecycle rules are not automatically transitioned to any state</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>When an object is eligible for both an S3 Glacier Flexible Retrieval and S3 Standard-IA transition, Amazon S3 chooses the S3 Standard-IA transition</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>When an object is eligible for both an S3 Glacier Flexible Retrieval and S3 One Zone-IA transition, Amazon S3 chooses the S3 One Zone-IA transition</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Transition takes precedence over creation of delete markers</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Permanent deletion takes precedence over transition</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Permanent deletion takes precedence over transition</strong></p>\n\n<p><strong>Transition takes precedence over creation of delete markers</strong></p>\n\n<p>When you have multiple rules in an S3 Lifecycle configuration, an object can become eligible for multiple S3 Lifecycle actions. In such cases, Amazon S3 follows these general rules:</p>\n\n<ol>\n<li><p>Permanent deletion takes precedence over transition.</p></li>\n<li><p>Transition takes precedence over the creation of delete markers.</p></li>\n<li><p>When an object is eligible for both an S3 Glacier Flexible Retrieval and S3 Standard-IA (or S3 One Zone-IA) transition, Amazon S3 chooses the S3 Glacier Flexible Retrieval transition.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3 objects that fall under multiple lifecycle rules are not automatically transitioned to any state</strong></p>\n\n<p><strong>When an object is eligible for both an S3 Glacier Flexible Retrieval and S3 Standard-IA transition, Amazon S3 chooses the S3 Standard-IA transition</strong></p>\n\n<p><strong>When an object is eligible for both an S3 Glacier Flexible Retrieval and S3 One Zone-IA transition, Amazon S3 chooses the S3 One Zone-IA transition</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Permanent deletion takes precedence over transition</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Transition takes precedence over creation of delete markers</strong>"
      },
      {
        "answer": "",
        "explanation": "When you have multiple rules in an S3 Lifecycle configuration, an object can become eligible for multiple S3 Lifecycle actions. In such cases, Amazon S3 follows these general rules:"
      },
      {}
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>S3 objects that fall under multiple lifecycle rules are not automatically transitioned to any state</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>When an object is eligible for both an S3 Glacier Flexible Retrieval and S3 Standard-IA transition, Amazon S3 chooses the S3 Standard-IA transition</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>When an object is eligible for both an S3 Glacier Flexible Retrieval and S3 One Zone-IA transition, Amazon S3 chooses the S3 One Zone-IA transition</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html"
    ]
  },
  {
    "id": 64,
    "question": "<p>A multi-national retail company wants to migrate its on-premises Oracle database to Aurora MySQL. The company has hired you as an AWS Certified Data Engineer Associate to carry out the migration with minimal downtime using AWS DMS. The company has mandated that the migration must have minimal impact on the performance of the source database and you must validate that the data was migrated accurately from the source to the target before the cutover.</p>\n\n<p>Which of the following solutions will MOST effectively address this use-case?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure DMS premigration assessment on the migration task so the assessment can compare the source and target data and report any mismatches</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Schema Conversion Tool for the migration task so it can compare the source and target data and report any mismatches</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the table metrics of the DMS task to verify the statistics for tables being migrated including the DDL statements completed</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure DMS data validation on the migration task so it can compare the source and target data for the DMS task and report any mismatches</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure DMS data validation on the migration task so it can compare the source and target data for the DMS task and report any mismatches</strong></p>\n\n<p>You can use AWS DMS data validation to ensure that your data has migrated accurately from the source to the target. DMS compares the source and target records and then reports any mismatches. In addition, for a CDC-enabled task, AWS DMS compares the incremental changes and reports any mismatches. As part of data validation, DMS compares each row in the source with its corresponding row at the target and verifies that those rows contain the same data. For this comparison, DMS issues appropriate queries to retrieve the data. These queries consume additional resources at the source and the target as well as additional network resources.</p>\n\n<p>DMS data validation overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q34-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q34-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the table metrics of the DMS task to verify the statistics for tables being migrated including the DDL statements completed</strong> - You can use table metrics to capture statistics such as insert, update, delete, and DDL statements completed for the tables being migrated. This option will not help you compare the source and target data for the DMS task and report any mismatches.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q34-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q34-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics</a><p></p>\n\n<p><strong>Configure DMS premigration assessment on the migration task so the assessment can compare the source and target data and report any mismatches</strong> - A premigration assessment evaluates specified components of a database migration task to help identify any problems that might prevent a migration task from running as expected. This assessment gives you a chance to identify issues before you run a new or modified task. You can then fix problems before they occur while running the migration task itself. This can avoid delays in completing a given database migration needed to repair data and your database environment. This option will not help you compare the source and target data for the DMS task and report any mismatches.</p>\n\n<p><strong>Use AWS Schema Conversion Tool for the migration task so it can compare the source and target data and report any mismatches</strong> - You can use the AWS Schema Conversion Tool (AWS SCT) to convert your existing database schema from one database engine to another. You can convert relational OLTP schema or data warehouse schema. Your converted schema is suitable for an Amazon Relational Database Service (Amazon RDS) MySQL, MariaDB, Oracle, SQL Server, PostgreSQL DB, an Amazon Aurora DB cluster, or an Amazon Redshift cluster. This option will not help you compare the source and target data for the DMS task and report any mismatches.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure DMS data validation on the migration task so it can compare the source and target data for the DMS task and report any mismatches</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use AWS DMS data validation to ensure that your data has migrated accurately from the source to the target. DMS compares the source and target records and then reports any mismatches. In addition, for a CDC-enabled task, AWS DMS compares the incremental changes and reports any mismatches. As part of data validation, DMS compares each row in the source with its corresponding row at the target and verifies that those rows contain the same data. For this comparison, DMS issues appropriate queries to retrieve the data. These queries consume additional resources at the source and the target as well as additional network resources."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q34-i1.jpg",
        "answer": "",
        "explanation": "DMS data validation overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the table metrics of the DMS task to verify the statistics for tables being migrated including the DDL statements completed</strong> - You can use table metrics to capture statistics such as insert, update, delete, and DDL statements completed for the tables being migrated. This option will not help you compare the source and target data for the DMS task and report any mismatches."
      },
      {
        "link": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics"
      },
      {
        "answer": "",
        "explanation": "<strong>Configure DMS premigration assessment on the migration task so the assessment can compare the source and target data and report any mismatches</strong> - A premigration assessment evaluates specified components of a database migration task to help identify any problems that might prevent a migration task from running as expected. This assessment gives you a chance to identify issues before you run a new or modified task. You can then fix problems before they occur while running the migration task itself. This can avoid delays in completing a given database migration needed to repair data and your database environment. This option will not help you compare the source and target data for the DMS task and report any mismatches."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Schema Conversion Tool for the migration task so it can compare the source and target data and report any mismatches</strong> - You can use the AWS Schema Conversion Tool (AWS SCT) to convert your existing database schema from one database engine to another. You can convert relational OLTP schema or data warehouse schema. Your converted schema is suitable for an Amazon Relational Database Service (Amazon RDS) MySQL, MariaDB, Oracle, SQL Server, PostgreSQL DB, an Amazon Aurora DB cluster, or an Amazon Redshift cluster. This option will not help you compare the source and target data for the DMS task and report any mismatches."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html",
      "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics"
    ]
  },
  {
    "id": 65,
    "question": "<p>A company wants to optimize its daily Extract-Transform-Load (ETL) process that migrates and transforms data from its Amazon S3 based data lake to an Amazon Redshift cluster. The data engineering team wants to manage this daily job in a serverless environment.</p>\n\n<p>Which AWS service is the best fit to manage this process without the need to configure or manage the underlying compute resources?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Database Migration Service (DMS)</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Glue</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS Data Pipeline</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon EMR</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Glue</strong></p>\n\n<p>AWS Glue provides a managed ETL service that runs on a serverless Apache Spark environment. This allows you to focus on your ETL job and not worry about configuring and managing the underlying compute resources. AWS Glue takes a data-first approach and allows you to focus on the data properties and data manipulation to transform the data to a form where you can derive business insights. It provides an integrated data catalog that makes metadata available for ETL as well as querying via Amazon Athena and Amazon Redshift Spectrum.</p>\n\n<p>Create a unified catalog to find data across multiple data stores using AWS Glue:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q63-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q63-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a><p></p>\n\n<p>AWS Glue automates much of the effort required for data integration. AWS Glue crawls your data sources, identifies data formats, and suggests schemas to store your data. It automatically generates the code to run your data transformations and loading processes. You can use AWS Glue to easily run and manage thousands of ETL jobs or to combine and replicate data across multiple data stores using SQL.</p>\n\n<p>AWS Glue runs in a serverless environment. There is no infrastructure to manage, and AWS Glue provisions, configures, and scales the resources required to run your data integration jobs. You pay only for the resources your jobs use while running.</p>\n\n<p>AWS Glue is the right fit since the company is looking at a managed ETL service without having the overhead of configuring, maintaining, or managing any servers.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Data Pipeline</strong> - AWS Data Pipeline provides a managed orchestration service that gives you greater flexibility in terms of the execution environment, access and control over the compute resources that run your code, as well as the code itself that does data processing. AWS Data Pipeline launches compute resources in your account allowing you direct access to the Amazon EC2 instances or Amazon EMR clusters. As this option provides access to the underlying EC2 instances, so it's not a serverless solution. Therefore this option is incorrect for the given use case.</p>\n\n<p><strong>Amazon EMR</strong> - EMR is a web service to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). As this option provides access to the underlying Amazon EC2 instances, so it's not a serverless solution. Therefore this option is incorrect for the given use case.</p>\n\n<p><strong>AWS Database Migration Service (DMS)</strong> - AWS Database Migration Service (DMS) helps you migrate databases to AWS easily and securely. For use cases that require a database migration from on-premises to AWS or database replication between on-premises sources and sources on AWS, AWS recommends you use AWS DMS. Once your data is in AWS, you can use AWS Glue to move, combine, replicate, and transform data from your data source into another database or data warehouse, such as Amazon Redshift. As the use-case talks about data migration and transformation between AWS services, AWS Glue is a better fit than DMS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/glue/faqs/\">https://aws.amazon.com/glue/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Glue</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Glue provides a managed ETL service that runs on a serverless Apache Spark environment. This allows you to focus on your ETL job and not worry about configuring and managing the underlying compute resources. AWS Glue takes a data-first approach and allows you to focus on the data properties and data manipulation to transform the data to a form where you can derive business insights. It provides an integrated data catalog that makes metadata available for ETL as well as querying via Amazon Athena and Amazon Redshift Spectrum."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt4-q63-i1.jpg",
        "answer": "",
        "explanation": "Create a unified catalog to find data across multiple data stores using AWS Glue:"
      },
      {
        "link": "https://aws.amazon.com/glue/"
      },
      {
        "answer": "",
        "explanation": "AWS Glue automates much of the effort required for data integration. AWS Glue crawls your data sources, identifies data formats, and suggests schemas to store your data. It automatically generates the code to run your data transformations and loading processes. You can use AWS Glue to easily run and manage thousands of ETL jobs or to combine and replicate data across multiple data stores using SQL."
      },
      {
        "answer": "",
        "explanation": "AWS Glue runs in a serverless environment. There is no infrastructure to manage, and AWS Glue provisions, configures, and scales the resources required to run your data integration jobs. You pay only for the resources your jobs use while running."
      },
      {
        "answer": "",
        "explanation": "AWS Glue is the right fit since the company is looking at a managed ETL service without having the overhead of configuring, maintaining, or managing any servers."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Data Pipeline</strong> - AWS Data Pipeline provides a managed orchestration service that gives you greater flexibility in terms of the execution environment, access and control over the compute resources that run your code, as well as the code itself that does data processing. AWS Data Pipeline launches compute resources in your account allowing you direct access to the Amazon EC2 instances or Amazon EMR clusters. As this option provides access to the underlying EC2 instances, so it's not a serverless solution. Therefore this option is incorrect for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EMR</strong> - EMR is a web service to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). As this option provides access to the underlying Amazon EC2 instances, so it's not a serverless solution. Therefore this option is incorrect for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Database Migration Service (DMS)</strong> - AWS Database Migration Service (DMS) helps you migrate databases to AWS easily and securely. For use cases that require a database migration from on-premises to AWS or database replication between on-premises sources and sources on AWS, AWS recommends you use AWS DMS. Once your data is in AWS, you can use AWS Glue to move, combine, replicate, and transform data from your data source into another database or data warehouse, such as Amazon Redshift. As the use-case talks about data migration and transformation between AWS services, AWS Glue is a better fit than DMS."
      }
    ],
    "references": [
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/glue/faqs/",
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html"
    ]
  }
]