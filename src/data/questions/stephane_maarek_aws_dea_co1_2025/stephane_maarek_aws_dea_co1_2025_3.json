[
  {
    "id": 1,
    "question": "<p>A logistics company has multiple AWS accounts hosting its portfolio of IT applications that serve the company's retail and enterprise customers. A CloudWatch Logs agent is installed on each of the EC2 instances running these IT applications. The company wants to aggregate all security events in a centralized AWS account dedicated to log storage. The centralized operations team at the company needs to perform near-real-time gathering and collating events across multiple AWS accounts.</p>\n\n<p>Which of the following solutions would you recommend to address these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up Kinesis Data Firehose in the logging account and then subscribe the delivery stream to CloudWatch Logs streams in each application AWS account via subscription filters. Persist the log data in an Amazon S3 bucket inside the logging AWS account</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up CloudWatch Logs agents to publish data to a Kinesis Data Firehose stream in the centralized logging AWS account. Create a Lambda function to read messages from the stream and push messages to Kinesis Data Firehose and then store the data in S3</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up CloudWatch Logs streams in each application AWS account to forward events to CloudWatch Logs in the centralized logging AWS account. In the centralized logging AWS account, subscribe a Kinesis Data Firehose stream to Amazon CloudWatch Events and further use the Firehose stream to store the log data in S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a new IAM role in each application AWS account with permissions to view CloudWatch Logs. Create a Lambda function to assume this new role and perform an hourly export of each AWS account's CloudWatch Logs data to an S3 bucket in the centralized logging AWS account</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up Kinesis Data Firehose in the logging account and then subscribe the delivery stream to CloudWatch Logs streams in each application AWS account via subscription filters. Persist the log data in an Amazon S3 bucket inside the logging AWS account</strong></p>\n\n<p>You can configure Amazon Kinesis Data Firehose to aggregate and collate CloudWatch Logs from different AWS accounts and receive their log events in a centralized logging AWS Account (this is known as cross-account data sharing) by using a CloudWatch Logs destination and then creating a Subscription Filter. This log event data can be read from a centralized Amazon Kinesis Firehose delivery stream to perform downstream processing and analysis.</p>\n\n<p>You can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis or Amazon Kinesis Data Firehose stream (this is known as cross-account data sharing). You can use a subscription filter with Kinesis Streams, Lambda, or Kinesis Data Firehose. Logs that are sent to a receiving service through a subscription filter are Base64 encoded and compressed with the gzip format.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a new IAM role in each application AWS account with permissions to view CloudWatch Logs. Create a Lambda function to assume this new role and perform an hourly export of each AWS account's CloudWatch Logs data to an S3 bucket in the centralized logging AWS account</strong> - As the Lambda function is performing an hourly export, it's not a near-real-time solution. In addition, Lambda is not the right choice to build a high volume and high-velocity streaming solution which is better handled by using the Kinesis Family of services.</p>\n\n<p><strong>Set up CloudWatch Logs agents to publish data to a Kinesis Data Firehose stream in the centralized logging AWS account. Create a Lambda function to read messages from the stream and push messages to Kinesis Data Firehose and then store the data in S3</strong> - The CloudWatch Logs agent (on the path to deprecation) supports the collection of logs from only servers running Linux. It is recommended to use the unified CloudWatch agent. It enables you to collect both logs and advanced metrics with one agent. It offers support across operating systems, including servers running Windows Server. This agent also provides better performance. CloudWatch Logs agent cannot publish data to a Kinesis Data Firehose stream, so this option is incorrect.</p>\n\n<p><strong>Set up CloudWatch Logs streams in each application AWS account to forward events to CloudWatch Logs in the centralized logging AWS account. In the centralized logging AWS account, subscribe a Kinesis Data Firehose stream to Amazon CloudWatch Events and further use the Firehose stream to store the log data in S3</strong> - You can use a subscription filter with Kinesis Streams, Lambda, or Kinesis Data Firehose. So you cannot just forward events directly to CloudWatch Logs in another account. In addition, the Kinesis Data Firehose stream cannot subscribe to CloudWatch Events, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/\">https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up Kinesis Data Firehose in the logging account and then subscribe the delivery stream to CloudWatch Logs streams in each application AWS account via subscription filters. Persist the log data in an Amazon S3 bucket inside the logging AWS account</strong>"
      },
      {
        "answer": "",
        "explanation": "You can configure Amazon Kinesis Data Firehose to aggregate and collate CloudWatch Logs from different AWS accounts and receive their log events in a centralized logging AWS Account (this is known as cross-account data sharing) by using a CloudWatch Logs destination and then creating a Subscription Filter. This log event data can be read from a centralized Amazon Kinesis Firehose delivery stream to perform downstream processing and analysis."
      },
      {
        "answer": "",
        "explanation": "You can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis or Amazon Kinesis Data Firehose stream (this is known as cross-account data sharing). You can use a subscription filter with Kinesis Streams, Lambda, or Kinesis Data Firehose. Logs that are sent to a receiving service through a subscription filter are Base64 encoded and compressed with the gzip format."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up a new IAM role in each application AWS account with permissions to view CloudWatch Logs. Create a Lambda function to assume this new role and perform an hourly export of each AWS account's CloudWatch Logs data to an S3 bucket in the centralized logging AWS account</strong> - As the Lambda function is performing an hourly export, it's not a near-real-time solution. In addition, Lambda is not the right choice to build a high volume and high-velocity streaming solution which is better handled by using the Kinesis Family of services."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up CloudWatch Logs agents to publish data to a Kinesis Data Firehose stream in the centralized logging AWS account. Create a Lambda function to read messages from the stream and push messages to Kinesis Data Firehose and then store the data in S3</strong> - The CloudWatch Logs agent (on the path to deprecation) supports the collection of logs from only servers running Linux. It is recommended to use the unified CloudWatch agent. It enables you to collect both logs and advanced metrics with one agent. It offers support across operating systems, including servers running Windows Server. This agent also provides better performance. CloudWatch Logs agent cannot publish data to a Kinesis Data Firehose stream, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up CloudWatch Logs streams in each application AWS account to forward events to CloudWatch Logs in the centralized logging AWS account. In the centralized logging AWS account, subscribe a Kinesis Data Firehose stream to Amazon CloudWatch Events and further use the Firehose stream to store the log data in S3</strong> - You can use a subscription filter with Kinesis Streams, Lambda, or Kinesis Data Firehose. So you cannot just forward events directly to CloudWatch Logs in another account. In addition, the Kinesis Data Firehose stream cannot subscribe to CloudWatch Events, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html",
      "https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/"
    ]
  },
  {
    "id": 2,
    "question": "<p>An Internet-of-Things (IoT) devices company uses Amazon S3 as the data lake to store the input data that is ingested from the field devices on an hourly basis. The ingested data has attributes such as the device type, ID of the device, the status of the device, the timestamp of the event, the source IP address, etc. The data runs into millions of records per day and the company wants to run complex analytical queries on this data on a daily basis for product improvements for each device type.</p>\n\n<p>Which is the most optimal way to save this data to get the best performance from the millions of data points saved on a daily basis?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the data in compressed .csv, partitioned by date and sorted by device type</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store the data in compressed .csv, partitioned by date and sorted by the status of the device</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Store the data in Apache Parquet, partitioned by device type and sorted by date</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the data in Apache ORC, partitioned by date and sorted by device type of the device</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the data in Apache ORC, partitioned by date and sorted by device type of the device</strong> - Apache Parquet and ORC are columnar storage formats that are optimized for fast retrieval of data and used in AWS analytical applications.</p>\n\n<p>By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but that is loaded only once per day, might partition by a data source identifier and date.</p>\n\n<p>For the given use case, as the company does daily analysis, so it only needs to look at the data generated for a given date. Hence partitioning by date offers significant performance and cost advantages. Since the company also wants to do analysis for product improvements for each device type, it is better to keep the data sorted by device type, so it allows faster query execution.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the data in Apache Parquet, partitioned by device type and sorted by date</strong> -  Apache Parquet is a columnar storage format that is optimized for fast retrieval of data and used in AWS analytical applications. However, partitioning by device type is incorrect for this use case, and partitioning by date is optimal.</p>\n\n<p><strong>Store the data in compressed .csv, partitioned by date and sorted by the status of the device</strong></p>\n\n<p><strong>Store the data in compressed .csv, partitioned by date and sorted by device type</strong></p>\n\n<p>Both the above options are not columnar storage formats, they are row-based formats that are not optimal for big data retrievals for complex analytical queries.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the data in Apache ORC, partitioned by date and sorted by device type of the device</strong> - Apache Parquet and ORC are columnar storage formats that are optimized for fast retrieval of data and used in AWS analytical applications."
      },
      {
        "answer": "",
        "explanation": "By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but that is loaded only once per day, might partition by a data source identifier and date."
      },
      {
        "answer": "",
        "explanation": "For the given use case, as the company does daily analysis, so it only needs to look at the data generated for a given date. Hence partitioning by date offers significant performance and cost advantages. Since the company also wants to do analysis for product improvements for each device type, it is better to keep the data sorted by device type, so it allows faster query execution."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the data in Apache Parquet, partitioned by device type and sorted by date</strong> -  Apache Parquet is a columnar storage format that is optimized for fast retrieval of data and used in AWS analytical applications. However, partitioning by device type is incorrect for this use case, and partitioning by date is optimal."
      },
      {
        "answer": "",
        "explanation": "<strong>Store the data in compressed .csv, partitioned by date and sorted by the status of the device</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Store the data in compressed .csv, partitioned by date and sorted by device type</strong>"
      },
      {
        "answer": "",
        "explanation": "Both the above options are not columnar storage formats, they are row-based formats that are not optimal for big data retrievals for complex analytical queries."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/partitions.html"
    ]
  },
  {
    "id": 3,
    "question": "<p>A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency.</p>\n\n<p>Which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon DynamoDB</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS Lambda</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon ElastiCache</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Amazon RDS</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>AWS Lambda</strong></p>\n\n<p>With AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there’s no charge when your code isn’t running. You can run code for virtually any type of application or backend service—all with zero administration.</p>\n\n<p><strong>Amazon DynamoDB</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. Amazon DynamoDB is a NoSQL database and it's best suited to store data in key-value pairs.</p>\n\n<p>AWS Lambda can be combined with DynamoDB to process and capture the key-value data from the IoT sources described in the use case. So both these options are correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. You cannot use Redshift to capture data in key-value pairs from the IoT sources, so this option is not correct.</p>\n\n<p><strong>Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Elasticache is used as a caching layer in front of relational databases. It is not a good fit to store data in key-value pairs from the IoT sources, so this option is not correct.</p>\n\n<p><strong>Amazon RDS</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Relational databases are not a good fit to store data in key-value pairs, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n\n<p><a href=\"https://aws.amazon.com/lambda/faqs/\">https://aws.amazon.com/lambda/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Lambda</strong>"
      },
      {
        "answer": "",
        "explanation": "With AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there’s no charge when your code isn’t running. You can run code for virtually any type of application or backend service—all with zero administration."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon DynamoDB</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. Amazon DynamoDB is a NoSQL database and it's best suited to store data in key-value pairs."
      },
      {
        "answer": "",
        "explanation": "AWS Lambda can be combined with DynamoDB to process and capture the key-value data from the IoT sources described in the use case. So both these options are correct."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. You cannot use Redshift to capture data in key-value pairs from the IoT sources, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Elasticache is used as a caching layer in front of relational databases. It is not a good fit to store data in key-value pairs from the IoT sources, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon RDS</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Relational databases are not a good fit to store data in key-value pairs, so this option is not correct."
      }
    ],
    "references": [
      "https://aws.amazon.com/dynamodb/",
      "https://aws.amazon.com/lambda/faqs/"
    ]
  },
  {
    "id": 4,
    "question": "<p>An e-commerce company stores a copy of its order details in an Amazon S3 bucket (Orders bucket). The company wants to log all writes to the Orders bucket into another Amazon S3 bucket (Audit bucket) that is in the same AWS Region.</p>\n\n<p>Which solution will address this requirement with the LEAST operational effort?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a trail to log data events using the AWS CloudTrail console. Configure the trail to receive data events from the Orders bucket by specifying the prefix as <code>All-objects</code> and the option to log <code>Write</code> data events. Configure the Audit bucket as the destination bucket for the trail</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure Amazon S3 Event notification to trigger Amazon EventBridge. The EventBridge event will trigger an AWS Lambda function to copy the Orders bucket object metadata to the Audit bucket. The EventBridge event will then log the event into AWS CloudTrail logs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure Amazon S3 Event notification to trigger an AWS Lambda function on the <code>New object created</code> event for all the objects in the Orders bucket. The Lambda function will write the data event to the Audit bucket. The IAM role assigned to the Lambda function should have full access privileges on both the S3 buckets</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a trail to log data events using the AWS CloudTrail console. Configure the trail to receive data events from the Orders bucket by specifying an empty prefix and the option to log <code>Write</code> data events. Configure the Audit bucket as the destination bucket for the trail</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a trail to log data events using the AWS CloudTrail console. Configure the trail to receive data events from the Orders bucket by specifying an empty prefix and the option to log <code>Write</code> data events. Configure the Audit bucket as the destination bucket for the trail</strong></p>\n\n<p>An event in CloudTrail is the record of an activity in an AWS account. This activity can be an action taken by an IAM identity, or service that is monitorable by CloudTrail. By default, trails and event data stores log management events, but not data or Insights events. Additional charges apply for data events. Data events provide visibility into the resource operations performed on or within a resource. These are also known as data plane operations. Data events are often high-volume activities.</p>\n\n<p>Data event types available for Amazon S3 are Amazon S3 object-level API activity (for example, GetObject, DeleteObject, and PutObject API operations) on buckets and objects in buckets.</p>\n\n<p>If you have to log write events on all objects in an S3 bucket then you specify an empty prefix along with the option to log Write data events for the current requirement.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a trail to log data events using the AWS CloudTrail console. Configure the trail to receive data events from the Orders bucket by specifying the prefix as <code>All-objects</code> and the option to log <code>Write</code> data events. Configure the Audit bucket as the destination bucket for the trail</strong> - If you want the data events for all objects in the S3 bucket then the prefix should be left empty. If a prefix is defined, the CloudTrail will only look for objects per the defined prefix. Hence, this option is incorrect.</p>\n\n<p><strong>Configure Amazon S3 Event notification to trigger an AWS Lambda function on the <code>New object created</code> event for all the objects in the Orders bucket. The Lambda function will write the data event to the Audit bucket. The IAM role assigned to the Lambda function should have full access privileges on both the S3 buckets</strong> - Using the AWS Lambda function adds unnecessary complexity to the solution. You can simply leverage AWS CloudTrail integration with Amazon S3 as the most elegant solution to meet this requirement.</p>\n\n<p><strong>Configure Amazon S3 Event notification to trigger Amazon EventBridge. The EventBridge event will trigger an AWS Lambda function to copy the Orders bucket object metadata to the Audit bucket. The EventBridge event will then log the event into AWS CloudTrail logs</strong> - Using Amazon EventBridge adds unnecessary complexity to the solution. You can simply leverage AWS CloudTrail integration with Amazon S3 as the most elegant solution to meet this requirement.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html#logging-data-events-examples\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html#logging-data-events-examples</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a trail to log data events using the AWS CloudTrail console. Configure the trail to receive data events from the Orders bucket by specifying an empty prefix and the option to log <code>Write</code> data events. Configure the Audit bucket as the destination bucket for the trail</strong>"
      },
      {
        "answer": "",
        "explanation": "An event in CloudTrail is the record of an activity in an AWS account. This activity can be an action taken by an IAM identity, or service that is monitorable by CloudTrail. By default, trails and event data stores log management events, but not data or Insights events. Additional charges apply for data events. Data events provide visibility into the resource operations performed on or within a resource. These are also known as data plane operations. Data events are often high-volume activities."
      },
      {
        "answer": "",
        "explanation": "Data event types available for Amazon S3 are Amazon S3 object-level API activity (for example, GetObject, DeleteObject, and PutObject API operations) on buckets and objects in buckets."
      },
      {
        "answer": "",
        "explanation": "If you have to log write events on all objects in an S3 bucket then you specify an empty prefix along with the option to log Write data events for the current requirement."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a trail to log data events using the AWS CloudTrail console. Configure the trail to receive data events from the Orders bucket by specifying the prefix as <code>All-objects</code> and the option to log <code>Write</code> data events. Configure the Audit bucket as the destination bucket for the trail</strong> - If you want the data events for all objects in the S3 bucket then the prefix should be left empty. If a prefix is defined, the CloudTrail will only look for objects per the defined prefix. Hence, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon S3 Event notification to trigger an AWS Lambda function on the <code>New object created</code> event for all the objects in the Orders bucket. The Lambda function will write the data event to the Audit bucket. The IAM role assigned to the Lambda function should have full access privileges on both the S3 buckets</strong> - Using the AWS Lambda function adds unnecessary complexity to the solution. You can simply leverage AWS CloudTrail integration with Amazon S3 as the most elegant solution to meet this requirement."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon S3 Event notification to trigger Amazon EventBridge. The EventBridge event will trigger an AWS Lambda function to copy the Orders bucket object metadata to the Audit bucket. The EventBridge event will then log the event into AWS CloudTrail logs</strong> - Using Amazon EventBridge adds unnecessary complexity to the solution. You can simply leverage AWS CloudTrail integration with Amazon S3 as the most elegant solution to meet this requirement."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html#logging-data-events-examples",
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html"
    ]
  },
  {
    "id": 5,
    "question": "<p>A company's data engineer is tasked with enhancing the performance of SQL table queries on data stored in an Amazon Redshift cluster. Due to budget constraints, expanding the cluster size is not an option. The company utilizes the EVEN distribution style for loading data across multiple tables, with some tables containing hundreds of GB of data while others have less than 20 MB.</p>\n\n<p>What solution would address these needs effectively?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Opt for ALL distribution style for large tables. Declare primary and foreign keys for all tables</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Update the distribution style for all tables to AUTO. Declare primary and foreign keys for all tables</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>For the rarely updated small tables, opt for ALL distribution style. Declare primary and foreign keys for all tables</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Switch to Amazon Redshift Spectrum to efficiently query and retrieve data from the tables</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>For the rarely updated small tables, opt for ALL distribution style. Declare primary and foreign keys for all tables</strong></p>\n\n<p>For ALL distribution, a copy of the entire table is distributed to every node. While EVEN distribution or KEY distribution places only a portion of a table's rows on each node, ALL distribution ensures that every row is collocated for every join that the table participates in.</p>\n\n<p>ALL distribution multiplies the storage required by the number of nodes in the cluster, so it takes much longer to load, update, or insert data into multiple tables. ALL distribution is appropriate only for relatively slow-moving tables; that is, tables that are not updated frequently or extensively. Because the cost of redistributing small tables during a query is low, there isn't a significant benefit to defining small dimension tables as DISTSTYLE ALL.</p>\n\n<p>Hence, ALL distribution is suited for the rarely updated small tables while maintaining the EVEN distribution style for the larger tables.</p>\n\n<p>Define primary key and foreign key constraints between tables wherever appropriate. Even though they are informational only, the query optimizer uses those constraints to generate more efficient query plans.</p>\n\n<p>Note: Do not define primary key and foreign key constraints unless your application enforces the constraints. Amazon Redshift does not enforce unique, primary-key, or foreign-key constraints.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for ALL distribution style for large tables. Declare primary and foreign keys for all tables</strong> - ALL distribution multiplies the storage required by the number of nodes in the cluster, and so it takes much longer to load, update, or insert data into multiple tables. ALL distribution is appropriate only for relatively slow-moving tables; that is, tables that are not updated frequently or extensively.</p>\n\n<p><strong>Update the distribution style for all tables to AUTO. Declare primary and foreign keys for all tables</strong> - If a table is largely denormalized and does not participate in joins, or if you don't have a clear choice for another distribution style, use AUTO distribution. For this use case, AUTO is not an optimal choice since tables have clearly defined access patterns.</p>\n\n<p><strong>Switch to Amazon Redshift Spectrum to efficiently query and retrieve data from the tables</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. This option acts as a distractor since the given use case does not mention using Amazon S3 for storing data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/t_designating_distribution_styles.html\">https://docs.aws.amazon.com/redshift/latest/dg/t_designating_distribution_styles.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-defining-constraints.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-defining-constraints.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>For the rarely updated small tables, opt for ALL distribution style. Declare primary and foreign keys for all tables</strong>"
      },
      {
        "answer": "",
        "explanation": "For ALL distribution, a copy of the entire table is distributed to every node. While EVEN distribution or KEY distribution places only a portion of a table's rows on each node, ALL distribution ensures that every row is collocated for every join that the table participates in."
      },
      {
        "answer": "",
        "explanation": "ALL distribution multiplies the storage required by the number of nodes in the cluster, so it takes much longer to load, update, or insert data into multiple tables. ALL distribution is appropriate only for relatively slow-moving tables; that is, tables that are not updated frequently or extensively. Because the cost of redistributing small tables during a query is low, there isn't a significant benefit to defining small dimension tables as DISTSTYLE ALL."
      },
      {
        "answer": "",
        "explanation": "Hence, ALL distribution is suited for the rarely updated small tables while maintaining the EVEN distribution style for the larger tables."
      },
      {
        "answer": "",
        "explanation": "Define primary key and foreign key constraints between tables wherever appropriate. Even though they are informational only, the query optimizer uses those constraints to generate more efficient query plans."
      },
      {
        "answer": "",
        "explanation": "Note: Do not define primary key and foreign key constraints unless your application enforces the constraints. Amazon Redshift does not enforce unique, primary-key, or foreign-key constraints."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Opt for ALL distribution style for large tables. Declare primary and foreign keys for all tables</strong> - ALL distribution multiplies the storage required by the number of nodes in the cluster, and so it takes much longer to load, update, or insert data into multiple tables. ALL distribution is appropriate only for relatively slow-moving tables; that is, tables that are not updated frequently or extensively."
      },
      {
        "answer": "",
        "explanation": "<strong>Update the distribution style for all tables to AUTO. Declare primary and foreign keys for all tables</strong> - If a table is largely denormalized and does not participate in joins, or if you don't have a clear choice for another distribution style, use AUTO distribution. For this use case, AUTO is not an optimal choice since tables have clearly defined access patterns."
      },
      {
        "answer": "",
        "explanation": "<strong>Switch to Amazon Redshift Spectrum to efficiently query and retrieve data from the tables</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. This option acts as a distractor since the given use case does not mention using Amazon S3 for storing data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/dg/t_designating_distribution_styles.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-defining-constraints.html"
    ]
  },
  {
    "id": 6,
    "question": "<p>The data engineering team at a social media company has noticed that while some of the images stored in Amazon S3 are frequently accessed, others sit idle for a considerable time.</p>\n\n<p>What is your recommendation to build the MOST cost-effective solution?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is invoked daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store the images using the Amazon S3 Intelligent-Tiering storage class</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is invoked daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the images using the Amazon S3 Standard-IA storage class</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the images using the Amazon S3 Intelligent-Tiering storage class</strong></p>\n\n<p>The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.</p>\n\n<p>For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. Therefore using the Amazon S3 Intelligent-Tiering storage class is the correct solution for the given problem statement.</p>\n\n<p>Amazon S3 Storage Classes Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q55-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q55-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the images using the Amazon S3 Standard-IA storage class</strong></p>\n\n<p>Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. As some of the objects are frequently accessed, the per GB retrieval fee for Amazon S3 Standard-IA can cause the costs to shoot up, hence this option is incorrect.</p>\n\n<p><strong>Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is invoked daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class</strong></p>\n\n<p><strong>Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is invoked daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class</strong></p>\n\n<p>Creating a data monitoring application on an Amazon EC2 instance for managing the desired Amazon S3 storage class entails significant development cost as well as infrastructure maintenance effort. The Amazon S3 Intelligent-Tiering storage class does the job cost-effectively. Therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the images using the Amazon S3 Intelligent-Tiering storage class</strong>"
      },
      {
        "answer": "",
        "explanation": "The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access."
      },
      {
        "answer": "",
        "explanation": "For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. Therefore using the Amazon S3 Intelligent-Tiering storage class is the correct solution for the given problem statement."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q55-i1.jpg",
        "answer": "",
        "explanation": "Amazon S3 Storage Classes Overview:"
      },
      {
        "link": "https://aws.amazon.com/s3/storage-classes/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Store the images using the Amazon S3 Standard-IA storage class</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. As some of the objects are frequently accessed, the per GB retrieval fee for Amazon S3 Standard-IA can cause the costs to shoot up, hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is invoked daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is invoked daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class</strong>"
      },
      {
        "answer": "",
        "explanation": "Creating a data monitoring application on an Amazon EC2 instance for managing the desired Amazon S3 storage class entails significant development cost as well as infrastructure maintenance effort. The Amazon S3 Intelligent-Tiering storage class does the job cost-effectively. Therefore both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "id": 7,
    "question": "<p>A data engineer needs to monitor query performance on the Amazon Redshift cluster on a daily basis and send a report by the end of the day to the team lead. The report should have data about the following:</p>\n\n<p>1) Any long-running queries that have performance issues\n2) Any transactions that currently hold locks on tables</p>\n\n<p>Which tables/views will help get this information?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>STL_USAGE_CONTROL, STL_SESSIONS</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>STL_PLAN_INFO, STL_SESSIONS</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>STL_ALERT_EVENT_LOG, SVV_TRANSACTIONS</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>STL_QUERY_METRICS, SVV_TRANSACTIONS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>STL_ALERT_EVENT_LOG, SVV_TRANSACTIONS</strong></p>\n\n<p>STL_ALERT_EVENT_LOG - When a query runs, Amazon Redshift notes the query performance and indicates whether the query is running efficiently. If the query is identified as inefficient, then Amazon Redshift notes the query ID and provides recommendations for query performance improvement. These recommendations are logged in the STL_ALERT_EVENT_LOG internal system table. If you experience a long-running or inefficient query, then check the STL_ALERT_EVENT_LOG entries.</p>\n\n<p>SVV_TRANSACTIONS - Records information about transactions that currently hold locks on tables in the database. Use the SVV_TRANSACTIONS view to identify open transactions and lock contention issues</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>STL_USAGE_CONTROL, STL_SESSIONS</strong></p>\n\n<p><strong>STL_PLAN_INFO, STL_SESSIONS</strong></p>\n\n<p><strong>STL_QUERY_METRICS, SVV_TRANSACTIONS</strong></p>\n\n<p>These three options are incorrect. Here is a detailed description:</p>\n\n<p>STL_QUERY_METRICS - Contains metrics information, such as the number of rows processed, CPU usage, input/output, and disk use, for queries that have completed running in user-defined query queues (service classes).</p>\n\n<p>STL_USAGE_CONTROL - The STL_USAGE_CONTROL view contains information that is logged when a usage limit is reached</p>\n\n<p>STL_SESSIONS** - Use the STL_SESSIONS table to check for long-running sessions.</p>\n\n<p>STL_PLAN_INFO - Use the STL_PLAN_INFO view to look at the EXPLAIN output for a query in terms of a set of rows. This is an alternative way to look at query plans.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_STL_ALERT_EVENT_LOG.html\">https://docs.aws.amazon.com/redshift/latest/dg/r_STL_ALERT_EVENT_LOG.html</a></p>\n\n<p><a href=\"https://repost.aws/knowledge-center/redshift-cluster-degrade\">https://repost.aws/knowledge-center/redshift-cluster-degrade</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_STL_PLAN_INFO.html\">https://docs.aws.amazon.com/redshift/latest/dg/r_STL_PLAN_INFO.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>STL_ALERT_EVENT_LOG, SVV_TRANSACTIONS</strong>"
      },
      {
        "answer": "",
        "explanation": "STL_ALERT_EVENT_LOG - When a query runs, Amazon Redshift notes the query performance and indicates whether the query is running efficiently. If the query is identified as inefficient, then Amazon Redshift notes the query ID and provides recommendations for query performance improvement. These recommendations are logged in the STL_ALERT_EVENT_LOG internal system table. If you experience a long-running or inefficient query, then check the STL_ALERT_EVENT_LOG entries."
      },
      {
        "answer": "",
        "explanation": "SVV_TRANSACTIONS - Records information about transactions that currently hold locks on tables in the database. Use the SVV_TRANSACTIONS view to identify open transactions and lock contention issues"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>STL_USAGE_CONTROL, STL_SESSIONS</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>STL_PLAN_INFO, STL_SESSIONS</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>STL_QUERY_METRICS, SVV_TRANSACTIONS</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options are incorrect. Here is a detailed description:"
      },
      {
        "answer": "",
        "explanation": "STL_QUERY_METRICS - Contains metrics information, such as the number of rows processed, CPU usage, input/output, and disk use, for queries that have completed running in user-defined query queues (service classes)."
      },
      {
        "answer": "",
        "explanation": "STL_USAGE_CONTROL - The STL_USAGE_CONTROL view contains information that is logged when a usage limit is reached"
      },
      {
        "answer": "",
        "explanation": "STL_SESSIONS** - Use the STL_SESSIONS table to check for long-running sessions."
      },
      {
        "answer": "",
        "explanation": "STL_PLAN_INFO - Use the STL_PLAN_INFO view to look at the EXPLAIN output for a query in terms of a set of rows. This is an alternative way to look at query plans."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/dg/r_STL_ALERT_EVENT_LOG.html",
      "https://repost.aws/knowledge-center/redshift-cluster-degrade",
      "https://docs.aws.amazon.com/redshift/latest/dg/r_STL_PLAN_INFO.html"
    ]
  },
  {
    "id": 8,
    "question": "<p>A data engineer is configuring Amazon Athena to create a table for each file stored under the same prefix in Amazon S3. After running CREATE TABLE statement in Athena with expected columns and their data types, the engineer has issued a SELECT query. However, the query has returned zero records.</p>\n\n<p>Which of the following is the right way to configure the Amazon S3 location path?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>S3 table location path should be similar to this: <code>s3://doc-example-bucket/myprefix//input//</code></p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>S3 file names should be similar to the following - <code>s3://doc-example-bucket/athena/inputdata/_file1</code></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>S3 table location should be similar to the following - <code>s3://doc-example-bucket/table1.csv</code>, <code>s3://doc-example-bucket/table2.csv</code></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create individual S3 prefixes for each table like so - <code>s3://doc-example-bucket/table1/table1.csv</code>, <code>s3://doc-example-bucket/table2/table2.csv</code></p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create individual S3 prefixes for each table like so - <code>s3://doc-example-bucket/table1/table1.csv</code>, <code>s3://doc-example-bucket/table2/table2.csv</code></strong> - Athena query returns zero records if your table location is similar to the following:</p>\n\n<p>s3://doc-example-bucket/table1.csv\ns3://doc-example-bucket/table2.csv</p>\n\n<p>To resolve this issue, create individual S3 prefixes for each table similar to the following:</p>\n\n<p>s3://doc-example-bucket/table1/table1.csv\ns3://doc-example-bucket/table2/table2.csv</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q33-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q33-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/athena-empty-results/\">https://aws.amazon.com/premiumsupport/knowledge-center/athena-empty-results/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3 table location should be similar to the following - <code>s3://doc-example-bucket/table1.csv</code>, <code>s3://doc-example-bucket/table2.csv</code></strong> - Glue crawlers create separate tables for data that is stored in the same S3 prefix. However, when you query those tables in Athena, you get zero records.</p>\n\n<p>For example, your Athena query returns zero records if your table location is similar to the following:</p>\n\n<p>s3://doc-example-bucket/table1.csv\ns3://doc-example-bucket/table2.csv</p>\n\n<p>To resolve this issue, create individual S3 prefixes for each table similar to the following:</p>\n\n<p>s3://doc-example-bucket/table1/table1.csv\ns3://doc-example-bucket/table2/table2.csv</p>\n\n<p><strong>S3 table location path should be similar to this: <code>s3://doc-example-bucket/myprefix//input//</code></strong> - Athena doesn't support table location paths that include a double slash (//). For example, the following LOCATION path returns empty results:</p>\n\n<p>s3://doc-example-bucket/myprefix//input//</p>\n\n<p>To resolve this issue, copy the files to a location that doesn't have double slashes.</p>\n\n<p><strong>S3 file names should be similar to the following - <code>s3://doc-example-bucket/athena/inputdata/_file1</code></strong> - If the files in your S3 path have names that start with an underscore or a dot, then Athena considers these files as placeholders. Athena ignores these files when processing a query. If all the files in your S3 path have names that start with an underscore or a dot, then you get zero records.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/athena-empty-results/\">https://aws.amazon.com/premiumsupport/knowledge-center/athena-empty-results/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create individual S3 prefixes for each table like so - <code>s3://doc-example-bucket/table1/table1.csv</code>, <code>s3://doc-example-bucket/table2/table2.csv</code></strong> - Athena query returns zero records if your table location is similar to the following:"
      },
      {
        "answer": "",
        "explanation": "s3://doc-example-bucket/table1.csv\ns3://doc-example-bucket/table2.csv"
      },
      {
        "answer": "",
        "explanation": "To resolve this issue, create individual S3 prefixes for each table similar to the following:"
      },
      {
        "answer": "",
        "explanation": "s3://doc-example-bucket/table1/table1.csv\ns3://doc-example-bucket/table2/table2.csv"
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/athena-empty-results/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>S3 table location should be similar to the following - <code>s3://doc-example-bucket/table1.csv</code>, <code>s3://doc-example-bucket/table2.csv</code></strong> - Glue crawlers create separate tables for data that is stored in the same S3 prefix. However, when you query those tables in Athena, you get zero records."
      },
      {
        "answer": "",
        "explanation": "For example, your Athena query returns zero records if your table location is similar to the following:"
      },
      {
        "answer": "",
        "explanation": "s3://doc-example-bucket/table1.csv\ns3://doc-example-bucket/table2.csv"
      },
      {
        "answer": "",
        "explanation": "To resolve this issue, create individual S3 prefixes for each table similar to the following:"
      },
      {
        "answer": "",
        "explanation": "s3://doc-example-bucket/table1/table1.csv\ns3://doc-example-bucket/table2/table2.csv"
      },
      {
        "answer": "",
        "explanation": "<strong>S3 table location path should be similar to this: <code>s3://doc-example-bucket/myprefix//input//</code></strong> - Athena doesn't support table location paths that include a double slash (//). For example, the following LOCATION path returns empty results:"
      },
      {
        "answer": "",
        "explanation": "s3://doc-example-bucket/myprefix//input//"
      },
      {
        "answer": "",
        "explanation": "To resolve this issue, copy the files to a location that doesn't have double slashes."
      },
      {
        "answer": "",
        "explanation": "<strong>S3 file names should be similar to the following - <code>s3://doc-example-bucket/athena/inputdata/_file1</code></strong> - If the files in your S3 path have names that start with an underscore or a dot, then Athena considers these files as placeholders. Athena ignores these files when processing a query. If all the files in your S3 path have names that start with an underscore or a dot, then you get zero records."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/athena-empty-results/"
    ]
  },
  {
    "id": 9,
    "question": "<p>A bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order.</p>\n\n<p>Which of the following options can be used to implement this system?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon SQS standard queue to process the messages</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues - Standard queues vs FIFO queues.</p>\n\n<p>For FIFO queues, the order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out). On the other hand, the standard SQS queues offer best-effort ordering. This means that occasionally, messages might be delivered in an order different from which they were sent.</p>\n\n<p>By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). When you batch 10 messages per operation (maximum), FIFO queues can support up to 3,000 messages per second. Therefore you need to process 4 messages per operation so that the FIFO queue can support up to 1200 messages per second, which is well within the peak rate.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q43-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q43-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon SQS standard queue to process the messages</strong> - As messages need to be processed in order, therefore standard queues are ruled out.</p>\n\n<p><strong>Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages</strong> - By default, FIFO queues support up to 300 messages per second and this is not sufficient to meet the message processing throughput per the given use case. Hence this option is incorrect.</p>\n\n<p><strong>Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate</strong> - As mentioned earlier in the explanation, you need to use FIFO queues in batch mode and process 4 messages per operation, so that the FIFO queue can support up to 1200 messages per second. With 2 messages per operation, you can only support up to 600 messages per second.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues - Standard queues vs FIFO queues."
      },
      {
        "answer": "",
        "explanation": "For FIFO queues, the order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out). On the other hand, the standard SQS queues offer best-effort ordering. This means that occasionally, messages might be delivered in an order different from which they were sent."
      },
      {
        "answer": "",
        "explanation": "By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). When you batch 10 messages per operation (maximum), FIFO queues can support up to 3,000 messages per second. Therefore you need to process 4 messages per operation so that the FIFO queue can support up to 1200 messages per second, which is well within the peak rate."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS standard queue to process the messages</strong> - As messages need to be processed in order, therefore standard queues are ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages</strong> - By default, FIFO queues support up to 300 messages per second and this is not sufficient to meet the message processing throughput per the given use case. Hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate</strong> - As mentioned earlier in the explanation, you need to use FIFO queues in batch mode and process 4 messages per operation, so that the FIFO queue can support up to 1200 messages per second. With 2 messages per operation, you can only support up to 600 messages per second."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/sqs/features/"
    ]
  },
  {
    "id": 10,
    "question": "<p>A retail company stores its customer data in an Amazon S3 bucket. The data is accessed by employees from various countries for analytics purposes. The governance team needs to implement a solution ensuring that data analysts can only access customer data from their respective countries.</p>\n\n<p>What solution would satisfy these requirements with minimal operational effort?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Migrate the data to AWS Regions that are close to the countries where the customers are. Restrict access to each analyst based on the country that the analyst serves</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up S3 Access Point to read data from the S3 bucket. Leverage the S3 Access Points policy to dynamically filter data based on the country of the analyst making the read request</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a separate bucket for each country's customer data. Provide access to each analyst based on the country that the analyst serves</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the S3 bucket as a data lake location in AWS Lake Formation and leverage the Lake Formation row-level security features to enforce the company's access policies</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the S3 bucket as a data lake location in AWS Lake Formation and leverage the Lake Formation row-level security features to enforce the company's access policies</strong></p>\n\n<p>AWS Lake Formation helps you centrally govern, secure, and globally share data for analytics and machine learning. With Lake Formation, you can manage fine-grained access control for your data lake data on Amazon Simple Storage Service (Amazon S3) and its metadata in the AWS Glue Data Catalog.</p>\n\n<p>Lake Formation provides its own permissions model that augments the IAM permissions model. The Lake Formation permissions model enables fine-grained access to data stored in data lakes through a simple grant or revoke mechanism, much like a relational database management system (RDBMS). Lake Formation provides data filters that allow you to restrict access to a combination of columns and rows. You can use row and cell-level security to protect sensitive data like Personal Identifiable Information (PII).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q6-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q6-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up S3 Access Point to read data from the S3 bucket. Leverage the S3 Access Points policy to dynamically filter data based on the country of the analyst making the read request</strong> - Amazon S3 Access Points simplify data access for any AWS service or customer application that stores data in S3. With S3 Access Points, customers can create unique access control policies for each access point to easily control access to shared datasets.</p>\n\n<p>Each S3 Access Point is configured with an access policy specific to a use case or application. For example, you can create an access point for your S3 bucket that grants access to groups of users or applications for your data lake. An Access Point can support a single user or application, or groups of users or applications within and across accounts, allowing separate management of each access point.</p>\n\n<p>You cannot use an S3 Access Points policy to dynamically filter data based on the country of the analyst making the read request.</p>\n\n<p><strong>Create a separate bucket for each country's customer data. Provide access to each analyst based on the country that the analyst serves</strong> - Creating a separate bucket for each country's customer data is wasteful use of resources and involves significant operational effort to manage the data on an ongoing basis, so this option is incorrect.</p>\n\n<p><strong>Migrate the data to AWS Regions that are close to the countries where the customers are. Restrict access to each analyst based on the country that the analyst serves</strong> - This option also involves creating multiple buckets (since each S3 bucket is specific to an AWS Region), therefore it would also involve a significant operational effort to manage the data on an ongoing basis, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/register-data-lake.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/register-data-lake.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/registration-role.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/registration-role.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/features/access-points/\">https://aws.amazon.com/s3/features/access-points/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure the S3 bucket as a data lake location in AWS Lake Formation and leverage the Lake Formation row-level security features to enforce the company's access policies</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Lake Formation helps you centrally govern, secure, and globally share data for analytics and machine learning. With Lake Formation, you can manage fine-grained access control for your data lake data on Amazon Simple Storage Service (Amazon S3) and its metadata in the AWS Glue Data Catalog."
      },
      {
        "answer": "",
        "explanation": "Lake Formation provides its own permissions model that augments the IAM permissions model. The Lake Formation permissions model enables fine-grained access to data stored in data lakes through a simple grant or revoke mechanism, much like a relational database management system (RDBMS). Lake Formation provides data filters that allow you to restrict access to a combination of columns and rows. You can use row and cell-level security to protect sensitive data like Personal Identifiable Information (PII)."
      },
      {
        "link": "https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up S3 Access Point to read data from the S3 bucket. Leverage the S3 Access Points policy to dynamically filter data based on the country of the analyst making the read request</strong> - Amazon S3 Access Points simplify data access for any AWS service or customer application that stores data in S3. With S3 Access Points, customers can create unique access control policies for each access point to easily control access to shared datasets."
      },
      {
        "answer": "",
        "explanation": "Each S3 Access Point is configured with an access policy specific to a use case or application. For example, you can create an access point for your S3 bucket that grants access to groups of users or applications for your data lake. An Access Point can support a single user or application, or groups of users or applications within and across accounts, allowing separate management of each access point."
      },
      {
        "answer": "",
        "explanation": "You cannot use an S3 Access Points policy to dynamically filter data based on the country of the analyst making the read request."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a separate bucket for each country's customer data. Provide access to each analyst based on the country that the analyst serves</strong> - Creating a separate bucket for each country's customer data is wasteful use of resources and involves significant operational effort to manage the data on an ongoing basis, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Migrate the data to AWS Regions that are close to the countries where the customers are. Restrict access to each analyst based on the country that the analyst serves</strong> - This option also involves creating multiple buckets (since each S3 bucket is specific to an AWS Region), therefore it would also involve a significant operational effort to manage the data on an ongoing basis, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html",
      "https://docs.aws.amazon.com/lake-formation/latest/dg/register-data-lake.html",
      "https://docs.aws.amazon.com/lake-formation/latest/dg/registration-role.html",
      "https://aws.amazon.com/s3/features/access-points/"
    ]
  },
  {
    "id": 11,
    "question": "<p>The content division at a digital media agency has an application that generates a large number of files on Amazon S3, each approximately 10 megabytes in size. The agency mandates that the files be stored for 5 years before they can be deleted. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days, however, immediate accessibility is always required. The files contain critical business data that is not easy to reproduce.</p>\n\n<p>Which solution is the MOST cost-effective for the given use case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation</strong></p>\n\n<p>Amazon S3 Standard-IA class is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per gigabyte storage price and per GB retrieval charge.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q61-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q61-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a><p></p>\n\n<p>For the given use case, you can set up an Amazon S3 lifecycle configuration and create a transition action to move objects from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. You can set up an expiration action to delete the object 5 years after object creation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q61-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q61-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q61-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q61-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation</strong> - Amazon S3 Glacier Flexible Retrieval storage class has the best case retrieval time of the order of minutes, so this option is incorrect for the given requirement.</p>\n\n<p><strong>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation</strong> - The files can simply be deleted 5 years after object creation instead of archiving the files to Amazon S3 Glacier Deep Archive. There is no need to incur the cost of archival.</p>\n\n<p><strong>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation</strong> - Unlike other Amazon S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single AZ and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. The given scenario clearly states that the business-critical data is not easy to reproduce, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/glacier/\">https://aws.amazon.com/s3/storage-classes/glacier/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Standard-IA class is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per gigabyte storage price and per GB retrieval charge."
      },
      {
        "link": "https://aws.amazon.com/s3/storage-classes/"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can set up an Amazon S3 lifecycle configuration and create a transition action to move objects from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. You can set up an expiration action to delete the object 5 years after object creation."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation</strong> - Amazon S3 Glacier Flexible Retrieval storage class has the best case retrieval time of the order of minutes, so this option is incorrect for the given requirement."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation</strong> - The files can simply be deleted 5 years after object creation instead of archiving the files to Amazon S3 Glacier Deep Archive. There is no need to incur the cost of archival."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation</strong> - Unlike other Amazon S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single AZ and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. The given scenario clearly states that the business-critical data is not easy to reproduce, so this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html",
      "https://aws.amazon.com/s3/storage-classes/glacier/"
    ]
  },
  {
    "id": 12,
    "question": "<p>A stock trading company uses Amazon Redshift to power the Business Intelligence (BI) specific queries which are run on Redshift. The data engineering team at the company needs to provide the sales team access to a historical trades table whose data is stored in Apache Parquet format in an S3 bucket of the company's data lake. The data engineering team should provide access to only a few specific columns in the historical trades table so that the access does not violate the compliance regulations.</p>\n\n<p>Which of the following options should be combined together to build a solution for the given use case? (Select three)</p>",
    "corrects": [
      1,
      5,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Grant permissions in Lake Formation to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create an IAM role for Amazon Redshift which has a policy to allow Redshift to access the S3 bucket having data for the historical trades table</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Grant permissions in the S3 bucket policy to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an internal schema in Amazon Redshift by using the Amazon Redshift IAM role</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Create an external schema in Amazon Redshift by using the Amazon Redshift IAM role</p>",
        "correct": true
      },
      {
        "id": 6,
        "answer": "<p>Create an IAM role for Amazon Redshift which has a policy to allow Redshift to access AWS Lake Formation</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an IAM role for Amazon Redshift which has a policy to allow Redshift to access AWS Lake Formation</strong></p>\n\n<p><strong>Create an external schema in Amazon Redshift by using the Amazon Redshift IAM role</strong></p>\n\n<p><strong>Grant permissions in Lake Formation to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table</strong></p>\n\n<p>Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale. Using Redshift Spectrum, Amazon Redshift customers can easily query their data in Amazon S3. Redshift Spectrum is a built-in feature of Amazon Redshift, and your existing queries and BI tools will continue to work seamlessly. Under the hood, AWS manages a fleet of thousands of Redshift Spectrum nodes spread across multiple Availability Zones. These are transparently scaled and allocated to your queries based on the data that you need to process, with no provisioning or commitments. Redshift Spectrum is also highly concurrent—you can access your Amazon S3 data from any number of Amazon Redshift clusters. To access this data on S3 via Redshift Spectrum, you need to create an external schema in Amazon Redshift.</p>\n\n<p>Amazon Redshift Spectrum supports column level access control for data stored in Amazon S3 and managed by AWS Lake Formation. Column-level access control can be used to limit access to only the specific columns of a table rather than allowing access to all columns of a table. To use this feature, an administrator needs to create an IAM role for Amazon Redshift and create the policy to allow Redshift to access AWS Lake Formation. The administrator can then use the Lake Formation console to specify the tables and columns that the role is allowed access to. The column-level access control policies can also be created and managed by the SQL grant statements.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an internal schema in Amazon Redshift by using the Amazon Redshift IAM role</strong> - As mentioned in the explanation above, you need to create an external schema in Amazon Redshift to access the data on S3 via Redshift spectrum.</p>\n\n<p><strong>Grant permissions in the S3 bucket policy to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table</strong> - You need to grant permissions in Lake Formation to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table.</p>\n\n<p><strong>Create an IAM role for Amazon Redshift which has a policy to allow Redshift to access the S3 bucket having data for the historical trades table</strong> - You need to create an IAM role for Amazon Redshift which has a policy to allow Redshift to access AWS Lake Formation.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/08/amazon-redshift-spectrum-now-supports-column-level-access-control-with-aws-lake-formation/\">https://aws.amazon.com/about-aws/whats-new/2019/08/amazon-redshift-spectrum-now-supports-column-level-access-control-with-aws-lake-formation/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/achieve-finer-grained-data-security-with-column-level-access-control-in-amazon-redshift/\">https://aws.amazon.com/blogs/big-data/achieve-finer-grained-data-security-with-column-level-access-control-in-amazon-redshift/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an IAM role for Amazon Redshift which has a policy to allow Redshift to access AWS Lake Formation</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Create an external schema in Amazon Redshift by using the Amazon Redshift IAM role</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Grant permissions in Lake Formation to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale. Using Redshift Spectrum, Amazon Redshift customers can easily query their data in Amazon S3. Redshift Spectrum is a built-in feature of Amazon Redshift, and your existing queries and BI tools will continue to work seamlessly. Under the hood, AWS manages a fleet of thousands of Redshift Spectrum nodes spread across multiple Availability Zones. These are transparently scaled and allocated to your queries based on the data that you need to process, with no provisioning or commitments. Redshift Spectrum is also highly concurrent—you can access your Amazon S3 data from any number of Amazon Redshift clusters. To access this data on S3 via Redshift Spectrum, you need to create an external schema in Amazon Redshift."
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift Spectrum supports column level access control for data stored in Amazon S3 and managed by AWS Lake Formation. Column-level access control can be used to limit access to only the specific columns of a table rather than allowing access to all columns of a table. To use this feature, an administrator needs to create an IAM role for Amazon Redshift and create the policy to allow Redshift to access AWS Lake Formation. The administrator can then use the Lake Formation console to specify the tables and columns that the role is allowed access to. The column-level access control policies can also be created and managed by the SQL grant statements."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an internal schema in Amazon Redshift by using the Amazon Redshift IAM role</strong> - As mentioned in the explanation above, you need to create an external schema in Amazon Redshift to access the data on S3 via Redshift spectrum."
      },
      {
        "answer": "",
        "explanation": "<strong>Grant permissions in the S3 bucket policy to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table</strong> - You need to grant permissions in Lake Formation to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an IAM role for Amazon Redshift which has a policy to allow Redshift to access the S3 bucket having data for the historical trades table</strong> - You need to create an IAM role for Amazon Redshift which has a policy to allow Redshift to access AWS Lake Formation."
      }
    ],
    "references": [
      "https://aws.amazon.com/about-aws/whats-new/2019/08/amazon-redshift-spectrum-now-supports-column-level-access-control-with-aws-lake-formation/",
      "https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/",
      "https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html",
      "https://aws.amazon.com/blogs/big-data/achieve-finer-grained-data-security-with-column-level-access-control-in-amazon-redshift/"
    ]
  },
  {
    "id": 13,
    "question": "<p>A company is building an application on Amazon EC2 instances that generates temporary data in the staging environment. The application is now being migrated to the production environment, so the company requires a solution to ensure data persistence, even if the EC2 instances are terminated. A data engineer is tasked with launching new EC2 instances from an Amazon Machine Image (AMI) and setting them up to retain the data permanently.</p>\n\n<p>What solution would fulfill this requirement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch an EC2 instance using an AMI that is backed by an EC2 instance store volume. Attach an Amazon EBS volume to store the application data. Apply the default settings to the EC2 instances</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Launch an EC2 instance by using an AMI that is backed by an Amazon EBS volume. Apply the default settings to the EC2 instances</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Launch an EC2 instance by using an AMI that is backed by an Amazon EC2 instance store volume. Apply the default settings to the EC2 instances</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Launch an EC2 instance using an AMI that is backed by an Amazon EBS volume. Attach an additional EC2 instance store volume to store the application data. Apply the default settings to the EC2 instances</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Launch an EC2 instance using an AMI that is backed by an EC2 instance store volume. Attach an Amazon EBS volume to store the application data. Apply the default settings to the EC2 instances</strong></p>\n\n<p>When an instance terminates, the data on any instance store volumes, and the data stored in the instance RAM is erased. Any Elastic IP addresses associated with the instance are detached. For Amazon EBS volumes and the data on those volumes, the outcome depends on the Delete on termination setting for the volume. By default, the root volume is deleted and the data volumes are preserved.</p>\n\n<p>By default, Amazon EBS root device volumes are automatically deleted when the instance terminates. However, any additional EBS volumes that you attach at launch, or any EBS volumes that you attach to an existing instance persist even after the instance terminates. Hence, for the given use case, you should choose an AMI backed by Amazon EC2 instance store with an additional EBS volume to persist data on termination.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q16-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q16-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-ec2-instance-termination-works.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-ec2-instance-termination-works.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Launch an EC2 instance by using an AMI that is backed by an Amazon EBS volume. Apply the default settings to the EC2 instances</strong></p>\n\n<p><strong>Launch an EC2 instance by using an AMI that is backed by an Amazon EC2 instance store volume. Apply the default settings to the EC2 instances</strong></p>\n\n<p>As discussed above, data on root volumes is terminated by default and hence these two options are incorrect.</p>\n\n<p><strong>Launch an EC2 instance using an AMI that is backed by an Amazon EBS volume. Attach an additional EC2 instance store volume to store the application data. Apply the default settings to the EC2 instances</strong> - Data on all root volumes is deleted on termination. So, data on the EBS root volume will be deleted. You cannot attach an EC2 instance store volume as an additional data volume for an instance that has an EBS root volume. Hence, all data will still be lost on termination.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html#storage-for-the-root-device\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html#storage-for-the-root-device</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-ec2-instance-termination-works.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-ec2-instance-termination-works.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Launch an EC2 instance using an AMI that is backed by an EC2 instance store volume. Attach an Amazon EBS volume to store the application data. Apply the default settings to the EC2 instances</strong>"
      },
      {
        "answer": "",
        "explanation": "When an instance terminates, the data on any instance store volumes, and the data stored in the instance RAM is erased. Any Elastic IP addresses associated with the instance are detached. For Amazon EBS volumes and the data on those volumes, the outcome depends on the Delete on termination setting for the volume. By default, the root volume is deleted and the data volumes are preserved."
      },
      {
        "answer": "",
        "explanation": "By default, Amazon EBS root device volumes are automatically deleted when the instance terminates. However, any additional EBS volumes that you attach at launch, or any EBS volumes that you attach to an existing instance persist even after the instance terminates. Hence, for the given use case, you should choose an AMI backed by Amazon EC2 instance store with an additional EBS volume to persist data on termination."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-ec2-instance-termination-works.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Launch an EC2 instance by using an AMI that is backed by an Amazon EBS volume. Apply the default settings to the EC2 instances</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Launch an EC2 instance by using an AMI that is backed by an Amazon EC2 instance store volume. Apply the default settings to the EC2 instances</strong>"
      },
      {
        "answer": "",
        "explanation": "As discussed above, data on root volumes is terminated by default and hence these two options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Launch an EC2 instance using an AMI that is backed by an Amazon EBS volume. Attach an additional EC2 instance store volume to store the application data. Apply the default settings to the EC2 instances</strong> - Data on all root volumes is deleted on termination. So, data on the EBS root volume will be deleted. You cannot attach an EC2 instance store volume as an additional data volume for an instance that has an EBS root volume. Hence, all data will still be lost on termination."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-ec2-instance-termination-works.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html#storage-for-the-root-device"
    ]
  },
  {
    "id": 14,
    "question": "<p>The data engineering team is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the data engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams.</p>\n\n<p>Which of the following would you recommend for improving the performance for the given use-case?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Enhanced Fanout feature of Amazon Kinesis Data Streams</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Enhanced Fanout feature of Amazon Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream.</p>\n\n<p>Amazon Kinesis Data Streams Fanout:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q59-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q59-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose</strong> -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Amazon Kinesis Data Firehose can only write to Amazon S3, Amazon Redshift, Amazon Elasticsearch or Splunk. You can't have applications consuming data streams from Amazon Kinesis Data Firehose, that's the job of Amazon Kinesis Data Streams. Therefore this option is not correct.</p>\n\n<p><strong>Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues</strong></p>\n\n<p><strong>Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues</strong></p>\n\n<p>Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both Amazon SQS Standard and Amazon SQS FIFO are not the right fit for the given use case.</p>\n\n<p>Exam Alert:</p>\n\n<p>You should understand the differences between the capabilities of Amazon Kinesis Data Streams vs Amazon SQS, as you may be asked scenario-based questions on this topic in the exam.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q59-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q59-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a><p></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Enhanced Fanout feature of Amazon Kinesis Data Streams</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events."
      },
      {
        "answer": "",
        "explanation": "By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q59-i1.jpg",
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams Fanout:"
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose</strong> -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Amazon Kinesis Data Firehose can only write to Amazon S3, Amazon Redshift, Amazon Elasticsearch or Splunk. You can't have applications consuming data streams from Amazon Kinesis Data Firehose, that's the job of Amazon Kinesis Data Streams. Therefore this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both Amazon SQS Standard and Amazon SQS FIFO are not the right fit for the given use case."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "answer": "",
        "explanation": "You should understand the differences between the capabilities of Amazon Kinesis Data Streams vs Amazon SQS, as you may be asked scenario-based questions on this topic in the exam."
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/faqs/"
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/",
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 15,
    "question": "<p>An Internet-of-Things (IoT) company is looking for a database solution on AWS Cloud that has Auto Scaling capabilities and is highly available. The database should be able to handle any changes in data attributes over time, in case the company updates the data feed from its IoT devices. The database must provide the capability to output a continuous stream with details of any changes to the underlying data.</p>\n\n<p>Which database will you recommend?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon Aurora</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon DynamoDB</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon Relational Database Service (Amazon RDS)</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon DynamoDB</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. DynamoDB is serverless with no servers to provision, patch, or manage and no software to install, maintain, or operate.</p>\n\n<p>A Amazon DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, Amazon DynamoDB captures information about every modification to data items in the table.</p>\n\n<p>Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the \"before\" and \"after\" images of modified items.</p>\n\n<p>Amazon DynamoDB is horizontally scalable, has a DynamoDB streams capability and is multi-AZ by default. On top of it, we can adjust the RCU and WCU automatically using Auto Scaling. This is the right choice for current requirements.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Relational Database Service (Amazon RDS)</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. Schema changes on relational databases are not straight forward and are hard to maintain if the schema requirements change often.</p>\n\n<p><strong>Amazon Aurora</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database. Schema changes on relational databases are not straight forward and are hard to maintain if the schema requirements change often.</p>\n\n<p><strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud based data warehouse product designed for large scale data set storage and analysis. It is a powerful warehousing service from Amazon. The current requirement, however, is not looking for a warehousing solution and hence Redshift is not an option here.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon DynamoDB</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. DynamoDB is serverless with no servers to provision, patch, or manage and no software to install, maintain, or operate."
      },
      {
        "answer": "",
        "explanation": "A Amazon DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, Amazon DynamoDB captures information about every modification to data items in the table."
      },
      {
        "answer": "",
        "explanation": "Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the \"before\" and \"after\" images of modified items."
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is horizontally scalable, has a DynamoDB streams capability and is multi-AZ by default. On top of it, we can adjust the RCU and WCU automatically using Auto Scaling. This is the right choice for current requirements."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Relational Database Service (Amazon RDS)</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. Schema changes on relational databases are not straight forward and are hard to maintain if the schema requirements change often."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Aurora</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database. Schema changes on relational databases are not straight forward and are hard to maintain if the schema requirements change often."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud based data warehouse product designed for large scale data set storage and analysis. It is a powerful warehousing service from Amazon. The current requirement, however, is not looking for a warehousing solution and hence Redshift is not an option here."
      }
    ],
    "references": [
      "https://aws.amazon.com/dynamodb/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"
    ]
  },
  {
    "id": 16,
    "question": "<p>A social media startup uses AWS Cloud to manage its IT infrastructure. The data engineering team at the startup wants to perform weekly database rollovers for a MySQL database server using a serverless cron job that typically takes about 5 minutes to execute the database rollover script written in Python. The database rollover will archive the past week’s data from the production database to keep the database small while still keeping its data accessible.</p>\n\n<p>Which of the following would you recommend as the MOST cost-efficient and reliable solution?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Provision an Amazon EC2 on-demand instance to run the database rollover script to be run via an OS-based weekly cron expression</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. AWS Lambda supports standard rate and cron expressions for frequencies of up to once per minute.</p>\n\n<p>Schedule expressions using rate or cron:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q53-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q53-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing and it's not the right fit for running a database rollover script. Although AWS Glue is also serverless, AWS Lambda is a more cost-effective option compared to AWS Glue.</p>\n\n<p><strong>Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression</strong> - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly (up to 90% off the On-Demand price).\nAs the Spot Instance runs whenever capacity is available, there is no guarantee that the weekly job will be executed during the defined time window. Additionally, the given use case requires a serverless solution, therefore this option is incorrect.</p>\n\n<p><strong>Provision an Amazon EC2 on-demand instance to run the database rollover script to be run via an OS-based weekly cron expression</strong> - An Amazon EC2 on-demand instance can certainly be used to run the database rollover script which is run via an OS-based weekly cron expression. However, the given use case requires a serverless solution, therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents-expressions.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents-expressions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. AWS Lambda supports standard rate and cron expressions for frequencies of up to once per minute."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q53-i1.jpg",
        "answer": "",
        "explanation": "Schedule expressions using rate or cron:"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing and it's not the right fit for running a database rollover script. Although AWS Glue is also serverless, AWS Lambda is a more cost-effective option compared to AWS Glue."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression</strong> - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly (up to 90% off the On-Demand price).\nAs the Spot Instance runs whenever capacity is available, there is no guarantee that the weekly job will be executed during the defined time window. Additionally, the given use case requires a serverless solution, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision an Amazon EC2 on-demand instance to run the database rollover script to be run via an OS-based weekly cron expression</strong> - An Amazon EC2 on-demand instance can certainly be used to run the database rollover script which is run via an OS-based weekly cron expression. However, the given use case requires a serverless solution, therefore this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents-expressions.html"
    ]
  },
  {
    "id": 17,
    "question": "<p>A retail company recently migrated to a data lake design based on Amazon S3. Amazon Redshift and Amazon QuickSight are being used by the company's data engineering team to analyze data for better insights. To ensure access to the most up-to-date actionable data, the team has now shifted to a nightly Amazon Redshift refresh utilizing terabytes of the previous day's changes. The team has noticed that post the switchover to nightly refresh, several popular dashboards that had good performance earlier, are now seeing degraded performance during business hours as well. Amazon CloudWatch shows no notifications regarding the performance metrics.</p>\n\n<p>Which of the following represents the MOST LIKELY cause for this issue?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The Redshift cluster is undersized for the queries being executed by the dashboards</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Inefficient SQL queries are being run by the dashboards</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The nightly data refreshes left the dashboard tables in need of a vacuum operation that could not be automatically performed by Amazon Redshift due to ongoing user workloads</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>The nightly data refreshes are causing the queries to hang during the business hours</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The nightly data refreshes left the dashboard tables in need of a vacuum operation that could not be automatically performed by Amazon Redshift due to ongoing user workloads</strong></p>\n\n<p>The Redshift vacuum operation re-sorts rows and reclaims space in either a specified table or all tables in the current database. Amazon Redshift does not reclaim free space automatically. Such available space is created whenever you delete or update rows on a table. This process is a design choice inherited from PostgreSQL and a routine maintenance process that you need to follow for your tables to maximize the performance of your Redshift cluster. By running a vacuum command on one of your tables, you can reclaim any free space that is the result of delete and update operations. At the same time, the data of the table gets sorted. Thus, you end up with a compact and sorted table, which is useful for the performance of your cluster.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q24-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q24-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html\">https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html</a><p></p>\n\n<p>You should note that the automatic vacuum operations can pause if the cluster experiences a period of high load. So, in case the cluster continues to see consistently high loads during the nightly refresh, the vacuum operations won't complete, thereby causing persistent performance degradation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q24-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q24-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html\">https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The nightly data refreshes are causing the queries to hang during the business hours</strong> - The nightly data refreshes cannot cause the queries to hang during the business hours since these refreshes are carried out via ETL steps that could use long-running queries such as a COPY command. This can only cause the queries to hang for the duration of the nightly refresh and not during business hours.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q24-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q24-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/queries-troubleshooting.html#queries-troubleshooting-query-hangs\">https://docs.aws.amazon.com/redshift/latest/dg/queries-troubleshooting.html#queries-troubleshooting-query-hangs</a><p></p>\n\n<p><strong>Inefficient SQL queries are being run by the dashboards</strong></p>\n\n<p><strong>The Redshift cluster is undersized for the queries being executed by the dashboards</strong></p>\n\n<p>These two options have been added as distractors. Had inefficient SQL queries or an undersized Redshift cluster been the reasons behind the cluster's performance deterioration, these would have impacted the cluster even before the switchover to the nightly refresh process. Therefore, both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html\">https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-redshift-automatic-vacuum/\">https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-redshift-automatic-vacuum/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/queries-troubleshooting.html#queries-troubleshooting-query-hangs\">https://docs.aws.amazon.com/redshift/latest/dg/queries-troubleshooting.html#queries-troubleshooting-query-hangs</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The nightly data refreshes left the dashboard tables in need of a vacuum operation that could not be automatically performed by Amazon Redshift due to ongoing user workloads</strong>"
      },
      {
        "answer": "",
        "explanation": "The Redshift vacuum operation re-sorts rows and reclaims space in either a specified table or all tables in the current database. Amazon Redshift does not reclaim free space automatically. Such available space is created whenever you delete or update rows on a table. This process is a design choice inherited from PostgreSQL and a routine maintenance process that you need to follow for your tables to maximize the performance of your Redshift cluster. By running a vacuum command on one of your tables, you can reclaim any free space that is the result of delete and update operations. At the same time, the data of the table gets sorted. Thus, you end up with a compact and sorted table, which is useful for the performance of your cluster."
      },
      {
        "link": "https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html"
      },
      {
        "answer": "",
        "explanation": "You should note that the automatic vacuum operations can pause if the cluster experiences a period of high load. So, in case the cluster continues to see consistently high loads during the nightly refresh, the vacuum operations won't complete, thereby causing persistent performance degradation."
      },
      {
        "link": "https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The nightly data refreshes are causing the queries to hang during the business hours</strong> - The nightly data refreshes cannot cause the queries to hang during the business hours since these refreshes are carried out via ETL steps that could use long-running queries such as a COPY command. This can only cause the queries to hang for the duration of the nightly refresh and not during business hours."
      },
      {
        "link": "https://docs.aws.amazon.com/redshift/latest/dg/queries-troubleshooting.html#queries-troubleshooting-query-hangs"
      },
      {
        "answer": "",
        "explanation": "<strong>Inefficient SQL queries are being run by the dashboards</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>The Redshift cluster is undersized for the queries being executed by the dashboards</strong>"
      },
      {
        "answer": "",
        "explanation": "These two options have been added as distractors. Had inefficient SQL queries or an undersized Redshift cluster been the reasons behind the cluster's performance deterioration, these would have impacted the cluster even before the switchover to the nightly refresh process. Therefore, both these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/queries-troubleshooting.html#queries-troubleshooting-query-hangs",
      "https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-redshift-automatic-vacuum/"
    ]
  },
  {
    "id": 18,
    "question": "<p>A company is looking at transferring its archived digital media assets of around 20 petabytes to AWS Cloud in the shortest possible time.</p>\n\n<p>Which of the following is an optimal solution for this requirement, given that the company's archives are located at a remote location?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Snowball</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Snowmobile</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS DataSync</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Storage Gateway</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Snowmobile</strong></p>\n\n<p>AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective.  AWS recommends using Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Snowball</strong> - The AWS Snowball service uses physical storage devices to transfer large amounts of data between Amazon Simple Storage Service (Amazon S3) and client's onsite data storage location at faster-than-internet speeds. Snowball provides powerful interfaces that you can use to create jobs, track data, and track the status of your jobs through to completion. AWS recommends snowball only if you want to transfer greater than 10 TB of data between your on-premises data centers and Amazon S3.</p>\n\n<p><strong>AWS Storage Gateway</strong> - AWS DataSync is an online data movement and discovery service that simplifies and accelerates data migrations to AWS as well as moving data to and from on-premises storage, edge locations, other cloud providers, and AWS Storage services. This is not an optimal solution since the company's archives are in a remote location where internet speed may not be optimal, thereby increasing both cost and time for migrating 20TB of data.</p>\n\n<p><strong>AWS DataSync</strong> - AWS Direct Connect is a network service that provides an alternative to using the Internet to connect a customer’s on-premises sites to AWS. Data is transmitted through a private network connection between AWS and a customer’s data center or corporate network. Direct Connect connection takes significant cost as well as time to provision. This is not the correct solution since the company wants the data transfer to be done in the shortest possible time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/snowmobile/\">https://aws.amazon.com/snowmobile/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Snowmobile</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective.  AWS recommends using Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Snowball</strong> - The AWS Snowball service uses physical storage devices to transfer large amounts of data between Amazon Simple Storage Service (Amazon S3) and client's onsite data storage location at faster-than-internet speeds. Snowball provides powerful interfaces that you can use to create jobs, track data, and track the status of your jobs through to completion. AWS recommends snowball only if you want to transfer greater than 10 TB of data between your on-premises data centers and Amazon S3."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Storage Gateway</strong> - AWS DataSync is an online data movement and discovery service that simplifies and accelerates data migrations to AWS as well as moving data to and from on-premises storage, edge locations, other cloud providers, and AWS Storage services. This is not an optimal solution since the company's archives are in a remote location where internet speed may not be optimal, thereby increasing both cost and time for migrating 20TB of data."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS DataSync</strong> - AWS Direct Connect is a network service that provides an alternative to using the Internet to connect a customer’s on-premises sites to AWS. Data is transmitted through a private network connection between AWS and a customer’s data center or corporate network. Direct Connect connection takes significant cost as well as time to provision. This is not the correct solution since the company wants the data transfer to be done in the shortest possible time."
      }
    ],
    "references": [
      "https://aws.amazon.com/snowmobile/"
    ]
  },
  {
    "id": 19,
    "question": "<p>A company intends to deploy a provisioned Amazon EMR cluster running Apache Spark jobs for big data analysis and requires a highly reliable configuration. The big data team needs to adhere to best practices for managing cost-effective, long-duration workloads on Amazon EMR while preserving existing performance levels.</p>\n\n<p>What two resource combinations will most economically meet these requirements? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Utilize EMR File System (EMRFS) as a persistent data store to read/write data directly to Amazon S3</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Provision spot instances for core nodes</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize Hadoop Distributed File System (HDFS) as a persistent data store</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Provision x86-based instances for core nodes and task nodes</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Provision Graviton instances for core nodes and task nodes</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Utilize EMR File System (EMRFS) as a persistent data store to read/write data directly to Amazon S3</strong></p>\n\n<p>You can use EMRFS when you read the dataset one time in each run. EMRFS decouples storage from compute, so you don’t need to provision core nodes specifically to store data, and you don’t need to pay for data replication in HDFS. This results in lower costs, and it provides availability of the data for multiple clusters. You also have the advantage of retaining data after shutting down the cluster. For long-duration workloads on Amazon EMR, EMRFS represents a cost-effective and highly reliable storage solution.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q3-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q3-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/amazon-emr-hardware/storage.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/amazon-emr-hardware/storage.html</a><p></p>\n\n<p><strong>Provision Graviton instances for core nodes and task nodes</strong></p>\n\n<p>AWS Graviton is a family of processors designed to deliver the best price-performance for your cloud workloads running in Amazon Elastic Compute Cloud (Amazon EC2). AWS Graviton-based instances cost up to 20% less than comparable x86-based Amazon EC2 instances.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provision x86-based instances for core nodes and task nodes</strong> - As mentioned above, AWS Graviton-based instances cost up to 20% less than comparable x86-based Amazon EC2 instances. So, this option is incorrect.</p>\n\n<p><strong>Provision spot instances for core nodes</strong> - Amazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud and are available at up to a 90% discount compared to On-Demand prices. Core nodes process data and store information using HDFS. Terminating a core instance risks data loss. For this reason, you should only run core nodes on Spot Instances when partial HDFS data loss is tolerable. Since the given use case requires high reliability, so this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q3-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q3-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-dev-master-instance-group-spot\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-dev-master-instance-group-spot</a><p></p>\n\n<p><strong>Utilize Hadoop Distributed File System (HDFS) as a persistent data store</strong> - Hadoop Distributed File System (HDFS) is a distributed, scalable, and portable file system for Hadoop. An advantage of HDFS is data awareness between the Hadoop cluster nodes managing the clusters and the Hadoop cluster nodes managing the individual steps. HDFS is ephemeral, which means it is reclaimed when the instances are terminated. Since you need to pay for data replication in HDFS, it is not best suited as a persistent data store for long-duration workloads on Amazon EMR.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/amazon-emr-hardware/storage.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/amazon-emr-hardware/storage.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/ec2/graviton/\">https://aws.amazon.com/ec2/graviton/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-dev-master-instance-group-spot\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-dev-master-instance-group-spot</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Utilize EMR File System (EMRFS) as a persistent data store to read/write data directly to Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use EMRFS when you read the dataset one time in each run. EMRFS decouples storage from compute, so you don’t need to provision core nodes specifically to store data, and you don’t need to pay for data replication in HDFS. This results in lower costs, and it provides availability of the data for multiple clusters. You also have the advantage of retaining data after shutting down the cluster. For long-duration workloads on Amazon EMR, EMRFS represents a cost-effective and highly reliable storage solution."
      },
      {
        "link": "https://docs.aws.amazon.com/prescriptive-guidance/latest/amazon-emr-hardware/storage.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Provision Graviton instances for core nodes and task nodes</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Graviton is a family of processors designed to deliver the best price-performance for your cloud workloads running in Amazon Elastic Compute Cloud (Amazon EC2). AWS Graviton-based instances cost up to 20% less than comparable x86-based Amazon EC2 instances."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Provision x86-based instances for core nodes and task nodes</strong> - As mentioned above, AWS Graviton-based instances cost up to 20% less than comparable x86-based Amazon EC2 instances. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Provision spot instances for core nodes</strong> - Amazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud and are available at up to a 90% discount compared to On-Demand prices. Core nodes process data and store information using HDFS. Terminating a core instance risks data loss. For this reason, you should only run core nodes on Spot Instances when partial HDFS data loss is tolerable. Since the given use case requires high reliability, so this option is incorrect."
      },
      {
        "link": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-dev-master-instance-group-spot"
      },
      {
        "answer": "",
        "explanation": "<strong>Utilize Hadoop Distributed File System (HDFS) as a persistent data store</strong> - Hadoop Distributed File System (HDFS) is a distributed, scalable, and portable file system for Hadoop. An advantage of HDFS is data awareness between the Hadoop cluster nodes managing the clusters and the Hadoop cluster nodes managing the individual steps. HDFS is ephemeral, which means it is reclaimed when the instances are terminated. Since you need to pay for data replication in HDFS, it is not best suited as a persistent data store for long-duration workloads on Amazon EMR."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/prescriptive-guidance/latest/amazon-emr-hardware/storage.html",
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-dev-master-instance-group-spot",
      "https://aws.amazon.com/ec2/graviton/"
    ]
  },
  {
    "id": 20,
    "question": "<p>The data engineering team at a retail organization is running batch workloads on AWS Cloud. The team has embedded RDS database connection strings within each web server hosting the flagship application. After failing a security audit, the team is looking at a different approach to store the database secrets securely and automatically rotate the database credentials.</p>\n\n<p>Which of the following solutions would you recommend to meet this requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>KMS</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Secrets Manager</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS Config</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SSM Parameter Store</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Secrets Manager</strong></p>\n\n<p>AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.</p>\n\n<p>Benefits of Secrets Manager:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q40-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q40-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSM Parameter Store</strong> - AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. SSM Parameter Store cannot be used to automatically rotate the database credentials.</p>\n\n<p><strong>AWS Config</strong> - AWS Config helps with auditing and recording compliance of your AWS resources. It cannot be used to store and automatically rotate the database credentials.</p>\n\n<p><strong>KMS</strong> - AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. KMS cannot be used to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Secrets Manager</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q40-i1.jpg",
        "answer": "",
        "explanation": "Benefits of Secrets Manager:"
      },
      {
        "link": "https://aws.amazon.com/secrets-manager/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>SSM Parameter Store</strong> - AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. SSM Parameter Store cannot be used to automatically rotate the database credentials."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Config</strong> - AWS Config helps with auditing and recording compliance of your AWS resources. It cannot be used to store and automatically rotate the database credentials."
      },
      {
        "answer": "",
        "explanation": "<strong>KMS</strong> - AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. KMS cannot be used to store your secrets securely and automatically rotate the database credentials."
      }
    ],
    "references": [
      "https://aws.amazon.com/secrets-manager/",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html"
    ]
  },
  {
    "id": 21,
    "question": "<p>Your company has deployed an application that will perform a lot of overwrites and deletes on data and require the latest information to be available anytime data is read via queries on database tables.</p>\n\n<p>Which database technology will you recommend?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Simple Storage Service (Amazon S3)</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon ElastiCache</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Relational Database Service (Amazon RDS)</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon Neptune</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Relational Database Service (Amazon RDS)</strong></p>\n\n<p>Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. RDS allows you to create, read, update, and delete records without any item lock or ambiguity. All RDS transactions must be ACID compliant or be Atomic, Consistent, Isolated, and Durable to ensure data integrity.</p>\n\n<p>Atomicity requires that either transaction as a whole is successfully executed or if a part of the transaction fails, then the entire transaction be invalidated. Consistency mandates the data written to the database as part of the transaction must adhere to all defined rules, and restrictions including constraints, cascades, and triggers. Isolation is critical to achieving concurrency control and makes sure each transaction is independent of itself. Durability requires that all of the changes made to the database be permanent once a transaction is completed.\nHence, the best fit is Amazon RDS.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. ElastiCache could work but it's a better fit as a caching technology to enhance reads.</p>\n\n<p><strong>Amazon Simple Storage Service (Amazon S3)</strong> - This option is incorrect as Amazon S3 is not a database technology that supports queries on database tables out of the box. It is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3.</p>\n\n<p>After a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. Amazon S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected. Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects.</p>\n\n<p><strong>Amazon Neptune</strong> - Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency.</p>\n\n<p>Amazon Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune is secure with support for HTTPS encrypted client connections and encryption at rest. Amazon Neptune is fully managed, so you no longer need to worry about database management tasks such as hardware provisioning, software patching, setup, configuration, or backups. Amazon Neptune is a graph database so it's not the best fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/relational-database/\">https://aws.amazon.com/relational-database/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/\">https://aws.amazon.com/rds/</a></p>\n\n<p><a href=\"https://aws.amazon.com/neptune/\">https://aws.amazon.com/neptune/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Relational Database Service (Amazon RDS)</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. RDS allows you to create, read, update, and delete records without any item lock or ambiguity. All RDS transactions must be ACID compliant or be Atomic, Consistent, Isolated, and Durable to ensure data integrity."
      },
      {
        "answer": "",
        "explanation": "Atomicity requires that either transaction as a whole is successfully executed or if a part of the transaction fails, then the entire transaction be invalidated. Consistency mandates the data written to the database as part of the transaction must adhere to all defined rules, and restrictions including constraints, cascades, and triggers. Isolation is critical to achieving concurrency control and makes sure each transaction is independent of itself. Durability requires that all of the changes made to the database be permanent once a transaction is completed.\nHence, the best fit is Amazon RDS."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. ElastiCache could work but it's a better fit as a caching technology to enhance reads."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Storage Service (Amazon S3)</strong> - This option is incorrect as Amazon S3 is not a database technology that supports queries on database tables out of the box. It is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3."
      },
      {
        "answer": "",
        "explanation": "After a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. Amazon S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected. Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Neptune</strong> - Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency."
      },
      {
        "answer": "",
        "explanation": "Amazon Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune is secure with support for HTTPS encrypted client connections and encryption at rest. Amazon Neptune is fully managed, so you no longer need to worry about database management tasks such as hardware provisioning, software patching, setup, configuration, or backups. Amazon Neptune is a graph database so it's not the best fit for the given use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/relational-database/",
      "https://aws.amazon.com/rds/",
      "https://aws.amazon.com/neptune/",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel"
    ]
  },
  {
    "id": 22,
    "question": "<p>An ad-hoc task has to be created to read objects of Apache Parquet format from an Amazon S3 bucket. The task needs to query only one column of the data.</p>\n\n<p>How would you build a solution that involves the LEAST operational overhead?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Run an AWS Glue crawler on the S3 objects to create the schema. Use AWS Glue ETL job to read the required column using an Apache Spark dataframe</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon S3 Select to filter the required column from the contents of the Amazon S3 object</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure an AWS Lambda function to load data from the S3 bucket into a pandas DataFrame. Use SQL SELECT statement to query the required column from the DataFrame</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Run an AWS Glue crawler on the S3 objects to create the schema. Use Amazon Athena SQL queries to select the required column</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 Select to filter the required column from the contents of the Amazon S3 object</strong></p>\n\n<p>With Amazon S3 Select, you can use structured query language (SQL) statements to filter the contents of an Amazon S3 object and retrieve only the subset of data that you need. By using Amazon S3 Select to filter this data, you can reduce the amount of data that Amazon S3 transfers, which reduces the cost and latency to retrieve this data.</p>\n\n<p>Amazon S3 Select only allows you to query one object at a time. It works on an object stored in CSV, JSON, or Apache Parquet format. It also works with an object that is compressed with GZIP or BZIP2 (for CSV and JSON objects only), and a server-side encrypted object. You can specify the format of the results as either CSV or JSON, and you can determine how the records in the result are delimited.</p>\n\n<p>Amazon S3 Select supports only the SELECT SQL command. The following ANSI standard clauses are supported for SELECT:\nSELECT list\nFROM clause\nWHERE clause\nLIMIT clause</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Run an AWS Glue crawler on the S3 objects to create the schema. Use Amazon Athena SQL queries to select the required column</strong> - You can certainly use Amazon Athena to query the required column, however, this option involves more operational overhead of configuring the AWS Glue crawler on the S3 objects to create the schema. So, this option is not the best fit for the given use case.</p>\n\n<p><strong>Configure an AWS Lambda function to load data from the S3 bucket into a pandas DataFrame. Use SQL SELECT statement to query the required column from the DataFrame</strong> - This option involves developing custom code using pandas to meet the requirements of the given use case, which represents unnecessary overhead to build a solution for the given use case.</p>\n\n<p><strong>Run an AWS Glue crawler on the S3 objects to create the schema. Use AWS Glue ETL job to read the required column using an Apache Spark dataframe</strong> - This option involves developing custom code using Apache Spark to meet the requirements of the given use case, which represents unnecessary overhead to build a solution for the given use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Select to filter the required column from the contents of the Amazon S3 object</strong>"
      },
      {
        "answer": "",
        "explanation": "With Amazon S3 Select, you can use structured query language (SQL) statements to filter the contents of an Amazon S3 object and retrieve only the subset of data that you need. By using Amazon S3 Select to filter this data, you can reduce the amount of data that Amazon S3 transfers, which reduces the cost and latency to retrieve this data."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Select only allows you to query one object at a time. It works on an object stored in CSV, JSON, or Apache Parquet format. It also works with an object that is compressed with GZIP or BZIP2 (for CSV and JSON objects only), and a server-side encrypted object. You can specify the format of the results as either CSV or JSON, and you can determine how the records in the result are delimited."
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Select supports only the SELECT SQL command. The following ANSI standard clauses are supported for SELECT:\nSELECT list\nFROM clause\nWHERE clause\nLIMIT clause"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Run an AWS Glue crawler on the S3 objects to create the schema. Use Amazon Athena SQL queries to select the required column</strong> - You can certainly use Amazon Athena to query the required column, however, this option involves more operational overhead of configuring the AWS Glue crawler on the S3 objects to create the schema. So, this option is not the best fit for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure an AWS Lambda function to load data from the S3 bucket into a pandas DataFrame. Use SQL SELECT statement to query the required column from the DataFrame</strong> - This option involves developing custom code using pandas to meet the requirements of the given use case, which represents unnecessary overhead to build a solution for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Run an AWS Glue crawler on the S3 objects to create the schema. Use AWS Glue ETL job to read the required column using an Apache Spark dataframe</strong> - This option involves developing custom code using Apache Spark to meet the requirements of the given use case, which represents unnecessary overhead to build a solution for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html"
    ]
  },
  {
    "id": 23,
    "question": "<p>A company has developed an end-to-end AWS cloud-based Internet-of-Things (IoT) solution that provides customers with integrated IoT functionality in devices including baby monitors, security cameras, and entertainment systems. The company is using Kinesis Data Streams (KDS) to process IoT data from these devices. Multiple consumer applications are using the incoming data streams and the data engineers have noticed a performance lag in the data delivery speed between producers and consumers of the data streams.</p>\n\n<p>Which of the following would you suggest to improve the performance for the given use case?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Swap out Kinesis Data Streams with Kinesis Data Firehose to support the desired read throughput for the downstream applications</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Swap out Kinesis Data Streams with SQS Standard queues to support the desired read throughput for the downstream applications</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Swap out Kinesis Data Streams with SQS FIFO queues to support the desired read throughput for the downstream applications</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Enhanced Fanout feature of Kinesis Data Streams to support the desired read throughput for the downstream applications</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Enhanced Fanout feature of Kinesis Data Streams to support the desired read throughput for the downstream applications</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream.</p>\n\n<p>You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q22-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q22-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a><p></p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2018/08/16/enhanced_fan-out-1024x504.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2018/08/16/enhanced_fan-out-1024x504.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Swap out Kinesis Data Streams with Kinesis Data Firehose to support the desired read throughput for the downstream applications</strong> -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Kinesis Data Firehose can only write to S3, Redshift, Elasticsearch or Splunk. You can't have applications consuming data streams from Kinesis Data Firehose, that's the job of Kinesis Data Streams. Therefore this option is not correct.</p>\n\n<p><strong>Swap out Kinesis Data Streams with SQS Standard queues to support the desired read throughput for the downstream applications</strong></p>\n\n<p><strong>Swap out Kinesis Data Streams with SQS FIFO queues to support the desired read throughput for the downstream applications</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both SQS Standard and SQS FIFO are not the right fit for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q22-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q22-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - https://aws.amazon.com/kinesis/data-streams/faqs/<p></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Enhanced Fanout feature of Kinesis Data Streams to support the desired read throughput for the downstream applications</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream."
      },
      {
        "answer": "",
        "explanation": "You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream."
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/"
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Swap out Kinesis Data Streams with Kinesis Data Firehose to support the desired read throughput for the downstream applications</strong> -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Kinesis Data Firehose can only write to S3, Redshift, Elasticsearch or Splunk. You can't have applications consuming data streams from Kinesis Data Firehose, that's the job of Kinesis Data Streams. Therefore this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Swap out Kinesis Data Streams with SQS Standard queues to support the desired read throughput for the downstream applications</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Swap out Kinesis Data Streams with SQS FIFO queues to support the desired read throughput for the downstream applications</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both SQS Standard and SQS FIFO are not the right fit for the given use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/",
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 24,
    "question": "<p>A company has noticed several provisioned throughput exceptions on its Amazon DynamoDB database due to major spikes in the writes to the database. The development team wants to decouple the application layer from the database layer and dedicate a worker process to writing the data to Amazon DynamoDB.</p>\n\n<p>Which of the following options can scale infinitely and meet these requirements in the most cost-effective way?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Kinesis Data Streams</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon DynamoDB DAX</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Simple Queue Service (Amazon SQS)</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon Simple Notification Service (Amazon SNS)</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Simple Queue Service (Amazon SQS)</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Using Amazon SQS as a middleware will help us sustain the write throughput during write peaks and therefore this option is the best fit for the given use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon DynamoDB DAX</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.</p>\n\n<p>DAX is used for caching reads, not to help with writes. So this option is ruled out.</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service.  KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Kinesis is used to process consistent real-time data and does not scale as cost effectively as SQS to handle spikes in traffic.</p>\n\n<p><strong>Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS won't keep our data if it cannot be delivered, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (Amazon SQS)</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent."
      },
      {
        "answer": "",
        "explanation": "Using Amazon SQS as a middleware will help us sustain the write throughput during write peaks and therefore this option is the best fit for the given use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon DynamoDB DAX</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management."
      },
      {
        "answer": "",
        "explanation": "DAX is used for caching reads, not to help with writes. So this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service.  KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Kinesis is used to process consistent real-time data and does not scale as cost effectively as SQS to handle spikes in traffic."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS won't keep our data if it cannot be delivered, so this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 25,
    "question": "<p>A company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, is served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users.</p>\n\n<p>Which of the following would you suggest as the MOST efficient solution to improve the application performance?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable ElastiCache for both Amazon DynamoDB and Amazon S3</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache for Amazon S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable ElastiCache for DynamoDB and Amazon CloudFront for Amazon S3</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3</strong></p>\n\n<p>Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second.</p>\n\n<p>Amazon DynamoDB Accelerator (DAX) is tightly integrated with Amazon DynamoDB—you simply provision a DAX cluster, use the DAX client SDK to point your existing Amazon DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with Amazon DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache Amazon DynamoDB reads.</p>\n\n<p>Amazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the Amazon S3 bucket where you’ve stored your content.</p>\n\n<p>So, you can use Amazon CloudFront to improve application performance to serve static content from Amazon S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable ElastiCache for DynamoDB and Amazon CloudFront for Amazon S3</strong></p>\n\n<p>Amazon ElastiCache is a blazing-fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.</p>\n\n<p>Amazon ElastiCache Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a><p></p>\n\n<p>Although you can integrate Elasticache with DynamoDB, it's much more involved than using DAX which is a much better fit.</p>\n\n<p><strong>Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache for Amazon S3</strong></p>\n\n<p><strong>Enable ElastiCache for both Amazon DynamoDB and Amazon S3</strong></p>\n\n<p>ElastiCache cannot be used as a cache to serve static content from Amazon S3, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second."
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB Accelerator (DAX) is tightly integrated with Amazon DynamoDB—you simply provision a DAX cluster, use the DAX client SDK to point your existing Amazon DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with Amazon DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache Amazon DynamoDB reads."
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from S3 directly to your users."
      },
      {
        "answer": "",
        "explanation": "When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the Amazon S3 bucket where you’ve stored your content."
      },
      {
        "answer": "",
        "explanation": "So, you can use Amazon CloudFront to improve application performance to serve static content from Amazon S3."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable ElastiCache for DynamoDB and Amazon CloudFront for Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache is a blazing-fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store."
      },
      {
        "image": "https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png",
        "answer": "",
        "explanation": "Amazon ElastiCache Overview:"
      },
      {
        "link": "https://aws.amazon.com/elasticache/redis/"
      },
      {
        "answer": "",
        "explanation": "Although you can integrate Elasticache with DynamoDB, it's much more involved than using DAX which is a much better fit."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache for Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Enable ElastiCache for both Amazon DynamoDB and Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "ElastiCache cannot be used as a cache to serve static content from Amazon S3, so both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/elasticache/redis/",
      "https://aws.amazon.com/dynamodb/dax/",
      "https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/"
    ]
  },
  {
    "id": 26,
    "question": "<p>A company utilizes AWS Step Functions to manage a data pipeline that includes Amazon EMR jobs for data ingestion from various sources and subsequent storage in an Amazon S3 bucket. This pipeline also incorporates EMR jobs that transfer the data to Amazon Redshift. The cloud infrastructure team has manually configured a Step Functions state machine and initiated an EMR cluster within a VPC to facilitate the EMR jobs. However, the Step Functions state machine is currently unable to execute the EMR jobs.</p>\n\n<p>What are the two steps that the company should take to determine the root cause behind the AWS Step Functions state machine's failure to run the EMR jobs? (Select two)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Examine the VPC flow logs to assess whether traffic from the EMR cluster can effectively reach the data providers. Also, check if the security groups associated with the Amazon EMR cluster permit connections to the data source servers through the specified ports</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Ensure that the AWS Step Functions state machine has the necessary IAM permissions to both create and execute the EMR jobs. Additionally, confirm that it has the required IAM permissions to interact with the Amazon S3 buckets utilized by the EMR jobs. To verify the access settings of the S3 buckets, utilize Access Analyzer for Amazon S3</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Ensure that the AWS Step Functions state machine has the necessary IAM permissions to both create and execute the EMR jobs. Additionally, confirm that it has the required IAM permissions to interact with the Amazon S3 buckets utilized by the EMR jobs. To verify the access settings of the S3 buckets, utilize S3 Analytics storage class analysis for Amazon S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Add a Fail state in the AWS Step Functions state machine to handle the failure of the EMR jobs. Address the failure in a Catch block to send an SNS notification to a human user for further action</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Add a Fail state in the AWS Step Functions state machine to handle the failure of the EMR jobs. Address the failure in a Retry block by increasing the number of seconds in the interval between each EMR task</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Ensure that the AWS Step Functions state machine has the necessary IAM permissions to both create and execute the EMR jobs. Additionally, confirm that it has the required IAM permissions to interact with the Amazon S3 buckets utilized by the EMR jobs. To verify the access settings of the S3 buckets, utilize Access Analyzer for Amazon S3</strong></p>\n\n<p>AWS Step Functions can execute code and access AWS resources (such as invoking an AWS Lambda function). To maintain security, you must grant necessary IAM permissions to the Step Functions for accessing the resources. When creating an IAM policy for your state machines to use, the policy should include the permissions that you would like the state machines to assume. You can use an existing AWS managed policy as an example or you can create a custom policy from scratch that meets your specific needs. For the given use case, you need to ensure that the AWS Step Functions state machine has the appropriate permissions to run EMR jobs and access Amazon S3. You can utilize Access Analyzer for Amazon S3 to validate the access settings of the S3 buckets.</p>\n\n<p><strong>Examine the VPC flow logs to assess whether traffic from the EMR cluster can effectively reach the data providers. Also, check if the security groups associated with the Amazon EMR cluster permit connections to the data source through the specified ports</strong></p>\n\n<p>VPC Flow Logs enable you to capture information about the IP traffic going to and from network interfaces in your VPC. After you create a flow log, you can retrieve and view the flow log records in the log group, bucket, or delivery stream that you configured. You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored. Flow log data can be published to the following locations: Amazon CloudWatch Logs, Amazon S3, or Amazon Data Firehose.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q4-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q4-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a><p></p>\n\n<p>For the given use case, you can enable the VPC Flow Logs within the VPC that has the state machine and the EMR jobs. You can then analyze the flow log to validate whether traffic from the EMR cluster can effectively reach the data providers. Additionally, you should also ascertain that the security groups associated with the Amazon EMR cluster permit connections to the data source through the specified ports.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add a Fail state in the AWS Step Functions state machine to handle the failure of the EMR jobs. Address the failure in a Catch block to send an SNS notification to a human user for further action</strong></p>\n\n<p><strong>Add a Fail state in the AWS Step Functions state machine to handle the failure of the EMR jobs. Address the failure in a Retry block by increasing the number of seconds in the interval between each EMR task</strong></p>\n\n<p>A Fail state (\"Type\": \"Fail\") stops the execution of the state machine and marks it as a failure unless it is caught by a Catch block. The state machine enters the fail state upon the failure of the EMR jobs, but it does not point toward the root cause, so both these options are incorrect.</p>\n\n<p><strong>Ensure that the AWS Step Functions state machine has the necessary IAM permissions to both create and execute the EMR jobs. Additionally, confirm that it has the required IAM permissions to interact with the Amazon S3 buckets utilized by the EMR jobs. To verify the access settings of the S3 buckets, utilize S3 Analytics storage class analysis for Amazon S3</strong> - By using Amazon S3 Analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. So, you cannot utilize S3 Analytics storage class analysis to verify the access settings of the S3 buckets.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/procedure-create-iam-role.html\">https://docs.aws.amazon.com/step-functions/latest/dg/procedure-create-iam-role.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/service-integration-iam-templates.html\">https://docs.aws.amazon.com/step-functions/latest/dg/service-integration-iam-templates.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-fail-state.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-fail-state.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html\">https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-man-sec-groups.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-man-sec-groups.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Ensure that the AWS Step Functions state machine has the necessary IAM permissions to both create and execute the EMR jobs. Additionally, confirm that it has the required IAM permissions to interact with the Amazon S3 buckets utilized by the EMR jobs. To verify the access settings of the S3 buckets, utilize Access Analyzer for Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Step Functions can execute code and access AWS resources (such as invoking an AWS Lambda function). To maintain security, you must grant necessary IAM permissions to the Step Functions for accessing the resources. When creating an IAM policy for your state machines to use, the policy should include the permissions that you would like the state machines to assume. You can use an existing AWS managed policy as an example or you can create a custom policy from scratch that meets your specific needs. For the given use case, you need to ensure that the AWS Step Functions state machine has the appropriate permissions to run EMR jobs and access Amazon S3. You can utilize Access Analyzer for Amazon S3 to validate the access settings of the S3 buckets."
      },
      {
        "answer": "",
        "explanation": "<strong>Examine the VPC flow logs to assess whether traffic from the EMR cluster can effectively reach the data providers. Also, check if the security groups associated with the Amazon EMR cluster permit connections to the data source through the specified ports</strong>"
      },
      {
        "answer": "",
        "explanation": "VPC Flow Logs enable you to capture information about the IP traffic going to and from network interfaces in your VPC. After you create a flow log, you can retrieve and view the flow log records in the log group, bucket, or delivery stream that you configured. You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored. Flow log data can be published to the following locations: Amazon CloudWatch Logs, Amazon S3, or Amazon Data Firehose."
      },
      {
        "link": "https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can enable the VPC Flow Logs within the VPC that has the state machine and the EMR jobs. You can then analyze the flow log to validate whether traffic from the EMR cluster can effectively reach the data providers. Additionally, you should also ascertain that the security groups associated with the Amazon EMR cluster permit connections to the data source through the specified ports."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Add a Fail state in the AWS Step Functions state machine to handle the failure of the EMR jobs. Address the failure in a Catch block to send an SNS notification to a human user for further action</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Add a Fail state in the AWS Step Functions state machine to handle the failure of the EMR jobs. Address the failure in a Retry block by increasing the number of seconds in the interval between each EMR task</strong>"
      },
      {
        "answer": "",
        "explanation": "A Fail state (\"Type\": \"Fail\") stops the execution of the state machine and marks it as a failure unless it is caught by a Catch block. The state machine enters the fail state upon the failure of the EMR jobs, but it does not point toward the root cause, so both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Ensure that the AWS Step Functions state machine has the necessary IAM permissions to both create and execute the EMR jobs. Additionally, confirm that it has the required IAM permissions to interact with the Amazon S3 buckets utilized by the EMR jobs. To verify the access settings of the S3 buckets, utilize S3 Analytics storage class analysis for Amazon S3</strong> - By using Amazon S3 Analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. So, you cannot utilize S3 Analytics storage class analysis to verify the access settings of the S3 buckets."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html",
      "https://docs.aws.amazon.com/step-functions/latest/dg/procedure-create-iam-role.html",
      "https://docs.aws.amazon.com/step-functions/latest/dg/service-integration-iam-templates.html",
      "https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-fail-state.html",
      "https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html",
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-man-sec-groups.html"
    ]
  },
  {
    "id": 27,
    "question": "<p>An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report-creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3. The data runs into hundreds of Terabytes and should be available with milliseconds latency.</p>\n\n<p>Which is the MOST cost-effective storage class that you would recommend for this use-case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon S3 Standard-Infrequent Access (S3 Standard-IA)</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon S3 Glacier Deep Archive</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon S3 Standard</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon S3 Standard-Infrequent Access (S3 Standard-IA)</strong></p>\n\n<p>Since the data is accessed only twice in a financial year but needs rapid access when required, the most cost-effective storage class for this use case is Amazon S3 Standard-IA. The S3 Standard-IA storage class is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. Amazon Standard-IA is designed for 99.9% availability compared to 99.99% availability of Amazon S3 Standard. However, the report creation process has failover and retry scenarios built into the workflow, so in case the data is not available owing to the 99.9% availability of Amazon S3 Standard-IA, the job will be auto re-invoked till data is successfully retrieved. Therefore this is the correct option.</p>\n\n<p>Amazon S3 Storage Classes Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q42-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q42-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 Standard</strong> - Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. As described above, Amazon S3 Standard-IA storage is a better fit than Amazon S3 Standard, hence using S3 standard is ruled out for the given use-case.</p>\n\n<p><strong>Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)</strong> - For a small monthly object monitoring and automation charge, Amazon S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers. The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Intelligent-Tiering, with a low per GB storage price and per GB retrieval fee. Moreover, Standard-IA has the same availability as that of Amazon S3 Intelligent-Tiering. So, it's cost-efficient to use S3 Standard-IA instead of S3 Intelligent-Tiering.</p>\n\n<p><strong>Amazon S3 Glacier Deep Archive</strong> - Amazon S3 Glacier Deep Archive is a secure, durable, and low-cost storage class for data archiving. Amazon S3 Glacier Deep Archive does not support millisecond latency, so this option is ruled out.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes\">https://aws.amazon.com/s3/storage-classes</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Standard-Infrequent Access (S3 Standard-IA)</strong>"
      },
      {
        "answer": "",
        "explanation": "Since the data is accessed only twice in a financial year but needs rapid access when required, the most cost-effective storage class for this use case is Amazon S3 Standard-IA. The S3 Standard-IA storage class is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. Amazon Standard-IA is designed for 99.9% availability compared to 99.99% availability of Amazon S3 Standard. However, the report creation process has failover and retry scenarios built into the workflow, so in case the data is not available owing to the 99.9% availability of Amazon S3 Standard-IA, the job will be auto re-invoked till data is successfully retrieved. Therefore this is the correct option."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q42-i1.jpg",
        "answer": "",
        "explanation": "Amazon S3 Storage Classes Overview:"
      },
      {
        "link": "https://aws.amazon.com/s3/storage-classes/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Standard</strong> - Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. As described above, Amazon S3 Standard-IA storage is a better fit than Amazon S3 Standard, hence using S3 standard is ruled out for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)</strong> - For a small monthly object monitoring and automation charge, Amazon S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers. The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Intelligent-Tiering, with a low per GB storage price and per GB retrieval fee. Moreover, Standard-IA has the same availability as that of Amazon S3 Intelligent-Tiering. So, it's cost-efficient to use S3 Standard-IA instead of S3 Intelligent-Tiering."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Glacier Deep Archive</strong> - Amazon S3 Glacier Deep Archive is a secure, durable, and low-cost storage class for data archiving. Amazon S3 Glacier Deep Archive does not support millisecond latency, so this option is ruled out."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://aws.amazon.com/s3/storage-classes"
    ]
  },
  {
    "id": 28,
    "question": "<p>A sports analytics firm uses AWS DynamoDB to store information about user's favorite sports teams and allows the information to be searchable from their home page. The data engineering team at the firm has a requirement wherein all 1 million records in a DynamoDB staging table should be deleted and then re-loaded at 3:00 AM each night.</p>\n\n<p>Which option is an efficient way to delete with minimal costs?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Delete then re-create the table</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Scan and delete items individually</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the purge table option</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Scan and delete items using batch mode</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Delete then re-create the table</strong></p>\n\n<p>The DeleteTable operation deletes a table and all of its items. After a <code>DeleteTable</code> request, the specified table is in the <code>DELETING</code> state until DynamoDB completes the deletion. This is the most cost-efficient option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Scan and delete items using batch mode</strong> - Scan is a very slow operation for 10 million items and this is not the best-fit option for the given use case.</p>\n\n<p><strong>Scan and delete items individually</strong> - Scan is a very slow operation for 10 million items and this is not the best-fit option for the given use case.</p>\n\n<p><strong>Use the purge table option</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/dynamodb/delete-table.html\">https://docs.aws.amazon.com/cli/latest/reference/dynamodb/delete-table.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Delete then re-create the table</strong>"
      },
      {
        "answer": "",
        "explanation": "The DeleteTable operation deletes a table and all of its items. After a <code>DeleteTable</code> request, the specified table is in the <code>DELETING</code> state until DynamoDB completes the deletion. This is the most cost-efficient option."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Scan and delete items using batch mode</strong> - Scan is a very slow operation for 10 million items and this is not the best-fit option for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Scan and delete items individually</strong> - Scan is a very slow operation for 10 million items and this is not the best-fit option for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the purge table option</strong> - This is a made-up option and has been added as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/cli/latest/reference/dynamodb/delete-table.html"
    ]
  },
  {
    "id": 29,
    "question": "<p>To improve the performance and security of the application, the data engineering team at a company has created an Amazon CloudFront distribution with an Application Load Balancer as the custom origin. The team has also set up an AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution. The security team at the company has noticed a surge in malicious attacks from a specific IP address to steal sensitive data stored on Amazon EC2 instances.</p>\n\n<p>Which of the following actions would you recommend to stop the attacks?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a deny rule for the malicious IP in the Security Groups associated with each of the instances</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a ticket with AWS support to take action against the malicious IP</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an IP match condition in the AWS WAF to block the malicious IP address</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IP match condition in the AWS WAF to block the malicious IP address</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.</p>\n\n<p>How AWS WAF Works:\n<img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a><p></p>\n\n<p>If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from. So, this option is correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances</strong> - Network access control list (network ACL) are not associated with instances. So this option is also ruled out.</p>\n\n<p><strong>Create a deny rule for the malicious IP in the Security Groups associated with each of the instances</strong> - You cannot create deny rules in Security Groups. So this option is ruled out.</p>\n\n<p><strong>Create a ticket with AWS support to take action against the malicious IP</strong> - Managing the security of your application is your responsibility, not that of AWS, so you cannot raise a ticket for this issue.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html\">https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an IP match condition in the AWS WAF to block the malicious IP address</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define."
      },
      {
        "image": "https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png",
        "answer": "",
        "explanation": "How AWS WAF Works:"
      },
      {
        "link": "https://aws.amazon.com/waf/"
      },
      {
        "answer": "",
        "explanation": "If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from. So, this option is correct."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances</strong> - Network access control list (network ACL) are not associated with instances. So this option is also ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a deny rule for the malicious IP in the Security Groups associated with each of the instances</strong> - You cannot create deny rules in Security Groups. So this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a ticket with AWS support to take action against the malicious IP</strong> - Managing the security of your application is your responsibility, not that of AWS, so you cannot raise a ticket for this issue."
      }
    ],
    "references": [
      "https://aws.amazon.com/waf/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html"
    ]
  },
  {
    "id": 30,
    "question": "<p>A digital media company has hired you to improve the data backup solution for applications running on the AWS Cloud. Currently, all of the applications running on AWS use at least two Availability Zones (AZs). The updated backup policy at the company mandates that all nightly backups for its data are durably stored in at least two geographically distinct Regions for Production and Disaster Recovery (DR) and the backup processes for both Regions must be fully automated. The new backup solution must ensure that the backup is available to be restored immediately for the Production Region and should be restored within 24 hours in the DR Region.</p>\n\n<p>Which of the following represents the MOST cost-effective solution that will address the given use-case?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a backup process to persist all the data to an S3 bucket A using the S3 standard storage class in the Production Region. Set up cross-Region replication of this S3 bucket A to an S3 bucket B using S3 standard-IA storage class in the DR Region and set up a lifecycle policy in the DR Region to immediately move this data to Amazon Glacier Deep Archive</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a backup process to persist all the data to Amazon Glacier Deep Archive in the Production Region. Set up cross-Region replication of this data to Amazon Glacier Deep Archive in the DR Region to ensure minimum possible costs in both Regions</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a backup process to persist all the data to an S3 bucket A using the S3 standard storage class in the Production Region. Set up cross-Region replication of this S3 bucket A to an S3 bucket B using S3 standard storage class in the DR Region and set up a lifecycle policy in the DR Region to immediately move this data to Amazon Glacier Deep Archive</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create a backup process to persist all the data to a large Amazon EBS volume attached to the backup server in the Production Region. Run nightly cron jobs to snapshot these volumes and then copy these snapshots to the DR Region</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a backup process to persist all the data to an S3 bucket A using the S3 standard storage class in the Production Region. Set up cross-Region replication of this S3 bucket A to an S3 bucket B using S3 standard storage class in the DR Region and set up a lifecycle policy in the DR Region to immediately move this data to Amazon Glacier Deep Archive</strong></p>\n\n<p>S3 Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. There are two types of Replications:</p>\n\n<p>Cross-Region Replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions.</p>\n\n<p>Same-Region replication (SRR) is used to copy objects across Amazon S3 buckets in the same AWS Region.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q23-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q23-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html</a><p></p>\n\n<p>For the given use-case, you can set up cross-Region replication from S3 bucket A using the S3 standard storage class in the production Region to S3 bucket B using the S3 standard storage class in the DR Region and further create a lifecycle policy to transition this data in bucket B from standard storage class to Amazon Glacier Deep Archive.</p>\n\n<p>Please note the allowed transitions for the S3 Lifecycle Policy:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q23-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q23-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a><p></p>\n\n<p>By default, Amazon S3 stores object replicas using the same storage class as the source object. You can also specify a different storage class for the replicas. This allows you to use something like an S3 Standard-IA for the replica bucket, however, S3 standard IA has a minimum storage duration charge of 30 days thereby making it costlier than using S3 Standard storage class for the given scenario because the data would be moved to Glacier Deep Archive via a Lifecycle policy immediately.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q23-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q23-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a backup process to persist all the data to an S3 bucket A using the S3 standard storage class in the Production Region. Set up cross-Region replication of this S3 bucket A to an S3 bucket B using S3 standard-IA storage class in the DR Region and set up a lifecycle policy in the DR Region to immediately move this data to Amazon Glacier Deep Archive</strong> - As mentioned in the explanation above, S3 standard IA has a minimum storage duration charge of 30 days thereby making it costlier than using S3 Standard storage class for the given scenario, so this option is incorrect.</p>\n\n<p><strong>Create a backup process to persist all the data to Amazon Glacier Deep Archive in the Production Region. Set up cross-Region replication of this data to Amazon Glacier Deep Archive in the DR Region to ensure minimum possible costs in both Regions</strong> - One of the key requirements of the given scenario is to ensure that the backup is available to be restored immediately for the Production Region. However, Glacier Deep Archive has a first-byte latency of hours while restoring data, hence this option is not correct for the given use-case.</p>\n\n<p><strong>Create a backup process to persist all the data to a large Amazon EBS volume attached to the backup server in the Production Region. Run nightly cron jobs to snapshot these volumes and then copy these snapshots to the DR Region</strong> - One of the key requirements of the given scenario is to ensure that the backup is durable but the data in an EBS volume is only replicated within its Availability Zone so it is not highly durable. However, the EBS snapshots are stored on Amazon S3, which is durable. The issue with this option is that it introduces the additional cost of an EBS volume and also does not optimize the storage cost in the DR Region as it does not leverage Glacier Deep Archive for the backup data storage.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a backup process to persist all the data to an S3 bucket A using the S3 standard storage class in the Production Region. Set up cross-Region replication of this S3 bucket A to an S3 bucket B using S3 standard storage class in the DR Region and set up a lifecycle policy in the DR Region to immediately move this data to Amazon Glacier Deep Archive</strong>"
      },
      {
        "answer": "",
        "explanation": "S3 Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. There are two types of Replications:"
      },
      {
        "answer": "",
        "explanation": "Cross-Region Replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions."
      },
      {
        "answer": "",
        "explanation": "Same-Region replication (SRR) is used to copy objects across Amazon S3 buckets in the same AWS Region."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html"
      },
      {
        "answer": "",
        "explanation": "For the given use-case, you can set up cross-Region replication from S3 bucket A using the S3 standard storage class in the production Region to S3 bucket B using the S3 standard storage class in the DR Region and further create a lifecycle policy to transition this data in bucket B from standard storage class to Amazon Glacier Deep Archive."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q23-i2.jpg",
        "answer": "",
        "explanation": "Please note the allowed transitions for the S3 Lifecycle Policy:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html"
      },
      {
        "answer": "",
        "explanation": "By default, Amazon S3 stores object replicas using the same storage class as the source object. You can also specify a different storage class for the replicas. This allows you to use something like an S3 Standard-IA for the replica bucket, however, S3 standard IA has a minimum storage duration charge of 30 days thereby making it costlier than using S3 Standard storage class for the given scenario because the data would be moved to Glacier Deep Archive via a Lifecycle policy immediately."
      },
      {
        "link": "https://aws.amazon.com/s3/storage-classes/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a backup process to persist all the data to an S3 bucket A using the S3 standard storage class in the Production Region. Set up cross-Region replication of this S3 bucket A to an S3 bucket B using S3 standard-IA storage class in the DR Region and set up a lifecycle policy in the DR Region to immediately move this data to Amazon Glacier Deep Archive</strong> - As mentioned in the explanation above, S3 standard IA has a minimum storage duration charge of 30 days thereby making it costlier than using S3 Standard storage class for the given scenario, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a backup process to persist all the data to Amazon Glacier Deep Archive in the Production Region. Set up cross-Region replication of this data to Amazon Glacier Deep Archive in the DR Region to ensure minimum possible costs in both Regions</strong> - One of the key requirements of the given scenario is to ensure that the backup is available to be restored immediately for the Production Region. However, Glacier Deep Archive has a first-byte latency of hours while restoring data, hence this option is not correct for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a backup process to persist all the data to a large Amazon EBS volume attached to the backup server in the Production Region. Run nightly cron jobs to snapshot these volumes and then copy these snapshots to the DR Region</strong> - One of the key requirements of the given scenario is to ensure that the backup is durable but the data in an EBS volume is only replicated within its Availability Zone so it is not highly durable. However, the EBS snapshots are stored on Amazon S3, which is durable. The issue with this option is that it introduces the additional cost of an EBS volume and also does not optimize the storage cost in the DR Region as it does not leverage Glacier Deep Archive for the backup data storage."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html",
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "id": 31,
    "question": "<p>A media company uses an ad-hoc Kinesis Firehose-based solution to ingest raw data in JSON format and then deliver it to an Amazon S3 bucket. The data engineering team at the company uses Apache Spark SQL to analyze this data via Amazon EMR, which is configured to use AWS Glue Data Catalog as the metastore. An AWS Glue crawler runs every four hours to update the schema of the data catalog. The team has noticed that it sometimes obtains outdated data. You have been hired by the company as an AWS Certified Data Engineer Associate to build a solution for ensuring that the team always has access to the current data.</p>\n\n<p>Which of the following represents the best solution to meet this requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon CloudWatch Events with the rate (5 minutes) expression to execute the AWS Glue crawler every 5 minutes</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Invoke the AWS Glue crawler via an AWS Lambda function that is triggered by an S3:ObjectCreated:* event notification on the S3 bucket</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Modify the execution schedule of the AWS Glue crawler from 4 hours to 1 minute</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Athena to directly analyze the current data in Amazon S3</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Invoke the AWS Glue crawler via an AWS Lambda function that is triggered by an S3:ObjectCreated:* event notification on the S3 bucket</strong></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q20-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q20-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a><p></p>\n\n<p>An AWS Glue crawler is a program that connects to a data store (source or target) such as Amazon S3, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in the AWS Glue Data Catalog. You can use a Lambda function that is triggered by an S3:ObjectCreated:* event notification on the S3 bucket to invoke the AWS Glue crawler on-demand. This obviates the need to periodically run the crawler on a schedule to update the new data in the existing data catalog tables.</p>\n\n<p>Data Lake Architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q20-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q20-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/\">https://aws.amazon.com/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon CloudWatch Events with the rate (5 minutes) expression to execute the AWS Glue crawler every 5 minutes</strong> - CloudWatch Events do not support AWS Glue crawler as a destination type, so this option just acts as a distractor.</p>\n\n<p><strong>Modify the execution schedule of the AWS Glue crawler from 4 hours to 1 minute</strong> - For AWS Glue crawlers, the minimum precision for a schedule is 5 minutes. So this option just acts as a distractor.</p>\n\n<p><strong>Use Amazon Athena to directly analyze the current data in Amazon S3</strong> - The use case mentions that the analysis must be done by using Apache Spark SQL on Amazon EMR, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html\">https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/\">https://aws.amazon.com/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Invoke the AWS Glue crawler via an AWS Lambda function that is triggered by an S3:ObjectCreated:* event notification on the S3 bucket</strong>"
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html"
      },
      {
        "answer": "",
        "explanation": "An AWS Glue crawler is a program that connects to a data store (source or target) such as Amazon S3, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in the AWS Glue Data Catalog. You can use a Lambda function that is triggered by an S3:ObjectCreated:* event notification on the S3 bucket to invoke the AWS Glue crawler on-demand. This obviates the need to periodically run the crawler on a schedule to update the new data in the existing data catalog tables."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q20-i2.jpg",
        "answer": "",
        "explanation": "Data Lake Architecture:"
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon CloudWatch Events with the rate (5 minutes) expression to execute the AWS Glue crawler every 5 minutes</strong> - CloudWatch Events do not support AWS Glue crawler as a destination type, so this option just acts as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Modify the execution schedule of the AWS Glue crawler from 4 hours to 1 minute</strong> - For AWS Glue crawlers, the minimum precision for a schedule is 5 minutes. So this option just acts as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Athena to directly analyze the current data in Amazon S3</strong> - The use case mentions that the analysis must be done by using Apache Spark SQL on Amazon EMR, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html",
      "https://aws.amazon.com/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/",
      "https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html"
    ]
  },
  {
    "id": 32,
    "question": "<p>An e-commerce company performs analytics on the company's data using the Amazon Redshift cluster. The Redshift cluster has two important tables: the orders table and the product table which have millions of rows each. A few small tables with supporting data are also present. The team is looking for the right distribution patterns for the tables, to optimize query speed.</p>\n\n<p>Which of the following are the key points to consider while planning for the best distribution style for your data? (Select two)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Choose a column with low cardinality in the filtered result set</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Small Dimension tables should be marked to use KEY distribution style, which will cause them to be replicated to each physical node in the cluster</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>If a dimension table cannot be collocated with the fact table or other important joining tables, use ALL distribution style for such tables</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>A fact table with multiple distribution keys is useful when multiple dimension tables have to be joined to it</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Data should be distributed in such a way that the rows that participate in joins are already collocated on the nodes with their joining rows in other tables</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Data should be distributed in such a way that the rows that participate in joins are already collocated on the nodes with their joining rows in other tables</strong></p>\n\n<p>A prime goal of data distribution is to minimize data movement when queries run. Therefore, if the rows that participate in joins or aggregates are already collocated on the nodes with their joining rows in other tables, the optimizer doesn't need to redistribute as much data when queries run.</p>\n\n<p><strong>If a dimension table cannot be collocated with the fact table or other important joining tables, use ALL distribution style for such tables</strong></p>\n\n<p>When you execute a query, the query optimizer redistributes the rows to the compute nodes as needed to perform any joins and aggregations. The goal in selecting a table distribution style is to minimize the impact of the redistribution step by locating the data where it needs to be before the query is run.</p>\n\n<p>One of the best approaches is to change some dimension tables to use ALL distribution. If a dimension table cannot be collocated with the fact table or other important joining tables, you can improve query performance significantly by distributing the entire table to all of the nodes. Using ALL distribution multiplies storage space requirements and increases load times and maintenance operations, so you should weigh all factors before choosing ALL distribution.</p>\n\n<p>Choosing the best distribution style:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q32-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q32-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Choose a column with low cardinality in the filtered result set</strong> - This is incorrect. A column with high cardinality should be considered in the filtered result set. If you distribute a sales table on a date column, for example, you should probably get fairly even data distribution, unless most of your sales are seasonal. However, if you commonly use a range-restricted predicate to filter for a narrow date period, most of the filtered rows occur on a limited set of slices and the query workload is skewed.</p>\n\n<p><strong>A fact table with multiple distribution keys is useful when multiple dimension tables have to be joined to it</strong> - Your fact table can have only one distribution key. Any tables that join on another key aren't collocated with the fact table. Choose one dimension to collocate based on how frequently it is joined and the size of the joining rows.</p>\n\n<p><strong>Small Dimension tables should be marked to use KEY distribution style, which will cause them to be replicated to each physical node in the cluster</strong> - In a KEY distribution style, the rows are distributed according to the values in one column. The leader node places matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns. This way, matching values from the common columns are physically stored together.</p>\n\n<p>A copy of the entire table is distributed to every node in ALL distribution styles.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Data should be distributed in such a way that the rows that participate in joins are already collocated on the nodes with their joining rows in other tables</strong>"
      },
      {
        "answer": "",
        "explanation": "A prime goal of data distribution is to minimize data movement when queries run. Therefore, if the rows that participate in joins or aggregates are already collocated on the nodes with their joining rows in other tables, the optimizer doesn't need to redistribute as much data when queries run."
      },
      {
        "answer": "",
        "explanation": "<strong>If a dimension table cannot be collocated with the fact table or other important joining tables, use ALL distribution style for such tables</strong>"
      },
      {
        "answer": "",
        "explanation": "When you execute a query, the query optimizer redistributes the rows to the compute nodes as needed to perform any joins and aggregations. The goal in selecting a table distribution style is to minimize the impact of the redistribution step by locating the data where it needs to be before the query is run."
      },
      {
        "answer": "",
        "explanation": "One of the best approaches is to change some dimension tables to use ALL distribution. If a dimension table cannot be collocated with the fact table or other important joining tables, you can improve query performance significantly by distributing the entire table to all of the nodes. Using ALL distribution multiplies storage space requirements and increases load times and maintenance operations, so you should weigh all factors before choosing ALL distribution."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q32-i1.jpg",
        "answer": "",
        "explanation": "Choosing the best distribution style:"
      },
      {
        "link": "https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Choose a column with low cardinality in the filtered result set</strong> - This is incorrect. A column with high cardinality should be considered in the filtered result set. If you distribute a sales table on a date column, for example, you should probably get fairly even data distribution, unless most of your sales are seasonal. However, if you commonly use a range-restricted predicate to filter for a narrow date period, most of the filtered rows occur on a limited set of slices and the query workload is skewed."
      },
      {
        "answer": "",
        "explanation": "<strong>A fact table with multiple distribution keys is useful when multiple dimension tables have to be joined to it</strong> - Your fact table can have only one distribution key. Any tables that join on another key aren't collocated with the fact table. Choose one dimension to collocate based on how frequently it is joined and the size of the joining rows."
      },
      {
        "answer": "",
        "explanation": "<strong>Small Dimension tables should be marked to use KEY distribution style, which will cause them to be replicated to each physical node in the cluster</strong> - In a KEY distribution style, the rows are distributed according to the values in one column. The leader node places matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns. This way, matching values from the common columns are physically stored together."
      },
      {
        "answer": "",
        "explanation": "A copy of the entire table is distributed to every node in ALL distribution styles."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html"
    ]
  },
  {
    "id": 33,
    "question": "<p>The data engineering team at a leading gaming company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data.</p>\n\n<p>Which of the following solutions would you recommend? (Select two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Develop the leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Develop the leaderboard using RDS Aurora as it meets the in-memory, high availability, low latency requirements</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Develop the leaderboard using ElastiCache Redis as it meets the in-memory, high availability, low latency requirements</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Develop the leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Develop the leaderboard using AWS Neptune as it meets the in-memory, high availability, low latency requirements</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Develop the leaderboard using ElastiCache Redis as it meets the in-memory, high availability, low latency requirements</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.\nElastiCache for Redis can be used to power the live leaderboard, so this option is correct.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a><p></p>\n\n<p><strong>Develop the leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. So DynamoDB with DAX can be used to power the live leaderboard.</p>\n\n<p>DAX Overview:\n<img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Develop the leaderboard using AWS Neptune as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Neptune is not an in-memory database, so this option is not correct.</p>\n\n<p><strong>Develop the leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements</strong> - DynamoDB is not an in-memory database, so this option is not correct.</p>\n\n<p><strong>Develop the leaderboard using RDS Aurora as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Develop the leaderboard using ElastiCache Redis as it meets the in-memory, high availability, low latency requirements</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.\nElastiCache for Redis can be used to power the live leaderboard, so this option is correct."
      },
      {
        "image": "https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png",
        "answer": "",
        "explanation": "ElastiCache for Redis Overview:"
      },
      {
        "link": "https://aws.amazon.com/elasticache/redis/"
      },
      {
        "answer": "",
        "explanation": "<strong>Develop the leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications."
      },
      {
        "answer": "",
        "explanation": "DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. So DynamoDB with DAX can be used to power the live leaderboard."
      },
      {
        "image": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png",
        "answer": "",
        "explanation": "DAX Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Develop the leaderboard using AWS Neptune as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Neptune is not an in-memory database, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Develop the leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements</strong> - DynamoDB is not an in-memory database, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Develop the leaderboard using RDS Aurora as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database, so this option is not correct."
      }
    ],
    "references": [
      "https://aws.amazon.com/elasticache/redis/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html",
      "https://aws.amazon.com/elasticache/",
      "https://aws.amazon.com/dynamodb/dax/"
    ]
  },
  {
    "id": 34,
    "question": "<p>You are a data engineer working for an IT company. You have been tasked with building a reporting application that includes dashboards for data visualization. You are provisioning your AWS DynamoDB table and need to perform 10 strongly consistent reads per second of 8 KB in size each.</p>\n\n<p>How many Read Capacity Units (RCUs) are needed?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>10</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>40</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>20</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>5</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q39-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q39-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q39-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q39-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>20</strong></p>\n\n<p>One Read Capacity Unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of Read Capacity Units required depends on the item size, and whether you want an eventually consistent or strongly consistent read.</p>\n\n<p>1) Item Size / 4KB, rounding to the nearest whole number.</p>\n\n<p>So, in the above case, 8KB / 4 KB = 2 read capacity units per item.</p>\n\n<p>2) 2 read capacity units per item (since strongly consistent read) × No of reads per second</p>\n\n<p>So, in the above case, 2 x 10 = 20 read capacity units.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>40</strong></p>\n\n<p><strong>10</strong></p>\n\n<p><strong>5</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Before proceeding with the calculations, please review the following:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html",
        "answer": "",
        "explanation": "via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a>"
      },
      {
        "answer": "",
        "explanation": "<strong>20</strong>"
      },
      {
        "answer": "",
        "explanation": "One Read Capacity Unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of Read Capacity Units required depends on the item size, and whether you want an eventually consistent or strongly consistent read."
      },
      {
        "answer": "",
        "explanation": "1) Item Size / 4KB, rounding to the nearest whole number."
      },
      {
        "answer": "",
        "explanation": "So, in the above case, 8KB / 4 KB = 2 read capacity units per item."
      },
      {
        "answer": "",
        "explanation": "2) 2 read capacity units per item (since strongly consistent read) × No of reads per second"
      },
      {
        "answer": "",
        "explanation": "So, in the above case, 2 x 10 = 20 read capacity units."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>40</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>10</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>5</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the details provided in the explanation above, so these are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html"
    ]
  },
  {
    "id": 35,
    "question": "<p>For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints.</p>\n\n<p>Which of the following services would you suggest for this requirement? (Select two)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon DynamoDB</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon Simple Notification Service (Amazon SNS)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Kinesis</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon S3</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Amazon Simple Queue Service (Amazon SQS)</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Amazon S3</strong></p>\n\n<p><strong>Amazon DynamoDB</strong></p>\n\n<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p>\n\n<p>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p>\n\n<p>There are two types of VPC endpoints: Interface Endpoints and Gateway Endpoints. An Interface Endpoint is an Elastic Network Interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.</p>\n\n<p>A Gateway Endpoint is a gateway that you specify as a target for a route in your route table for traffic destined for a supported AWS service. The following AWS services are supported: Amazon S3 and Amazon DynamoDB.</p>\n\n<p>You can use two types of VPC endpoints to access Amazon S3: gateway endpoints and interface endpoints. A gateway endpoint is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network. Interface endpoints extend the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on-premises, or from a VPC in another AWS Region using VPC peering or AWS Transit Gateway.</p>\n\n<p>Gateway VPC endpoints:\n<img src=\"https://docs.aws.amazon.com/images/vpc/latest/privatelink/images/gateway-endpoints.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/images/vpc/latest/privatelink/images/gateway-endpoints.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Queue Service (Amazon SQS)</strong></p>\n\n<p><strong>Amazon Simple Notification Service (Amazon SNS)</strong></p>\n\n<p><strong>Amazon Kinesis</strong></p>\n\n<p>As mentioned in the description above, these three options use interface endpoints, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon DynamoDB</strong>"
      },
      {
        "answer": "",
        "explanation": "A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network."
      },
      {
        "answer": "",
        "explanation": "Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic."
      },
      {
        "answer": "",
        "explanation": "There are two types of VPC endpoints: Interface Endpoints and Gateway Endpoints. An Interface Endpoint is an Elastic Network Interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service."
      },
      {
        "answer": "",
        "explanation": "A Gateway Endpoint is a gateway that you specify as a target for a route in your route table for traffic destined for a supported AWS service. The following AWS services are supported: Amazon S3 and Amazon DynamoDB."
      },
      {
        "answer": "",
        "explanation": "You can use two types of VPC endpoints to access Amazon S3: gateway endpoints and interface endpoints. A gateway endpoint is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network. Interface endpoints extend the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on-premises, or from a VPC in another AWS Region using VPC peering or AWS Transit Gateway."
      },
      {
        "image": "https://docs.aws.amazon.com/images/vpc/latest/privatelink/images/gateway-endpoints.png",
        "answer": "",
        "explanation": "Gateway VPC endpoints:"
      },
      {
        "link": "https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (Amazon SQS)</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Notification Service (Amazon SNS)</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis</strong>"
      },
      {
        "answer": "",
        "explanation": "As mentioned in the description above, these three options use interface endpoints, so these are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html"
    ]
  },
  {
    "id": 36,
    "question": "<p>An IT company needs to set up a data lake on Amazon S3 for a healthcare client. The data lake is split into raw and curated zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue-based ETL job into the curated zone. The data engineering team runs ad-hoc queries only on the data in the curated zone using Athena. The team is concerned about the cost of data storage in both the raw and curated zones as the data is increasing at a rate of 2 TB daily in each zone.</p>\n\n<p>Which of the following options would you implement together as the MOST cost-optimal solution? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Glue ETL job to write the transformed data in the curated zone using CSV format</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Glue ETL job to write the transformed data in the curated zone using a compressed file format</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a Lambda function based job to delete the raw zone data after 1 day</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Setup a lifecycle policy to transition the curated zone data into Glacier Deep Archive after 1 day of object creation</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Setup a lifecycle policy to transition the raw zone data into Glacier Deep Archive after 1 day of object creation</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Setup a lifecycle policy to transition the raw zone data into Glacier Deep Archive after 1 day of object creation</strong></p>\n\n<p>You can manage your objects so that they are stored cost-effectively throughout their lifecycle by configuring their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them.</p>\n\n<p>For the given use-case, the raw zone consists of the source data, so it cannot be deleted due to compliance reasons. Therefore, you should use a lifecycle policy to transition the raw zone data into Glacier Deep Archive after 1 day of object creation.</p>\n\n<p>Please read more about S3 Object Lifecycle Management:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q25-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q25-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a><p></p>\n\n<p><strong>Use Glue ETL job to write the transformed data in the curated zone using a compressed file format</strong></p>\n\n<p>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics.</p>\n\n<p>You cannot transition the curated zone data into Glacier Deep Archive because it is used by business analysts for ad-hoc querying. Therefore, the best optimization is to have the curated zone data stored in a compressed format via the Glue job. The compressed data would reduce the storage cost incurred on the data in the curated zone.</p>\n\n<p>Please see this example for a Glue ETL Pipeline:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Glue_Event-driven-ETL-Pipelines.e24d59bb79a9e24cdba7f43ffd234ec0482a60e2.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Glue_Event-driven-ETL-Pipelines.e24d59bb79a9e24cdba7f43ffd234ec0482a60e2.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lambda function-based job to delete the raw zone data after 1 day</strong> - As mentioned in the use case, the source data needs to be kept for a minimum of 5 years for compliance reasons. Therefore the data in the raw zone cannot be deleted after 1 day.</p>\n\n<p><strong>Setup a lifecycle policy to transition the curated zone data into Glacier Deep Archive after 1 day of object creation</strong> - You cannot transition the curated zone data into Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Hence this option is incorrect.</p>\n\n<p><strong>Use Glue ETL job to write the transformed data in the curated zone using CSV format</strong> - It is cost-optimal to write the data in the curated zone using a compressed format instead of CSV format. The compressed data would reduce the storage cost incurred on the data in the curated zone. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Setup a lifecycle policy to transition the raw zone data into Glacier Deep Archive after 1 day of object creation</strong>"
      },
      {
        "answer": "",
        "explanation": "You can manage your objects so that they are stored cost-effectively throughout their lifecycle by configuring their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them."
      },
      {
        "answer": "",
        "explanation": "For the given use-case, the raw zone consists of the source data, so it cannot be deleted due to compliance reasons. Therefore, you should use a lifecycle policy to transition the raw zone data into Glacier Deep Archive after 1 day of object creation."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q25-i1.jpg",
        "answer": "",
        "explanation": "Please read more about S3 Object Lifecycle Management:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Use Glue ETL job to write the transformed data in the curated zone using a compressed file format</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics."
      },
      {
        "answer": "",
        "explanation": "You cannot transition the curated zone data into Glacier Deep Archive because it is used by business analysts for ad-hoc querying. Therefore, the best optimization is to have the curated zone data stored in a compressed format via the Glue job. The compressed data would reduce the storage cost incurred on the data in the curated zone."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Glue_Event-driven-ETL-Pipelines.e24d59bb79a9e24cdba7f43ffd234ec0482a60e2.png",
        "answer": "",
        "explanation": "Please see this example for a Glue ETL Pipeline:"
      },
      {
        "link": "https://aws.amazon.com/glue/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a Lambda function-based job to delete the raw zone data after 1 day</strong> - As mentioned in the use case, the source data needs to be kept for a minimum of 5 years for compliance reasons. Therefore the data in the raw zone cannot be deleted after 1 day."
      },
      {
        "answer": "",
        "explanation": "<strong>Setup a lifecycle policy to transition the curated zone data into Glacier Deep Archive after 1 day of object creation</strong> - You cannot transition the curated zone data into Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Glue ETL job to write the transformed data in the curated zone using CSV format</strong> - It is cost-optimal to write the data in the curated zone using a compressed format instead of CSV format. The compressed data would reduce the storage cost incurred on the data in the curated zone. So, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html",
      "https://aws.amazon.com/glue/"
    ]
  },
  {
    "id": 37,
    "question": "<p>A data engineer has been tasked to design a low-latency solution for a static, single-page application, accessed by users through a custom domain name. The solution must be serverless, provide in-transit data encryption and needs to be cost-effective.</p>\n\n<p>Which AWS services can be combined to build the simplest possible solution for the company's requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Host the application on Amazon EC2 instance with instance store volume for high performance and low latency access to users</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure Amazon S3 to store the static data and use AWS Fargate for hosting the application</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 to host the static website and Amazon CloudFront to distribute the content for low latency access</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Host the application on Amazon Elastic Container Service (Amazon ECS) using Fargate launch type and front it with Elastic Load Balancing for an improved performance</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 to host the static website and Amazon CloudFront to distribute the content for low latency access</strong></p>\n\n<p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you must enable website hosting, set permissions, and create and add an index document. Depending on your website requirements, you can also configure redirects, web traffic logging, and a custom error document.</p>\n\n<p>After you configure your bucket as a static website, you can access the bucket through the AWS Region-specific Amazon S3 website endpoints for your bucket. Website endpoints are different from the endpoints where you send REST API requests. Amazon S3 doesn't support HTTPS access for website endpoints. If you want to use HTTPS, you can use CloudFront to serve a static website hosted on Amazon S3.</p>\n\n<p>You can use Amazon CloudFront to improve the performance of your website. CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (called edge locations). When a visitor requests a file from your website, Amazon CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away.</p>\n\n<p>Amazon CloudFront caches content at edge locations for a period of time that you specify. If a visitor requests content that has been cached for longer than the expiration date, Amazon CloudFront checks the origin server to see if a newer version of the content is available. If a newer version is available, Amazon CloudFront copies the new version to the edge location. Changes that you make to the original content are replicated to edge locations as visitors request the content.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Host the application on Amazon EC2 instance with instance store volume for high performance and low latency access to users</strong> - Since the use case speaks about a serverless solution, Amazon EC2 cannot be the answer, since Amazon EC2 is not serverless.</p>\n\n<p><strong>Host the application on Amazon Elastic Container Service (Amazon ECS) using Fargate launch type and front it with Elastic Load Balancing for an improved performance</strong> - AWS Fargate is a serverless compute engine for containers that work with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Elastic Load Balancing can spread the incoming requests across a fleet of Amazon EC2 instances. This additional complexity is not needed since we are looking at a static single-page webpage.</p>\n\n<p><strong>Configure Amazon S3 to store the static data and use AWS Fargate for hosting the application</strong> - Using Amazon ECS with Fargate is an overkill for hosting a static single-page webpage.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 to host the static website and Amazon CloudFront to distribute the content for low latency access</strong>"
      },
      {
        "answer": "",
        "explanation": "To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you must enable website hosting, set permissions, and create and add an index document. Depending on your website requirements, you can also configure redirects, web traffic logging, and a custom error document."
      },
      {
        "answer": "",
        "explanation": "After you configure your bucket as a static website, you can access the bucket through the AWS Region-specific Amazon S3 website endpoints for your bucket. Website endpoints are different from the endpoints where you send REST API requests. Amazon S3 doesn't support HTTPS access for website endpoints. If you want to use HTTPS, you can use CloudFront to serve a static website hosted on Amazon S3."
      },
      {
        "answer": "",
        "explanation": "You can use Amazon CloudFront to improve the performance of your website. CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (called edge locations). When a visitor requests a file from your website, Amazon CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away."
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront caches content at edge locations for a period of time that you specify. If a visitor requests content that has been cached for longer than the expiration date, Amazon CloudFront checks the origin server to see if a newer version of the content is available. If a newer version is available, Amazon CloudFront copies the new version to the edge location. Changes that you make to the original content are replicated to edge locations as visitors request the content."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Host the application on Amazon EC2 instance with instance store volume for high performance and low latency access to users</strong> - Since the use case speaks about a serverless solution, Amazon EC2 cannot be the answer, since Amazon EC2 is not serverless."
      },
      {
        "answer": "",
        "explanation": "<strong>Host the application on Amazon Elastic Container Service (Amazon ECS) using Fargate launch type and front it with Elastic Load Balancing for an improved performance</strong> - AWS Fargate is a serverless compute engine for containers that work with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Elastic Load Balancing can spread the incoming requests across a fleet of Amazon EC2 instances. This additional complexity is not needed since we are looking at a static single-page webpage."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon S3 to store the static data and use AWS Fargate for hosting the application</strong> - Using Amazon ECS with Fargate is an overkill for hosting a static single-page webpage."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html"
    ]
  },
  {
    "id": 38,
    "question": "<p>A company uses Amazon Athena for ad-hoc analysis of customer data in its data lake hosted on Amazon S3. A data engineer at the company wants to understand the detailed breakdown for each of the SQL query’s run plan and the computational cost of each operation.</p>\n\n<p>Which of the following Amazon Athena statements can be used to accomplish this task?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>EXPLAIN ANALYZE</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>PREPARE</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>EXPLAIN</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>OPTIMIZE</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>EXPLAIN ANALYZE</strong></p>\n\n<p>The EXPLAIN ANALYZE statement shows both the distributed execution plan of a specified SQL statement and the computational cost of each operation in a SQL query. You can output the results in text or JSON format. Administrators can benefit from using EXPLAIN ANALYZE because it provides a scanned data count, which helps you reduce financial impact due to user queries and apply optimizations for better cost control.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q2-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q2-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/databrew/latest/dg/what-is.html\">https://docs.aws.amazon.com/databrew/latest/dg/what-is.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EXPLAIN</strong> - The EXPLAIN statement only provides a detailed breakdown of a query’s run plan. You can analyze the plan to identify and reduce query complexity and improve its runtime. You can also use EXPLAIN to validate SQL syntax prior to running the query. Doing so helps prevent errors that would have occurred while running the query.</p>\n\n<p><strong>PREPARE</strong> - Creates an SQL statement with the name statement_name to be run at a later time. The statement can include parameters represented by question marks.</p>\n\n<p><strong>OPTIMIZE</strong> - Optimizes rows in an Apache Iceberg table by rewriting data files into a more optimized layout based on their size and the number of associated delete files.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/optimize-federated-query-performance-using-explain-and-explain-analyze-in-amazon-athena/\">https://aws.amazon.com/blogs/big-data/optimize-federated-query-performance-using-explain-and-explain-analyze-in-amazon-athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/athena-explain-statement.html\">https://docs.aws.amazon.com/athena/latest/ug/athena-explain-statement.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>EXPLAIN ANALYZE</strong>"
      },
      {
        "answer": "",
        "explanation": "The EXPLAIN ANALYZE statement shows both the distributed execution plan of a specified SQL statement and the computational cost of each operation in a SQL query. You can output the results in text or JSON format. Administrators can benefit from using EXPLAIN ANALYZE because it provides a scanned data count, which helps you reduce financial impact due to user queries and apply optimizations for better cost control."
      },
      {
        "link": "https://docs.aws.amazon.com/databrew/latest/dg/what-is.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>EXPLAIN</strong> - The EXPLAIN statement only provides a detailed breakdown of a query’s run plan. You can analyze the plan to identify and reduce query complexity and improve its runtime. You can also use EXPLAIN to validate SQL syntax prior to running the query. Doing so helps prevent errors that would have occurred while running the query."
      },
      {
        "answer": "",
        "explanation": "<strong>PREPARE</strong> - Creates an SQL statement with the name statement_name to be run at a later time. The statement can include parameters represented by question marks."
      },
      {
        "answer": "",
        "explanation": "<strong>OPTIMIZE</strong> - Optimizes rows in an Apache Iceberg table by rewriting data files into a more optimized layout based on their size and the number of associated delete files."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/databrew/latest/dg/what-is.html",
      "https://aws.amazon.com/blogs/big-data/optimize-federated-query-performance-using-explain-and-explain-analyze-in-amazon-athena/",
      "https://docs.aws.amazon.com/athena/latest/ug/athena-explain-statement.html"
    ]
  },
  {
    "id": 39,
    "question": "<p>A company operates a chain of pet-care shops and it stores all of its data in an Amazon S3 bucket. The company uses Amazon Athena to analyze customer preferences to launch new products for the different types of pets. The company's data is of the order of tens of terabytes and the company is looking at reducing the costs of running these queries.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Partition the data by store region</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Migrate the data to Redshift tables for a more cost-efficient analysis</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Partition the data by customer</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Partition the data by pet type</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Partition the data by pet type</strong></p>\n\n<p>By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but that is loaded only once per day, might partition by a data source identifier and date.</p>\n\n<p>While analyzing the new product launches for the different types of pets, most of the queries will be related to the pet type, so partitioning data by pet type will reduce the amount of data scanned by Athena and hence reduce the associated costs.</p>\n\n<p>Partitioning data in Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q28-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q28-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><em>*Migrate the data to Redshift tables for a more cost-efficient analysis</em> - Migrating data to Redshift will increase the costs compared to directly querying data on S3 via Athena.</p>\n\n<p><strong>Partition the data by customer</strong></p>\n\n<p><strong>Partition the data by store region</strong></p>\n\n<p>Partitioning data by customers or by store region will not help in optimizing the queries relevant to the given use case, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Partition the data by pet type</strong>"
      },
      {
        "answer": "",
        "explanation": "By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but that is loaded only once per day, might partition by a data source identifier and date."
      },
      {
        "answer": "",
        "explanation": "While analyzing the new product launches for the different types of pets, most of the queries will be related to the pet type, so partitioning data by pet type will reduce the amount of data scanned by Athena and hence reduce the associated costs."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q28-i1.jpg",
        "answer": "",
        "explanation": "Partitioning data in Athena:"
      },
      {
        "link": "https://docs.aws.amazon.com/athena/latest/ug/partitions.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<em>*Migrate the data to Redshift tables for a more cost-efficient analysis</em> - Migrating data to Redshift will increase the costs compared to directly querying data on S3 via Athena."
      },
      {
        "answer": "",
        "explanation": "<strong>Partition the data by customer</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Partition the data by store region</strong>"
      },
      {
        "answer": "",
        "explanation": "Partitioning data by customers or by store region will not help in optimizing the queries relevant to the given use case, so both these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/partitions.html"
    ]
  },
  {
    "id": 40,
    "question": "<p>You are a data engineer at an IT company. The company has multiple enterprise customers that use the company's mobile app to capture and send data to Amazon Kinesis Data Streams. The customers have been getting a <code>ProvisionedThroughputExceededException</code> exception. Upon analysis, you notice that messages are being sent one by one at a high rate.</p>\n\n<p>Which of the following options will help with the exception while keeping costs at a minimum?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use batch messages</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Exponential Backoff</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Increase the number of shards</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Decrease the Stream retention duration</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use batch messages</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Kinesis Data Streams Overview:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a><p></p>\n\n<p>When a host needs to send many records per second (RPS) to Amazon Kinesis, simply calling the basic PutRecord API action in a loop is inadequate. To reduce overhead and increase throughput, the application must batch records and implement parallel HTTP requests. This will increase the efficiency overall and ensure you are optimally using the shards.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Exponential Backoff</strong> - While this may help in the short term, as soon as the request rate increases, you will see the <code>ProvisionedThroughputExceededException</code> exception again.</p>\n\n<p><strong>Increase the number of shards</strong> - Increasing shards could be a short-term fix but will substantially increase the cost, so this option is ruled out.</p>\n\n<p><strong>Decrease the Stream retention duration</strong> - This operation may result in data loss and won't help with the exceptions, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/\">https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use batch messages</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png",
        "answer": "",
        "explanation": "Kinesis Data Streams Overview:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/"
      },
      {
        "answer": "",
        "explanation": "When a host needs to send many records per second (RPS) to Amazon Kinesis, simply calling the basic PutRecord API action in a loop is inadequate. To reduce overhead and increase throughput, the application must batch records and implement parallel HTTP requests. This will increase the efficiency overall and ensure you are optimally using the shards."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Exponential Backoff</strong> - While this may help in the short term, as soon as the request rate increases, you will see the <code>ProvisionedThroughputExceededException</code> exception again."
      },
      {
        "answer": "",
        "explanation": "<strong>Increase the number of shards</strong> - Increasing shards could be a short-term fix but will substantially increase the cost, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Decrease the Stream retention duration</strong> - This operation may result in data loss and won't help with the exceptions, so this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/"
    ]
  },
  {
    "id": 41,
    "question": "<p>An Amazon Athena query is lagging in performance. The query has a join on two tables using the equality condition. You have been hired as an AWS Certified Data Engineer Associate to optimize the query. Upon analysis, you have noticed that one of the tables is large with millions of rows of data and the other table is small.</p>\n\n<p>What do you recommend to address the given requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Specify the smaller table on the left side of the join and the larger table on the right side</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Specify the larger table on the left side of the join and the smaller table on the right side</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Update the Athena workgroup settings for the given query, so it has access to the most stable version of Athena engine</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Update the Athena workgroup settings for the given query, so it has access to the latest version of Athena engine</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Specify the larger table on the left side of the join and the smaller table on the right side</strong></p>\n\n<p>Choosing the right join order is critical for better query performance. When you join two tables using the equality condition, specify the larger table on the left side of the join and the smaller table on the right side. For the most common type of joins that use equality conditions, Athena builds a lookup table from the table on the right and distributes it to the worker nodes. It then streams the table on the left, joining rows by looking up matching values in the lookup table. This is called a distributed hash join. Because the lookup table built from the table on the right side is kept in memory, the smaller that table is, the less memory will be used, and the faster the join will run.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q10-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q10-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the Athena workgroup settings for the given query, so it has access to the latest version of Athena engine</strong></p>\n\n<p><strong>Update the Athena workgroup settings for the given query, so it has access to the most stable version of Athena engine</strong></p>\n\n<p>You can use workgroups to separate users, teams, applications, or workloads, to set limits on the amount of data each query or the entire workgroup can process, and to track costs. Because workgroups act as resources, you can use resource-level identity-based policies to control access to a specific workgroup. You can also view query-related metrics in Amazon CloudWatch, control costs by configuring limits on the amount of data scanned, create thresholds, and trigger actions, such as Amazon SNS, when these thresholds are breached.</p>\n\n<p>Athena occasionally releases a new engine version to provide improved performance, functionality, and code fixes. When a new engine version is available, Athena notifies you in the console. You can choose to let Athena decide when to upgrade, or manually specify an Athena engine version per workgroup.</p>\n\n<p>These two options have been added as distractors since the version of the Athena engine used for the query is not relevant to the given use case.</p>\n\n<p><strong>Specify the smaller table on the left side of the join and the larger table on the right side</strong> - This violates the aforementioned recommendation for optimizing join queries having two tables of asymmetric sizes.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html\">https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Specify the larger table on the left side of the join and the smaller table on the right side</strong>"
      },
      {
        "answer": "",
        "explanation": "Choosing the right join order is critical for better query performance. When you join two tables using the equality condition, specify the larger table on the left side of the join and the smaller table on the right side. For the most common type of joins that use equality conditions, Athena builds a lookup table from the table on the right and distributes it to the worker nodes. It then streams the table on the left, joining rows by looking up matching values in the lookup table. This is called a distributed hash join. Because the lookup table built from the table on the right side is kept in memory, the smaller that table is, the less memory will be used, and the faster the join will run."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Update the Athena workgroup settings for the given query, so it has access to the latest version of Athena engine</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Update the Athena workgroup settings for the given query, so it has access to the most stable version of Athena engine</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use workgroups to separate users, teams, applications, or workloads, to set limits on the amount of data each query or the entire workgroup can process, and to track costs. Because workgroups act as resources, you can use resource-level identity-based policies to control access to a specific workgroup. You can also view query-related metrics in Amazon CloudWatch, control costs by configuring limits on the amount of data scanned, create thresholds, and trigger actions, such as Amazon SNS, when these thresholds are breached."
      },
      {
        "answer": "",
        "explanation": "Athena occasionally releases a new engine version to provide improved performance, functionality, and code fixes. When a new engine version is available, Athena notifies you in the console. You can choose to let Athena decide when to upgrade, or manually specify an Athena engine version per workgroup."
      },
      {
        "answer": "",
        "explanation": "These two options have been added as distractors since the version of the Athena engine used for the query is not relevant to the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Specify the smaller table on the left side of the join and the larger table on the right side</strong> - This violates the aforementioned recommendation for optimizing join queries having two tables of asymmetric sizes."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
      "https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html"
    ]
  },
  {
    "id": 42,
    "question": "<p>The data engineering team at an e-commerce company has set up a workflow to ingest the clickstream data into the raw zone of the Amazon S3 data lake. The team wants to run some SQL-based data sanity checks on the raw zone of the data lake.</p>\n\n<p>What AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Load the incremental raw zone data into an Amazon EMR-based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Athena to run SQL based analytics against Amazon S3 data</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Athena to run SQL based analytics against Amazon S3 data</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Amazon Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>Amazon Athena Benefits:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q56-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q56-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis.\nAs the development team would have to maintain and monitor the Amazon Redshift cluster size and would require significant development time to set up the processes to consume the data periodically, so this option is ruled out.</p>\n\n<p><strong>Load the incremental raw zone data into an Amazon EMR-based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open-source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing an Amazon EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the correct solution for the given use-case should require the least amount of development effort and ongoing maintenance.</p>\n\n<p><strong>Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks</strong> - Loading the incremental data into Amazon RDS implies data migration jobs will have to be written via a AWS Lambda function or an Amazon EC2 based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance. Hence this option is not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Athena to run SQL based analytics against Amazon S3 data</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Amazon Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q56-i1.jpg",
        "answer": "",
        "explanation": "Amazon Athena Benefits:"
      },
      {
        "link": "https://aws.amazon.com/athena/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis.\nAs the development team would have to maintain and monitor the Amazon Redshift cluster size and would require significant development time to set up the processes to consume the data periodically, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Load the incremental raw zone data into an Amazon EMR-based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open-source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing an Amazon EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the correct solution for the given use-case should require the least amount of development effort and ongoing maintenance."
      },
      {
        "answer": "",
        "explanation": "<strong>Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks</strong> - Loading the incremental data into Amazon RDS implies data migration jobs will have to be written via a AWS Lambda function or an Amazon EC2 based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance. Hence this option is not correct."
      }
    ],
    "references": [
      "https://aws.amazon.com/athena/"
    ]
  },
  {
    "id": 43,
    "question": "<p>A company has hired you to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs.</p>\n\n<p>Which solution will you recommend to address this use-case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong></p>\n\n<p>Amazon Kinesis Data Streams is useful for rapidly moving data off data producers and then continuously processing the data, be it to transform the data before emitting to a data store, run real-time metrics and analytics, or derive more complex data streams for further processing. Kinesis data streams can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>Kinesis Data Streams Overview:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong> - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. SNS cannot be used to decouple the producers and consumers for the real-time data processor as described in the given use case.</p>\n\n<p><strong>Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong> - Amazon Simple Queue Service (Amazon SQS) offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. SQS cannot be used to decouple the producers and consumers for the real-time data processor as described in the given use case.</p>\n\n<p><strong>Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. Kinesis Firehose cannot be used to process and analyze the streaming data in custom applications. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics.</p>\n\n<p>Amazon Kinesis Data Firehose Overview\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a><p></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams is useful for rapidly moving data off data producers and then continuously processing the data, be it to transform the data before emitting to a data store, run real-time metrics and analytics, or derive more complex data streams for further processing. Kinesis data streams can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png",
        "answer": "",
        "explanation": "Kinesis Data Streams Overview:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong> - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. SNS cannot be used to decouple the producers and consumers for the real-time data processor as described in the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong> - Amazon Simple Queue Service (Amazon SQS) offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. SQS cannot be used to decouple the producers and consumers for the real-time data processor as described in the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. Kinesis Firehose cannot be used to process and analyze the streaming data in custom applications. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png",
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose Overview"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-firehose/"
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/kinesis/data-firehose/"
    ]
  },
  {
    "id": 44,
    "question": "<p>A company uses an Amazon Redshift provisioned cluster as its database service and the cluster uses key distribution. While running the maintenance tasks on the cluster, a data engineer noticed that of the four ra3 cluster nodes, one node had a CPU load of over 90% most of the time, while the other three nodes were averaging around 10%. The company has tasked the data engineer to balance the load more evenly across all four compute nodes.</p>\n\n<p>What changes are needed if provisioning additional nodes is not an option?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The distribution key should be changed to the table column that has the largest dimension</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Change the distribution style to ALL for the cluster</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Update the sort key to equal distribution key</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the sort key to be the column most used in JOIN of tables</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The distribution key should be changed to the table column that has the largest dimension</strong></p>\n\n<p>In key distribution, the rows are distributed according to the values in one column. The leader node places matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns. This way, matching values from the common columns are physically stored together.</p>\n\n<p>Choosing the right distribution key that matches the data characteristics and the patterns in which queries are made can lead to a more balanced distribution of data and workload. This approach helps avoid scenarios where one node is overloaded while others remain underutilized.</p>\n\n<p>How to choose the correct key:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q9-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q9-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Distribution_examples.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_Distribution_examples.html</a><p></p>\n\n<p>An example showcasing how the key can cause distribution skew:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q9-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q9-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Distribution_examples.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_Distribution_examples.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the sort key to be the column most used in JOIN of tables</strong> - When you create a table, you can alternatively define one or more of its columns as sort keys. When data is initially loaded into the empty table, the rows are stored on disk in sorted order. Information about sort key columns is passed to the query planner, and the planner uses this information to construct plans that exploit the way that the data is sorted. The general rule is to Set the DISTKEY to the column most commonly used in JOIN and Set the SORTKEY to the column most used in WHERE.</p>\n\n<p><strong>Change the distribution style to ALL for the cluster</strong> - In ALL distribution, a copy of the entire table is distributed to every node. Where EVEN distribution or KEY distribution places only a portion of a table's rows on each node, ALL distribution ensures that every row is collocated for every join that the table participates in. ALL distribution multiplies the storage required by the number of nodes in the cluster, so it takes much longer to load, update, or insert data into multiple tables. ALL distribution is appropriate only for relatively slow-moving tables; that is, tables that are not updated frequently or extensively. This would necessitate provisioning additional nodes, so this option is incorrect.</p>\n\n<p><strong>Update the sort key to equal distribution key</strong> - Updating the sort key will not have an impact on the distribution skew seen in the use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/viewing-distribution-styles.html\">https://docs.aws.amazon.com/redshift/latest/dg/viewing-distribution-styles.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Distribution_examples.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_Distribution_examples.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html\">https://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The distribution key should be changed to the table column that has the largest dimension</strong>"
      },
      {
        "answer": "",
        "explanation": "In key distribution, the rows are distributed according to the values in one column. The leader node places matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns. This way, matching values from the common columns are physically stored together."
      },
      {
        "answer": "",
        "explanation": "Choosing the right distribution key that matches the data characteristics and the patterns in which queries are made can lead to a more balanced distribution of data and workload. This approach helps avoid scenarios where one node is overloaded while others remain underutilized."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q9-i1.jpg",
        "answer": "",
        "explanation": "How to choose the correct key:"
      },
      {
        "link": "https://docs.aws.amazon.com/redshift/latest/dg/c_Distribution_examples.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q9-i2.jpg",
        "answer": "",
        "explanation": "An example showcasing how the key can cause distribution skew:"
      },
      {
        "link": "https://docs.aws.amazon.com/redshift/latest/dg/c_Distribution_examples.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure the sort key to be the column most used in JOIN of tables</strong> - When you create a table, you can alternatively define one or more of its columns as sort keys. When data is initially loaded into the empty table, the rows are stored on disk in sorted order. Information about sort key columns is passed to the query planner, and the planner uses this information to construct plans that exploit the way that the data is sorted. The general rule is to Set the DISTKEY to the column most commonly used in JOIN and Set the SORTKEY to the column most used in WHERE."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the distribution style to ALL for the cluster</strong> - In ALL distribution, a copy of the entire table is distributed to every node. Where EVEN distribution or KEY distribution places only a portion of a table's rows on each node, ALL distribution ensures that every row is collocated for every join that the table participates in. ALL distribution multiplies the storage required by the number of nodes in the cluster, so it takes much longer to load, update, or insert data into multiple tables. ALL distribution is appropriate only for relatively slow-moving tables; that is, tables that are not updated frequently or extensively. This would necessitate provisioning additional nodes, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Update the sort key to equal distribution key</strong> - Updating the sort key will not have an impact on the distribution skew seen in the use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/dg/c_Distribution_examples.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/viewing-distribution-styles.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html"
    ]
  },
  {
    "id": 45,
    "question": "<p>Reporters at a news agency upload/download video files (about 500 megabytes each) to/from an Amazon S3 bucket as part of their daily work. As the agency has started offices in remote locations, it has resulted in poor latency for uploading and accessing data to/from the given Amazon S3 bucket. The agency wants to continue using a serverless storage solution such as Amazon S3 but wants to improve the performance.</p>\n\n<p>Which of the following solutions do you propose to address this issue? (Select two)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create new Amazon S3 buckets in every region where the agency has a remote office so that each office can maintain its storage for the media assets</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files</strong></p>\n\n<p>Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, within a developer-friendly environment.\nWhen an object from Amazon S3 that is set up with Amazon CloudFront CDN is requested, the request would come through the Edge Location transfer paths only for the first request. Thereafter, it would be served from the nearest edge location to the users until it expires. So in this way, you can speed up uploads as well as downloads for the video files.</p>\n\n<p>Following is a good reference blog for a deep dive on this topic:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/\">https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/</a></p>\n\n<p><strong>Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files</strong></p>\n\n<p>Amazon S3 Transfer Acceleration (Amazon S3TA) can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. So this option is also correct.</p>\n\n<p>Amazon S3TA:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q57-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q57-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/s3/transfer-acceleration/\">https://aws.amazon.com/s3/transfer-acceleration/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create new Amazon S3 buckets in every region where the agency has a remote office so that each office can maintain its storage for the media assets</strong> - Creating new Amazon S3 buckets in every region is not an option, since the agency maintains centralized storage. Hence this option is incorrect.</p>\n\n<p><strong>Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection</strong></p>\n\n<p><strong>Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances</strong></p>\n\n<p>Both these options using Amazon EC2 instances are not correct for the given use-case, as the agency wants a serverless storage solution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/transfer-acceleration/\">https://aws.amazon.com/s3/transfer-acceleration/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/\">https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, within a developer-friendly environment.\nWhen an object from Amazon S3 that is set up with Amazon CloudFront CDN is requested, the request would come through the Edge Location transfer paths only for the first request. Thereafter, it would be served from the nearest edge location to the users until it expires. So in this way, you can speed up uploads as well as downloads for the video files."
      },
      {
        "answer": "",
        "explanation": "Following is a good reference blog for a deep dive on this topic:"
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/",
        "answer": "",
        "explanation": "<a href=\"https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/\">https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/</a>"
      },
      {
        "answer": "",
        "explanation": "<strong>Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Transfer Acceleration (Amazon S3TA) can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. So this option is also correct."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q57-i1.jpg",
        "answer": "",
        "explanation": "Amazon S3TA:"
      },
      {
        "link": "https://aws.amazon.com/s3/transfer-acceleration/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create new Amazon S3 buckets in every region where the agency has a remote office so that each office can maintain its storage for the media assets</strong> - Creating new Amazon S3 buckets in every region is not an option, since the agency maintains centralized storage. Hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances</strong>"
      },
      {
        "answer": "",
        "explanation": "Both these options using Amazon EC2 instances are not correct for the given use-case, as the agency wants a serverless storage solution."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/",
      "https://aws.amazon.com/s3/transfer-acceleration/",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html"
    ]
  },
  {
    "id": 46,
    "question": "<p>Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary algorithm running on Amazon EC2 instances. The ideal storage would facilitate high-performance IOPS when doing file processing in a temporary storage space before uploading the results back into Amazon S3.</p>\n\n<p>Which of the following AWS storage options would you recommend as the MOST performant as well as cost-optimal?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon EC2 instances with Instance Store as the storage option</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon EC2 instances with Instance Store as the storage option</strong></p>\n\n<p>An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures.</p>\n\n<p>As the Instance Store delivers high random I/O performance, it can act as a temporary storage space, and these volumes are included as part of the instance's usage cost, therefore this is the correct option.</p>\n\n<p>Amazon EC2 Instance Store:\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/instance_storage.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/instance_storage.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option</strong> - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver its provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB.\nAmazon EBS gp2 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the Amazon EC2 instance), therefore this option is not correct.</p>\n\n<p><strong>Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option</strong> - Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.\nAmazon EBS io1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the Amazon EC2 instance), therefore this option is not correct.</p>\n\n<p><strong>Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option</strong> - Throughput Optimized HDD (st1) are low-cost HDD volumes designed for frequently accessed, throughput-intensive workloads such as Big data and Data warehouses. Amazon EBS st1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the Amazon EC2 instance), therefore this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EC2 instances with Instance Store as the storage option</strong>"
      },
      {
        "answer": "",
        "explanation": "An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures."
      },
      {
        "answer": "",
        "explanation": "As the Instance Store delivers high random I/O performance, it can act as a temporary storage space, and these volumes are included as part of the instance's usage cost, therefore this is the correct option."
      },
      {
        "image": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/instance_storage.png",
        "answer": "",
        "explanation": "Amazon EC2 Instance Store:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option</strong> - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver its provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB.\nAmazon EBS gp2 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the Amazon EC2 instance), therefore this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option</strong> - Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.\nAmazon EBS io1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the Amazon EC2 instance), therefore this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option</strong> - Throughput Optimized HDD (st1) are low-cost HDD volumes designed for frequently accessed, throughput-intensive workloads such as Big data and Data warehouses. Amazon EBS st1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the Amazon EC2 instance), therefore this option is not correct."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
    ]
  },
  {
    "id": 47,
    "question": "<p>An advertising technology company is looking at moving their on-premises infrastructure to AWS Cloud. The company's flagship application uses a massive PostgreSQL database and the data engineering team would like to maintain consistent performance with high IOPS. The team wants to install the database on an EC2 instance with the optimal storage type on the attached EBS volume.</p>\n\n<p>Which of the following configurations would you suggest to the team?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon EC2 with EBS volume of Provisioned IOPS SSD (io1) type</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon EC2 with EBS volume of Throughput Optimized HDD (st1) type</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon EC2 with EBS volume of cold HDD (sc1) type</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon EC2 with EBS volume of General Purpose SSD (gp2) type</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EC2 with EBS volume of Provisioned IOPS SSD (io1) type</strong></p>\n\n<p>Amazon EBS provides the following volume types, which differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications.</p>\n\n<p>The volumes types fall into two categories:</p>\n\n<p>SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS</p>\n\n<p>HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS</p>\n\n<p>Provision IOPS type supports critical business applications that require sustained IOPS performance, or more than 16,000 IOPS or 250 MiB/s of throughput per volume. Examples are large database workloads, such as:\nMongoDB\nCassandra\nMicrosoft SQL Server\nMySQL\nPostgreSQL\nOracle</p>\n\n<p>Therefore, Amazon EC2 with EBS volume of Provisioned IOPS SSD (io1) type is the right fit for the given use case.</p>\n\n<p>EBS Volume Characteristics:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q37-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q37-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EC2 with EBS volume of General Purpose SSD (gp2) type</strong></p>\n\n<p><strong>Amazon EC2 with EBS volume of Throughput Optimized HDD (st1) type</strong></p>\n\n<p><strong>Amazon EC2 with EBS volume of cold HDD (sc1) type</strong></p>\n\n<p>According to the details provided in the explanation above, these three options are incorrect for the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 with EBS volume of Provisioned IOPS SSD (io1) type</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon EBS provides the following volume types, which differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications."
      },
      {
        "answer": "",
        "explanation": "The volumes types fall into two categories:"
      },
      {
        "answer": "",
        "explanation": "SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS"
      },
      {
        "answer": "",
        "explanation": "HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS"
      },
      {
        "answer": "",
        "explanation": "Provision IOPS type supports critical business applications that require sustained IOPS performance, or more than 16,000 IOPS or 250 MiB/s of throughput per volume. Examples are large database workloads, such as:\nMongoDB\nCassandra\nMicrosoft SQL Server\nMySQL\nPostgreSQL\nOracle"
      },
      {
        "answer": "",
        "explanation": "Therefore, Amazon EC2 with EBS volume of Provisioned IOPS SSD (io1) type is the right fit for the given use case."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q37-i1.jpg",
        "answer": "",
        "explanation": "EBS Volume Characteristics:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 with EBS volume of General Purpose SSD (gp2) type</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 with EBS volume of Throughput Optimized HDD (st1) type</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 with EBS volume of cold HDD (sc1) type</strong>"
      },
      {
        "answer": "",
        "explanation": "According to the details provided in the explanation above, these three options are incorrect for the given use-case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
    ]
  },
  {
    "id": 48,
    "question": "<p>An e-commerce company uses Amazon Kinesis Data Streams to process real-time clickstream data from its website. The company needs to perform near real-time analytics of the streaming data in conjunction with the previous day's data. The streaming data should be stored in Amazon Redshift Serverless service.</p>\n\n<p>Which solution meets these requirements with the LEAST operational overhead?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use federated queries feature of Amazon Redshift to retrieve and efficiently query the streaming data for analytics</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Direct the Kinesis Data Streams output into a Kinesis Data Firehose delivery stream. Leverage the direct integration of Kinesis Data Firehose with Amazon Redshift Serverless to deliver the streaming data</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the streaming ingestion feature of Amazon Redshift</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Redshift Spectrum to retrieve and efficiently query the streaming data for analytics</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the streaming ingestion feature of Amazon Redshift</strong></p>\n\n<p>Streaming ingestion provides low-latency, high-speed ingestion of stream data from Amazon Kinesis Data Streams and Amazon Managed Streaming for Apache Kafka into an Amazon Redshift provisioned or Amazon Redshift Serverless materialized view. It lowers the time it takes to access data and it reduces storage cost. You can configure streaming ingestion for your Amazon Redshift cluster or for Amazon Redshift Serverless and create a materialized view. After that, using materialized-view refresh, you can ingest hundreds of megabytes of data per second. This results in fast access to external data that is quickly refreshed.</p>\n\n<p>Use cases for Amazon Redshift streaming ingestion involve working with data that is generated continually (streamed) and must be processed within a short period (latency) of its generation. This is called near real-time analytics. Sources of data can vary and include IoT devices, system telemetry data, or clickstream data from a busy website or application.</p>\n\n<p>With streaming ingestion, you bypass several steps that involve other service(s) needed between Kinesis Data Streams and Redshift. The advantages of this configuration are:</p>\n\n<ol>\n<li><p>You don't have to send data to an Amazon Data Firehose delivery stream, because with streaming ingestion, data can be sent directly from Kinesis Data Streams to a materialized view in a Redshift database.</p></li>\n<li><p>You don't have to land streamed data in Amazon S3, because streaming ingestion data goes directly to the Redshift materialized view.</p></li>\n<li><p>You don't have to write and run COPY commands because the data in the materialized view is refreshed directly from the stream. Loading data from Amazon S3 to Redshift isn't part of the process.</p></li>\n</ol>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q8-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q8-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\">https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use federated queries feature of Amazon Redshift to retrieve and efficiently query the streaming data for analytics</strong> - By using federated queries in Amazon Redshift, you can query and analyze data across operational databases, data warehouses, and data lakes. With the Federated Query feature, you can integrate queries from Amazon Redshift on live data in external databases with queries across your Amazon Redshift and Amazon S3 environments. This option acts as a distractor since Redshift federated queries cannot be used to process streaming data from external sources.</p>\n\n<p><strong>Direct the Kinesis Data Streams output into a Kinesis Data Firehose delivery stream. Leverage the direct integration of Kinesis Data Firehose with Amazon Redshift Serverless to deliver the streaming data</strong> - This option introduces an unnecessary hop in the form of Kinesis Data Firehose to the solution. The ideal solution should have the LEAST operational overhead, so this option is incorrect.</p>\n\n<p><strong>Use Amazon Redshift Spectrum to retrieve and efficiently query the streaming data for analytics</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. This option acts as a distractor since Redshift Spectrum cannot be used to process streaming data from external sources.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\">https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-create-sql-command.html\">https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-create-sql-command.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the streaming ingestion feature of Amazon Redshift</strong>"
      },
      {
        "answer": "",
        "explanation": "Streaming ingestion provides low-latency, high-speed ingestion of stream data from Amazon Kinesis Data Streams and Amazon Managed Streaming for Apache Kafka into an Amazon Redshift provisioned or Amazon Redshift Serverless materialized view. It lowers the time it takes to access data and it reduces storage cost. You can configure streaming ingestion for your Amazon Redshift cluster or for Amazon Redshift Serverless and create a materialized view. After that, using materialized-view refresh, you can ingest hundreds of megabytes of data per second. This results in fast access to external data that is quickly refreshed."
      },
      {
        "answer": "",
        "explanation": "Use cases for Amazon Redshift streaming ingestion involve working with data that is generated continually (streamed) and must be processed within a short period (latency) of its generation. This is called near real-time analytics. Sources of data can vary and include IoT devices, system telemetry data, or clickstream data from a busy website or application."
      },
      {
        "answer": "",
        "explanation": "With streaming ingestion, you bypass several steps that involve other service(s) needed between Kinesis Data Streams and Redshift. The advantages of this configuration are:"
      },
      {},
      {
        "link": "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use federated queries feature of Amazon Redshift to retrieve and efficiently query the streaming data for analytics</strong> - By using federated queries in Amazon Redshift, you can query and analyze data across operational databases, data warehouses, and data lakes. With the Federated Query feature, you can integrate queries from Amazon Redshift on live data in external databases with queries across your Amazon Redshift and Amazon S3 environments. This option acts as a distractor since Redshift federated queries cannot be used to process streaming data from external sources."
      },
      {
        "answer": "",
        "explanation": "<strong>Direct the Kinesis Data Streams output into a Kinesis Data Firehose delivery stream. Leverage the direct integration of Kinesis Data Firehose with Amazon Redshift Serverless to deliver the streaming data</strong> - This option introduces an unnecessary hop in the form of Kinesis Data Firehose to the solution. The ideal solution should have the LEAST operational overhead, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Redshift Spectrum to retrieve and efficiently query the streaming data for analytics</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. This option acts as a distractor since Redshift Spectrum cannot be used to process streaming data from external sources."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html",
      "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-create-sql-command.html"
    ]
  },
  {
    "id": 49,
    "question": "<p>A company wants to store all of its consumer data on Amazon S3. Before storing the data, the company must clean it by standardizing the formats of a few of the data columns. A single data record might range in size from 500 KB to 10 MB.</p>\n\n<p>Which of these options represents the right solution?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Kinesis Data Streams. Configure a stream for incoming raw data. Kinesis Agent can be used to write data to the stream. Configure an Amazon Kinesis Data Analytics application to read the raw data and transform it to the necessary format before writing it to Amazon S3</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Managed Streaming for Apache Kafka. Create a topic for the initial raw data. Use a Kafka producer to write data on this topic. Use the Apache Kafka consumer API to create a consumer application (that can be hosted on Amazon EC2 instance) that reads data from this topic, transforms the data as needed, and writes it to Amazon S3 for final storage</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Simple Queue Service (Amazon SQS) to ingest incoming data. Configure an AWS Lambda function to read events from the SQS queue and upload the events to Amazon S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Kinesis Data Firehose to ingest data. Configure an AWS Lambda function to cleanse/transform the data written into the Firehose delivery stream which is then delivered to Amazon S3</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Managed Streaming for Apache Kafka. Create a topic for the initial raw data. Use a Kafka producer to write data on this topic. Use the Apache Kafka consumer API to create a consumer application (that can be hosted on Amazon EC2 instance) that reads data from this topic, transforms the data as needed, and writes it to Amazon S3 for final storage</strong></p>\n\n<p>Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data. Apache Kafka is an open-source platform for building real-time streaming data pipelines and applications. With Amazon MSK, you can use native Apache Kafka APIs to populate data lakes, stream changes to and from databases, and power machine learning and analytics applications.</p>\n\n<p>Apache Kafka stores records in topics. Data producers write records to topics and consumers read records from topics. Each record in Apache Kafka consists of a key, a value, and a timestamp.</p>\n\n<p>Amazon MSK is preferred over Amazon Kinesis Data Streams and Kinesis Firehose because of the limitation on record size. The maximum record size of an Amazon MSK is 100 MB.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Kinesis Data Firehose to ingest data. Configure an AWS Lambda function to cleanse/transform the data written into the Firehose delivery stream which is then delivered to Amazon S3</strong> - The maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB. Hence, Firehose cannot be used for this requirement where data can be of the order of 10MB.</p>\n\n<p><strong>Use Amazon Kinesis Data Streams. Configure a stream for incoming raw data. Kinesis Agent can be used to write data to the stream. Configure an Amazon Kinesis Data Analytics application to read the raw data and transform it to the necessary format before writing it to Amazon S3</strong> - For Amazon Kinesis Data Streams, the maximum size of a data blob (the data payload before Base64-encoding) within one record is 1 megabyte (MB). Hence, Kinesis Data Streams cannot be used for this requirement where data can be of the order of 10MB.</p>\n\n<p><strong>Use Amazon Simple Queue Service (Amazon SQS) to ingest incoming data. Configure an AWS Lambda function to read events from the SQS queue and upload the events to Amazon S3</strong> - Amazon SQS offers a reliable, highly scalable hosted queue for storing messages as they travel between applications or microservices. It moves data between distributed application components and helps you decouple these components. Amazon SQS provides common middleware constructs such as dead-letter queues and poison-pill management. It is not the best fit as a real-time data stream processing solution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/msk/latest/developerguide/limits.html\">https://docs.aws.amazon.com/msk/latest/developerguide/limits.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/limits.html\">https://docs.aws.amazon.com/firehose/latest/dev/limits.html</a></p>\n\n<p><a href=\"https://www.amazonaws.cn/en/kinesis/data-streams/faqs/\">https://www.amazonaws.cn/en/kinesis/data-streams/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Managed Streaming for Apache Kafka. Create a topic for the initial raw data. Use a Kafka producer to write data on this topic. Use the Apache Kafka consumer API to create a consumer application (that can be hosted on Amazon EC2 instance) that reads data from this topic, transforms the data as needed, and writes it to Amazon S3 for final storage</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data. Apache Kafka is an open-source platform for building real-time streaming data pipelines and applications. With Amazon MSK, you can use native Apache Kafka APIs to populate data lakes, stream changes to and from databases, and power machine learning and analytics applications."
      },
      {
        "answer": "",
        "explanation": "Apache Kafka stores records in topics. Data producers write records to topics and consumers read records from topics. Each record in Apache Kafka consists of a key, a value, and a timestamp."
      },
      {
        "answer": "",
        "explanation": "Amazon MSK is preferred over Amazon Kinesis Data Streams and Kinesis Firehose because of the limitation on record size. The maximum record size of an Amazon MSK is 100 MB."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Firehose to ingest data. Configure an AWS Lambda function to cleanse/transform the data written into the Firehose delivery stream which is then delivered to Amazon S3</strong> - The maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB. Hence, Firehose cannot be used for this requirement where data can be of the order of 10MB."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Streams. Configure a stream for incoming raw data. Kinesis Agent can be used to write data to the stream. Configure an Amazon Kinesis Data Analytics application to read the raw data and transform it to the necessary format before writing it to Amazon S3</strong> - For Amazon Kinesis Data Streams, the maximum size of a data blob (the data payload before Base64-encoding) within one record is 1 megabyte (MB). Hence, Kinesis Data Streams cannot be used for this requirement where data can be of the order of 10MB."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Simple Queue Service (Amazon SQS) to ingest incoming data. Configure an AWS Lambda function to read events from the SQS queue and upload the events to Amazon S3</strong> - Amazon SQS offers a reliable, highly scalable hosted queue for storing messages as they travel between applications or microservices. It moves data between distributed application components and helps you decouple these components. Amazon SQS provides common middleware constructs such as dead-letter queues and poison-pill management. It is not the best fit as a real-time data stream processing solution."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/msk/latest/developerguide/limits.html",
      "https://docs.aws.amazon.com/firehose/latest/dev/limits.html",
      "https://www.amazonaws.cn/en/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 50,
    "question": "<p>A company uses the S3 Standard storage class for all its data storage needs on Amazon S3. A data engineer analyzed the data access patterns and discovered that for the first six months, data files are accessed multiple times daily. From six months to three years, the data files are accessed about once or twice a month. After three years, each file is accessed only once or twice annually. The data engineer is tasked with creating S3 Lifecycle policies to develop new data storage rules, ensuring high availability while seeking cost-effectiveness.</p>\n\n<p>What solution will best meet these needs?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Move objects to S3 Standard-Infrequent Access (S3 Standard-IA) after six months. Transition objects to S3 Glacier Deep Archive after three years</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Move objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after six months. Transition objects to S3 Glacier Deep Archive after three years</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Move objects to S3 Intelligent-Tiering after six months. Transition objects to S3 Glacier Deep Archive after three years</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Move objects to S3 Standard-Infrequent Access (S3 Standard-IA) after six months. Transition objects to S3 Glacier Flexible Archive after three years</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Move objects to S3 Standard-Infrequent Access (S3 Standard-IA) after six months. Transition objects to S3 Glacier Deep Archive after three years</strong></p>\n\n<p>The S3 Standard-IA storage class is designed for long-lived and infrequently accessed data. (IA stands for infrequent access.) S3 Standard-IA and S3 One Zone-IA objects are available for millisecond access (similar to the S3 Standard storage class). Amazon S3 charges a retrieval fee for these objects, so they are most suitable for infrequently accessed data.</p>\n\n<p>Use S3 Glacier Deep Archive for archiving data that rarely need to be accessed. Data stored in the S3 Glacier Deep Archive storage class has a minimum storage duration period of 180 days and a default retrieval time of 12 hours. It is the lowest-cost storage option in AWS. Storage costs for the S3 Glacier Deep Archive are less expensive than using the S3 Glacier Flexible Retrieval storage class.</p>\n\n<p>Since the use case demands high availability while maintaining cost-effectiveness, so both S3 Standard-IA and S3 Glacier Deep Archive offer the best fit.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q11-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q11-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Move data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days. Transfer the data objects to S3 Glacier Flexible Retrieval after 1 year</strong> - S3 Glacier Flexible Retrieval is used for archives where portions of the data might need to be retrieved in minutes. Data stored in the S3 Glacier Flexible Retrieval storage class has a minimum storage duration period of 90 days and can be accessed in as little as 1-5 minutes by using expedited retrieval. Storage costs for the S3 Glacier Flexible Retrieval are more than the S3 Glacier Deep Archive. Since the use case poses no constraints on the first-byte latency, using S3 Glacier Deep Archive is more cost-effective while still maintaining high availability. So, this option is incorrect.</p>\n\n<p><strong>Move objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after six months. Transition objects to S3 Glacier Deep Archive after three years</strong> - Using S3 One Zone-Infrequent Access (S3 One Zone-IA) is ruled out as it does not guarantee high availability. So, this option is incorrect.</p>\n\n<p><strong>Move objects to S3 Intelligent-Tiering after six months. Transition objects to S3 Glacier Deep Archive after three years</strong> - For a small monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers. S3 Intelligent-Tiering automatically stores objects in three access tiers: one tier that is optimized for frequent access, a 40% lower-cost tier that is optimized for infrequent access, and a 68% lower-cost tier optimized for rarely accessed data. Since access patterns are very clear, Amazon S3 Intelligent-Tiering is not needed for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Move objects to S3 Standard-Infrequent Access (S3 Standard-IA) after six months. Transition objects to S3 Glacier Deep Archive after three years</strong>"
      },
      {
        "answer": "",
        "explanation": "The S3 Standard-IA storage class is designed for long-lived and infrequently accessed data. (IA stands for infrequent access.) S3 Standard-IA and S3 One Zone-IA objects are available for millisecond access (similar to the S3 Standard storage class). Amazon S3 charges a retrieval fee for these objects, so they are most suitable for infrequently accessed data."
      },
      {
        "answer": "",
        "explanation": "Use S3 Glacier Deep Archive for archiving data that rarely need to be accessed. Data stored in the S3 Glacier Deep Archive storage class has a minimum storage duration period of 180 days and a default retrieval time of 12 hours. It is the lowest-cost storage option in AWS. Storage costs for the S3 Glacier Deep Archive are less expensive than using the S3 Glacier Flexible Retrieval storage class."
      },
      {
        "answer": "",
        "explanation": "Since the use case demands high availability while maintaining cost-effectiveness, so both S3 Standard-IA and S3 Glacier Deep Archive offer the best fit."
      },
      {
        "link": "https://aws.amazon.com/s3/storage-classes/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Move data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days. Transfer the data objects to S3 Glacier Flexible Retrieval after 1 year</strong> - S3 Glacier Flexible Retrieval is used for archives where portions of the data might need to be retrieved in minutes. Data stored in the S3 Glacier Flexible Retrieval storage class has a minimum storage duration period of 90 days and can be accessed in as little as 1-5 minutes by using expedited retrieval. Storage costs for the S3 Glacier Flexible Retrieval are more than the S3 Glacier Deep Archive. Since the use case poses no constraints on the first-byte latency, using S3 Glacier Deep Archive is more cost-effective while still maintaining high availability. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Move objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after six months. Transition objects to S3 Glacier Deep Archive after three years</strong> - Using S3 One Zone-Infrequent Access (S3 One Zone-IA) is ruled out as it does not guarantee high availability. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Move objects to S3 Intelligent-Tiering after six months. Transition objects to S3 Glacier Deep Archive after three years</strong> - For a small monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers. S3 Intelligent-Tiering automatically stores objects in three access tiers: one tier that is optimized for frequent access, a 40% lower-cost tier that is optimized for infrequent access, and a 68% lower-cost tier optimized for rarely accessed data. Since access patterns are very clear, Amazon S3 Intelligent-Tiering is not needed for this use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html"
    ]
  },
  {
    "id": 51,
    "question": "<p>A ride-sharing company wants to use an Amazon DynamoDB table for data storage. The table will not be used during the night hours whereas the read and write traffic will often be unpredictable during day hours. When traffic spikes occur they will happen very quickly.</p>\n\n<p>Which of the following will you recommend as the best-fit solution?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up Amazon DynamoDB global table in the provisioned capacity mode</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up Amazon DynamoDB table in the on-demand capacity mode</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up Amazon DynamoDB table with a global secondary index</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up Amazon DynamoDB table in the on-demand capacity mode</strong></p>\n\n<p>Amazon DynamoDB has two read/write capacity modes for processing reads and writes on your tables:</p>\n\n<p>On-demand</p>\n\n<p>Provisioned (default, free-tier eligible)</p>\n\n<p>Amazon DynamoDB on-demand is a flexible billing option capable of serving thousands of requests per second without capacity planning. DynamoDB on-demand offers pay-per-request pricing for read and write requests so that you pay only for what you use.</p>\n\n<p>The on-demand mode is a good option if any of the following are true:</p>\n\n<p>You create new tables with unknown workloads.</p>\n\n<p>You have unpredictable application traffic.</p>\n\n<p>You prefer the ease of paying for only what you use.</p>\n\n<p>If you choose provisioned mode, you specify the number of reads and writes per second that you require for your application. You can use auto-scaling to adjust your table’s provisioned capacity automatically in response to traffic changes. This helps you govern your DynamoDB use to stay at or below a defined request rate to obtain cost predictability.</p>\n\n<p>Provisioned mode is a good option if any of the following are true:</p>\n\n<p>You have predictable application traffic.</p>\n\n<p>You run applications whose traffic is consistent or ramps gradually.</p>\n\n<p>You can forecast capacity requirements to control costs.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q51-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q51-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/\">https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/</a><p></p>\n\n<p>With on-demand, Amazon DynamoDB instantly allocates capacity as it is needed. There is no concept of provisioned capacity, and there is no delay waiting for Amazon CloudWatch thresholds or the subsequent table updates. On-demand is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes, and when underprovisioned capacity would impact the user experience. On-demand is a perfect solution if your team is moving to a NoOps or serverless environment.</p>\n\n<p>The given use case clearly states that when the traffic spikes occur they happen very quickly, thereby implying an unpredictable traffic pattern, therefore the on-demand capacity mode is the correct option for the given use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled</strong> - As mentioned in the explanation above, you should use the provisioned capacity mode (even with auto-scaling) only when you have predictable application traffic.</p>\n\n<p>When you create Amazon DynamoDB table, auto-scaling is the default capacity setting, but you can also enable auto-scaling on any table that does not have it active. Behind the scenes, as illustrated in the following diagram, DynamoDB auto scaling uses a scaling policy in Application Auto Scaling. To configure auto-scaling in DynamoDB, you set the minimum and maximum levels of read and write capacity in addition to the target utilization percentage. Auto-scaling uses Amazon CloudWatch to monitor a table’s read and write capacity metrics. To do so, it creates CloudWatch alarms that track consumed capacity.</p>\n\n<p><strong>Set up Amazon DynamoDB table with a global secondary index</strong> - A global secondary index (GSI) is an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. A global secondary index is stored in its own partition space away from the base table and scales separately from the base table. GSI cannot be used to handle an unpredictable load on a DynamoDB table.</p>\n\n<p><strong>Set up Amazon DynamoDB global table in the provisioned capacity mode</strong> - Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions.</p>\n\n<p>Global tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application's business logic. In addition, global tables enable your applications to stay highly available even in the unlikely event of isolation or degradation of an entire Region.</p>\n\n<p>Amazon DynamoDB global tables:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q51-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q51-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a><p></p>\n\n<p>Amazon DynamoDB global table cannot be used to handle an unpredictable load on a Amazon DynamoDB table.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/\">https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon DynamoDB table in the on-demand capacity mode</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB has two read/write capacity modes for processing reads and writes on your tables:"
      },
      {
        "answer": "",
        "explanation": "On-demand"
      },
      {
        "answer": "",
        "explanation": "Provisioned (default, free-tier eligible)"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB on-demand is a flexible billing option capable of serving thousands of requests per second without capacity planning. DynamoDB on-demand offers pay-per-request pricing for read and write requests so that you pay only for what you use."
      },
      {
        "answer": "",
        "explanation": "The on-demand mode is a good option if any of the following are true:"
      },
      {
        "answer": "",
        "explanation": "You create new tables with unknown workloads."
      },
      {
        "answer": "",
        "explanation": "You have unpredictable application traffic."
      },
      {
        "answer": "",
        "explanation": "You prefer the ease of paying for only what you use."
      },
      {
        "answer": "",
        "explanation": "If you choose provisioned mode, you specify the number of reads and writes per second that you require for your application. You can use auto-scaling to adjust your table’s provisioned capacity automatically in response to traffic changes. This helps you govern your DynamoDB use to stay at or below a defined request rate to obtain cost predictability."
      },
      {
        "answer": "",
        "explanation": "Provisioned mode is a good option if any of the following are true:"
      },
      {
        "answer": "",
        "explanation": "You have predictable application traffic."
      },
      {
        "answer": "",
        "explanation": "You run applications whose traffic is consistent or ramps gradually."
      },
      {
        "answer": "",
        "explanation": "You can forecast capacity requirements to control costs."
      },
      {
        "link": "https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/"
      },
      {
        "answer": "",
        "explanation": "With on-demand, Amazon DynamoDB instantly allocates capacity as it is needed. There is no concept of provisioned capacity, and there is no delay waiting for Amazon CloudWatch thresholds or the subsequent table updates. On-demand is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes, and when underprovisioned capacity would impact the user experience. On-demand is a perfect solution if your team is moving to a NoOps or serverless environment."
      },
      {
        "answer": "",
        "explanation": "The given use case clearly states that when the traffic spikes occur they happen very quickly, thereby implying an unpredictable traffic pattern, therefore the on-demand capacity mode is the correct option for the given use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled</strong> - As mentioned in the explanation above, you should use the provisioned capacity mode (even with auto-scaling) only when you have predictable application traffic."
      },
      {
        "answer": "",
        "explanation": "When you create Amazon DynamoDB table, auto-scaling is the default capacity setting, but you can also enable auto-scaling on any table that does not have it active. Behind the scenes, as illustrated in the following diagram, DynamoDB auto scaling uses a scaling policy in Application Auto Scaling. To configure auto-scaling in DynamoDB, you set the minimum and maximum levels of read and write capacity in addition to the target utilization percentage. Auto-scaling uses Amazon CloudWatch to monitor a table’s read and write capacity metrics. To do so, it creates CloudWatch alarms that track consumed capacity."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon DynamoDB table with a global secondary index</strong> - A global secondary index (GSI) is an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. A global secondary index is stored in its own partition space away from the base table and scales separately from the base table. GSI cannot be used to handle an unpredictable load on a DynamoDB table."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon DynamoDB global table in the provisioned capacity mode</strong> - Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions."
      },
      {
        "answer": "",
        "explanation": "Global tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application's business logic. In addition, global tables enable your applications to stay highly available even in the unlikely event of isolation or degradation of an entire Region."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q51-i2.jpg",
        "answer": "",
        "explanation": "Amazon DynamoDB global tables:"
      },
      {
        "link": "https://aws.amazon.com/dynamodb/global-tables/"
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB global table cannot be used to handle an unpredictable load on a Amazon DynamoDB table."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/",
      "https://aws.amazon.com/dynamodb/global-tables/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html"
    ]
  },
  {
    "id": 52,
    "question": "<p>A company operates a frontend website built with VueJS, which interacts with REST APIs via Amazon API Gateway to facilitate its functionalities. A data engineer is required to develop a Python script that can be executed on demand through the API Gateway, with the script's output returned to API Gateway.</p>\n\n<p>What is the most efficient solution to achieve this with minimal operational overhead?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Develop an AWS Lambda Python function to implement the backend of the REST API using the code in the Python script. Keep the Lambda function warm by invoking it via an Amazon EventBridge Scheduler on a per-minute basis</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a custom Python script on an Amazon Elastic Container Service (Amazon ECS) Fargate cluster</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a custom Python script on an Amazon Elastic Container Service (Amazon ECS) cluster running on EC2 instances</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Develop an AWS Lambda Python function with provisioned concurrency to implement the backend of the REST API using the code in the Python script</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Develop an AWS Lambda Python function with provisioned concurrency to implement the backend of the REST API using the code in the Python script</strong></p>\n\n<p>In Lambda, concurrency is the number of in-flight requests that your function is currently handling. There are two types of concurrency controls available:</p>\n\n<p>Reserved concurrency – This represents the maximum number of concurrent instances allocated to your function. When a function has reserved concurrency, no other function can use that concurrency. Configuring reserved concurrency for a function incurs no additional charges.</p>\n\n<p>Provisioned concurrency – This is the number of pre-initialized execution environments allocated to your function. These execution environments are ready to respond immediately to incoming function requests. Configuring provisioned concurrency incurs additional charges to your AWS account.</p>\n\n<p>Configuring provisioned concurrency for the Lambda function would keep it warm to respond immediately to any requests from the API Gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a custom Python script on an Amazon Elastic Container Service (Amazon ECS) Fargate cluster</strong></p>\n\n<p><strong>Create a custom Python script on an Amazon Elastic Container Service (Amazon ECS) cluster running on EC2 instances</strong></p>\n\n<p>Using an ECS cluster is overkill to deploy a Python script to act as the backend of a REST API for the given use case. The task is to build a solution with the least operational overhead, which is best achieved by using a Lambda function, as mentioned in the explanation above. So, both these options are incorrect.</p>\n\n<p><strong>Develop an AWS Lambda Python function to implement the backend of the REST API using the code in the Python script. Keep the Lambda function warm by invoking it via an Amazon EventBridge Scheduler on a per-minute basis</strong> - This option acts as a distractor. You cannot keep a Lambda function warm by invoking it via an Amazon EventBridge Scheduler on a per-minute basis.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/using-eventbridge-scheduler.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/using-eventbridge-scheduler.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Develop an AWS Lambda Python function with provisioned concurrency to implement the backend of the REST API using the code in the Python script</strong>"
      },
      {
        "answer": "",
        "explanation": "In Lambda, concurrency is the number of in-flight requests that your function is currently handling. There are two types of concurrency controls available:"
      },
      {
        "answer": "",
        "explanation": "Reserved concurrency – This represents the maximum number of concurrent instances allocated to your function. When a function has reserved concurrency, no other function can use that concurrency. Configuring reserved concurrency for a function incurs no additional charges."
      },
      {
        "answer": "",
        "explanation": "Provisioned concurrency – This is the number of pre-initialized execution environments allocated to your function. These execution environments are ready to respond immediately to incoming function requests. Configuring provisioned concurrency incurs additional charges to your AWS account."
      },
      {
        "answer": "",
        "explanation": "Configuring provisioned concurrency for the Lambda function would keep it warm to respond immediately to any requests from the API Gateway."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a custom Python script on an Amazon Elastic Container Service (Amazon ECS) Fargate cluster</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Create a custom Python script on an Amazon Elastic Container Service (Amazon ECS) cluster running on EC2 instances</strong>"
      },
      {
        "answer": "",
        "explanation": "Using an ECS cluster is overkill to deploy a Python script to act as the backend of a REST API for the given use case. The task is to build a solution with the least operational overhead, which is best achieved by using a Lambda function, as mentioned in the explanation above. So, both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Develop an AWS Lambda Python function to implement the backend of the REST API using the code in the Python script. Keep the Lambda function warm by invoking it via an Amazon EventBridge Scheduler on a per-minute basis</strong> - This option acts as a distractor. You cannot keep a Lambda function warm by invoking it via an Amazon EventBridge Scheduler on a per-minute basis."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html",
      "https://docs.aws.amazon.com/eventbridge/latest/userguide/using-eventbridge-scheduler.html"
    ]
  },
  {
    "id": 53,
    "question": "<p>An IT training company hosted its website on Amazon S3 a couple of years ago. Due to COVID-19 related travel restrictions, the training website has suddenly gained traction. With an almost 300% increase in the requests served per day, the company's AWS costs have sky-rocketed for just the Amazon S3 outbound data costs.</p>\n\n<p>Can you suggest an alternate method to reduce costs while keeping the latency low?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure Amazon S3 Batch Operations to read data in bulk at one go as it would reduce the number of calls made to Amazon S3 buckets</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively</strong></p>\n\n<p>Storing content with Amazon S3 provides a lot of advantages. But to help optimize your application’s performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your Amazon S3 bucket to serve and protect the content.</p>\n\n<p>Amazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from Amazon S3 directly to your users.</p>\n\n<p>Amazon CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located. Amazon CloudFront has edge servers in locations all around the world.</p>\n\n<p>When a user requests content that you serve with Amazon CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, Amazon CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content. Then, for the next local request for the same content, it’s already cached nearby and can be served immediately.</p>\n\n<p>By caching your content in Edge Locations, Amazon CloudFront reduces the load on your Amazon S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using CloudFront is often more cost-effective than serving files directly from Amazon S3, and there is no data transfer fee from Amazon S3 to CloudFront. You only pay for what is delivered to the internet from Amazon CloudFront, plus request fees.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application</strong> - Amazon EBS volumes are fast and relatively cheap (though Amazon S3 is still a cheaper alternative). But, Amazon EBS volumes are accessible only through Amazon EC2 instances and are bound to a specific region.</p>\n\n<p><strong>Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data</strong> - Amazon EFS is a shareable file system that can be mounted onto Amazon EC2 instances. Amazon EFS is costlier than Amazon EBS and not a solution if the company is looking at reducing costs.</p>\n\n<p><strong>Configure Amazon S3 Batch Operations to read data in bulk at one go as it would reduce the number of calls made to Amazon S3 buckets</strong> - This statement is incorrect and given only as a distractor. You can use Amazon S3 Batch Operations to perform large-scale batch operations on Amazon S3 objects, and it has nothing to do with content distribution.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively</strong>"
      },
      {
        "answer": "",
        "explanation": "Storing content with Amazon S3 provides a lot of advantages. But to help optimize your application’s performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your Amazon S3 bucket to serve and protect the content."
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from Amazon S3 directly to your users."
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located. Amazon CloudFront has edge servers in locations all around the world."
      },
      {
        "answer": "",
        "explanation": "When a user requests content that you serve with Amazon CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, Amazon CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content. Then, for the next local request for the same content, it’s already cached nearby and can be served immediately."
      },
      {
        "answer": "",
        "explanation": "By caching your content in Edge Locations, Amazon CloudFront reduces the load on your Amazon S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using CloudFront is often more cost-effective than serving files directly from Amazon S3, and there is no data transfer fee from Amazon S3 to CloudFront. You only pay for what is delivered to the internet from Amazon CloudFront, plus request fees."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application</strong> - Amazon EBS volumes are fast and relatively cheap (though Amazon S3 is still a cheaper alternative). But, Amazon EBS volumes are accessible only through Amazon EC2 instances and are bound to a specific region."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data</strong> - Amazon EFS is a shareable file system that can be mounted onto Amazon EC2 instances. Amazon EFS is costlier than Amazon EBS and not a solution if the company is looking at reducing costs."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon S3 Batch Operations to read data in bulk at one go as it would reduce the number of calls made to Amazon S3 buckets</strong> - This statement is incorrect and given only as a distractor. You can use Amazon S3 Batch Operations to perform large-scale batch operations on Amazon S3 objects, and it has nothing to do with content distribution."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/"
    ]
  },
  {
    "id": 54,
    "question": "<p>A healthcare company collects user health information on a daily basis and stores this data on Amazon S3 which is then queried by Amazon Athena for analysis. Any user health information older than a week is never used in the queries. The data engineering team at the company has now set up a Glue crawler to automate the process but the crawler has been running for several hours and is still unable to identify the schema of the data store.</p>\n\n<p>Which of the following options can be used to fix this issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Change the <code>Scanning rate</code> parameter of Glue Crawler to a higher value for the Crawler to complete the scan faster</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use an exclude pattern for the Glue crawler to filter out the unwanted files</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Split larger files into smaller ones, to reduce the overhead of reading large files for the Glue Crawler</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Compressed files, like Apache Parquet, take longer to crawl. Instead, use uncompressed files</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use an exclude pattern for the Glue crawler to filter out the unwanted files</strong></p>\n\n<p>An exclude pattern tells the crawler to skip certain files or paths. Exclude patterns reduce the number of files that the crawler must list, making the crawler run faster. For example, use an exclude pattern to exclude metafiles and files that have already been crawled.</p>\n\n<p>More on crawler exclude patterns:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q30-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q30-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude\">https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Compressed files, like Apache Parquet, take longer to crawl. Instead, use uncompressed files</strong> - Compressed files take longer to crawl. That's because the crawler must download the file and decompress it before reading the first megabyte or listing the file. For Apache Parquet, Apache Avro, and Apache Orc files, the crawler doesn't crawl the first megabyte. Instead, the crawler reads the metadata stored in each file. So this option is not relevant for the given use case.</p>\n\n<p><strong>Change the <code>Scanning rate</code> parameter of Glue Crawler to a higher value for the Crawler to complete the scan faster</strong> - Scanning rate specifies the percentage of the configured read capacity units to use by the AWS Glue crawler. This parameter is available only for DynamoDB data stores.</p>\n\n<p><strong>Split larger files into smaller ones, to reduce the overhead of reading large files for the Glue Crawler</strong> - This statement is incorrect. It takes more time to crawl a large number of small files than a small number of large files. That's because the crawler must list each file and must read the first megabyte of each new file.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/long-running-glue-crawler/\">https://aws.amazon.com/premiumsupport/knowledge-center/long-running-glue-crawler/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html\">https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use an exclude pattern for the Glue crawler to filter out the unwanted files</strong>"
      },
      {
        "answer": "",
        "explanation": "An exclude pattern tells the crawler to skip certain files or paths. Exclude patterns reduce the number of files that the crawler must list, making the crawler run faster. For example, use an exclude pattern to exclude metafiles and files that have already been crawled."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q30-i1.jpg",
        "answer": "",
        "explanation": "More on crawler exclude patterns:"
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Compressed files, like Apache Parquet, take longer to crawl. Instead, use uncompressed files</strong> - Compressed files take longer to crawl. That's because the crawler must download the file and decompress it before reading the first megabyte or listing the file. For Apache Parquet, Apache Avro, and Apache Orc files, the crawler doesn't crawl the first megabyte. Instead, the crawler reads the metadata stored in each file. So this option is not relevant for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the <code>Scanning rate</code> parameter of Glue Crawler to a higher value for the Crawler to complete the scan faster</strong> - Scanning rate specifies the percentage of the configured read capacity units to use by the AWS Glue crawler. This parameter is available only for DynamoDB data stores."
      },
      {
        "answer": "",
        "explanation": "<strong>Split larger files into smaller ones, to reduce the overhead of reading large files for the Glue Crawler</strong> - This statement is incorrect. It takes more time to crawl a large number of small files than a small number of large files. That's because the crawler must list each file and must read the first megabyte of each new file."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude",
      "https://aws.amazon.com/premiumsupport/knowledge-center/long-running-glue-crawler/",
      "https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html"
    ]
  },
  {
    "id": 55,
    "question": "<p>The data engineering team at a company is running batch workloads on AWS Cloud. The team has embedded Amazon RDS database connection strings within each web server hosting the flagship application. After failing a security audit, the team is looking at a different approach to store the database secrets securely and automatically rotate the database credentials.</p>\n\n<p>Which of the following solutions would you recommend to meet this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Systems Manager Parameter Store</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Key Management Service (KMS)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Config</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Secrets Manager</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Secrets Manager</strong></p>\n\n<p>AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.</p>\n\n<p>AWS Secrets Manager:\n<img src=\"https://d1.awsstatic.com/diagrams/Secrets-HIW.e84b6533ffb6bd688dad66cfca36622c2fa7c984.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/diagrams/Secrets-HIW.e84b6533ffb6bd688dad66cfca36622c2fa7c984.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Systems Manager Parameter Store</strong> - AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. AWS SSM Parameter Store cannot be used to automatically rotate the database credentials.</p>\n\n<p><strong>AWS Config</strong> - AWS Config helps with auditing and recording compliance of your AWS resources. AWS Config cannot be used to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p><strong>AWS Key Management Service (KMS)</strong> - AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. KMS cannot be used to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Secrets Manager</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB."
      },
      {
        "image": "https://d1.awsstatic.com/diagrams/Secrets-HIW.e84b6533ffb6bd688dad66cfca36622c2fa7c984.png",
        "answer": "",
        "explanation": "AWS Secrets Manager:"
      },
      {
        "link": "https://aws.amazon.com/secrets-manager/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Systems Manager Parameter Store</strong> - AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. AWS SSM Parameter Store cannot be used to automatically rotate the database credentials."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Config</strong> - AWS Config helps with auditing and recording compliance of your AWS resources. AWS Config cannot be used to store your secrets securely and automatically rotate the database credentials."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Key Management Service (KMS)</strong> - AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. KMS cannot be used to store your secrets securely and automatically rotate the database credentials."
      }
    ],
    "references": [
      "https://aws.amazon.com/secrets-manager/",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html"
    ]
  },
  {
    "id": 56,
    "question": "<p>A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Data Engineer Associate to build a multi-tier solution to store and retrieve this location data for analysis.</p>\n\n<p>Which of the following options addresses the given use case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage Amazon API Gateway with AWS Lambda</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Leverage Amazon API Gateway with Amazon Kinesis Data Analytics</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Leverage Amazon QuickSight with Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage Amazon Athena with Amazon S3</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage Amazon API Gateway with Amazon Kinesis Data Analytics</strong></p>\n\n<p>You can use Kinesis Data Analytics to transform and analyze streaming data in real time with Apache Flink. Kinesis Data Analytics enables you to quickly build end-to-end stream processing applications for log analytics, clickstream analytics, Internet of Things (IoT), ad tech, gaming, etc. The four most common use cases are streaming extract-transform-load (ETL), continuous metric generation, responsive real-time analytics, and interactive querying of data streams. Kinesis Data Analytics for Apache Flink applications provides your application with 50 GB of running application storage per Kinesis Processing Unit (KPU).</p>\n\n<p>Amazon API Gateway is a fully managed service that allows you to publish, maintain, monitor, and secure APIs at any scale. Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs, and REST APIs, as well as an option to create WebSocket APIs.</p>\n\n<p>Amazon API Gateway:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q44-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q44-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/api-gateway/faqs/\">https://aws.amazon.com/api-gateway/faqs/</a><p></p>\n\n<p>For the given use case, you can use Amazon API Gateway to create a REST API that handles incoming requests having location data from the trucks and sends it to the Kinesis Data Analytics application on the back end.</p>\n\n<p>Amazon Kinesis Data Analytics:\n<img src=\"https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Amazon-Kinesis-Data-Analytics_HIW.82e3aa53a5c87db03c766218b3d51f1a110c60eb.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Amazon-Kinesis-Data-Analytics_HIW.82e3aa53a5c87db03c766218b3d51f1a110c60eb.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon Athena with Amazon S3</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena cannot be used to build a REST API to consume data from the source. So this option is incorrect.</p>\n\n<p><strong>Leverage Amazon QuickSight with Amazon Redshift</strong> - QuickSight is a cloud-native, serverless business intelligence service. Quicksight cannot be used to build a REST API to consume data from the source. Redshift is a fully managed AWS cloud data warehouse. So this option is incorrect.</p>\n\n<p><strong>Leverage Amazon API Gateway with AWS Lambda</strong> - You cannot use Lambda to store and retrieve the location data for analysis, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/faqs/\">https://aws.amazon.com/api-gateway/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon API Gateway with Amazon Kinesis Data Analytics</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use Kinesis Data Analytics to transform and analyze streaming data in real time with Apache Flink. Kinesis Data Analytics enables you to quickly build end-to-end stream processing applications for log analytics, clickstream analytics, Internet of Things (IoT), ad tech, gaming, etc. The four most common use cases are streaming extract-transform-load (ETL), continuous metric generation, responsive real-time analytics, and interactive querying of data streams. Kinesis Data Analytics for Apache Flink applications provides your application with 50 GB of running application storage per Kinesis Processing Unit (KPU)."
      },
      {
        "answer": "",
        "explanation": "Amazon API Gateway is a fully managed service that allows you to publish, maintain, monitor, and secure APIs at any scale. Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs, and REST APIs, as well as an option to create WebSocket APIs."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q44-i1.jpg",
        "answer": "",
        "explanation": "Amazon API Gateway:"
      },
      {
        "link": "https://aws.amazon.com/api-gateway/faqs/"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can use Amazon API Gateway to create a REST API that handles incoming requests having location data from the trucks and sends it to the Kinesis Data Analytics application on the back end."
      },
      {
        "image": "https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Amazon-Kinesis-Data-Analytics_HIW.82e3aa53a5c87db03c766218b3d51f1a110c60eb.png",
        "answer": "",
        "explanation": "Amazon Kinesis Data Analytics:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-analytics/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon Athena with Amazon S3</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena cannot be used to build a REST API to consume data from the source. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon QuickSight with Amazon Redshift</strong> - QuickSight is a cloud-native, serverless business intelligence service. Quicksight cannot be used to build a REST API to consume data from the source. Redshift is a fully managed AWS cloud data warehouse. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Amazon API Gateway with AWS Lambda</strong> - You cannot use Lambda to store and retrieve the location data for analysis, so this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/api-gateway/faqs/",
      "https://aws.amazon.com/kinesis/data-analytics/",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html",
      "https://aws.amazon.com/kinesis/data-analytics/faqs/"
    ]
  },
  {
    "id": 57,
    "question": "<p>A company uses several analytics services from AWS such as AWS Glue, Amazon Athena, Amazon Redshift Spectrum, and Amazon EMR to query the data. The company wants to build a data lake in AWS that can provide row-level and column-level data access to specific teams/groups that use the aforementioned AWS services.</p>\n\n<p>Which solution can address these requirements with the LEAST operational overhead?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon S3 for data lake storage. Use Apache Ranger through Amazon EMR to restrict data access by rows and columns. Provide data access by using Apache Pig</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up Amazon S3 as the data lake service. Define the Amazon S3 access point for each AWS service. Configure each access point with distinct permissions and network controls needed for each AWS service</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up Amazon S3 as the data lake service. Use Amazon Redshift Spectrum to query data in Amazon S3 files without having to load the data into Amazon Redshift tables. Configure access control through the IAM roles attached to the Redshift cluster</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up Amazon S3 as the data lake service. Configure AWS Lake Formation permissions to provide fine-grained row and column-wise access. Provide access to data for the AWS services via Lake Formation permissions only</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up Amazon S3 as the data lake service. Configure AWS Lake Formation permissions to provide fine-grained row and column-wise access. Provide access to data for the AWS services via Lake Formation permissions only</strong></p>\n\n<p>AWS Lake Formation helps you centrally govern, secure, and globally share data for analytics and machine learning. With Lake Formation, you can manage fine-grained access control for your data lake data on Amazon Simple Storage Service (Amazon S3) and its metadata in the AWS Glue Data Catalog.</p>\n\n<p>AWS Lake Formation permissions are enforced using granular controls at the column, row, and cell levels across AWS analytics and machine learning services, including Amazon Athena, Amazon QuickSight, Amazon Redshift Spectrum, Amazon EMR, and AWS Glue.</p>\n\n<p>AWS Lake Formation permissions management workflow:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q14-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q14-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/how-it-works.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/how-it-works.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up Amazon S3 as the data lake service. Use Amazon Redshift Spectrum to query data in Amazon S3 files without having to load the data into Amazon Redshift tables. Configure access control through the IAM roles attached to the Redshift cluster</strong> - IAM roles cannot provide granular row-level and column-level data access permissions.</p>\n\n<p><strong>Use Amazon S3 as the data lake service. Define the Amazon S3 access point for each AWS service. Configure each access point with distinct permissions and network controls needed for each AWS service</strong> - Each access point has distinct permissions and network controls that S3 applies for any request that is made through that access point. However, fine-grained access (such as row-level and column-level data access) is not possible via S3 access points.</p>\n\n<p><strong>Use Amazon S3 for data lake storage. Use Apache Ranger through Amazon EMR to restrict data access by rows and columns. Provide data access by using Apache Pig</strong> - Amazon EMR natively integrates with Apache Ranger, allowing you to define, enforce, and audit fine-grained data access control. With this feature, you can define and enforce database, table, and column-level authorization policies for Apache Spark and Apache Hive users to access data through Hive Metastore. Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. The salient property of Pig programs is that their structure is amenable to substantial parallelization, which in turn enables them to handle very large data sets. For the given use case, configuring Amazon EMR adds a lot of operational overhead and additional costs compared to just implementing the required access through Lake Formation.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/service-integrations.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/service-integrations.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon S3 as the data lake service. Configure AWS Lake Formation permissions to provide fine-grained row and column-wise access. Provide access to data for the AWS services via Lake Formation permissions only</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Lake Formation helps you centrally govern, secure, and globally share data for analytics and machine learning. With Lake Formation, you can manage fine-grained access control for your data lake data on Amazon Simple Storage Service (Amazon S3) and its metadata in the AWS Glue Data Catalog."
      },
      {
        "answer": "",
        "explanation": "AWS Lake Formation permissions are enforced using granular controls at the column, row, and cell levels across AWS analytics and machine learning services, including Amazon Athena, Amazon QuickSight, Amazon Redshift Spectrum, Amazon EMR, and AWS Glue."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q14-i1.jpg",
        "answer": "",
        "explanation": "AWS Lake Formation permissions management workflow:"
      },
      {
        "link": "https://docs.aws.amazon.com/lake-formation/latest/dg/how-it-works.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up Amazon S3 as the data lake service. Use Amazon Redshift Spectrum to query data in Amazon S3 files without having to load the data into Amazon Redshift tables. Configure access control through the IAM roles attached to the Redshift cluster</strong> - IAM roles cannot provide granular row-level and column-level data access permissions."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 as the data lake service. Define the Amazon S3 access point for each AWS service. Configure each access point with distinct permissions and network controls needed for each AWS service</strong> - Each access point has distinct permissions and network controls that S3 applies for any request that is made through that access point. However, fine-grained access (such as row-level and column-level data access) is not possible via S3 access points."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 for data lake storage. Use Apache Ranger through Amazon EMR to restrict data access by rows and columns. Provide data access by using Apache Pig</strong> - Amazon EMR natively integrates with Apache Ranger, allowing you to define, enforce, and audit fine-grained data access control. With this feature, you can define and enforce database, table, and column-level authorization policies for Apache Spark and Apache Hive users to access data through Hive Metastore. Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. The salient property of Pig programs is that their structure is amenable to substantial parallelization, which in turn enables them to handle very large data sets. For the given use case, configuring Amazon EMR adds a lot of operational overhead and additional costs compared to just implementing the required access through Lake Formation."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lake-formation/latest/dg/how-it-works.html",
      "https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html",
      "https://docs.aws.amazon.com/lake-formation/latest/dg/service-integrations.html"
    ]
  },
  {
    "id": 58,
    "question": "<p>An e-commerce application provides a visual search feature by letting the customer take a picture of any item and provide a wide selection of similar items that the customer can buy with just a few clicks on the app. Creating the best user experience requires that the machine learning framework of the application identify objects and their attributes from the given set of images and return visually and contextually similar recommendations. With over a million users, the company is looking at the MOST cost-effective solution to store these images as well as run the underlying machine learning engine.</p>\n\n<p>Which of the following represents the best solution for this use case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Elastic File System (Amazon EFS) to store the images of objects. The machine learning framework of the application should be hosted on Amazon SageMaker</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon S3 to store the images of objects. The machine learning framework of the application should be hosted on Amazon EC2 instances</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 to store the images of objects. The machine learning framework of the application should be hosted on Amazon SageMaker</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Elastic File System (Amazon EFS) to store the images of objects. The machine learning framework of the application should be hosted on Amazon EC2 instances</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 to store the images of objects. The machine learning framework of the application should be hosted on Amazon EC2 instances</strong> - Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can store and protect any amount of data for virtually any use case, such as data lakes, cloud-native applications, and mobile apps. S3 offers the most cost-effective storage for storing the images of objects, thereby making it the right fit for this use case. In addition, you can use EC2 instances to deploy and host the machine learning framework of the application at the most cost-effective rate.</p>\n\n<p>Amazon S3 advantages:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q27-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q27-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/s3/\">https://aws.amazon.com/s3/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon S3 to store the images of objects. The machine learning framework of the application should be hosted on Amazon SageMaker</strong> - Amazon SageMaker is a fully managed service to build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. If cost is the primary optimization criteria, then SageMaker does not represent the best option, as the SageMaker instances are estimated to be 40% costlier than Amazon EC2 instances.\nvia - <a href=\"https://medium.com/radix-ai-blog/is-sagemaker-worth-it-4b78a2082ca9\">https://medium.com/radix-ai-blog/is-sagemaker-worth-it-4b78a2082ca9</a></p>\n\n<p><strong>Use Amazon Elastic File System (Amazon EFS) to store the images of objects. The machine learning framework of the application should be hosted on Amazon SageMaker</strong></p>\n\n<p><strong>Use Amazon Elastic File System (Amazon EFS) to store the images of objects. The machine learning framework of the application should be hosted on Amazon EC2 instances</strong></p>\n\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, serverless, set-and-forget elastic file system that lets you share file data without provisioning or managing storage. EFS is well suited to support a broad spectrum of use cases from home directories to business-critical applications. Use cases include storage for containerized and serverless applications, big data analytics, web serving and content management, application development, and testing, media and entertainment workflows, and database backups.</p>\n\n<p>Amazon EFS offers a wide range of features but it is a costlier option than Amazon S3. Hence, both these options are incorrect for this use case.</p>\n\n<p>via - <a href=\"https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system\">https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/\">https://aws.amazon.com/s3/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/industries/aws-is-how-pinterest-lens-helps-pinners-find-and-buy-the-perfect-item/\">https://aws.amazon.com/blogs/industries/aws-is-how-pinterest-lens-helps-pinners-find-and-buy-the-perfect-item/</a></p>\n\n<p><a href=\"https://medium.com/radix-ai-blog/is-sagemaker-worth-it-4b78a2082ca9\">https://medium.com/radix-ai-blog/is-sagemaker-worth-it-4b78a2082ca9</a></p>\n\n<p><a href=\"https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system\">https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 to store the images of objects. The machine learning framework of the application should be hosted on Amazon EC2 instances</strong> - Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can store and protect any amount of data for virtually any use case, such as data lakes, cloud-native applications, and mobile apps. S3 offers the most cost-effective storage for storing the images of objects, thereby making it the right fit for this use case. In addition, you can use EC2 instances to deploy and host the machine learning framework of the application at the most cost-effective rate."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q27-i1.jpg",
        "answer": "",
        "explanation": "Amazon S3 advantages:"
      },
      {
        "link": "https://aws.amazon.com/s3/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "link": "https://medium.com/radix-ai-blog/is-sagemaker-worth-it-4b78a2082ca9",
        "answer": "",
        "explanation": "<strong>Use Amazon S3 to store the images of objects. The machine learning framework of the application should be hosted on Amazon SageMaker</strong> - Amazon SageMaker is a fully managed service to build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. If cost is the primary optimization criteria, then SageMaker does not represent the best option, as the SageMaker instances are estimated to be 40% costlier than Amazon EC2 instances.\nvia - <a href=\"https://medium.com/radix-ai-blog/is-sagemaker-worth-it-4b78a2082ca9\">https://medium.com/radix-ai-blog/is-sagemaker-worth-it-4b78a2082ca9</a>"
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Elastic File System (Amazon EFS) to store the images of objects. The machine learning framework of the application should be hosted on Amazon SageMaker</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Elastic File System (Amazon EFS) to store the images of objects. The machine learning framework of the application should be hosted on Amazon EC2 instances</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic File System (Amazon EFS) provides a simple, serverless, set-and-forget elastic file system that lets you share file data without provisioning or managing storage. EFS is well suited to support a broad spectrum of use cases from home directories to business-critical applications. Use cases include storage for containerized and serverless applications, big data analytics, web serving and content management, application development, and testing, media and entertainment workflows, and database backups."
      },
      {
        "answer": "",
        "explanation": "Amazon EFS offers a wide range of features but it is a costlier option than Amazon S3. Hence, both these options are incorrect for this use case."
      },
      {
        "link": "https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system",
        "answer": "",
        "explanation": "via - <a href=\"https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system\">https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system</a>"
      }
    ],
    "references": [
      "https://aws.amazon.com/s3/",
      "https://medium.com/radix-ai-blog/is-sagemaker-worth-it-4b78a2082ca9",
      "https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system",
      "https://aws.amazon.com/blogs/industries/aws-is-how-pinterest-lens-helps-pinners-find-and-buy-the-perfect-item/"
    ]
  },
  {
    "id": 59,
    "question": "<p>A retail company uses Amazon RDS to store sales data. For the analytics workloads that require high performance, only the last six months of data (approximately 50 TB) will be frequently queried. At the end of each month, the monthly sales data will be merged with the historical sales data for the last 5 years, which should also be available for analysis. The CTO at the company is looking at a cost-optimal solution that offers the best performance for this use case.</p>\n\n<p>Which of the following would you select for the given requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure a read replica of the RDS database to store the last six months of data and execute more frequent queries on the read replica. Export RDS data to S3 and schedule an AWS data pipeline for an incremental copy of RDS data to S3. Configure an AWS Glue Data Catalog of the data in S3 and use Amazon Athena to query the historical data in S3</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Export RDS data to S3 and schedule an AWS data pipeline for an incremental copy of RDS data to S3. Load and store the last six months of data from S3 in Amazon Redshift. Configure an Amazon Redshift Spectrum table to connect to all the historical data in S3</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS data pipeline to incrementally load the last six months of data into Amazon Redshift and execute more frequent queries on Redshift. Set up a read replica of the RDS database to run queries on the historical data</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Export RDS data to S3 and schedule an AWS data pipeline for an incremental copy of RDS data to S3. Configure an AWS Glue Data Catalog of the data in S3 and use Amazon Athena to query the entire data in S3</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Export RDS data to S3 and schedule an AWS data pipeline for an incremental copy of RDS data to S3. Load and store the last six months of data from S3 in Amazon Redshift. Configure an Amazon Redshift Spectrum table to connect to all the historical data in S3</strong></p>\n\n<p>You can use the AWS data pipeline to automate the movement and transformation of data. It allows you to quickly define a dependent chain of data sources, destinations, and predefined or custom data processing activities called a pipeline. Based on a schedule you define, your pipeline regularly performs processing activities such as distributed data copy, SQL transforms, MapReduce applications, or custom scripts against destinations such as Amazon S3, Amazon RDS, Amazon Redshift or Amazon DynamoDB.</p>\n\n<p>You can use the AWS data pipeline to set up a one-time export of RDS data to S3 as well as an ongoing incremental copy of RDS data to S3. You can also use the AWS data pipeline to load data from S3 to Redshift.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q18-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q18-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html</a><p></p>\n\n<p>Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale. For the given use case, you can use the AWS data pipeline to load data from S3 to Redshift and then use Redshift to query the last six months of data to ensure high performance. Further, you can use Amazon Redshift Spectrum to query the entire historical data in S3 which would be a cost-effective solution to facilitate analysis of the historical data.</p>\n\n<p>Using Redshift Spectrum, Amazon Redshift customers can easily query their data in Amazon S3. Redshift Spectrum is a built-in feature of Amazon Redshift, and your existing queries and BI tools will continue to work seamlessly. Under the hood, AWS manages a fleet of thousands of Redshift Spectrum nodes spread across multiple Availability Zones. These are transparently scaled and allocated to your queries based on the data that you need to process, with no provisioning or commitments. Redshift Spectrum is also highly concurrent—you can access your Amazon S3 data from any number of Amazon Redshift clusters.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q18-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q18-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a read replica of the RDS database to store the last six months of data and execute more frequent queries on the read replica. Export RDS data to S3 and schedule an AWS data pipeline for an incremental copy of RDS data to S3. Configure an AWS Glue Data Catalog of the data in S3 and use Amazon Athena to query the historical data in S3</strong> - You cannot use RDS to facilitate queries on the last six months of data as it would not match the high performance offered by Redshift, which is a better option for the given requirement.</p>\n\n<p><strong>Use AWS data pipeline to incrementally load the last six months of data into Amazon Redshift and execute more frequent queries on Redshift. Set up a read replica of the RDS database to run queries on the historical data</strong> - Using a read replica on RDS to store and analyze the entire historical data would turn out to be costlier than storing the data on S3 and enable analysis using Redshift spectrum. So this option is incorrect.</p>\n\n<p><strong>Export RDS data to S3 and schedule an AWS data pipeline for an incremental copy of RDS data to S3. Configure an AWS Glue Data Catalog of the data in S3 and use Amazon Athena to query the entire data in S3</strong> - It is certainly possible to query the entire data in S3 using Athena, however, it will not be able to match the high performance offered by Redshift to query the last six months of data. So this option is not the best fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Export RDS data to S3 and schedule an AWS data pipeline for an incremental copy of RDS data to S3. Load and store the last six months of data from S3 in Amazon Redshift. Configure an Amazon Redshift Spectrum table to connect to all the historical data in S3</strong>"
      },
      {
        "answer": "",
        "explanation": "You can use the AWS data pipeline to automate the movement and transformation of data. It allows you to quickly define a dependent chain of data sources, destinations, and predefined or custom data processing activities called a pipeline. Based on a schedule you define, your pipeline regularly performs processing activities such as distributed data copy, SQL transforms, MapReduce applications, or custom scripts against destinations such as Amazon S3, Amazon RDS, Amazon Redshift or Amazon DynamoDB."
      },
      {
        "answer": "",
        "explanation": "You can use the AWS data pipeline to set up a one-time export of RDS data to S3 as well as an ongoing incremental copy of RDS data to S3. You can also use the AWS data pipeline to load data from S3 to Redshift."
      },
      {
        "link": "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html"
      },
      {
        "answer": "",
        "explanation": "Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale. For the given use case, you can use the AWS data pipeline to load data from S3 to Redshift and then use Redshift to query the last six months of data to ensure high performance. Further, you can use Amazon Redshift Spectrum to query the entire historical data in S3 which would be a cost-effective solution to facilitate analysis of the historical data."
      },
      {
        "answer": "",
        "explanation": "Using Redshift Spectrum, Amazon Redshift customers can easily query their data in Amazon S3. Redshift Spectrum is a built-in feature of Amazon Redshift, and your existing queries and BI tools will continue to work seamlessly. Under the hood, AWS manages a fleet of thousands of Redshift Spectrum nodes spread across multiple Availability Zones. These are transparently scaled and allocated to your queries based on the data that you need to process, with no provisioning or commitments. Redshift Spectrum is also highly concurrent—you can access your Amazon S3 data from any number of Amazon Redshift clusters."
      },
      {
        "link": "https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure a read replica of the RDS database to store the last six months of data and execute more frequent queries on the read replica. Export RDS data to S3 and schedule an AWS data pipeline for an incremental copy of RDS data to S3. Configure an AWS Glue Data Catalog of the data in S3 and use Amazon Athena to query the historical data in S3</strong> - You cannot use RDS to facilitate queries on the last six months of data as it would not match the high performance offered by Redshift, which is a better option for the given requirement."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS data pipeline to incrementally load the last six months of data into Amazon Redshift and execute more frequent queries on Redshift. Set up a read replica of the RDS database to run queries on the historical data</strong> - Using a read replica on RDS to store and analyze the entire historical data would turn out to be costlier than storing the data on S3 and enable analysis using Redshift spectrum. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Export RDS data to S3 and schedule an AWS data pipeline for an incremental copy of RDS data to S3. Configure an AWS Glue Data Catalog of the data in S3 and use Amazon Athena to query the entire data in S3</strong> - It is certainly possible to query the entire data in S3 using Athena, however, it will not be able to match the high performance offered by Redshift to query the last six months of data. So this option is not the best fit for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html",
      "https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/",
      "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html",
      "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html",
      "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html"
    ]
  },
  {
    "id": 60,
    "question": "<p>A company wants to use AWS for its connected cab application that would collect sensor data from its electric cab fleet to give drivers dynamically updated map information. The company would like to build its new sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the company does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. The company has hired you to provide consultancy for this strategic initiative.</p>\n\n<p>Given these constraints, which of the following solutions would you suggest as the BEST fit to develop this service?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Ingest the sensor data in a Kinesis Data Stream, which is polled by an application running on an EC2 instance, and the data is written into an auto-scaled DynamoDB table for downstream processing</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Ingest the sensor data in an Amazon SQS standard queue, which is polled by a Lambda function in batches, and the data is written into an auto-scaled DynamoDB table for downstream processing</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Ingest the sensor data in an Amazon SQS standard queue, which is polled by an application running on an EC2 instance, and the data is written into an auto-scaled DynamoDB table for downstream processing</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Ingest the sensor data in Kinesis Data Firehose, which directly writes the data into an auto-scaled DynamoDB table for downstream processing</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Correct option:\n<strong>Ingest the sensor data in an Amazon SQS standard queue, which is polled by a Lambda function in batches, and the data is written into an auto-scaled DynamoDB table for downstream processing</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>You can use an AWS Lambda function to process messages in an Amazon Simple Queue Service (Amazon SQS) queue. Lambda event source mappings support standard queues and first-in, first-out (FIFO) queues. With Amazon SQS, you can offload tasks from one component of your application by sending them to a queue and processing them asynchronously.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q19-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q19-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html</a><p></p>\n\n<p>AWS manages all ongoing operations and underlying infrastructure needed to provide a highly available and scalable message queuing service. With SQS, there is no upfront cost, no need to acquire, install, and configure messaging software, and no time-consuming build-out and maintenance of supporting infrastructure. SQS queues are dynamically created and scale automatically so you can build and grow applications quickly and efficiently.\nAs there is no need to manually provision the capacity, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Ingest the sensor data in Kinesis Data Firehose, which directly writes the data into an auto-scaled DynamoDB table for downstream processing</strong></p>\n\n<p>Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic.</p>\n\n<p>Firehose cannot directly write into a DynamoDB table, so this option is incorrect.</p>\n\n<p><strong>Ingest the sensor data in an Amazon SQS standard queue, which is polled by an application running on an EC2 instance, and the data is written into an auto-scaled DynamoDB table for downstream processing</strong></p>\n\n<p><strong>Ingest the sensor data in a Kinesis Data Stream, which is polled by an application running on an EC2 instance, and the data is written into an auto-scaled DynamoDB table for downstream processing</strong></p>\n\n<p>Using an application on an EC2 instance is ruled out as the company wants to use fully serverless components. So both these options are incorrect.</p>\n\n<p>References:\n<a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Ingest the sensor data in Kinesis Data Firehose, which directly writes the data into an auto-scaled DynamoDB table for downstream processing</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic."
      },
      {
        "answer": "",
        "explanation": "Firehose cannot directly write into a DynamoDB table, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Ingest the sensor data in an Amazon SQS standard queue, which is polled by an application running on an EC2 instance, and the data is written into an auto-scaled DynamoDB table for downstream processing</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Ingest the sensor data in a Kinesis Data Stream, which is polled by an application running on an EC2 instance, and the data is written into an auto-scaled DynamoDB table for downstream processing</strong>"
      },
      {
        "answer": "",
        "explanation": "Using an application on an EC2 instance is ruled out as the company wants to use fully serverless components. So both these options are incorrect."
      },
      {
        "link": "https://aws.amazon.com/sqs/",
        "answer": "",
        "explanation": "References:\n<a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a>"
      },
      {
        "link": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
        "answer": "",
        "explanation": "<a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html</a>"
      },
      {
        "link": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
        "answer": "",
        "explanation": "<a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html</a>"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/faqs/",
        "answer": "",
        "explanation": "<a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a>"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
      "https://aws.amazon.com/sqs/",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 61,
    "question": "<p>A company is looking for an automated orchestration solution that will run a job once every day. Once triggered, the job searches for new data that is added to an Amazon S3 bucket and then performs an extract, transform, and load (ETL) sequence to transform the newly added data. The transformed data is saved back to another S3 bucket and then an AWS Lambda function is triggered to start the downstream process.</p>\n\n<p>Which AWS service/feature can be used to meet these requirements with the LEAST operational overhead?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Glue DataBrew</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Step Functions tasks</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon Managed Workflows for Apache Airflow (Amazon MWAA) workflows</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Glue workflows</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Step Functions tasks</strong></p>\n\n<p>AWS Step Functions is a serverless orchestration service that lets you integrate with AWS Lambda functions and other AWS services to build business-critical applications. Through Step Functions' graphical console, you see your application’s workflow as a series of event-driven steps.</p>\n\n<p>Step Functions automatically triggers and tracks each step and retries when there are errors, so your application executes in order and as expected. Step Functions logs the state of each step, so when things do go wrong, you can diagnose and debug problems more quickly. You can change and add steps without even writing code, so you can more easily evolve your application and innovate faster.</p>\n\n<p>Depending on your use case, you can have Step Functions call AWS services, such as Lambda, to perform tasks. You can create workflows that process and publish machine learning models. You can have Step Functions control AWS services, such as AWS Glue, to create extract, transform, and load (ETL) workflows. You also can create long-running, automated workflows for applications that require human interaction.</p>\n\n<p>Serverless orchestration with Step Functions:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q7-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q7-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Glue DataBrew</strong> - AWS Glue DataBrew is a visual data preparation tool that makes it easier for data analysts and data scientists to clean and normalize data to prepare it for analytics and machine learning (ML). You can choose from over 250 prebuilt transformations to automate data preparation tasks, all without the need to write any code. This option has been added as a distractor.</p>\n\n<p><strong>AWS Glue workflows</strong> - AWS Glue workflows provide a visual and programmatic tool to author data pipelines by combining AWS Glue crawlers for schema discovery and AWS Glue Spark and Python shell jobs to transform the data. A workflow consists of one of more task nodes arranged as a graph. AWS Glue workflows cannot be used as a generic orchestration service that can coordinate/invoke other AWS services such as AWS Lambda, so this option is incorrect.</p>\n\n<p><strong>Amazon Managed Workflows for Apache Airflow (Amazon MWAA) workflows</strong> - Amazon Managed Workflows for Apache Airflow is a managed orchestration service for Apache Airflow that you can use to set up and operate data pipelines in the cloud at scale. Apache Airflow is an open-source tool used to programmatically author, schedule, and monitor sequences of processes and tasks referred to as workflows. This option requires more operational overhead to configure and set up the components for the workflow, so this is not the best fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p><a href=\"https://aws.amazon.com/glue/features/databrew/\">https://aws.amazon.com/glue/features/databrew/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html\">https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Step Functions tasks</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS Step Functions is a serverless orchestration service that lets you integrate with AWS Lambda functions and other AWS services to build business-critical applications. Through Step Functions' graphical console, you see your application’s workflow as a series of event-driven steps."
      },
      {
        "answer": "",
        "explanation": "Step Functions automatically triggers and tracks each step and retries when there are errors, so your application executes in order and as expected. Step Functions logs the state of each step, so when things do go wrong, you can diagnose and debug problems more quickly. You can change and add steps without even writing code, so you can more easily evolve your application and innovate faster."
      },
      {
        "answer": "",
        "explanation": "Depending on your use case, you can have Step Functions call AWS services, such as Lambda, to perform tasks. You can create workflows that process and publish machine learning models. You can have Step Functions control AWS services, such as AWS Glue, to create extract, transform, and load (ETL) workflows. You also can create long-running, automated workflows for applications that require human interaction."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q7-i1.jpg",
        "answer": "",
        "explanation": "Serverless orchestration with Step Functions:"
      },
      {
        "link": "https://aws.amazon.com/step-functions/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Glue DataBrew</strong> - AWS Glue DataBrew is a visual data preparation tool that makes it easier for data analysts and data scientists to clean and normalize data to prepare it for analytics and machine learning (ML). You can choose from over 250 prebuilt transformations to automate data preparation tasks, all without the need to write any code. This option has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Glue workflows</strong> - AWS Glue workflows provide a visual and programmatic tool to author data pipelines by combining AWS Glue crawlers for schema discovery and AWS Glue Spark and Python shell jobs to transform the data. A workflow consists of one of more task nodes arranged as a graph. AWS Glue workflows cannot be used as a generic orchestration service that can coordinate/invoke other AWS services such as AWS Lambda, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Managed Workflows for Apache Airflow (Amazon MWAA) workflows</strong> - Amazon Managed Workflows for Apache Airflow is a managed orchestration service for Apache Airflow that you can use to set up and operate data pipelines in the cloud at scale. Apache Airflow is an open-source tool used to programmatically author, schedule, and monitor sequences of processes and tasks referred to as workflows. This option requires more operational overhead to configure and set up the components for the workflow, so this is not the best fit."
      }
    ],
    "references": [
      "https://aws.amazon.com/step-functions/",
      "https://aws.amazon.com/glue/features/databrew/",
      "https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html"
    ]
  },
  {
    "id": 62,
    "question": "<p>A company helps its customers legally sign highly confidential contracts. To meet the regulatory requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud and plans on using Amazon Simple Storage Service (Amazon S3) to store the signed contracts.</p>\n\n<p>What do you recommend?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Client Side Encryption</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Server-side encryption with Amazon S3 managed keys (SSE-S3)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Server-side encryption with customer-provided keys (SSE-C)</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Server-side encryption with AWS KMS keys (SSE-KMS)</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Client Side Encryption</strong></p>\n\n<p>Client-side encryption is the act of encrypting your data locally to help ensure its security in transit and at rest. To encrypt your objects before you send them to Amazon S3, use the Amazon S3 Encryption Client. When your objects are encrypted in this manner, your objects aren't exposed to any third party, including AWS. Amazon S3 receives your objects already encrypted; Amazon S3 does not play a role in encrypting or decrypting your objects. You can use both the Amazon S3 Encryption Client and server-side encryption to encrypt your data. When you send encrypted objects to Amazon S3, Amazon S3 doesn't recognize the objects as being encrypted, it only detects typical objects.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Server-side encryption with AWS KMS keys (SSE-KMS)</strong> - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom.</p>\n\n<p><strong>Server-side encryption with Amazon S3 managed keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key.</p>\n\n<p><strong>Server-side encryption with customer-provided keys (SSE-C)</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects.</p>\n\n<p>For these three options, the encryption is being done on the server side and the company's proprietary algorithm plays no role in this set up. So, these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Client Side Encryption</strong>"
      },
      {
        "answer": "",
        "explanation": "Client-side encryption is the act of encrypting your data locally to help ensure its security in transit and at rest. To encrypt your objects before you send them to Amazon S3, use the Amazon S3 Encryption Client. When your objects are encrypted in this manner, your objects aren't exposed to any third party, including AWS. Amazon S3 receives your objects already encrypted; Amazon S3 does not play a role in encrypting or decrypting your objects. You can use both the Amazon S3 Encryption Client and server-side encryption to encrypt your data. When you send encrypted objects to Amazon S3, Amazon S3 doesn't recognize the objects as being encrypted, it only detects typical objects."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Server-side encryption with AWS KMS keys (SSE-KMS)</strong> - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom."
      },
      {
        "answer": "",
        "explanation": "<strong>Server-side encryption with Amazon S3 managed keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key."
      },
      {
        "answer": "",
        "explanation": "<strong>Server-side encryption with customer-provided keys (SSE-C)</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects."
      },
      {
        "answer": "",
        "explanation": "For these three options, the encryption is being done on the server side and the company's proprietary algorithm plays no role in this set up. So, these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"
    ]
  },
  {
    "id": 63,
    "question": "<p>A data engineer is encountering slow query performance while executing Amazon Athena queries on datasets stored in an Amazon S3 bucket, with AWS Glue Data Catalog serving as the metadata repository. The data engineer has identified the root cause of the sluggish performance as the excessive number of partitions in the S3 bucket, leading to increased Athena query planning times.</p>\n\n<p>What are the two possible approaches to mitigate this issue and enhance query efficiency (Select two)?</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Compress the files in gzip format to improve query performance against the partitions</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an AWS Glue partition index and leverage partition filtering via the GetPartitions call</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Transform the data in each partition to Apache ORC format</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Perform bucketing on the data in each partition</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Set up Athena partition projection based on the S3 bucket prefix</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Set up an AWS Glue partition index and leverage partition filtering via the GetPartitions call</strong></p>\n\n<p>When you create a partition index, you specify a list of partition keys that already exist on a given table. The partition index is sub list of partition keys defined in the table. A partition index can be created on any permutation of partition keys defined in the table. For the above sales_data table, the possible indexes are (country, category, creationDate), (country, category, year), (country, category), (country), (category, country, year, month), and so on.</p>\n\n<p>Let's take a sales_data table as an example which is partitioned by the keys Country, Category, Year, Month, and creationDate. If you want to obtain sales data for all the items sold for the Books category in the year 2020 after 2020-08-15, you have to make a GetPartitions request with the expression \"Category = 'Books' and creationDate &gt; '2020-08-15'\" to the Data Catalog.</p>\n\n<p>If no partition indexes are present on the table, AWS Glue loads all the partitions of the table and then filters the loaded partitions using the query expression provided by the user in the GetPartitions request. The query takes more time to run as the number of partitions increases on a table with no indexes. With an index, the GetPartitions query will try to fetch a subset of the partitions instead of loading all the partitions in the table.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q1-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q1-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/partition-indexes.html\">https://docs.aws.amazon.com/glue/latest/dg/partition-indexes.html</a><p></p>\n\n<p><strong>Set up Athena partition projection based on the S3 bucket prefix</strong></p>\n\n<p>Processing partition information can be a bottleneck for Athena queries when you have a very large number of partitions and aren’t using AWS Glue partition indexing. You can use partition projection in Athena to speed up query processing of highly partitioned tables and automate partition management. Partition projection helps minimize this overhead by allowing you to query partitions by calculating partition information rather than retrieving it from a metastore. It eliminates the need to add partitions’ metadata to the AWS Glue table.</p>\n\n<p>In partition projection, partition values, and locations are calculated from configuration rather than read from a repository like the AWS Glue Data Catalog. Because in-memory operations are usually faster than remote operations, partition projection can reduce the runtime of queries against highly partitioned tables. Depending on the specific characteristics of the query and underlying data, partition projection can significantly reduce query runtime for queries that are constrained by partition metadata retrieval.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q1-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q1-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/partition-projection.html\">https://docs.aws.amazon.com/athena/latest/ug/partition-projection.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Transform the data in each partition to Apache ORC format</strong> - Apache ORC is a popular file format for analytics workloads. It is a columnar file format because it stores data not by row, but by column. ORC format also allows query engines to reduce the amount of data that needs to be loaded in different ways. For example, by storing and compressing columns separately, you can achieve higher compression ratios and only the columns referenced in a query need to be read. However, the data is being transformed within the existing partitions, this option does not resolve the root cause of under-performance (that is, the excessive number of partitions in the S3 bucket).</p>\n\n<p><strong>Compress the files in gzip format to improve query performance against the partitions</strong> - Compressing your data can speed up your queries significantly. The smaller data sizes reduce the data scanned from Amazon S3, resulting in lower costs of running queries. It also reduces the network traffic from Amazon S3 to Athena. Athena supports a variety of compression formats, including common formats like gzip, Snappy, and zstd. However, the data is being compressed within the existing partitions, this option does not resolve the root cause of under-performance (that is, the excessive number of partitions in the S3 bucket).</p>\n\n<p><strong>Perform bucketing on the data in each partition</strong> - Bucketing is a way to organize the records of a dataset into categories called buckets. This meaning of bucket and bucketing is different from, and should not be confused with Amazon S3 buckets. In data bucketing, records that have the same value for a property go into the same bucket. Records are distributed as evenly as possible among buckets so that each bucket has roughly the same amount of data. In practice, the buckets are files, and a hash function determines the bucket that a record goes into. A bucketed dataset will have one or more files per bucket per partition. The bucket that a file belongs to is encoded in the file name. Bucketing is useful when a dataset is bucketed by a certain property and you want to retrieve records in which that property has a certain value. Because the data is bucketed, Athena can use the value to determine which files to look at. For example, suppose a dataset is bucketed by customer_id and you want to find all records for a specific customer. Athena determines the bucket that contains those records and only reads the files in that bucket.</p>\n\n<p>Good candidates for bucketing occur when you have columns that have high cardinality (that is, have many distinct values), are uniformly distributed, and that you frequently query for specific values.</p>\n\n<p>Since bucketing is being done within the existing partitions, this option does not resolve the root cause of under-performance (that is, the excessive number of partitions in the S3 bucket).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/partition-indexes.html\">https://docs.aws.amazon.com/glue/latest/dg/partition-indexes.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/partition-projection.html\">https://docs.aws.amazon.com/athena/latest/ug/partition-projection.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/ctas-partitioning-and-bucketing.html\">https://docs.aws.amazon.com/athena/latest/ug/ctas-partitioning-and-bucketing.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an AWS Glue partition index and leverage partition filtering via the GetPartitions call</strong>"
      },
      {
        "answer": "",
        "explanation": "When you create a partition index, you specify a list of partition keys that already exist on a given table. The partition index is sub list of partition keys defined in the table. A partition index can be created on any permutation of partition keys defined in the table. For the above sales_data table, the possible indexes are (country, category, creationDate), (country, category, year), (country, category), (country), (category, country, year, month), and so on."
      },
      {
        "answer": "",
        "explanation": "Let's take a sales_data table as an example which is partitioned by the keys Country, Category, Year, Month, and creationDate. If you want to obtain sales data for all the items sold for the Books category in the year 2020 after 2020-08-15, you have to make a GetPartitions request with the expression \"Category = 'Books' and creationDate &gt; '2020-08-15'\" to the Data Catalog."
      },
      {
        "answer": "",
        "explanation": "If no partition indexes are present on the table, AWS Glue loads all the partitions of the table and then filters the loaded partitions using the query expression provided by the user in the GetPartitions request. The query takes more time to run as the number of partitions increases on a table with no indexes. With an index, the GetPartitions query will try to fetch a subset of the partitions instead of loading all the partitions in the table."
      },
      {
        "link": "https://docs.aws.amazon.com/glue/latest/dg/partition-indexes.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Set up Athena partition projection based on the S3 bucket prefix</strong>"
      },
      {
        "answer": "",
        "explanation": "Processing partition information can be a bottleneck for Athena queries when you have a very large number of partitions and aren’t using AWS Glue partition indexing. You can use partition projection in Athena to speed up query processing of highly partitioned tables and automate partition management. Partition projection helps minimize this overhead by allowing you to query partitions by calculating partition information rather than retrieving it from a metastore. It eliminates the need to add partitions’ metadata to the AWS Glue table."
      },
      {
        "answer": "",
        "explanation": "In partition projection, partition values, and locations are calculated from configuration rather than read from a repository like the AWS Glue Data Catalog. Because in-memory operations are usually faster than remote operations, partition projection can reduce the runtime of queries against highly partitioned tables. Depending on the specific characteristics of the query and underlying data, partition projection can significantly reduce query runtime for queries that are constrained by partition metadata retrieval."
      },
      {
        "link": "https://docs.aws.amazon.com/athena/latest/ug/partition-projection.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Transform the data in each partition to Apache ORC format</strong> - Apache ORC is a popular file format for analytics workloads. It is a columnar file format because it stores data not by row, but by column. ORC format also allows query engines to reduce the amount of data that needs to be loaded in different ways. For example, by storing and compressing columns separately, you can achieve higher compression ratios and only the columns referenced in a query need to be read. However, the data is being transformed within the existing partitions, this option does not resolve the root cause of under-performance (that is, the excessive number of partitions in the S3 bucket)."
      },
      {
        "answer": "",
        "explanation": "<strong>Compress the files in gzip format to improve query performance against the partitions</strong> - Compressing your data can speed up your queries significantly. The smaller data sizes reduce the data scanned from Amazon S3, resulting in lower costs of running queries. It also reduces the network traffic from Amazon S3 to Athena. Athena supports a variety of compression formats, including common formats like gzip, Snappy, and zstd. However, the data is being compressed within the existing partitions, this option does not resolve the root cause of under-performance (that is, the excessive number of partitions in the S3 bucket)."
      },
      {
        "answer": "",
        "explanation": "<strong>Perform bucketing on the data in each partition</strong> - Bucketing is a way to organize the records of a dataset into categories called buckets. This meaning of bucket and bucketing is different from, and should not be confused with Amazon S3 buckets. In data bucketing, records that have the same value for a property go into the same bucket. Records are distributed as evenly as possible among buckets so that each bucket has roughly the same amount of data. In practice, the buckets are files, and a hash function determines the bucket that a record goes into. A bucketed dataset will have one or more files per bucket per partition. The bucket that a file belongs to is encoded in the file name. Bucketing is useful when a dataset is bucketed by a certain property and you want to retrieve records in which that property has a certain value. Because the data is bucketed, Athena can use the value to determine which files to look at. For example, suppose a dataset is bucketed by customer_id and you want to find all records for a specific customer. Athena determines the bucket that contains those records and only reads the files in that bucket."
      },
      {
        "answer": "",
        "explanation": "Good candidates for bucketing occur when you have columns that have high cardinality (that is, have many distinct values), are uniformly distributed, and that you frequently query for specific values."
      },
      {
        "answer": "",
        "explanation": "Since bucketing is being done within the existing partitions, this option does not resolve the root cause of under-performance (that is, the excessive number of partitions in the S3 bucket)."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/glue/latest/dg/partition-indexes.html",
      "https://docs.aws.amazon.com/athena/latest/ug/partition-projection.html",
      "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
      "https://docs.aws.amazon.com/athena/latest/ug/ctas-partitioning-and-bucketing.html"
    ]
  },
  {
    "id": 64,
    "question": "<p>The data engineering team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\".</p>\n\n<p>Which of the following AWS database services would you suggest as the BEST fit to handle such use cases?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Neptune</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon OpenSearch Service</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Aurora</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Redshift</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Neptune</strong></p>\n\n<p>Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security.</p>\n\n<p>Amazon Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune is secure with support for HTTPS-encrypted client connections and encryption at rest. Neptune is fully managed, so you no longer need to worry about database management tasks such as hardware provisioning, software patching, setup, configuration, or backups.</p>\n\n<p>Amazon Neptune can quickly and easily process large sets of user profiles and interactions to build social networking applications. Neptune enables highly interactive graph queries with high throughput to bring social features into your applications. For example, if you are building a social feed into your application, you can use Neptune to provide results that prioritize showing your users the latest updates from their family, from friends whose updates they ‘Like,’ and from friends who live close to them.</p>\n\n<p>Social Networking example with Amazon Neptune:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q48-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q48-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/neptune/\">https://aws.amazon.com/neptune/</a><p></p>\n\n<p>Identity graphs example with Amazon Neptune:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q48-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q48-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/neptune/\">https://aws.amazon.com/neptune/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon OpenSearch Service</strong> - Amazon OpenSearch Service is a managed service that makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open-source, distributed search and analytics suite derived from Elasticsearch. Amazon OpenSearch Service offers the latest versions of OpenSearch, support for 19 versions of Elasticsearch (1.5 to 7.10 versions), as well as visualization capabilities powered by OpenSearch Dashboards and Kibana (1.5 to 7.10 versions). Amazon OpenSearch Service currently has tens of thousands of active customers with hundreds of thousands of clusters under management processing trillions of requests per month.</p>\n\n<p><strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis. The given use case is not about data warehousing, so this is not a correct option.</p>\n\n<p><strong>Amazon Aurora</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64 terabytes per database instance. Aurora is not an in-memory database. Here, we need a graph database due to the highly connected datasets and queries, therefore Neptune is the best option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/neptune/\">https://aws.amazon.com/neptune/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Neptune</strong>"
      },
      {
        "answer": "",
        "explanation": "Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security."
      },
      {
        "answer": "",
        "explanation": "Amazon Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune is secure with support for HTTPS-encrypted client connections and encryption at rest. Neptune is fully managed, so you no longer need to worry about database management tasks such as hardware provisioning, software patching, setup, configuration, or backups."
      },
      {
        "answer": "",
        "explanation": "Amazon Neptune can quickly and easily process large sets of user profiles and interactions to build social networking applications. Neptune enables highly interactive graph queries with high throughput to bring social features into your applications. For example, if you are building a social feed into your application, you can use Neptune to provide results that prioritize showing your users the latest updates from their family, from friends whose updates they ‘Like,’ and from friends who live close to them."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q48-i1.jpg",
        "answer": "",
        "explanation": "Social Networking example with Amazon Neptune:"
      },
      {
        "link": "https://aws.amazon.com/neptune/"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dea-pt/assets/pt3-q48-i2.jpg",
        "answer": "",
        "explanation": "Identity graphs example with Amazon Neptune:"
      },
      {
        "link": "https://aws.amazon.com/neptune/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon OpenSearch Service</strong> - Amazon OpenSearch Service is a managed service that makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open-source, distributed search and analytics suite derived from Elasticsearch. Amazon OpenSearch Service offers the latest versions of OpenSearch, support for 19 versions of Elasticsearch (1.5 to 7.10 versions), as well as visualization capabilities powered by OpenSearch Dashboards and Kibana (1.5 to 7.10 versions). Amazon OpenSearch Service currently has tens of thousands of active customers with hundreds of thousands of clusters under management processing trillions of requests per month."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis. The given use case is not about data warehousing, so this is not a correct option."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Aurora</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64 terabytes per database instance. Aurora is not an in-memory database. Here, we need a graph database due to the highly connected datasets and queries, therefore Neptune is the best option."
      }
    ],
    "references": [
      "https://aws.amazon.com/neptune/"
    ]
  },
  {
    "id": 65,
    "question": "<p>A data engineer is working on the throughput capacity of a newly provisioned table in Amazon DynamoDB. The data engineer has provisioned 20 Read Capacity Units for the table.</p>\n\n<p>Which of the following options represents the correct throughput that the table will support for the various read modes?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Read throughput of 80KB/sec with strong consistency, Read throughput of 160KB/sec with eventual consistency, Transactional read throughput of 40KB/sec</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Read throughput of 40KB/sec with strong consistency, Read throughput of 80KB/sec with eventual consistency, Transactional read throughput of 120KB/sec</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Read throughput of 80KB/sec with strong consistency, Read throughput of 160KB/sec with eventual consistency, Transactional read throughput of 320KB/sec</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Read throughput of 40KB/sec with strong consistency, Read throughput of 80KB/sec with eventual consistency, Transactional read throughput of 60KB/sec</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Correct option:</p>\n\n<p>A read capacity unit (RCU) represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. For example, suppose that you create a table with 10 provisioned read capacity units. This allows you to perform 10 strongly consistent reads per second, or 20 eventually consistent reads per second, for items up to 4 KB. Transactional read requests require two read capacity units to perform one read per second for items up to 4 KB.</p>\n\n<p><strong>Read throughput of 80KB/sec with strong consistency, Read throughput of 160KB/sec with eventual consistency, Transactional read throughput of 40KB/sec</strong></p>\n\n<p>Strongly consistent reads = 20 * 4 = 80KB/sec</p>\n\n<p>Eventually consistent reads = 20 * 4 * 2 = 160KB/sec</p>\n\n<p>Transactional read requests = (20 * 4)/2 = 40KB/sec</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Read throughput of 40KB/sec with strong consistency, Read throughput of 80KB/sec with eventual consistency, Transactional read throughput of 120KB/sec</strong></p>\n\n<p><strong>Read throughput of 80KB/sec with strong consistency, Read throughput of 160KB/sec with eventual consistency, Transactional read throughput of 320KB/sec</strong></p>\n\n<p><strong>Read throughput of 40KB/sec with strong consistency, Read throughput of 80KB/sec with eventual consistency, Transactional read throughput of 60KB/sec</strong></p>\n\n<p>These three options contradict the earlier details provided in the explanation. Hence these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "A read capacity unit (RCU) represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. For example, suppose that you create a table with 10 provisioned read capacity units. This allows you to perform 10 strongly consistent reads per second, or 20 eventually consistent reads per second, for items up to 4 KB. Transactional read requests require two read capacity units to perform one read per second for items up to 4 KB."
      },
      {
        "answer": "",
        "explanation": "<strong>Read throughput of 80KB/sec with strong consistency, Read throughput of 160KB/sec with eventual consistency, Transactional read throughput of 40KB/sec</strong>"
      },
      {
        "answer": "",
        "explanation": "Strongly consistent reads = 20 * 4 = 80KB/sec"
      },
      {
        "answer": "",
        "explanation": "Eventually consistent reads = 20 * 4 * 2 = 160KB/sec"
      },
      {
        "answer": "",
        "explanation": "Transactional read requests = (20 * 4)/2 = 40KB/sec"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Read throughput of 40KB/sec with strong consistency, Read throughput of 80KB/sec with eventual consistency, Transactional read throughput of 120KB/sec</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Read throughput of 80KB/sec with strong consistency, Read throughput of 160KB/sec with eventual consistency, Transactional read throughput of 320KB/sec</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Read throughput of 40KB/sec with strong consistency, Read throughput of 80KB/sec with eventual consistency, Transactional read throughput of 60KB/sec</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the earlier details provided in the explanation. Hence these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html"
    ]
  }
]