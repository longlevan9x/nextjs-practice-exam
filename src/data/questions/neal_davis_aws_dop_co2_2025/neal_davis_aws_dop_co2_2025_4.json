[
  {
    "id": 1,
    "question": "<p>A company runs a serverless application that includes video sharing functionality for logged in users. The video service has become popular, and the company need to know which videos are getting the most interest. A DevOps engineer has been asked to identify the access patterns of the videos. The video files are stored in an Amazon S3 bucket. The information that must be gathered includes the number of users who access specific video files per day and the number of access requests for these files.</p><p>How can the company meet these requirements with the LEAST amount of effort?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Analytics",
    "explanation": "<p>The simplest solution for this request is to enable server access logging on the bucket and then analyze the data that is logged using Amazon Athena. Server access logging provides detailed records for the requests that are made to a bucket. This includes the information requested by the company.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p><p><strong>CORRECT: </strong>\"Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket\" is incorrect $</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/\">https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields",
      "https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/",
      "https://digitalcloud.training/amazon-athena/"
    ]
  },
  {
    "id": 2,
    "question": "<p>A DevOps engineer needs to stream data from Amazon CloudWatch Logs to a VPC-based Amazon OpenSearch Service cluster. The DevOps engineer creates an OpenSearch subscription filter for the OpenSearch cluster. However, the DevOps engineer notices that the logs are not visible in Amazon OpenSearch.</p><p>What should the DevOps engineer do to solve this problem?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Analytics",
    "explanation": "<p>The configuration requires that the Lambda IAM execution role that is specified in the CloudWatch Logs configuration must have the trust relationship configured to allow lambda.amazonaws.com to assume the role. The AWSLambdaVPCAccessExecutionRole policy must also be added to the function to allow access to the VPC-based OpenSearch cluster.</p><p><strong>CORRECT: </strong>\"Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role\" is incorrect.</p><p>The trust relationship is defined in the role attached to the CloudWatch Logs configuration.</p><p><strong>INCORRECT:</strong> \"Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch\" is incorrect.</p><p>This policy should be attached to the Lambda IAM execution role, not the OpenSearch cluster.</p><p><strong>INCORRECT:</strong> \"Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch\" is incorrect.</p><p>The subscription filter is used instead of using an export which is a better and more automated solution.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/\">https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-opensearch/\">https://digitalcloud.training/amazon-opensearch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/",
      "https://digitalcloud.training/amazon-opensearch/"
    ]
  },
  {
    "id": 3,
    "question": "<p>A company leverages AWS CloudFormation to build their infrastructure on AWS. The security department is concerned about sensitive data being exposed. A DevOps engineer has been asked to implement steps to prevent the exposure of sensitive parameters such as passwords during infrastructure deployment operations.</p><p>Which combination of steps should be taken to improve security when deploying infrastructure using AWS CloudFormation? (Select TWO.)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Secrets Manager to encrypt the CloudFormation template at rest using an AWS KMS key.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Systems Manager Parameter Store to store sensitive data as secure strings and reference the strings using tags in the CloudFormation template.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the CloudFormation NoEcho parameter property to mask any sensitive parameter values.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Secrets Manager to store sensitive data as secret values and use dynamic references in AWS CloudFormation.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Use an Amazon S3 bucket with default encryption enabled to store the AWS CloudFormation template.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Management & Governance",
    "explanation": "<p>The key issue here is that some parameters such as passwords may be exposed when deploying infrastructure using AWS CloudFormation. This data could be exposed through the console, command line tools, or API.</p><p>There are a couple of ways the company can deal with this issue. Firstly, AWS Secrets Manager can be used to store secure encrypted values. These values can be securely used in CloudFormation by referencing them as dynamic references.</p><p>Another option is to use the NoEcho parameter property to mask sensitive data. If you set the NoEcho attribute to true, CloudFormation returns the parameter value masked as asterisks (*****) for any calls that describe the stack or stack events.</p><p><strong>CORRECT: </strong>\"Use AWS Secrets Manager to store sensitive data as secret values and use dynamic references in AWS CloudFormation\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use the CloudFormation NoEcho parameter property to mask any sensitive parameter values\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Parameter Store to store sensitive data as secure strings and reference the strings using tags in the CloudFormation template\" is incorrect.</p><p>You cannot securely share secure strings in SSM Parameter Store by using tags in the CloudFormation template. Tags are used for assigning optional metadata.</p><p><strong>INCORRECT:</strong> \"Use AWS Secrets Manager to encrypt the CloudFormation template at rest using an AWS KMS key\" is incorrect.</p><p>Secrets Manager cannot be used to encrypt the template and the main issue is not the encryption of the template but preventing exposure of sensitive data during deployment operations.</p><p><strong>INCORRECT:</strong> \"Use an Amazon S3 bucket with default encryption enabled to store the AWS CloudFormation template\" is incorrect.</p><p>This ensures the encryption of the template but does not protect sensitive data from being exposed during deployment operations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-secret.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-secret.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-secret.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html",
      "https://digitalcloud.training/aws-cloudformation/"
    ]
  },
  {
    "id": 4,
    "question": "<p>A service provider has created business relationships with several companies. The service provider plans to deploy an application to multiple AWS accounts managed by these partner companies using AWS CloudFormation. Each partner company has granted the permissions to create IAM roles with permissions for the deployment in their respective accounts. The organization must minimize operational overhead and stack management.</p><p>Which actions should be taken to deploy the application across these accounts?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create IAM roles in each partner account that grant administrators in the service provider’s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>A <em>stack set</em> lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that template requires.</p><p>You can create a stack set using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. You can create a stack set with either self-managed or service-managed permissions.</p><p>With self-managed permissions, you can deploy stack instances to specific AWS accounts in specific Regions. To do this, you must first create the necessary IAM roles to establish a trusted relationship between the account you're administering the stack set from and the account you're deploying stack instances to.</p><p>With service-managed permissions, you can deploy stack instances to accounts managed by AWS Organizations in specific Regions. With this model, you don't need to create the necessary IAM roles; StackSets creates the IAM roles on your behalf</p><p>In this case the best solution is to use self-managed permissions as the partner companies have only granted the ability to create IAM roles. They would certainly not want their entire account to be controlled by the service provider through AWS Organizations just so they can deploy a single application.</p><p><strong>CORRECT: </strong>\"Create IAM roles in each partner account that grant administrators in the service provider’s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>You cannot have a role in one account that has permissions in another account. The roles must be created in each AWS account and self-managed permissions must be used with AWS CloudFormation stack sets.</p><p><strong>INCORRECT:</strong> \"Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>The partner companies would not want the service provider to control their accounts with AWS Organizations just for a single application. They have granted the ability to create roles which is enough for the service provider to deploy the application using stack sets with self-managed permissions.</p><p><strong>INCORRECT:</strong> \"Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template\" is incorrect.</p><p>The partner companies have granted the ability to create roles, not user accounts. Also, this would be inefficient.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html",
      "https://digitalcloud.training/aws-cloudformation/"
    ]
  },
  {
    "id": 5,
    "question": "<p>As part of single click deployment strategy, a financial organization is utilizing AWS CodeCommit, CodeBuild and CodeDeploy in an automated pipeline using AWS CodePipeline. It was discovered that in some instances, code was deployed which led to vulnerabilities in production systems.</p><p>It is mandated that additional checks must be included before code is promoted into production.</p><p>What additional steps could be taken to ensure code quality?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Pause the deployment stage to perform a final validation on the deployment.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add test stages before and after the deploy action.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Have the build stage trigger an AWS Lambda function that can perform a verification before deployment.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>An approval action can be added to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop, so someone can manually approve or reject the action. This is the simplest and most effective option presented.</p><p><strong>CORRECT: </strong>\"Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Pause the deployment stage to perform a final validation on the deployment\" is incorrect.</p><p>Pausing can help in the above scenario but could lead to additional delay based on user input, adding a manual approval stage is a better solution.</p><p><strong>INCORRECT:</strong> \"Add test stages before and after the deploy action\" is incorrect.</p><p>This could work but would need more configuration and effort as compared to adding a stage.</p><p><strong>INCORRECT:</strong> \"Have the build stage trigger a Lambda function that can perform a verification before deployment\" is incorrect.</p><p>Since the verification could change, Lambda would need customization depending on scenario and would be a time-consuming option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html",
      "https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/"
    ]
  },
  {
    "id": 6,
    "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an application Load Balancer (ALB). The applications runs across two Availability Zones (AZs). Recently the application has become very popular and to increase reliability of the application a DevOps engineer has configured the Auto Scaling group to launch instances across three AZs. However, instances launched in the newly added AZ are not receiving any traffic.</p><p>What is the most likely cause of this issue?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Cross-zone load balancing has not been enabled for the ALB.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The AMI has not been updated to include the new AZ.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The new AZ has not been enabled for the ALB.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>There are no subnets associated with the new AZ.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>Elastic Load Balancing creates a load balancer node for each Availability Zone you enable for the load balancer. The most likely issue here is that the ALB has not been updated with the new AZ so it has not created a node in that AZ and will not distribute traffic there. The DevOps engineer simply needs to update the ALB to include the new AZ and it will then register the targets in the target group and start distributing traffic to them.</p><p><strong>CORRECT: </strong>\"The new AZ has not been enabled for the ALB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Cross-zone load balancing has not been enabled for the ALB\" is incorrect.</p><p>Cross-zone load balancing aims to balance the load between instances evenly across AZs. But you still need to enable the AZ for the ALB.</p><p><strong>INCORRECT:</strong> \"The AMI has not been updated to include the new AZ\" is incorrect.</p><p>AMIs do not have configuration settings to control which AZ they can be launched into.</p><p><strong>INCORRECT:</strong> \"There are no subnets associated with the new AZ\" is incorrect.</p><p>This cannot be the case as the ASG has already been updated with the new AZ and instances have already been launched there.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html",
      "https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/"
    ]
  },
  {
    "id": 7,
    "question": "<p>A DevOps Engineer needs a scalable Node.js application in AWS with a MySQL database. There should be no downtime during deployments and if issues occur rollback to a previous version must be easy to implement. The database may also be used by other applications.</p><p>Which solution meets these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create an Amazon RDS MySQL instance inside the Elastic Beanstalk stack.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create the Amazon RDS MySQL instance outside the Elastic Beanstalk stack.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Deploy the application on Amazon EC2. Configure Elastic Load Balancing and Auto Scaling. Schedule an AWS Lambda function to take regular snapshots of attached EBS volumes. Use an Amazon RDS MySQL instance for the database tier.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the application on Amazon ECS. Configure Elastic Load Balancing and Auto Scaling. Create an ECS service and specify the desired task count. Use an Amazon RDS MySQL instance for the database tier.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>AWS Elastic Beanstalk offers automatic rollback options for deployment updates. This coupled with auto scaling and the ALB meets the requirements for a scalable compute and web tier. The RDS database provides a managed solution for the MySQL database. The RDS MySQL database should be created outside of the Elastic Beanstalk environment as it may be used by other applications. If it is created within the Elastic Beanstalk environment it could be automatically deleted if the environment is deleted.</p><p><strong>CORRECT: </strong>\"Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create the Amazon RDS MySQL instance outside the Elastic Beanstalk stack\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create an Amazon RDS MySQL instance inside the Elastic Beanstalk stack\" is incorrect.</p><p>As explained above the RDS database should be created outside of the Elastic Beanstalk environment.</p><p><strong>INCORRECT:</strong> \"Deploy the application on Amazon ECS. Configure Elastic Load Balancing and Auto Scaling. Create an ECS service and specify the desired task count. Use an Amazon RDS MySQL instance for the database tier\" is incorrect.</p><p>ECS does not offer automatic rollback so Elastic Beanstalk is a better solution to meet the requirements.</p><p><strong>INCORRECT:</strong> \"Deploy the application on Amazon EC2. Configure Elastic Load Balancing and Auto Scaling. Schedule an AWS Lambda function to take regular snapshots of attached EBS volumes. Use an Amazon RDS MySQL instance for the database tier\" is incorrect.</p><p>Automating the creation of snapshots is not a suitable solution for rollback. Elastic Beanstalk offers several deployment options which offer automatic rollback.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html",
      "https://digitalcloud.training/aws-elastic-beanstalk/"
    ]
  },
  {
    "id": 8,
    "question": "<p>A financial services company requires that DevOps engineers should not log directly into Amazon EC2 instances that process highly sensitive data except in exceptional circumstances. The security team requires a notification within 15 minutes if a DevOps engineer does log in to an instance.</p><p>Which solution will meet these requirements with the least operational overhead?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Install the Amazon CloudWatch agent on the EC2 instances. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Systems Manager to automate the execution of a script on each Amazon EC2 instance that pushes all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function that checks if files in S3 contain user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure an AWS CloudTrail trail and send events to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to an AWS Lambda function. Configure the Lambda function to check if the logs contain user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Install the AWS Systems Manager agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message contains user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. The agent includes the following components:</p><p>· A plug-in to the AWS CLI that pushes log data to CloudWatch Logs.</p><p>· A script (daemon) that initiates the process to push data to CloudWatch Logs.</p><p>· A cron job that ensures that the daemon is always running.</p><p>You can create metric filters to match terms in your log events and convert log data into metrics. When a metric filter matches a term, it increments the metric's count. For example, you can create a metric filter that counts the number of times the word <strong><em>ERROR</em></strong> occurs in your log events.</p><p>In this case the metric filter can search for user login data and then if this information is found it can send an SNS notification to the security team.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch agent on the EC2 instances. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user login data. If this information is found, send a notification to the security team using Amazon SNS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install the AWS Systems Manager agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message contains user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>The Systems Manager agent will not gather this information from EC2 instances. The CloudWatch Logs agent must be installed.</p><p><strong>INCORRECT:</strong> \"Configure an AWS CloudTrail trail and send events to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to an AWS Lambda function. Configure the Lambda function to check if the logs contain user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>CloudTrail will only report on API activity, and this does not include login data from an Amazon EC2 instance.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automate the execution of a script on each Amazon EC2 instance that pushes all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function that checks if files in S3 contain user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>This is possible though it is not the best solution as it requires the script to be rerun on a regular basis and requires more operational overhead to create and maintain.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  },
  {
    "id": 9,
    "question": "<p>A fast-growing US based company has just opened an office in Frankfurt, Germany. The users in the new office have reported high latency for the company’s CRM application. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB) and stores data in an Amazon DynamoDB table. The CRM application runs in the us-east-1 Region.</p><p>A DevOps engineer must minimize application response times and improve availability for users in both Regions.</p><p>Which combination of actions should be taken to address the latency issues? (Select THREE.)</p>",
    "corrects": [
      1,
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use DynamoDB Global Tables to create a replica of the existing DynamoDB table in the eu-central-1 Region.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create Amazon Route 53 aliases, health checks, and failover routing policies to route to the ALB.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a new ALB and Auto Scaling group in the eu-central-1 Region and configure the new ALB to direct traffic to the new Auto Scaling group.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create a new Auto Scaling group in the eu-central-1 Region and configure the existing ALB to direct traffic to the new Auto Scaling group.</p>",
        "correct": false
      },
      {
        "id": 6,
        "answer": "<p>Create a new DynamoDB table in the eu-central-1 Region with cross-Region replication enabled.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Database",
    "explanation": "<p>To reduce latency the application must be deployed in a Region closest to the users, in this case eu-central-1. As ALBs and Auto Scaling groups are regional services they must be deployed in this new Region. Amazon Route 53 can be used to create records with latency-based routing to direct users to the Region that provides the lowest latency. This will typically be the eu-central-1 Region for users in Frankfurt.</p><p>The database must be replicated to ensure the CRM data is consistent. A multi-master database is required so that changes in either Region will be synchronized. DynamoDB tables is ideal for this requirement and can be easily enabled at any time by creating a replication configuration on the existing table.</p><p><strong>CORRECT: </strong>\"Create a new ALB and Auto Scaling group in the eu-central-1 Region and configure the new ALB to direct traffic to the new Auto Scaling group\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use DynamoDB Global Tables to create a replica of the existing DynamoDB table in the eu-central-1 Region\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a new DynamoDB table in the eu-central-1 Region with cross-Region replication enabled\" is incorrect.</p><p>DynamoDB Global Tables should be used and can be enabled by creating a replication configuration on the existing table.</p><p><strong>INCORRECT:</strong> \"Create a new Auto Scaling group in the eu-central-1 Region and configure the existing ALB to direct traffic to the new Auto Scaling group\" is incorrect.</p><p>You cannot direct traffic to an ASG that is in another Region as both ELBs and ASGs are regional services.</p><p><strong>INCORRECT:</strong> \"Create Amazon Route 53 aliases, health checks, and failover routing policies to route to the ALB\" is incorrect.</p><p>Latency-based routing should be used rather than failover routing as the objective is to direct users to the application endpoint that has the lowest latency.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.tutorial.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.tutorial.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.tutorial.html",
      "https://digitalcloud.training/amazon-dynamodb/",
      "https://digitalcloud.training/amazon-route-53/"
    ]
  },
  {
    "id": 10,
    "question": "<p>A DevOps engineer deployed an application to AWS which makes use of Amazon EC2 Auto Scaling to launch new instances. The engineer needs to modify the instance type used for all new instances that are launched through automatic scaling. The Auto Scaling group is configured to use a launch template.</p><p>Which of the following actions should be taken to achieve this requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an AWS Elastic Beanstalk environment to deploy the new instance type for all scaling events.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Modify the existing launch template to create a new version that uses the new instance type and modify the Auto Scaling group to use the new template version.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Launch new EC2 instances with the new instance type using the AWS CLI and attach them to the Auto Scaling group.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the Overrides structure to define a new launch template for individual instance types using the existing Auto Scaling group.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>When you make a change to an existing launch template it creates a new version of the template. It is then easy to modify the Auto Scaling group to use the new version. As you can see in the diagram below you can modify the launch template used or the version of the launch template used.</p><p>In this case the engineer can simply save a new version of the template and then update the version of template used by the Auto Scaling group. All subsequent scaling events will launch using the new version of the template.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-06-22-87f49932ae89a3d9a078c0a894e0c1f5.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-06-22-87f49932ae89a3d9a078c0a894e0c1f5.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Modify the existing launch template to create a new version that uses the new instance type and modify the Auto Scaling group to use the new template version\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the Overrides structure to define a new launch template for individual instance types using the existing Auto Scaling group\" is incorrect.</p><p>The Overrides structure is used to define a different launch template for specific instance types. The best solution for this scenario is to simply update the launch template.</p><p><strong>INCORRECT:</strong> \"Create an AWS Elastic Beanstalk environment to deploy the new instance type for all scaling events\" is incorrect.</p><p>There is no need to move the solution to Elastic Beanstalk as the launch template can simply be updated.</p><p><strong>INCORRECT:</strong> \"Launch new EC2 instances with the new instance type using the AWS CLI and attach them to the Auto Scaling group\" is incorrect.</p><p>This is not a good solution as it is highly manual. The solution should be automated.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/examples-launch-templates-aws-cli.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/examples-launch-templates-aws-cli.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/examples-launch-templates-aws-cli.html",
      "https://digitalcloud.training/amazon-ec2-auto-scaling/"
    ]
  },
  {
    "id": 11,
    "question": "<p>A company manages both Amazon EC2 instances and on-premises servers running Linux and Windows. A DevOps engineer needs to manage patching across these environments. All patching must take place outside of business hours.</p><p>Which combination of actions will meet these requirements? (Select THREE.)</p>",
    "corrects": [
      1,
      5,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create an AWS Systems Manager Automation document that installs that latest patches every hour.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations.</p>",
        "correct": true
      },
      {
        "id": 6,
        "answer": "<p>Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Management & Governance",
    "explanation": "<p>In a hybrid environment the most secure deployment includes using Systems Manager Hybrid activations for on-premises servers and an IAM role for the EC2 instances. The hybrid activations feature in systems manager is a more secure option than using access keys as it uses an IAM service role.</p><p>To ensure that systems are not affected during business hours the DevOps engineer needs to simply select a patching window during acceptable times and Systems Manager Patch Manager will only apply updates during that timeframe.</p><p><strong>CORRECT: </strong>\"Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager\" is incorrect.</p><p>This is a less secure method than using Hybrid Activations in Systems Manager and would also be harder to manage.</p><p><strong>INCORRECT:</strong> \"Create an AWS Systems Manager Automation document that installs that latest patches every hour\" is incorrect.</p><p>Automation documents are used for running tasks on managed instances using Systems Manager but are not used for patch management.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours\" is incorrect.</p><p>CloudWatch Events responds to state changes in resources but cannot be used to schedule patch installation using AWS Systems Manager Patch Manager.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html",
      "https://digitalcloud.training/aws-systems-manager/"
    ]
  },
  {
    "id": 12,
    "question": "<p>A company uses GitHub to store the code for their software development. The company’s DevOps team want to automate the build process of an application. When the GitHub repository is updated, the code should be compiled, tested, and then pushed to an Amazon S3 bucket.</p><p>Which combination of steps would address these requirements? (Select THREE.)</p>",
    "corrects": [
      1,
      4,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add a buildspec.yml file to the source code with build instructions.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create an AWS CodeDeploy application with the Amazon EC2/On-Premises compute platform.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Launch an Amazon EC2 instance to perform the build.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a GitHub webhook to trigger a build every time a code change is pushed to the repository.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create an AWS OpsWorks deployment with the install dependencies command.</p>",
        "correct": false
      },
      {
        "id": 6,
        "answer": "<p>Create an AWS CodeBuild project with GitHub as the source repository.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Developer Tools",
    "explanation": "<p>AWS CodeBuild can be used for this project to perform the compilation of code, testing, and pushing the package to S3. AWS CodeBuild supports webhooks when the source repository is GitHub. This means that for a CodeBuild build project that has its source code stored in a GitHub repository, webhooks can be used to rebuild the source code every time a code change is pushed to the repository.</p><p>The buildspec.yml file is used to define the build commands and sequence to run during the build process. This file is placed in the source code.</p><p><strong>CORRECT: </strong>\"Create an AWS CodeBuild project with GitHub as the source repository\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Add a buildspec.yml file to the source code with build instructions\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure a GitHub webhook to trigger a build every time a code change is pushed to the repository\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS CodeDeploy application with the Amazon EC2/On-Premises compute platform\" is incorrect.</p><p>CodeDeploy is used for deploying the code. In this case the code needs to be compiled and tested. CodeBuild is used for this purpose. There is no mention in the question about how the code will be deployed to servers.</p><p><strong>INCORRECT:</strong> \"Create an AWS OpsWorks deployment with the install dependencies command\" is incorrect.</p><p>OpsWorks is not needed in this scenario as this is a pure compile and build requirement which is a good use case for CodeBuild.</p><p><strong>INCORRECT:</strong> \"Launch an Amazon EC2 instance to perform the build\" is incorrect.</p><p>There is no need to launch an instance and CodeBuild can automatically do this on the platform of your choice.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 13,
    "question": "<p>An application sits behind a Network Load Balancer (NLB) that is configured with a TLS listener. The DevOps team must analyze traffic patterns and require information about the connections made by clients. The data that is captured must be stored securely with encryption at rest and should only be accessible to the DevOps team members.</p><p>Which actions should a DevOps engineer take?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal ‘delivery.logs.amazonaws.com’.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal ‘delivery.logs.amazonaws.com’. Apply an IAM permissions policy to the DevOps team that grants read access.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>Elastic Load Balancing provides access logs that capture detailed information about the TLS requests sent to your Network Load Balancer. You can use these access logs to analyze traffic patterns and troubleshoot issues. The logs are sent to an Amazon S3 bucket you configure as the logging destination. This bucket can be encrypted using one of the available server-side encryption options.</p><p>When you enable access logging, you must specify an S3 bucket for the access logs. The policy must grant permission to the AWS service account ‘delivery.logs.amazonaws.com’. In this case, the DevOps team also require permissions to access the bucket, and this can be granted through an IAM policy attached to the team members, most likely via an IAM group.</p><p><strong>CORRECT: </strong>\"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal ‘delivery.logs.amazonaws.com’. Apply an IAM permissions policy to the DevOps team that grants read access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account\" is incorrect.</p><p>This will not allow read access for the DevOps team as the only permission is write access.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal ‘delivery.logs.amazonaws.com’\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html",
      "https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/"
    ]
  },
  {
    "id": 14,
    "question": "<p>A global multi-player gaming application that was deployed in Europe now needs to be extended globally. Availability of the application must be extremely high, and latency should be low. The application is hosted on Amazon EC2 instances. It is anticipated that the traffic will be unevenly distributed with a few locations having much more traffic than others.</p><p>Which of the below routing strategies would be best fit considering above parameters?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy Amazon CloudFront in front of the instances to cache requests and deliver content from Edge Locations globally.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions. Create a Route 53 geolocation-based routing record. Point the record to each of your load balancers.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize Route 53 latency-based routing and deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions. Create a Route 53 geoproximity-based routing record. Point the record to each of your load balancers.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>Geo-proximity Routing - Lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources.</p><p>You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource.</p><p>The catch in the question is a few regions have heavier traffic load than the others so a bias needs to be configured and geoproximity routing is a better fit for this solution.</p><p><strong>CORRECT: </strong>\"Deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions. Create a Route 53 geoproximity-based routing record. Point the record to each of your load balancers\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions. Create a Route 53 geolocation-based routing record. Point the record to each of your load balancers\" is incorrect.</p><p>If the discrepancy between traffic volume was unknown, this would also been a correct option.</p><p><strong>INCORRECT:</strong> \"Utilize Route 53 latency-based routing and deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions\" is incorrect.</p><p>Latency-based routing is based on latency measurements performed over a period, and the measurements reflect changes in network connectivity and routing.</p><p><strong>INCORRECT:</strong> \"Deploy Amazon CloudFront in front of the instances to cache requests and deliver content from Edge Locations globally\" is incorrect.</p><p>It is unlikely that a CDN can be used to cache a dynamic gaming application. A load balanced deployment of auto scaling EC2 instances is needed to run the application and ensure availability and performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
      "https://digitalcloud.training/amazon-route-53/"
    ]
  },
  {
    "id": 15,
    "question": "<p>A DevOps engineer is deploying a new application with a MySQL-compatible Amazon Aurora database cluster. The Multi-AZ database will have a cross-Region read replica deployed for disaster recovery. The DevOps engineer wants to automate promotion of the replica in the event of a failure of the primary database.</p><p>Which deployment option should the DevOps engineer choose?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Aurora custom endpoint to point to the primary database instance. Configure the application to use this endpoint. Configure AWS CloudTrail to detect database failures and then run an AWS Lambda function. Configure the Lambda function to promote the replica instance and modify the custom endpoint to point to the newly promoted instance.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Save the Aurora endpoint in AWS Secrets Manager. Subscribe an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail and run an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Secrets Manager. Configure the application to retrieve the updated endpoint from the parameter store if the application fails.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon EventBridge event that detects the database failure and modifies the application's AWS CloudFormation template to promote the replica. Create an AWS Lambda function to apply the updated template to update the stack and point the application to the newly promoted instance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Save the Aurora endpoint in AWS Systems Manager Parameter Store. Create an Amazon EventBridge event that detects the database failure and runs an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Systems Manager Parameter Store. Configure the application to retrieve the updated endpoint from the parameter store if the application fails.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Systems Manager Parameter Store is an ideal service to use for storing database connection strings. The application can be configured to recheck for updated connection strings if the application experiences a failure.</p><p>Amazon EventBridge can be used to trigger an action based on state changes in the Aurora database. For instance, you can use the “RDS DB Instance Event” event pattern to detect state changes in the Aurora database.</p><p>When the database is in a failed state a Lambda function can be used to promote the replica and update the database connection endpoint in Systems Manager Parameter Store. This will fully automate the promotion of the replica and the application will automatically pick up the new endpoint.</p><p><strong>CORRECT: </strong>\"Save the Aurora endpoint in AWS Systems Manager Parameter Store. Create an Amazon EventBridge event that detects the database failure and runs an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Systems Manager Parameter Store. Configure the application to retrieve the updated endpoint from the parameter store if the application fails\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge event that detects the database failure and modifies the application's AWS CloudFormation template to promote the replica. Create an AWS Lambda function to apply the updated template to update the stack and point the application to the newly promoted instance\" is incorrect.</p><p>EventBridge cannot update CloudFormation templates. You would need to use an AWS Lambda function to update the template, but the answer states the EventBridge would do this work.</p><p><strong>INCORRECT:</strong> \"Save the Aurora endpoint in AWS Secrets Manager. Subscribe an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail and run an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Secrets Manager. Configure the application to retrieve the updated endpoint from the parameter store if the application fails\" is incorrect.</p><p>There is no such thing as publishing RDS failure notifications into AWS CloudTrail, so this solution is not possible.</p><p><strong>INCORRECT:</strong> \"Create an Aurora custom endpoint to point to the primary database instance. Configure the application to use this endpoint. Configure AWS CloudTrail to detect database failures and then run an AWS Lambda function. Configure the Lambda function to promote the replica instance and modify the custom endpoint to point to the newly promoted instance\" is incorrect.</p><p>As with the previous answer you cannot use CloudTrail to detect database failures as it is a service that is used for recording the API calls made within an account. It is not used for performance monitoring or recording the changes in state of resources (use CloudWatch/EventBridge instead).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-cloud-watch-events.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-cloud-watch-events.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-cloud-watch-events.html",
      "https://digitalcloud.training/amazon-aurora/"
    ]
  },
  {
    "id": 16,
    "question": "<p>An online retail chain processes thousands of orders each day from 100 countries and its website is localized in 15 languages. The company’s website faces continual security threats and challenges in the form of HTTP flood attacks, distributed denial of service (DDoS) attacks, and rogue robots that flood its website with traffic. Most of these attacks originate from certain countries.</p><p>The company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on EC2 instances running behind an Application Load Balancer (ALB) with AWS WAF.</p><p>How can a DevOps engineer protect workloads from DDoS attacks whilst allowing the development team to be productive? (Select TWO.)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an AWS WAF IP set statement that specifies the IP addresses that needs to be allowed.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use an ALB geo match statement listing the countries that needs to be blocked.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an ALB IP set statement that specifies the IP addresses that needs to be allow through.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a deny rule for the blocked countries in the NACL associated with each of the EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use an AWS WAF geo match statement listing the countries that need to be blocked.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns and rules that filter out specific traffic patterns you define.</p><p>You can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts your web servers or origin servers running on EC2, or Amazon API Gateway for your APIs.</p><p><strong>CORRECT: </strong>\"Use an AWS WAF IP set statement that specifies the IP addresses that needs to be allowed\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use an AWS WAF geo match statement listing the countries that need to be blocked\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an ALB geo match statement listing the countries that needs to be blocked\" is incorrect.</p><p>An ALB cannot block or allow traffic based on geographic match conditions or IP based conditions.</p><p><strong>INCORRECT:</strong> \"Use an ALB IP set statement that specifies the IP addresses that needs to be allow through\" is incorrect.</p><p>Geo match restriction works with WAF and not ALB. The correct way to add geo restriction is to bind the condition as an WAF rule.</p><p><strong>INCORRECT:</strong> \"Create a deny rule for the blocked countries in the NACL associated with each of the EC2 instances\" is incorrect.</p><p>Creating a Network Access Control List (NACL) would be tedious and won’t entirely cover the requirements here. AWS WAF is designed for protecting against the types of attacks mentioned.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html\">https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/\">https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html",
      "https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/",
      "https://digitalcloud.training/aws-waf-shield/"
    ]
  },
  {
    "id": 17,
    "question": "<p>A company is deploying a new application that uses Amazon EC2 instances for the web tier and a MySQL database for the database tier. An Application Load Balancer (ALB) will be used in front of the web tier. The company requires an RPO of 2 hours and an RTO of 10 minutes for the solution.</p><p>Which combination of deployment strategies will meet these requirements? (Select TWO.)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create two Amazon Aurora clusters spread across two Regions. Use AWS Database Migration Service (AWS DMS) to synchronize changes.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon Aurora global database in two Regions for the database tier. In the event of a failure, promote the secondary Region to take on read/write responsibilities.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Deploy the application in two Regions and create Amazon Route 53 latency-based routing records pointing to the ALB in each Regions. Enable health checks for the records.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the application in two Regions and create Amazon Route 53 failover-based routing records pointing to the ALB in each Regions. Enable health checks for the records.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create an Amazon Aurora multi-master cluster across multiple Regions for the database tier. Configure the database in an active-active mode across the Regions.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Database",
    "explanation": "<p>To meet the RTO and RPO requirements the best solution for the database tier is to use Amazon Aurora global database. This solution provides replication from the Aurora storage layer across Regions. The reader endpoint in the secondary endpoint can be promoted in the event of a DR scenario to be the main database which takes on read/write responsibilities.</p><p>For the web tier this can be placed behind ALBs in each Region. Amazon Route 53 failover-based routing policies with health checks should be created. These records will point to the ALBs in each Region and if the health checks fail in the primary Region, automatic failover to the secondary Region will occur.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-10-42-d7b7624f7e1003a9bd3098d979fb2910.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-10-42-d7b7624f7e1003a9bd3098d979fb2910.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Create an Amazon Aurora global database in two Regions for the database tier. In the event of a failure, promote the secondary Region to take on read/write responsibilities\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Deploy the application in two Regions and create Amazon Route 53 failover-based routing records pointing to the ALB in each Regions. Enable health checks for the records\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create two Amazon Aurora clusters spread across two Regions. Use AWS Database Migration Service (AWS DMS) to synchronize changes\" is incorrect.</p><p>The better solution is to use Aurora Global Database as this will simplify the failover process which can be instantiated through a few API calls. Also, this replication uses Aurora replication with low latency and the Aurora reader endpoint can also be utilized in the second Region.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Aurora multi-master cluster across multiple Regions for the database tier. Configure the database in an active-active mode across the Regions\" is incorrect.</p><p>It is not possible to create multi-master clusters across Regions, they work within a Region only.</p><p><strong>INCORRECT:</strong> \"Deploy the application in two Regions and create Amazon Route 53 latency-based routing records pointing to the ALB in each Regions. Enable health checks for the records\" is incorrect.</p><p>Failover routing policy records should be created, not latency routing records. With latency records users closer to the secondary Region will be directed there but the DB layer is not in read/write mode except in a DR scenario.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/rds/aurora/global-database/",
      "https://digitalcloud.training/amazon-aurora/"
    ]
  },
  {
    "id": 18,
    "question": "<p>A financial organization is using a multi-account strategy with AWS Organizations. The AWS accounts are organized into different organizational units. Amazon CloudWatch Logs is used for logging within each account. The company needs to ship all the logs to a single centralized account for archiving purposes. The solution must be secure, and centralized, and the logs must be stored cost-effectively.</p><p>How can a DevOps Engineer meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon EFS.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a log destination in the centralized account and create a log subscription on that destination. Create a subscription for the log stream that triggers and AWS Lambda function that copies the log data to Amazon EFS.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Analytics",
    "explanation": "<p>Subscriptions can be used to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. When log events are sent to the receiving service, they are Base64 encoded and compressed with the gzip format.</p><p>For cross-account log data sharing with subscriptions, you can collaborate with an owner of a different AWS account and receive their log events on the AWS resources, such as an Amazon Kinesis or Amazon Kinesis Data Firehose stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Kinesis or Kinesis Data Firehose stream to perform custom processing and analysis. Therefore, we must subscribe the log destination to a Kinesis Firehose delivery stream, which in turn has a destination of S3.</p><p><strong>CORRECT: </strong>\"Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon EFS\" is incorrect.</p><p>The issue with this option is that the target for Kinesis Firehose is set as Amazon EFS which is not a supported destination.</p><p><strong>INCORRECT:</strong> \"Create a log destination in the centralized account and create a log subscription on that destination. Create a subscription for the log stream that triggers and AWS Lambda function that copies the log data to Amazon EFS\" is incorrect.</p><p>This solution is possible, but Amazon EFS is not the most cost-efficient storage solution for archiving purposes. Amazon S3 is more suitable.</p><p><strong>INCORRECT:</strong> \"Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift\" is incorrect.</p><p>The issue with this option is that the target for Kinesis Firehose is set as Amazon Redshift which is not suitable for archiving data and would be very expensive. RedShift is used for analytics purposes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html",
      "https://digitalcloud.training/amazon-kinesis/"
    ]
  },
  {
    "id": 19,
    "question": "<p>A DevOps team is assisting with the deployment of a web application's infrastructure using AWS CloudFormation. The database management team maintains the database resources in a CloudFormation template, and the software development team maintains the web application resources in a separate CloudFormation template. The software development team needs to use resources maintained by the database engineering team. However, both teams have their own review and lifecycle management processes that they want to maintain. Both teams also require resource-level change-set reviews. The software development team would like to deploy changes to this template using their CI/CD pipeline.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>A nested stack is a stack that you create within another stack by using the AWS::CloudFormation::Stack resource. With nested stacks, you deploy and manage all resources from a single stack. You can use outputs from one stack in the nested stack group as inputs to another stack in the group. This differs from exporting values.</p><p>If you want to isolate information sharing to within a nested stack group, AWS suggests that you use nested stacks. To share information with other stacks (not just within the group of nested stacks), you should export values instead.</p><p>Change sets for nested stacks affect the entire stack hierarchy which does not meet the requirements as each team requires resource-level change-set reviews. Therefore, in this scenario it is better to export values rather than use a nested stack.</p><p><strong>CORRECT: </strong>\"Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks\" is incorrect.</p><p>As described above, to meet the requirements an export of values is preferable to using nested stacks in this scenario.</p><p><strong>INCORRECT:</strong> \"Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks\" is incorrect.</p><p>Stack sets are used for extending the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions.</p><p><strong>INCORRECT:</strong> \"Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack\" is incorrect.</p><p>Values should be exported as per the correct answer.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html#output-vs-nested\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html#output-vs-nested</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html#output-vs-nested",
      "https://digitalcloud.training/aws-cloudformation/"
    ]
  },
  {
    "id": 20,
    "question": "<p>A new application is being deployed on AWS using Amazon EC2 instances. The operations team must monitor the instance’s application logs and API activity and need a solution for querying both sets of logs.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Install the Amazon CloudWatch agent on the instances to send the application logs to Amazon CloudWatch Logs and configure AWS CloudTrail to log API activity to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Install the Amazon CloudWatch agent on the instances to send the application logs to an Amazon SQS queue and configure AWS CloudTrail to log API activity to the same SQS queue. Create an AWS Lambda function to query messages in the queue.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Install the Amazon CloudWatch agent on the instances to send the application logs to Amazon CloudWatch Logs and use AWS CloudTrail to log API activity to Amazon S3. Use CloudWatch Logs Insights to query both sets of logs.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Install the Amazon CloudWatch agent on the instances to send the application logs to an Amazon S3 bucket and configure AWS CloudTrail to log API activity to the same S3 bucket. Use Amazon Athena to query both sets of logs.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>The unified CloudWatch agent enables you to collect system logs, application logs, and metrics from your Amazon EC2 instances. The log files can be stored in Amazon CloudWatch Logs.</p><p>AWS CloudTrail logs API activity from your account and can also be configured to stream the log data to an Amazon CloudWatch Logs log group.</p><p>CloudWatch Logs Insights can then be used to query both sets of logs. This service is used to interactively search and analyze your log data in Amazon CloudWatch Logs.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch agent on the instances to send the application logs to Amazon CloudWatch Logs and configure AWS CloudTrail to log API activity to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch agent on the instances to send the application logs to Amazon CloudWatch Logs and use AWS CloudTrail to log API activity to Amazon S3. Use CloudWatch Logs Insights to query both sets of logs\" is incorrect.</p><p>CloudWatch Logs Insights can be used to query log data in CloudWatch Logs log groups but not data that is stored directly in an Amazon S3 bucket.</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch agent on the instances to send the application logs to an Amazon S3 bucket and configure AWS CloudTrail to log API activity to the same S3 bucket. Use Amazon Athena to query both sets of logs\" is incorrect.</p><p>The CloudWatch agent can be configured to send the log files to an Amazon CloudWatch Logs log group but cannot send data into an S3 bucket.</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch agent on the instances to send the application logs to an Amazon SQS queue and configure AWS CloudTrail to log API activity to the same SQS queue. Create an AWS Lambda function to query messages in the queue\" is incorrect.</p><p>The SQS queue is not a destination you can configure in the CloudWatch Logs agent and it not a suitable storage location as it is meant for temporary storage of data that is awaiting processing.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  }
]