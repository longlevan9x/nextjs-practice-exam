[
  {
    "id": 1,
    "question": "<p>A DevOps engineer needs to implement an automated deployment process for an application running on AWS. The implementation should minimize deployment costs whilst ensuring that at least half of the instances are available at any time to service user requests. If the application fails on some instances they should be automatically replaced.</p><p>Which deployment strategy will meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement Auto Scaling with an Elastic Load Balancer. Use AWS CodeDeploy with a blue/green deployment strategy. Enable an ELB health check and set the Auto Scaling health check to ELB.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an AWS OpsWorks stack. Configure the application layer to use rolling deployments as a deployment strategy. Add an Elastic Load Balancing layer. Enable auto healing on the application layer.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Implement Auto Scaling with an Elastic Load Balancer. Use AWS CodeDeploy with the CodeDeployDefault.HalfAtAtime deployment strategy. Enable an ELB health check and set the Auto Scaling health check to ELB.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an AWS Elastic Beanstalk environment and configure it to use Auto Scaling and an Elastic Load Balancer. Use a rolling deployment strategy and configure a batch size of 50%.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>With a CodeDeploy ‘CodeDeployDefault.HalfAtAtime’ deployment strategy, half of the instances will be updated at a time. The overall deployment succeeds if the application revision is deployed to at least half of the instances. Otherwise, the deployment fails.</p><p>Configuring an ELB health check and updating the ASG to use this health check ensures that the ALB can take the instances that fail out of service. If you do not enable ELB health checks on the ASG it will not know when the target group health checks have failed.</p><p><strong>CORRECT: </strong>\"Implement Auto Scaling with an Elastic Load Balancer. Use AWS CodeDeploy with the CodeDeployDefault.HalfAtAtime deployment strategy. Enable an ELB health check and set the Auto Scaling health check to ELB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS Elastic Beanstalk environment and configure it to use Auto Scaling and an Elastic Load Balancer. Use a rolling deployment strategy and configure a batch size of 50%\" is incorrect.</p><p>There are no health checks configured for the target group with this answer so if the application fails the ASG may not replace the instances. This is a key requirement that has been missed with this configuration.</p><p><strong>INCORRECT:</strong> \"Create an AWS OpsWorks stack. Configure the application layer to use rolling deployments as a deployment strategy. Add an Elastic Load Balancing layer. Enable auto healing on the application layer\" is incorrect.</p><p>Auto healing with OpsWorks only checks the instance health, it does not check the application health so this will not detect if the application has failed.</p><p><strong>INCORRECT:</strong> \"Implement Auto Scaling with an Elastic Load Balancer. Use AWS CodeDeploy with a blue/green deployment strategy. Enable an ELB health check and set the Auto Scaling health check to ELB\" is incorrect.</p><p>This is a more expensive option as a full additional deployment will be running alongside the existing deployment (doubling the number of instances) during the rollout.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 2,
    "question": "<p>A DevOps team is developing a PHP web application which will be deployed on Amazon EC2 instances. The application has been designed to use a MySQL database. The team requires a deployment model in which they can reliably build, test, and deploy new updates daily, without downtime or degraded performance. The application must also be able to scale to meet an unpredictable number of concurrent users.</p><p>Which action will allow the team to quickly meet these objectives?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create two auto scaling AWS Elastic Beanstalk environments: one for test and one for production. Build the application using AWS CodeBuild and use Amazon RDS to store data. When new versions of the applications have passed all tests, use the Elastic Beanstalk 'swap cname' to promote the test environment to production.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS OpsWorks to create a stack for the application with a DynamoDB database layer, an Application Load Balancing layer, and an Amazon EC2 instance layer. Use Chef recipes to build and deploy the application. Use custom health checks to run unit tests on each instance with rollback on failure.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an AWS CloudFormation template which deploys an Auto Scaling group of EC2 instances behind an Application Load Balancer. Use AWS CodeBuild to build and test the PHP application. Install the application and the MySQL database on each EC2 instance. Update the stack to deploy new application versions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create two Amazon ECS services: one for test and one for production. Upload the web application code to the test ECS tasks using the AWS CLI. Test the application and if the tests pass, upload the code to the production ECS tasks.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application might become unavailable to users for a short period of time. To avoid this, perform a blue/green deployment. To do this, deploy the new version to a separate environment, and then swap the CNAMEs of the two environments to redirect traffic to the new version instantly.</p><p>This solution that meets the requirements of this question by ensuring that the team can build, test, and deploy new updates daily, without downtime or degraded performance. The auto scaling support for the environments will automatically scale the infrastructure to ensure that the application can meet changing demands.</p><p><strong>CORRECT: </strong>\"Create two auto scaling AWS Elastic Beanstalk environments: one for test and one for production. Build the application using AWS CodeBuild and use Amazon RDS to store data. When new versions of the applications have passed all tests, use the Elastic Beanstalk 'swap cname' to promote the test environment to production\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create two Amazon ECS services: one for test and one for production. Upload the web application code to the test ECS tasks using the AWS CLI. Test the application and if the tests pass, upload the code to the production ECS tasks\" is incorrect.</p><p>Using the CLI to upload code is not a best practice. It is better to build and test with a service such as AWS CodeBuild. The main problem with this solution is there is no method of deploying updates in a blue/green strategy and to be able to rollback changes when necessary.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudFormation template which deploys an Auto Scaling group of EC2 instances behind an Application Load Balancer. Use AWS CodeBuild to build and test the PHP application. Install the application and the MySQL database on each EC2 instance. Update the stack to deploy new application versions\" is incorrect.</p><p>CloudFormation stacks are best used for infrastructure deployments and updates and not for application/code level changes.</p><p><strong>INCORRECT:</strong> \"Use AWS OpsWorks to create a stack for the application with a DynamoDB database layer, an Application Load Balancing layer, and an Amazon EC2 instance layer. Use Chef recipes to build and deploy the application. Use custom health checks to run unit tests on each instance with rollback on failure\" is incorrect.</p><p>DynamoDB is not a suitable solution when the question states that the application has been designed to work with MySQL which is an SQL type of database. A code rewrite would be required to support a NoSQL database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html",
      "https://digitalcloud.training/aws-elastic-beanstalk/"
    ]
  },
  {
    "id": 3,
    "question": "<p>A DevOps engineer is troubleshooting problems with an application which uses AWS Lambda to process messages in an Amazon SQS standard queue. The function sometimes fails to process the messages in the queue. The engineer needs to analyze the events to determine the cause of the issue and update the function code.</p><p>Which action should the engineer take to achieve this outcome?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable long-polling by increasing WaitTimeSeconds parameter.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable FIFO support for the queue to preserve ordering of the messages.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure a delay queue by increasing the DelaySeconds parameter.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a redrive policy to move the messages to a dead-letter queue.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>You can configure a dead-letter queue (DLQ) by specifying a redrive policy on the SQS queue. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.</p><p><strong>CORRECT: </strong>\"Configure a redrive policy to move the messages to a dead-letter queue\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable FIFO support for the queue to preserve ordering of the messages\" is incorrect.</p><p>You cannot enable FIFO on a standard queue, and it does not help with this issue anyway.</p><p><strong>INCORRECT:</strong> \"Enable long-polling by increasing WaitTimeSeconds parameter\" is incorrect.</p><p>Long polling just helps with efficiency of API calls as it waits for messages to appear in the queue rather than returning an immediate response. This does not assist with isolating messages for analysis.</p><p><strong>INCORRECT:</strong> \"Configure a delay queue by increasing the DelaySeconds parameter\" is incorrect.</p><p>A delay queue simply delays visibility of the messages for a specified time. This does not assist with this issue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 4,
    "question": "<p>A DevOps Engineer must deploy a three-tier web application on AWS. The application will run on Amazon EC2 instances and will use an Amazon RDS database tier. The engineer must select a deployment model the reduces operational overhead as much as possible.</p><p>Which solution best meets these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS CloudFormation to create an Application Load Balancer and an Auto Scaling group. Use AWS OpsWorks to create the application and database resources. Deploy application updates with OpsWorks using lifecycle events.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS CloudFormation to create an Application Load Balancer, an Auto Scaling group and database resources. Deploy application updates using CloudFormation rolling updates.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS OpsWorks to create an Application Load Balancer, an Auto Scaling group, and application and database resources. Deploy application updates using OpsWorks lifecycle events.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS OpsWorks to create an Application Load Balancer, an Auto Scaling group, and application resources. Use AWS CloudFormation to create the database resources. Deploy application updates using CloudFormation rolling updates.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>AWS OpsWorks Stacks provides a simple and flexible way to create and manage stacks and applications. Using layers, the various components of the three-tier application can be deployed, updated, and managed.</p><p>OpsWorks can handle deployment of all resources including application code. Lifecycle events can be used to manage updates to application code and changes to infrastructure.</p><p>The diagram below depicts an OpsWorks stack that includes three layers representing each tier of the application:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-07-05-2cc66e24169197dc611cfcb4efc3ae51.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-07-05-2cc66e24169197dc611cfcb4efc3ae51.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Use AWS OpsWorks to create an Application Load Balancer, an Auto Scaling group, and application and database resources. Deploy application updates using OpsWorks lifecycle events\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to create an Application Load Balancer and an Auto Scaling group. Use AWS OpsWorks to create the application and database resources. Deploy application updates with OpsWorks using lifecycle events\" is incorrect.</p><p>It would be better to use OpsWorks for all components of the application and infrastructure deployment and management as it will reduce operational overhead.</p><p><strong>INCORRECT:</strong> \"Use AWS OpsWorks to create an Application Load Balancer, an Auto Scaling group, and application resources. Use AWS CloudFormation to create the database resources. Deploy application updates using CloudFormation rolling updates\" is incorrect.</p><p>As with the previous answer, it is better to use OpsWorks to manage the stack. Also, CloudFormation is well suited to deploying infrastructure but is not as good at deploying application code.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to create an Application Load Balancer, an Auto Scaling group and database resources. Deploy application updates using CloudFormation rolling updates\" is incorrect.</p><p>CloudFormation is an infrastructure as code tool that deploys AWS infrastructure extremely well. However, it is not the best tool to use for deploying application updates. OpsWorks should be used in this scenario as it offers a better solution for reducing overall operational overhead whilst fully meeting the requirements of the solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/welcome_classic.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/welcome_classic.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-opsworks/\">https://digitalcloud.training/aws-opsworks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/opsworks/latest/userguide/welcome_classic.html",
      "https://digitalcloud.training/aws-opsworks/"
    ]
  },
  {
    "id": 5,
    "question": "<p>A DevOps Engineer manages an application running across accounts which is deployed via AWS CloudFormation.</p><p>The application stack has an Amazon ECS Fargate cluster which spins up multiple tasks for the application layer and utilizes an Amazon ElastiCache Redis cache to store frequently accessed data.</p><p>The accounts are labelled “sandbox” and “staging”. While the stack spins up fine, the application is unable to connect to Redis with the below error logged in Amazon CloudWatch Logs.</p><p>“Stopped reason ResourceInitializationError: unable to pull secrets or registry auth: pull command failed :: signal: killed“.</p><p>What is the possible fix for the above error? (Select TWO.)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Ensure the ECS tasks are launched in a public subnet and public IP addresses are assigned to them.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Ensure inbound connectivity is allowed in the application security group on port 6379 from the Redis cluster subnet.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Ensure an ENI is configured for the ECS tasks in the AWS Fargate cluster and there is an api.ecr endpoint.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Ensure that the IAM role provides the required permissions.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Ensure that the application security group has a rule that accepts connections from 0.0.0.0/0.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Compute",
    "explanation": "<p>AWS Fargate clusters with the 1.4 version upgrade provides ENI at the task level. Below is AWS documentation for the same:</p><p>Fargate tasks run on a fleet of virtual machines that AWS manages on behalf of the customer. These VMs are connected to AWS owned VPCs via so called “Fargate ENIs”. When a user launches a task on Fargate, the task is assigned an ENI and this ENI is connected to the customer owned VPC</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-05-26-622074b8a2ac8ab2c36144cac8b52c3e.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-05-26-622074b8a2ac8ab2c36144cac8b52c3e.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Ensure inbound connectivity is allowed in the application security group on port 6379 from the Redis cluster subnet\" is a correct answer (as explained above).</p><p><strong>CORRECT: </strong>\"Ensure an ENI is configured for the ECS tasks in the AWS Fargate cluster and there is an api.ecr endpoint\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Ensure the ECS tasks are launched in a public subnet and public IP addresses are assigned to them\" is incorrect since your tasks can sit in a private subnet as well.</p><p><strong>INCORRECT:</strong> \"Ensure that the IAM role provides the required permissions\" is incorrect since question is more around connectivity and stack worked fine in one of the environments.</p><p><strong>INCORRECT:</strong> \"Ensure that the application security group has a rule that accepts connections from 0.0.0.0/0\" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/containers/aws-fargate-launches-platform-version-1-4/\">AWS Fargate launches platform version 1.4.0 | Containers (amazon.com)</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/containers/aws-fargate-launches-platform-version-1-4/",
      "https://digitalcloud.training/amazon-ecs-and-eks/"
    ]
  },
  {
    "id": 6,
    "question": "<p>A company is migrating an important production application to the AWS Cloud. The application includes a containerized web tier and a MySQL database. The application must be deployed with high availability and fault tolerance and must minimize cost.</p><p>How should a DevOps engineer refactor the application? (Select TWO.)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Migrate the web tier to an AWS Fargate cluster and configure a service and attach an Application Load Balancer.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Migrate the MySQL database to an Amazon RDS instance with a Read Replica in another AZ.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Migrate the MySQL database to an Amazon RDS Multi-AZ deployment.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Migrate the MySQL database to an Amazon DynamoDB with Global Tables.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Migrate the web tier to an Auto Scaling group of EC2 instances across multiple AZs and use an Application Load Balancer.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Compute",
    "explanation": "<p>AWS Fargate is a managed and serverless service for running Docker containers. It is part of the Amazon ECS family of AWS services and supports multiple availability zones for all deployments. To attach a load balancer you must create a service and you can define the desired count of ECS tasks to run.</p><p>The database tier of the application can be deployed on a highly available Amazon RDS database that has a multi-AZ replica. This provides fault tolerance across multiple AZs.</p><p><strong>CORRECT: </strong>\"Migrate the web tier to an AWS Fargate cluster and configure a service and attach an Application Load Balancer\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Migrate the MySQL database to an Amazon RDS Multi-AZ deployment\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the web tier to an Auto Scaling group of EC2 instances across multiple AZs and use an Application Load Balancer\" is incorrect.</p><p>EC2 is not the most cost-effective platform for this solution as the application is already containerized and therefore should run on Amazon ECS for best performance and cost efficiency.</p><p><strong>INCORRECT:</strong> \"Migrate the MySQL database to an Amazon RDS instance with a Read Replica in another AZ\" is incorrect.</p><p>A Read replica in another AZ can be used as an asynchronous backup target but the promotion time can be quite long in a failure scenario. A multi-AZ instance allows fast failover.</p><p><strong>INCORRECT:</strong> \"Migrate the MySQL database to an Amazon DynamoDB with Global Tables\" is incorrect.</p><p>DynamoDB is a NoSQL database, so it is not suitable for migrating MySQL which is a relational database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html</a></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html",
      "https://aws.amazon.com/rds/features/multi-az/",
      "https://digitalcloud.training/amazon-ecs-and-eks/",
      "https://digitalcloud.training/amazon-rds/"
    ]
  },
  {
    "id": 7,
    "question": "<p>A DevOps engineer has created a CI/CD pipeline in which AWS CodeDeploy is used to deploy an AWS Lambda function as the last stage of the pipeline. For proper execution, the Lambda function relies on an Amazon API Gateway API to have been fully deployed and ready to accept requests.</p><p>The DevOps engineer needs to ensure that the API is ready to accept requests before traffic is shifted to the deployed Lambda function version.</p><p>How can this requirement be met?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the ValidateService hook in the AppSpec file to check that the deployment was completed successfully before shifting traffic to the deployed Lambda function.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the BeforeAllowTraffic hook in the AppSpec file to call a validation function that checks that the API is ready to accept requests.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use the AfterAllowTraffic hook in the AppSpec file to call a validation function that checks that the API is ready to accept requests.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the AfterInstall hook in the buildspec file to call AWS CodeBuild and run test commands to check that the API’s production stage endpoint is reachable.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>An AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed once per deployment. Here are descriptions of the hooks available for use in your AppSpec file.</p><ul><li><p><strong>BeforeAllowTraffic</strong> – Use to run tasks before traffic is shifted to the deployed Lambda function version.</p></li><li><p><strong>AfterAllowTraffic</strong> – Use to run tasks after all traffic is shifted to the deployed Lambda function version.</p></li></ul><p>In a serverless Lambda function version deployment, event hooks run in the following order: BeforeAllowTraffic &gt; AllowTraffic &gt; AfterAllowTraffic.</p><p>Use the 'hooks' section to specify a Lambda function that CodeDeploy can call to validate a Lambda deployment. You can use the same function or a different one for the BeforeAllowTraffic and AfterAllowTraffic deployment lifecyle events. Following completion of the validation tests, the Lambda validation function calls back CodeDeploy and delivers a result of Succeeded or Failed.</p><p><strong>CORRECT: </strong>\"Use the BeforeAllowTraffic hook in the AppSpec file to call a validation function that checks that the API is ready to accept requests\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the AfterAllowTraffic hook in the AppSpec file to call a validation function that checks that the API is ready to accept requests\" is incorrect.</p><p>This hook is run after traffic is shifted to the Lambda function, so it doesn’t offer any value here.</p><p><strong>INCORRECT:</strong> \"Use the ValidateService hook in the AppSpec file to check that the deployment was completed successfully before shifting traffic to the deployed Lambda function\" is incorrect.</p><p>The ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully. At that point traffic has already been shifted.</p><p><strong>INCORRECT:</strong> \"Use the AfterInstall hook in the buildspec file to call AWS CodeBuild and run test commands to check that the API’s production stage endpoint is reachable\" is incorrect.</p><p>CodeDeploy is being used as the last stage of the pipeline so any build commands would have already been completed at this point.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 8,
    "question": "<p>A DevOps engineer is troubleshooting an issue with an AWS CodeDeploy deployment to a deployment group containing Amazon EC2 instances. The engineer noticed that all events associated with the specific deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group.</p><p>Which of the following as possible reasons for this failure? (Select TWO.)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The target EC2 instances do not have an instance profile attached that provides the required permissions.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>The IAM user who initiated the deployment does not have the required permissions to interact with CodeDeploy.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The target EC2 instances were not properly registered with the CodeDeploy endpoint.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The EC2 instances were deployed using instance store volumes rather than EBS volumes.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>The EC2 instances cannot reach the internet or CodeDeploy public endpoints using a NAT gateway or internet gateway.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Developer Tools",
    "explanation": "<p>There are several reasons why the deployment may show a Skipped status for all events. These include the networking configuration of the instances not being correctly setup to include a path to a NAT gateway or internet gateway. This is required for internet access and to connect to the CodeDeploy public endpoints.</p><p>Another possible cause of this issue is that the EC2 instances do not have an instance profile attached that uses an IAM policy to assign the required permissions for the application deployment.</p><p><strong>CORRECT: </strong>\"The EC2 instances cannot reach the internet or CodeDeploy public endpoints using a NAT gateway or internet gateway\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"The target EC2 instances do not have an instance profile attached that provides the required permissions\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The IAM user who initiated the deployment does not have the required permissions to interact with CodeDeploy\" is incorrect.</p><p>The deployment uses a service role rather than the IAM user permissions.</p><p><strong>INCORRECT:</strong> The target EC2 instances were not properly registered with the CodeDeploy endpoint\" is incorrect.</p><p>An agent must be installed, and the instances must have the required permissions for CodeDeploy. However, they do not need to be registered.</p><p><strong>INCORRECT:</strong> \"The EC2 instances were deployed using instance store volumes rather than EBS volumes\" is incorrect.</p><p>There is no requirement to use EBS volumes over instance store volumes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 9,
    "question": "<p>A company manages several AWS accounts in an AWS Organizations organization. There are several applications running in each account. Many resources have not been tagged properly and the finance team cannot fully determine the costs that should be attributed to each application.</p><p>A DevOps engineer has been asked to remediate this issue and ensure it does not happen again.</p><p>Which combination of actions should the DevOps engineer take to meet these requirements? (Select TWO.)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Activate the user-defined cost allocation tags in each AWS account.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Define each application in AWS Budgets. Assign the required tag to each resource.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the budget report to find untagged resources. Assign tags to the resources.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Scan all accounts with Tag Editor. Assign the required tag to each resource.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create and attach an SCP that requires tags to be specified when creating resources.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Management & Governance",
    "explanation": "<p>You can use service control policies (SCPs) to require that tags are specified when creating resources. If the appropriate tags are not specified the resource creation fails. This will prevent the issue of resources not being properly tagged from occurring again in the future.</p><p>To resolve the issue now, the DevOps engineer can use Tag Editor to scan resources looking for those that are missing the appropriate tags. The resources can then be tagged.</p><p><strong>CORRECT: </strong>\"Create and attach an SCP that requires tags to be specified when creating resources\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Scan all accounts with Tag Editor. Assign the required tag to each resource\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Activate the user-defined cost allocation tags in each AWS account\" is incorrect.</p><p>We can assume that this has already been done. The question does not state that we need to enable cost allocation tagging. The context infers that this has already been done and the issue is simply that tagging has not been performed properly on some resources.</p><p><strong>INCORRECT:</strong> \"Define each application in AWS Budgets. Assign the required tag to each resource\" is incorrect.</p><p>You cannot define application in AWS Budgets or add tags to resources.</p><p><strong>INCORRECT:</strong> \"Use the budget report to find untagged resources. Assign tags to the resources\" is incorrect.</p><p>The Tag Editor should be used to find untagged resources and then add tags to them.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html\">https://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html",
      "https://digitalcloud.training/aws-organizations/"
    ]
  },
  {
    "id": 10,
    "question": "<p>A company is developing a continuous delivery pipeline using AWS CodePipeline. The application is deployed across multiple ALBs each of which has a dedicated Auto Scaling group. A DevOps engineer must configure the pipeline so that a simultaneous deployment is triggered to all ALBs and Auto Scaling groups when code is committed to an AWS CodeCommit repository.</p><p>Which deployment option meets these requirements with the LEAST amount of configuration?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a single AWS CodePipeline pipeline that deploys the application in parallel using separate AWS CodeDeploy applications and deployment groups for each pair of ALB and Auto Scaling group.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use a separate AWS CodePipeline pipeline for each pair of ALB and Auto Scaling group that deploys the application using a single AWS CodeDeploy application and deployment group for those resources.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group that includes all ALBs and Auto Scaling groups.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a single AWS CodePipeline pipeline that deploys the application in parallel using a single AWS CodeDeploy application and separate deployment groups for each pair of ALB and Auto Scaling group.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>You can associate more than one deployment group with an application in CodeDeploy. This makes it possible to deploy an application revision to different sets of instances at different times or in parallel.</p><p>In this case the configuration can deploy to each ALB and Auto Scaling group pair in parallel within a single AWS CodePipeline pipeline. This is the most efficient solution and requires the least configuration.</p><p>Deployment options include using a blue/green deployment strategy or an in-place deployment.</p><p><strong>CORRECT: </strong>\"Use a single AWS CodePipeline pipeline that deploys the application in parallel using a single AWS CodeDeploy application and separate deployment groups for each pair of ALB and Auto Scaling group\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use a single AWS CodePipeline pipeline that deploys the application in parallel using separate AWS CodeDeploy applications and deployment groups for each pair of ALB and Auto Scaling group\" is incorrect.</p><p>A separate deployment group can be specified to deploy the updates in parallel. These deployment groups can be created within a single CodeDeploy application. You cannot select multiple CodeDeploy applications within the pipeline.</p><p><strong>INCORRECT:</strong> \"Use a separate AWS CodePipeline pipeline for each pair of ALB and Auto Scaling group that deploys the application using a single AWS CodeDeploy application and deployment group for those resources\" is incorrect.</p><p>This is a more complex configuration that requires more configuration and management so is not the preferred option.</p><p><strong>INCORRECT:</strong> \"Use a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group that includes all ALBs and Auto Scaling groups\" is incorrect.</p><p>You cannot include all ALBs and Auto Scaling groups in a single deployment group within CodeDeploy. Each ALB and Auto Scaling group pair should be added to a separate deployment group within the single CodeDeploy application. The pipeline can then be configured to run the deployments in parallel.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 11,
    "question": "<p>A global health care company uses AWS CloudFormation templates to deploy applications across various environments. The template includes Amazon EC2 instances and an Amazon RDS database. During the testing phase before the application went live, the Amazon RDS instance type was changed and caused the instance to be re-created, resulting in the loss of test data. Also, when the CloudFormation stacks are deleted, it is mandatory to keep a snapshot of the EBS volumes for backup and compliance purposes.</p><p>How can this be achieved? (Select TWO.)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use DeletionPolicy=Snapshot to persist the EBS volumes attached to Amazon EC2 instances.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use an AWS CloudFormation stack policy to deny updates to the instance. Only allow UpdateStack permission to IAM principals that are denied SetStackPolicy.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>In the AWS CloudFormation template, set the DeletionPolicy of the AWS::RDS::DBInstance DeletionPolicy property to “Retain.”</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>In the AWS CloudFormation template, set the AWS::RDS::DBInstance DBlnstanceClass property to be read-only.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Enable termination protection for the Amazon EC2 instances and the Amazon RDS database.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Management & Governance",
    "explanation": "<p>To control how AWS CloudFormation handles the EBS volume when the stack is deleted, set a deletion policy for the volume. It could be deleted, retained, or stored as a snapshot.</p><p>CloudFormation keeps the resource without deleting the resource or its contents when the resource is replaced. This policy could be added to any resource type. Resources that are retained continue to exist and continue to incur applicable charges until the resources are deleted.</p><p><strong>CORRECT: </strong>\"Use DeletionPolicy=Snapshot to persist the EBS volumes attached to Amazon EC2 instances\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"In the AWS CloudFormation template, set the DeletionPolicy of the AWS::RDS::DBInstance DeletionPolicy property to “Retain.”\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable termination protection for the Amazon EC2 instances and the Amazon RDS database\" is incorrect.</p><p>With AWS CloudFormation stacks the template should be updated to configure retention of resources and persistence of snapshots. This will achieve the required outcome.</p><p><strong>INCORRECT:</strong> \"Use an AWS CloudFormation stack policy to deny updates to the instance. Only allow UpdateStack permission to IAM principals that are denied SetStackPolicy\" is incorrect.</p><p>Using permissions will complicate the stack creation and update process and may introduce issues. It is better to use the techniques in the correct answers instead.</p><p><strong>INCORRECT:</strong> \"In the AWS CloudFormation template, set the AWS::RDS::DBInstance DBlnstanceClass property to be read-only\" is incorrect.</p><p>The solution should not prevent the instance type from being changed as that may be a legitimate requirement. It should instead ensure that the database is not lost, and the snapshots are not deleted.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-ebs-volume.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-ebs-volume.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-ebs-volume.html",
      "https://digitalcloud.training/aws-cloudformation/"
    ]
  },
  {
    "id": 12,
    "question": "<p>A company is using AWS Storage Gateway for a branch office location. The gateway is configured in file gateway mode in front of an Amazon S3 bucket that contains files that must be processed by workers in the branch office. Each night a batch process uploads many files to the S3 bucket. Users have reported that the new files are not visible in the morning though they do exist in the S3 bucket.</p><p>How can a DevOps engineer ensure that the files become visible?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the batch process to upload files to the S3 bucket using Amazon S3 transfer acceleration.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure an Amazon EventBridge event to run on a schedule and trigger an AWS Lambda functions that executes the RefreshCache command.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure the Storage Gateway to run in cached Volume Gateway mode and use S3 event notifications to update the storage gateway when objects are uploaded.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>In file gateway mode the users and applications in the branch office can access the storage using either NFS or SMB protocols. The new objects in the bucket may sometimes not be visible. The RefreshCache operation refreshes the cached inventory of objects for the specified file share. This operation finds objects in the Amazon S3 bucket that were added, removed, or replaced since the gateway last listed the bucket's contents and cached the results.</p><p><strong>CORRECT: </strong>\"Configure an Amazon EventBridge event to run on a schedule and trigger an AWS Lambda functions that executes the RefreshCache command\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the Storage Gateway to run in cached Volume Gateway mode and use S3 event notifications to update the storage gateway when objects are uploaded\" is incorrect.</p><p>Volume Gateway mode uses block storage targets (iSCSI) rather than file storage targets (NFS/SMB) and would therefore be a major change to the architecture. Event notifications cannot be used to refresh the cache in a storage gateway.</p><p><strong>INCORRECT:</strong> \"Use S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway\" is incorrect. S3 replication cannot be used to replicate to storage gateway.</p><p><strong>INCORRECT:</strong> \"Configure the batch process to upload files to the S3 bucket using Amazon S3 transfer acceleration\" is incorrect. Transfer acceleration is used to improve upload speeds and does not assist at all in this situation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html\">https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html",
      "https://digitalcloud.training/aws-storage-gateway/"
    ]
  },
  {
    "id": 13,
    "question": "<p>An application was recently migrated to AWS. While the initial version worked, a new version is displaying an error relating to the database layer.</p><p>A DevOps engineer checked Amazon CloudWatch Logs and determined that the error was due to a database connection issue. However, no new database deployment has been performed and there were no changes to the database. Typically the application fetches the database credentials from AWS Secrets Manager.</p><p>What is the most likely cause of the issue?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The GetSecretValue API call in the application didn't include the version of the secret to return.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The secretsmanager:DescribeSecret permission is not enabled for the client-side application component and so it is unable to retrieve the database password.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The DB credentials and connection details in Secrets Manager have been encrypted using the default Secrets Manager KMS key for the account. The application and secret are in different AWS accounts though, and no cross-account access has been granted.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>When secret rotation is configured in Secrets Manager, it causes the secret to rotate once as soon as you store the secret. This can lead to a situation where the old credentials are not usable anymore after the initial rotation. It is possible that the team forgot to update the application to retrieve the secret from Secrets Manager.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>AWS Secrets manager is the preferred option to store database credentials rather than the application code base since Secrets Manager gives many options to manage, rotate and retrieve secrets out of the box. This is more secure as this information is not embedded in the application code.</p><p><strong>CORRECT: </strong>\"When secret rotation is configured in Secrets Manager, it causes the secret to rotate once as soon as you store the secret. This can lead to a situation where the old credentials are not usable anymore after the initial rotation. It is possible that the team forgot to update the application to retrieve the secret from Secrets Manager\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The DB credentials and connection details in Secrets Manager have been encrypted using the default Secrets Manager CMK for the account. The application and secret are in different AWS accounts though, and no cross-account access has been granted\" is incorrect.</p><p>As the account is the same and it is a subsequent deployment post initial version, this is unlikely to be the correct explanation.</p><p><strong>INCORRECT:</strong> \"The secretsmanager:DescribeSecret permission is not enabled for the client-side application component and so it is unable to retrieve the database password\" is incorrect.</p><p>Since permissions were not changed, this is not a correct explanation of the issue.</p><p><strong>INCORRECT:</strong> \"The GetSecretValue API call in the application didn't include the version of the secret to return\" is incorrect.</p><p>This is also a possible failure when there are multiple versions to a secret and you are fetching the incorrect version of secret but in this scenario, this is not the cause of the issue.</p><p><strong>References:</strong></p><p><a href=\"https://awscloudsecvirtualevent.com/workshops/module4/rds/\">https://awscloudsecvirtualevent.com/workshops/module4/rds/</a></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html</a></p><p><a href=\"https://docs.amazonaws.cn/en_us/secretsmanager/latest/userguide/cfn-example_reference-secret.html\">https://docs.amazonaws.cn/en_us/secretsmanager/latest/userguide/cfn-example_reference-secret.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-secrets-manager/\">https://digitalcloud.training/aws-secrets-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://awscloudsecvirtualevent.com/workshops/module4/rds/",
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html",
      "https://docs.amazonaws.cn/en_us/secretsmanager/latest/userguide/cfn-example_reference-secret.html",
      "https://digitalcloud.training/aws-secrets-manager/"
    ]
  },
  {
    "id": 14,
    "question": "<p>A DevOps engineer is migrating an application running on Docker containers from an on-premises data center to AWS. The application must run with minimal management overhead. Encrypted communications between the data center and AWS must be implemented for secure connectivity with on-premises resources.</p><p>Which actions should the DevOps engineer take to meet these requirements? (Select TWO.)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement a Network Load Balancer with IP-based targets and configure an HTTPS listener.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement an AWS Managed VPN to encrypt traffic between the on-premises data center and the VPC.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Migrate the container-based workload to Amazon ECS with the Fargate launch type in a custom VPC.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Migrate the container-based workload to Amazon ECS with the EC2 launch type in a custom VPC.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Implement a VPC endpoint and update security groups to enable access to Amazon ECS.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>The requirements are for low management overhead of the container-based application on AWS and encryption for traffic between AWS and the on-premises network. To minimize management overhead of the application the DevOps engineer should deploy the container-based application on Amazon ECS with the Fargate launch type. This is the serverless solution for Amazon ECS and will be the easiest to manage.</p><p>To encrypt communications between the on-premises data center and the VPC, the engineer should deploy an AWS Managed VPN which will used IPSec to encrypt the traffic between these networks.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-16-13-2d804006a971e4e66b25f15f06b9dd58.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-16-13-2d804006a971e4e66b25f15f06b9dd58.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Migrate the container-based workload to Amazon ECS with the Fargate launch type in a custom VPC\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Implement an AWS Managed VPN to encrypt traffic between the on-premises data center and the VPC\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the container-based workload to Amazon ECS with the EC2 launch type in a custom VPC\" is incorrect.</p><p>The EC2 launch type has a higher management overhead as you must maintain the EC2 instances that run the container agent.</p><p><strong>INCORRECT:</strong> \"Implement a VPC endpoint and update security groups to enable access to Amazon ECS\" is incorrect.</p><p>With the Fargate launch type there is no need to add a VPC endpoint to enable connectivity for the ECS cluster.</p><p><strong>INCORRECT:</strong> \"Implement a Network Load Balancer with IP-based targets and configure an HTTPS listener\" is incorrect.</p><p>You cannot create an HTTPS listener with an NLB and there is no need to use a load balancer to meet these requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html",
      "https://digitalcloud.training/amazon-ecs-and-eks/",
      "https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/"
    ]
  },
  {
    "id": 15,
    "question": "<p>A DevOps engineer is creating a pipeline in AWS CodePipeline for automation of a testing process. The engineer wants to be notified when the execution state fails and used the following custom event pattern in Amazon EventBridge:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question/2023-02-13_09-03-34-adc59425bdf24abb6630e1ead4f37af0.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question/2023-02-13_09-03-34-adc59425bdf24abb6630e1ead4f37af0.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p>Which type of events will match this event pattern?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Failed deploy and build actions across all the pipelines.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>All abandoned or cancelled approval actions across all pipelines.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Failed stage execution events for the executed stage.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>All rejected or failed approval actions across all the pipelines.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>You can monitor CodePipeline events in EventBridge, which delivers a stream of real-time data from your own applications, software-as-a-service (SaaS) applications, and AWS services. EventBridge routes that data to targets such as AWS Lambda and Amazon Simple Notification Service.</p><p>Events are composed of rules. A rule is configured by choosing the following:</p><ul><li><p><strong>Event Pattern.</strong> Each rule is expressed as an event pattern with the source and type of events to monitor, and event targets. To monitor events, you create a rule with the service you are monitoring as the event source, such as CodePipeline. For example, you can create a rule with an event pattern that that uses CodePipeline as an event source to trigger the rule when there are changes in the state of a pipeline, stage, or action.</p></li><li><p><strong>Targets.</strong> The new rule receives a selected service as the event target. You might want to set up a target service to send notifications, capture state information, take corrective action, initiate events, or take other actions. When you add your target, you must also grant permissions to Amazon CloudWatch Events to allow it to invoke the selected target service.</p></li></ul><p>For approval actions, the FAILED state means the action was either rejected by the reviewer or failed due to an incorrect action configuration.</p><p><strong>CORRECT: </strong>\"All rejected or failed approval actions across all the pipelines\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Failed stage execution events for the executed stage\" is incorrect.</p><p>The specified pattern applies to a CodePipeline Action Execution State Change rather than a CodePipeline Stage Execution State Change.</p><p><strong>INCORRECT:</strong> \"Failed deploy and build actions across all the pipelines\" is incorrect.</p><p>The specified pattern applies to rejected or failed approval actions only.</p><p><strong>INCORRECT:</strong> \"All abandoned or cancelled approval actions across all pipelines\" is incorrect.</p><p>The specified pattern applies to rejected or failed approval actions only.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 16,
    "question": "<p>A legacy application runs on a single Amazon EC2 instance and is backed with Amazon EBS storage. The company requires the ability to recover quickly with minimal data losses in the event of network connectivity issues or power failures on the EC2 instance.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon CloudWatch alarm for the StatusCheckFailed_Instance metric and select the EC2 action to reboot the instance.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon EC2 Auto Scaling group and configure the minimum, maximum, and desired capacity to 1.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a spread placement group configured across distinct underlying hardware. Enable automatic recovery.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon CloudWatch alarm for the StatusCheckFailed_System metric and select the EC2 action to recover the instance.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically recovers the instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair.</p><p>The alarm uses the StatusCheckFailed_System metric and can be configured to initiate a recover action. When this occurs you will be notified by the Amazon SNS topic you configured when creating the alarm.</p><p>During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost.</p><p>Examples of problems that cause system status checks to fail include:</p><ul><li><p>Loss of network connectivity</p></li><li><p>Loss of system power</p></li><li><p>Software issues on the physical host</p></li><li><p>Hardware issues on the physical host that impact network reachability</p></li></ul><p><strong>CORRECT: </strong>\"Create an Amazon CloudWatch alarm for the StatusCheckFailed_System metric and select the EC2 action to recover the instance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm for the StatusCheckFailed_Instance metric and select the EC2 action to reboot the instance\" is incorrect.</p><p>This metric indicates issues that involve your involvement to repair. The StatusCheckFailed_System metric should be used instead to identify issues with the underlying hardware or network.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EC2 Auto Scaling group and configure the minimum, maximum, and desired capacity to 1\" is incorrect.</p><p>Auto Scaling groups use health checks to identify if the instance is functioning. The StatusCheckFailed_System metric is more useful in this scenario as it can identify several issues that may affect the instance including hardware issues on the physical host that impact network reachability (an instance health check won’t do this).</p><p><strong>INCORRECT:</strong> \"Create a spread placement group configured across distinct underlying hardware. Enable automatic recovery\" is incorrect.</p><p>Spread placement groups are used when you have more than one instance and want to spread the instances across distinct underlying hardware. In this case there is only one instance, so it is of no benefit.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html#AddingRecoverActions\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html#AddingRecoverActions</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html#AddingRecoverActions",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  },
  {
    "id": 17,
    "question": "<p>A financial organization is already utilizing a CI/CD pipeline to deploy applications in production. A DevOps team has automated the entire upgrade flow for a database upgrade process.</p><p>During deployment, they triggered a CI/CD pipeline with no human intervention, and the upgrade progressed smoothly. Then, 20 minutes in, the pipeline became stuck, and the update halted. It was discovered that a planned outage in an AWS region caused the pipeline to fail.</p><p>What solution can a DevOps engineer implement to prevent this from happening again without causing unnecessary delay or cost?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Embed AWS Health API insights into the CI/CD pipelines using AWS Lambda functions to automatically stop deployments when an AWS Health event is reported in the Region.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Utilize an additional instance of database and apply upgrades on passive database. When the upgrade is complete, route traffic to the passive database and perform a switch between database instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an AWS CodePipeline pipeline stage to retrigger the pipeline and rerun the CI/CD process.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Schedule all database upgrades to avoid any planned outages from AWS in the relevant AWS Regions.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>In AWS CodePipeline, you can create a new stage with a single action to asynchronously invoke a Lambda function. The function will call AWS Health <a href=\"https://docs.aws.amazon.com/health/latest/APIReference/API_DescribeEvents.html\">DescribeEvents API</a> to retrieve the list of active health incidents. Then, the function will complete the event analysis and decide whether it may impact the running deployment.</p><p>Finally, the function will call back CodePipeline with the evaluation results through either <a href=\"https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_PutJobSuccessResult.html\">PutJobSuccessResult</a> or <a href=\"https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_PutJobFailureResult.html\">PutJobFailureResult</a> API operations.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-20-46-825daf1581271e9aa301dbc5540619c9.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-20-46-825daf1581271e9aa301dbc5540619c9.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Embed AWS Health API insights into the CI/CD pipelines using AWS Lambda functions to automatically stop deployments when an AWS Health event is reported in the Region\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS CodePipeline pipeline stage to retrigger the pipeline and rerun the CI/CD process\" is incorrect.</p><p>The issue with this option is that in case AWS health event is still there, the deployment might fail again.</p><p><strong>INCORRECT:</strong> \"Schedule all database upgrades to avoid any planned outages from AWS in the relevant AWS Regions\" is incorrect.</p><p>This is a manual step which will work but can unnecessarily delay the upgrade in case of business critical/ compliance upgrades.</p><p><strong>INCORRECT:</strong> \"Utilize an additional instance of database and apply upgrades on passive database. When the upgrade is complete, route traffic to the passive database and perform a switch between database instances\" is incorrect.</p><p>This is a high-cost option and above explained scenario is a better suited one.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/build-health-aware-ci-cd-pipelines/\">https://aws.amazon.com/blogs/devops/build-health-aware-ci-cd-pipelines/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/health/latest/APIReference/API_DescribeEvents.html",
      "https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_PutJobSuccessResult.html",
      "https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_PutJobFailureResult.html",
      "https://aws.amazon.com/blogs/devops/build-health-aware-ci-cd-pipelines/",
      "https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/"
    ]
  },
  {
    "id": 18,
    "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The application is used by users around the world who access the application using a custom DNS domain name. The application must support encryption in transit, be protected from DDoS attacks and web exploits, should be optimized for performance.</p><p>Which actions should a DevOps engineer take to meet these requirements? (Select TWO.)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an AWS WAF web ACL, configure a default action and rule, and specify the Auto Scaling group that WAF should inspect.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon CloudFront distribution with the Auto Scaling group as an origin. Configure the custom domain name and attach an SSL/TLS certificate.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an AWS WAF web ACL, configure a default action and rule, and specify the CloudFront distribution that WAF should inspect.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon CloudFront distribution with the ALB as an origin. Configure the custom domain name and attach an SSL/TLS certificate.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create an AWS WAF web ACL, configure a default action and rule, and specify the ALB that WAF should inspect.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>To improve performance for global users the solution should use Amazon CloudFront. The distribution should specify the ALB as the origin and use a custom domain name and SSL/TLS certificate. This will enable caching of content in Edge Locations around the world and CloudFront offers DDoS protection.</p><p>To protect against web exploits AWS WAF should be used. A Web ACL must be created with an action and rule specified to deal with threats. The web ACL can be specified in the CloudFront distribution.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudFront distribution with the ALB as an origin. Configure the custom domain name and attach an SSL/TLS certificate\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS WAF web ACL, configure a default action and rule, and specify the CloudFront distribution that WAF should inspect\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution with the Auto Scaling group as an origin. Configure the custom domain name and attach an SSL/TLS certificate\" is incorrect.</p><p>You can configure an ELB as an origin for the distribution, but you cannot specify an ASG.</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF web ACL, configure a default action and rule, and specify the ALB that WAF should inspect\" is incorrect.</p><p>The web ACL should be attached to the CloudFront distribution in this case as it sits in front of the ALB. It is always better to protect as close to the edge as possible.</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF web ACL, configure a default action and rule, and specify the Auto Scaling group that WAF should inspect\" is incorrect.</p><p>The web ACL must be attached to the CloudFront distribution, not the ASG.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html\">https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html",
      "https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html",
      "https://digitalcloud.training/amazon-cloudfront/"
    ]
  },
  {
    "id": 19,
    "question": "<p>A company is deploying an application in four AWS Regions across North America, Europe and Asia. The application will be used by millions of users. The application must allow users to submit data through the application layer in each Region and have it saved in a low-latency database layer. The company also must ensure that the data can be read through the application layer in each Region.</p><p>Which solution will meet these requirements with the LOWEST latency of reads and writes?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a table in Amazon DynamoDB and enable global tables in each of the four Regions.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a database in Amazon DocumentDB (with MongoDB compatibility) in each of the four Regions.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon ElastiCache for Redis cluster and configure replication groups in each of the four Regions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon RDS database in one Region and create Read Replicas in each of the three other Regions.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions.</p><p>This is the only workable solution in the list that provides both reads and writes in each Region that are replicated across the other Regions. This is also the best solution as it provides low latency reads and writes.</p><p><strong>CORRECT: </strong>\"Create a table in Amazon DynamoDB and enable global tables in each of the four Regions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon RDS database in one Region and create Read Replicas in each of the three other Regions\" is incorrect.</p><p>This solutions does not provide local writes in each Region as the Read Replicas cannot be written to. Therefore, this solution only offers low latency reads in each Region.</p><p><strong>INCORRECT:</strong> \"Create an Amazon ElastiCache for Redis cluster and configure replication groups in each of the four Regions\" is incorrect.</p><p>Replication groups in ElastiCache are used within a Region and not across Regions so this solution does not work.</p><p><strong>INCORRECT:</strong> \"Create a database in Amazon DocumentDB (with MongoDB compatibility) in each of the four Regions\" is incorrect.</p><p>This solution provides a database in each Region, there is no mechanism for replication. You can create replica instances in different Regions but that would only provide low latency reads (as with RDS), and not low latency writes.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/dynamodb/global-tables/",
      "https://digitalcloud.training/amazon-dynamodb/"
    ]
  },
  {
    "id": 20,
    "question": "<p>A web application runs on a custom port. The application has been deployed in an Auto Scaling group with an Application Load Balancer (ALB). After launching instances the Auto Scaling and Target Group health checks are returning a healthy status. However, users report that the application is not accessible.</p><p>Which steps should a DevOps engineer take to troubleshoot the issue? (Select TWO.)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a path-based routing rule to direct traffic to the custom port and path on the EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Modify the Target Group health check configuration to check the application process on the custom port and path.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Modify the Target Group configuration to specify targets by IP rather than instance ID to allow routing to any private IP address.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Modify the Auto Scaling group health check configuration to check the application process on the custom port and path.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Inspect the listener configuration on the ALB and check it is configured with the TCP protocol and the custom port.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Compute",
    "explanation": "<p>By default health checks are configured to use the HTTP protocol and port 80. For an ALB the traffic protocol must be HTTP or HTTPS, but the port can be customized. The most likely cause of the issue is that the web service is running on the instances and the default protocol/port is used for health checks on the ALB and ASG. This will result in instances becoming “healthy” despite the actual application service not functioning correctly.</p><p>The engineer should check the ASG health check configuration and the target group health check configuration. If the default values are used then the correct custom port number should be entered instead. The path may also be updated if a specific web page should be checked.</p><p>The image below shows how you can override the default port number and path in a target group health check:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-08-15-0585684e22874b6c255bd441d63a099e.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-08-15-0585684e22874b6c255bd441d63a099e.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Modify the Target Group health check configuration to check the application process on the custom port and path\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Modify the Auto Scaling group health check configuration to check the application process on the custom port and path\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Inspect the listener configuration on the ALB and check it is configured with the TCP protocol and the custom port\" is incorrect.</p><p>You cannot use a TCP listener with an ALB. It must be HTTP or HTTPS though a custom port number can certainly be used.</p><p><strong>INCORRECT:</strong> \"Modify the Target Group configuration to specify targets by IP rather than instance ID to allow routing to any private IP address\" is incorrect.</p><p>The issue is not related to the IP addresses traffic is being directed to on the instances; it is related to the port number and path the health checks are configured to check.</p><p><strong>INCORRECT:</strong> \"Create a path-based routing rule to direct traffic to the custom port and path on the EC2 instances\" is incorrect.</p><p>Path-based routing rules are used to route traffic to different target groups based on the path in the HTTP request. In this case there is only one target group, so a path-based routing rule is useless. Instead, the ALB must direct traffic to a custom port number (configured in the listener) and validate the application is healthy be running health checks against the appropriate port and path.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html",
      "https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/"
    ]
  }
]