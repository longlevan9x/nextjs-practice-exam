[
  {
    "id": 1,
    "question": "<p>An application is being deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The security team requires that the traffic is secured with SSL/TLS certificates. Protection against common web exploits must also be implemented. The solution should not have a performance impact on the EC2 instances.</p><p>What steps should be taken to secure the web application? (Select TWO.)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an AWS WAF web ACL and attach it to the ALB.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable EBS encryption for the EC2 volumes to encrypt all traffic.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Install SSL/TLS certificates on the EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure Server-Side Encryption with KMS managed keys.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Add an SSL/TLS certificate to a secure listener on the ALB.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>To secure the traffic in transit an SSL/TLS certificate should be attached to a secure listener on the ALB. This will not affect the performance of the EC2 instances as the encryption takes place only between the client and the ALB. The certificate can be issued through AWS Certificate Manager (ACM).</p><p>The AWS Web Application Firewall (AWS WAF) protects against common web exploits. The company can create a web ACL with a rule and action and then attach it to the ALB. This will protect against web exploits.</p><p><strong>CORRECT: </strong>\"Add an SSL/TLS certificate to a secure listener on the ALB\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS WAF web ACL and attach it to the ALB\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install SSL/TLS certificates on the EC2 instances\" is incorrect.</p><p>Encryption on the EC2 instances would impact the performance of those instances.</p><p><strong>INCORRECT:</strong> \"Configure Server-Side Encryption with KMS managed keys\" is incorrect.</p><p>This is not relevant to in transit encryption, this is used to encrypt data at rest on services such as Amazon S3.</p><p><strong>INCORRECT:</strong> \"Enable EBS encryption for the EC2 volumes to encrypt all traffic\" is incorrect.</p><p>EBS encryption is used for encrypting data at rest. The question requires encryption using SSL/TLS certificates which is encryption in transit.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/\">https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/</a></p><p><a href=\"https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\">https://aws.amazon.com/elasticloadbalancing/application-load-balancer/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/"
    ]
  },
  {
    "id": 2,
    "question": "<p>A company has deployed AWS Single Sign-On (AWS SSO) and needs to ensure that user accounts are not created within AWS Identity and Access Management (AWS IAM). A DevOps engineer must create an automated solution for immediately disabling credentials of any new IAM user that is created. The security team must be notified when user creation events take place.</p><p>Which combination of steps should the DevOps engineer take to meet these requirements? (Select THREE.)</p>",
    "corrects": [
      3,
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon EventBridge rule that is triggered by IAM GetLoginProfile API calls in AWS CloudTrail.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an AWS Config rule that sends a notification to the security team using Amazon SNS when user accounts are modified.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an AWS Lambda function that disables the access keys and deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon SNS topic that is a target of the EventBridge rule. Subscribe the security team's group email address to the topic.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create an Amazon EventBridge rule that is triggered by IAM CreateUser API calls in AWS CloudTrail.</p>",
        "correct": true
      },
      {
        "id": 6,
        "answer": "<p>Create an AWS Lambda function that deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>The company is using AWS SSO and we can presume have an identity source that is outside of AWS IAM. They therefore want to control creation of IAM users. The solution uses an EventBridge rule that monitors for CreateUser API calls in AWS CloudTrail. This will pick up all user creation events.</p><p>Then, an AWS Lambda function will disable both the access keys (if created) and login profile (if created) that are associated with the newly created user account. Then, an SNS notification will be sent to the security team.</p><p>This solution meets all the stated requirements.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule that is triggered by IAM CreateUser API calls in AWS CloudTrail\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS Lambda function that disables the access keys and deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an Amazon SNS topic that is a target of the EventBridge rule. Subscribe the security team's group email address to the topic\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule that is triggered by IAM GetLoginProfile API calls in AWS CloudTrail\" is incorrect.</p><p>This would only be triggered when user accounts with console (password) access are created. A user with programmatic access does not have a login profile unless you create a password for the user to access the AWS Management Console. Therefore, this would miss users that are created with programmatic-only access.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule\" is incorrect.</p><p>As above, login profiles are associated with console-based password access only, they do not apply to users with programmatic-only access.</p><p><strong>INCORRECT:</strong> \"Create an AWS Config rule that sends a notification to the security team using Amazon SNS when user accounts are modified\" is incorrect.</p><p>The DevOps engineer should directly configure Amazon SNS to be triggered by EventBridge. There is no need to send notifications related to user modifications, only creation events.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/APIReference/API_GetLoginProfile.html\">https://docs.aws.amazon.com/IAM/latest/APIReference/API_GetLoginProfile.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/APIReference/API_GetLoginProfile.html",
      "https://digitalcloud.training/aws-iam/"
    ]
  },
  {
    "id": 3,
    "question": "<p>A company requires an automated solution that terminates Amazon EC2 instances that have been logged into manually within 24 hours of the login event. The applications running in the account are launched using Auto Scaling groups and the CloudWatch Logs agent is configured on all instances.</p><p>How should a DevOps engineer build the automation?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a CloudWatch alarm that will trigger on login events. Send the notification to a Kinesis Data Firehose stream. Configure the stream to send notifications to an SNS topic and instruct the operations team to subscribe to the topic and terminate EC2 instances that produced login events within 24 hours.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a CloudWatch Logs subscription filter that delivers logs to an AWS Lambda function. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a CloudWatch Logs subscription filter that delivers logs to an AWS Step Functions state machine. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a CloudWatch alarm that will trigger on AWS API call events in CloudTrail. Configure the alarm to send a message to an Amazon SQS queue. Create an AWS Lambda function that processes messages from the queue and terminates the instances that produced the login event.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to Kinesis, Lambda, or Kinesis Data Firehose. When log events are sent to the receiving service, they are base64 encoded and compressed with the gzip format.</p><p>The Lambda function that receives the log events can process the log files looking for entries that indicate that a manual login event occurred and add a tag. Another Lambda function that runs on a schedule can then look for instances that have been tagged and terminate them.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Logs subscription filter that delivers logs to an AWS Lambda function. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch Logs subscription filter that delivers logs to an AWS Step Functions state machine. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged\" is incorrect.</p><p>You cannot configure Step Functions to receive the events.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that will trigger on AWS API call events in CloudTrail. Configure the alarm to send a message to an Amazon SQS queue. Create an AWS Lambda function that processes messages from the queue and terminates the instances that produced the login event\" is incorrect.</p><p>The API calls logged via CloudTrail will not show that a manual login event occurred as that does not require an AWS API call to be made. The manual login data would be included in the logs delivered to CloudWatch Logs by the CloudWatch agent.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that will trigger on login events. Send the notification to a Kinesis Data Firehose stream. Configure the stream to send notifications to an SNS topic and instruct the operations team to subscribe to the topic and terminate EC2 instances that produced login events within 24 hours\" is incorrect.</p><p>You can configure a KDS stream to receive the events but would then need a record processor (KCL worker) to process the events, you cannot send directly to an SNS topic. Also, requiring the operations team to process the terminations is not automated.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  },
  {
    "id": 4,
    "question": "<p>A data intelligence and analytics company has implemented a CI/CD pipeline using AWS CodePipeline which takes code from an AWS CodeCommit repository and then builds it using AWS CodeBuild. During the deploy stage, the application is deployed onto an Amazon ECS cluster. During deployment, the application is only partly updated on some ECS tasks which are running an older version of the image.</p><p>A DevOps engineer investigated and found that terminating the task or clearing the local Docker cache fixes the issue, but a more robust solution is required that provides visibility and identification to track where container images are deployed. Also, the start-up time of the containers needs to be optimized.</p><p>Which actions should the DevOps engineer take to achieve these requirements? (Select TWO.)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Move secondary dependencies to be downloaded at application startup rather than including them within a static container image.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Move all the dependencies into a single image and pull them from a single container registry.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Developer Tools",
    "explanation": "<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. The IMAGEID property is the SHA digest for the Docker image used to start the container.</p><p>Ideally, a container image is intended to be a complete snapshot of everything that the application requires to function. With a complete container image, the application could be run by downloading one container image from one place.</p><p>There is no need to download several separate pieces from different locations. Therefore, as a best practice, store all application dependencies as static files inside the container image. This will improve performance and start up time.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-00-17-d1b2e063cff66b89f7caee81435b0971.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-00-17-d1b2e063cff66b89f7caee81435b0971.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Move all the dependencies into a single image and pull them from a single container registry\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time\" is incorrect.</p><p>The exact image tag needs to be pulled because the latest tag will only pull the image that was pushed last.</p><p><strong>INCORRECT:</strong> \"After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task\" is incorrect.</p><p>Using Lambda to SSH onto instances is not a workable solution. SSM Run Command may be better but it’s also not ideal.</p><p><strong>INCORRECT:</strong> \"Move secondary dependencies to be downloaded at application startup rather than including them within a static container image\" is incorrect.</p><p>This would delay the image spin up time rather than improving it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html\">https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html</a></p><p><br></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 5,
    "question": "<p>A development team is running a project that will involve deploying applications across several Amazon VPCs. The applications will require fully meshed network connectivity to enable transitive routing between VPCs. The development lead is concerned about security and has requested centralized control over network access controls.</p><p>Which deployment will satisfy the requirements with the most operational efficiency?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an AWS PrivateLink connection between each VPC and configure VPC endpoints to enable fully meshed connectivity. Use AWS Security Hub to centrally deploy and manage security policies across the VPCs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy an AWS Site-to-Site VPN between each VPC and configure route tables to enable fully meshed routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create VPC peering connections between the VPCs and configure a fully meshed network topology. Use AWS Web Application Firewall (WAF) to centrally deploy and manage WebACLs across the VPCs.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy AWS Transit Gateway to create a fully meshed network topology with transitive routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>AWS Transit Gateway can be configured to enable a fully meshed network topology which allows transitive routing between all the VPCs. With AWS Network Firewall, you can define firewall rules that provide fine-grained control over network traffic.</p><p>AWS Firewall Manager allows you to build policies based on Network Firewall rules and then centrally apply those policies across your virtual private clouds (VPCs) and accounts.</p><p><strong>CORRECT: </strong>\"Deploy AWS Transit Gateway to create a fully meshed network topology with transitive routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create VPC peering connections between the VPCs and configure a fully meshed network topology. Use AWS Web Application Firewall (WAF) to centrally deploy and manage WebACLs across the VPCs\" is incorrect.</p><p>VPC peering requires creating complex peering relationships and does not support transitive routing (though this can be achieved through a mesh of peering connections). VPC peering is less operationally efficient compared to using a Transit Gateway. AWS WAF is not the best solution for enforcing centralized network access controls, it is used for preventing web based attacks.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Site-to-Site VPN between each VPC and configure route tables to enable fully meshed routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs\" is incorrect.</p><p>You cannot connect Amazon VPCs using AWS S2S VPNs. You can only use an AWS VPN to connect on-premises networks.</p><p><strong>INCORRECT:</strong> \"Create an AWS PrivateLink connection between each VPC and configure VPC endpoints to enable fully meshed connectivity. Use AWS Security Hub to centrally deploy and manage security policies across the VPC\" is incorrect.</p><p>PrivateLink is not used for creating this kind of network deployment. It is used for private access to AWS services using private IP addresses.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/network-firewall/\">https://aws.amazon.com/network-firewall/</a></p><p><a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/network-firewall/",
      "https://aws.amazon.com/transit-gateway/"
    ]
  },
  {
    "id": 6,
    "question": "<p>A web application runs on Amazon EC2 instances in an EC2 Auto Scaling group behind an Application Load Balancer (ALB). A DevOps engineer needs to implement a strategy for deploying updates that meets the following requirements:</p><p>· Automatically launches the new version of the application on a second set of instances with the same capacity as the old version of the application.</p><p>· Maintains the old version unchanged while the new version is launched.</p><p>· Shifts traffic to the new version when the instances are fully deployed.</p><p>· Terminates the old fleet of instances automatically 1 hour after shifting traffic.</p><p>Which solution will satisfy these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use two AWS Elastic Beanstalk environments to perform a blue/green deployment from old set of instances to the new one. Create an application version lifecycle policy that terminates the original environment after 1 hour.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS CodeDeploy and create a deployment group that uses a blue/green deployment configuration. Use the BlueInstanceTerminationOption to terminate the instances in the blue environment after 1 hour.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create an AWS CloudFormation template and configure a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to redirect traffic to the new ALB.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using the Resources key that sets the deletion policy of the ALB to 1 hour and deploy the application.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>When you use a blue/green deployment strategy with AWS CodeDeploy it can be configured to automatically copy the auto scaling group and launch instances running the new version of the application in a new environment. The capacity of the new auto scaling group can be the same and the original ASG can be maintained until the launch is complete.</p><p>The instances in the new environment can be a new target group behind the ALB. The traffic can be shifted across to the new environment when the launch is complete by modifying the target group setting in the ALB. This is performed automatically by CodeDeploy.</p><p>The BlueInstanceTerminationOption can be used to configure whether instances in the original environment are terminated when a blue/green deployment is successful. The options are:</p><ul><li><p>TERMINATE: Instances are terminated after a specified wait time.</p></li><li><p>KEEP_ALIVE: Instances are left running after they are deregistered from the load balancer and removed from the deployment group.</p></li></ul><p><strong>CORRECT: </strong>\"Use AWS CodeDeploy and create a deployment group that uses a blue/green deployment configuration. Use the BlueInstanceTerminationOption to terminate the instances in the blue environment after 1 hour\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudFormation template and configure a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to redirect traffic to the new ALB\" is incorrect.</p><p>A CloudFormation retention policy configures which resources should be retained when a stack is deleted. Retention policies are not suitable for implementing blue/green deployments.</p><p><strong>INCORRECT:</strong> \"Use two AWS Elastic Beanstalk environments to perform a blue/green deployment from old set of instances to the new one. Create an application version lifecycle policy that terminates the original environment after 1 hour\" is incorrect.</p><p>A lifecycle policy tells Elastic Beanstalk to delete application versions that are old, or to delete application versions when the total number of versions for an application exceeds a specified number. This is not used for blue/green deployments.</p><p><strong>INCORRECT:</strong> \"Use AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using the Resources key that sets the deletion policy of the ALB to 1 hour and deploy the application\" is incorrect.</p><p>You can use the Resources key in a configuration file to create and customize AWS resources in your environment. However this is not suitable for implementing a blue/green deployment strategy and the ALB does not need to be deleted. Instead, traffic simply needs to be shifted from one target group to another (when using the correct deployment configuration).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html\">https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html",
      "https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 7,
    "question": "<p>The DevOps team at a global retail company wants to deploy the latest application code to through build, staging, beta &amp; prod environments. While doing the staging deployment, an automated functional test suite needs to be executed which runs for approximately two hours to complete regression testing. The code is managed via AWS CodeCommit.</p><p>How can a DevOps engineer optimize the configuration and automate the pipeline?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a CodeBuild build that will run the test suite. If the stage doesn’t fail, the last stage will deploy the application to production.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will run the test suite. If the stage succeeds, the last stage will deploy the application to production.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a CodePipeline pointing to the master branch of the CodeCommit repository and as a first stage run a CodeBuild build that will run the test suite against the staging environment. Upon passing, deploy to staging using CodeDeploy and if it succeeds, deploy to production.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a Step Function which will run the test suite. Create a CloudWatch Event Rule on the execution termination of the Step Function to invoke a Lambda function and signal CodePipeline the success or failure. If the stage doesn’t fail, the last stage will deploy the application to production.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Developer Tools",
    "explanation": "<p>The solution to these requirements uses an AWS CodePipeline pipeline to automate the entire CI/CD pipeline. When code is committed it will be picked up as a change and the pipeline execution commences. This will automate the testing suites and deployment into production using CodeDeploy.</p><p>Lambda should be ruled out for running the test suite as the maximum timeout for a Lambda function is 15 minutes, so it will not support the given use-case since the functional test suite runs for over two hours.</p><p><strong>INCORRECT:</strong> \"Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a CodeBuild build that will run the test suite. If the stage doesn’t fail, the last stage will deploy the application to production\" is incorrect.</p><p><strong>CORRECT: </strong>\"Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will run the test suite. If the stage succeeds, the last stage will deploy the application to production\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CodePipeline pointing to the master branch of the CodeCommit repository and as a first stage run a CodeBuild build that will run the test suite against the staging environment. Upon passing, deploy to staging using CodeDeploy and if it succeeds, deploy to production\" is incorrect.</p><p>Since CodeBuild Test can’t be as a stage prior to deploying in the staging environment, so this option is incorrect.</p><p><strong>INCORRECT:</strong> \"Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a Step Function which will run the test suite. Create a CloudWatch Event Rule on the execution termination of the Step Function to invoke a Lambda function and signal CodePipeline the success or failure. If the stage doesn’t fail, the last stage will deploy the application to production\" is incorrect.</p><p>AWS Step Functions is a low-code, visual workflow service that developers use to build distributed applications, automate IT and business processes, and build data and machine learning pipelines using AWS services. Workflows manage failures, retries, parallelization, service integrations, and observability so developers can focus on higher-value business logic. This is not an ideal use case for Step Functions.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/\">https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/</a></p><p><a href=\"https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/\">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/",
      "https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/",
      "https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 8,
    "question": "<p>A company plans to deploy a high-performance computing (HPC) workload on Amazon EC2 instances in a shared Amazon VPC. Developers in multiple participant accounts must be granted access to the cluster to perform analytics. The cluster requires a shared file system that supports file-based access to objects stored in Amazon S3 buckets.</p><p>Which deployment steps should be implemented to support the required features and access control?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy an Amazon FSx for NetApp ONTAP file system with NFS access. Create a resource based policy that allows cross-account access for members of the participant accounts. Use access keys to authenticate users and use security groups to enable file system access.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy an Amazon FSx for Windows file server file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy and Amazon FSx for OpenZFS file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy an Amazon FSx for Lustre file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>Amazon FSx for Lustre is the only option that can present objects stored in S3 buckets as files. The question specifically asks for file-based access for objects stored in buckets, so this requirement is satisfied by using FSx for Lustre. The access control requirements can be implemented through an AWS IAM role that can be assumed by cross-account participants. Permissions should be assigned through an identity-based permissions policy assigned to the role.</p><p><strong>CORRECT: </strong>\"Deploy an Amazon FSx for Lustre file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon FSx for NetApp ONTAP file system with NFS access. Create a resource based policy that allows cross-account access for members of the participant accounts. Use access keys to authenticate users and use security groups to enable file system access\" is incorrect.</p><p>This FSx file system does not allow file-based access to S3 objects. You also cannot use resource based policies with FSx file systems and access keys would be unsuitable for cross-account access.</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon FSx for Windows file server file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access\" is incorrect.</p><p>This FSx file system does not allow file-based access to S3 objects. This file system should be used with Windows servers that need an NTFS file system.</p><p><strong>INCORRECT:</strong> \"Deploy and Amazon FSx for OpenZFS file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access\" is incorrect.</p><p>This FSx file system does not allow file-based access to S3 objects.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/LustreGuide/security_iam_service-with-iam.html\">https://docs.aws.amazon.com/fsx/latest/LustreGuide/security_iam_service-with-iam.html</a></p><p><a href=\"https://aws.amazon.com/fsx/lustre/faqs/\">https://aws.amazon.com/fsx/lustre/faqs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/fsx/latest/LustreGuide/security_iam_service-with-iam.html",
      "https://aws.amazon.com/fsx/lustre/faqs/"
    ]
  },
  {
    "id": 9,
    "question": "<p>A legacy application uses IAM user credentials to access resources in the company's AWS Organizations organization. It should not be possible to create IAM users unless the user account making the requires is specific on an exception list. A DevOps engineer must apply these restrictions.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user’s name against the exception list and delete the user account if it is not listed.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user’s name against the exception list and delete the user account if it is not listed.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>With AWS Organizations Service Control Policies (SCPs) you can restrict access to specific resources or define conditions for when SCPs are in effect. The condition element is supported when the Effect element has a value of Deny.</p><p>The StringNotLike condition will check the exception list for the aws:username that made the request. If the username is not present the request will be denied.</p><p><strong>CORRECT: </strong>\"Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list\" is incorrect.</p><p>The API action iam:CreateLoginProfile creates a password for the specified IAM user. This is not used to create the user account.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user’s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The API action iam:GetUser retrieves information about the specified IAM user, including the user's creation date, path, unique ID, and ARN.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user’s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The function should be used to validate that the iam:CreateUser request is being initiated by a user on the exception list. It should not delete the requesting user account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html",
      "https://digitalcloud.training/aws-organizations/"
    ]
  },
  {
    "id": 10,
    "question": "<p>The launch template that is used by an Auto Scaling group has been modified to use a new instance type and AMI. The Auto Scaling group is deployed using AWS CloudFormation. There are 8 production EC2 instances running in the Auto Scaling group.</p><p>A DevOps engineer needs to modify the Auto Scaling group to use the new template version without causing any interruption to the application and must ensure that at least 4 instances are always running.</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AutoScalingRollingUpdate attribute with the MinInstancesInService property.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use the AutoScalingScheduledAction attribute with the MaxBatchSize property.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the AutoScalingReplacingUpdate attribute and specify the WaitOnResourceSignals and PauseTime properties.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the UpdateReplacePolicy attribute and specify the MinSuccessfulInstancesPercent and MaxBatchSize properties.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>You can use the UpdatePolicy attribute to specify how AWS CloudFormation handles updates to several types of resource including Auto Scaling groups. This attribute can be configured with several properties to determine how the update process works. This is important to ensure that the correct number of instances are still available to service requests whilst the update process occurs.</p><p>The MinInstancesInService property specifies the minimum number of instances that must be in service within the Auto Scaling group while CloudFormation updates old instances. You can also specify the MaxBatchSize which specifies the maximum number of instances that CloudFormation updates.</p><p><strong>CORRECT: </strong>\"Use the AutoScalingRollingUpdate attribute with the MinInstancesInService property\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the AutoScalingScheduledAction attribute with the MaxBatchSize property\" is incorrect.</p><p>This is the wrong attribute to use; the engineer must use the AutoScalingRollingUpdate attribute instead.</p><p><strong>INCORRECT:</strong> \"Use the AutoScalingReplacingUpdate attribute and specify the WaitOnResourceSignals and PauseTime properties\" is incorrect.</p><p>A rolling update should be used to specify the number of instances to update in a batch. The replacing update either replaces all instances or creates a new ASG. The properties specified are also inappropriate for the situation (check the link below for more detail).</p><p><strong>INCORRECT:</strong> \"Use the UpdateReplacePolicy attribute and specify the MinSuccessfulInstancesPercent and MaxBatchSize properties\" is incorrect.</p><p>This attribute is used to retain or, in some cases, backup the existing physical instance of a resource when it's replaced during a stack update operation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html",
      "https://digitalcloud.training/amazon-ec2-auto-scaling/"
    ]
  },
  {
    "id": 11,
    "question": "<p>A company runs a Java application in an on-premises data center. The application is not currently containerized but the company plans to migrate it to the AWS Cloud and modernize the application into a containerized deployment. A CI/CD pipeline should also be created to automate application updates.</p><p>Which solution can a DevOps engineer use to meet all these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>App2Container (A2C) is a command-line tool for modernizing .NET and Java applications into containerized applications. A2C analyzes and builds an inventory of all applications running in virtual machines on-premises or in the cloud.</p><p>With A2C you can simply select the application you want to containerize, and A2C packages the application artifact and identified dependencies into container images, configures the network ports, and generates the ECS task and Kubernetes pod definitions.</p><p>App2Container creates CI/CD pipelines for Amazon DevOps services such as CodeBuild and CodeDeploy to build and deploy containers. If you have existing CI/CD tooling (for example, Azure DevOps and Jenkins), then you can integrate A2C provided artifacts – dockerfile, ECS task definition, EKS deployment YAML - into your existing CI/CD workflows.</p><p><strong>CORRECT: </strong>\"Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates\" is incorrect.</p><p>AWS Copilot is a command line interface (CLI) that you can use to quickly launch and manage containerized applications on AWS. It simplifies running applications on Amazon Elastic Container Service (ECS), AWS Fargate, and AWS App Runner. It is not able to inventory and analyze existing applications or migrate them to containers.</p><p><strong>INCORRECT:</strong> \"Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository\" is incorrect.</p><p>AWS Proton is used to create application stack templates and build infrastructure for applications using the CLI, API, or UI. It is not used for refactoring applications or migrating them to containerized deployments.</p><p><strong>INCORRECT:</strong> \"Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy\" is incorrect.</p><p>Amazon AppFlow is used to automate bi-directional data flows between SaaS applications and AWS services. It is not used to migrate applications between on-premises deployments and Amazon ECS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/app2container/features/\">https://aws.amazon.com/app2container/features/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/app2container/features/"
    ]
  },
  {
    "id": 12,
    "question": "<p>A development team run a fleet of Amazon EC2 instances in a dev/test environment. A manager is concerned about the costs of the workloads. While the developers are unable to determine the exact resource requirements for each workload, they also cannot terminate any instances as all are currently in operational use.</p><p>What is the best way to ensure the most efficient use of the underlying hardware?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Detective to run statistical analysis on the log data collected from EC2 instances to determine services that are underutilized. Disable unnecessary services automatically using an AWS Lambda function.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Compute Optimizer to report on resource utilization on EC2 instances and use the recommendations to manually reconfigure resources for improved utilization.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Control Tower to inspect the utilization metrics of the workloads and use the reports to right-size the instance types to the workloads. Implement the changes using EC2 Image Builder.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS CloudWatch to collect performance metrics and create a dashboard displaying utilization metrics across the EC2 instances. Auto remediate instance types with AWS Systems Manager automation documents.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>AWS Compute Optimizer helps you identify the optimal AWS resource configurations, such as Amazon Elastic Compute Cloud (EC2) instance types, Amazon Elastic Block Store (EBS) volume configurations, task sizes of Amazon Elastic Container Service (ECS) services on AWS Fargate, and AWS Lambda function memory sizes, using machine learning to analyze historical utilization metrics.</p><p>The recommendations provided by AWS Computer Optimizer can be used by the development team to right-size the workloads which will ensure cost-efficiency.</p><p><strong>CORRECT: </strong>\"Use AWS Computer Optimizer to report on resource utilization on EC2 instances and use the recommendations to manually reconfigure resources for improved utilization\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Control Tower to inspect the utilization metrics of the workloads and use the reports to right-size the instance types to the workloads. Implement the changes using EC2 Image Builder\" is incorrect.</p><p>Control Tower is used for managing multiple AWS accounts and applying best practice baselines and guardrails. It is not used for monitoring instance utilization metrics. EC2 Image Builder is used for automating the image management processes for AMIs.</p><p><strong>INCORRECT:</strong> \"Use Amazon Detective to run statistical analysis on the log data collected from EC2 instances to determine services that are underutilized. Disable unnecessary services automatically using an AWS Lambda function\" is incorrect.</p><p>Amazon Detective is a security service that runs statistical analysis on log data using machine learning algorithms. It is used to identify security issues and does not provide performance recommendations for right-sizing workloads.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudWatch to collect performance metrics and create a dashboard displaying utilization metrics across the EC2 instances. Auto remediate instance types with AWS Systems Manager automation documents\" is incorrect.</p><p>Amazon CloudWatch is used for performance monitoring. However, it would be more efficient for the development team to leverage AWS Computer Optimizer for the purpose of right-sizing workloads as it is a tool that is designed for exactly that purpose.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/compute-optimizer/faqs/\">https://aws.amazon.com/compute-optimizer/faqs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/compute-optimizer/faqs/"
    ]
  },
  {
    "id": 13,
    "question": "<p>A company manages several legacy applications that all generate different log formats. The logs need to be standardized so they can be queried and analyzed. A DevOps engineer needs a solution for standardizing the log formats before writing them to<br>an Amazon S3 bucket.</p><p>How can this requirement be met at the LOWEST cost?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the application to send its logs to an Amazon OpenSearch cluster and use Lambda to normalize the logs and export to S3.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure the application to send its logs to Amazon QuickSight, then use the Amazon QuickSight SPICE engine to normalize the logs. Do the analysis directly from Amazon QuickSight.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure the application to send the log files to an Amazon S3 bucket and use Amazon Redshift Spectrum to normalize the logs in place.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the Amazon Kinesis Agent to upload the logs to Amazon Kinesis Data Firehose and use an AWS Lambda function to normalize the log files before they are loaded to Amazon S3.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Analytics",
    "explanation": "<p>Kinesis Data Firehose can invoke an AWS Lambda function to transform incoming source data and deliver the transformed data to destinations. You can enable Kinesis Data Firehose data transformation when you create your delivery stream.</p><p>Each server can run the Kinesis agent and upload logs to the Kinesis Data Firehose delivery stream. Then the Lambda function will normalize the log files before they are loaded to the S3 bucket.</p><p><strong>CORRECT: </strong>\"Configure the Amazon Kinesis Agent to upload the logs to Amazon Kinesis Data Firehose and use an AWS Lambda function to normalize the log files before they are loaded to Amazon S3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the application to send its logs to an Amazon OpenSearch cluster and use Lambda to normalize the logs and export to S3\" is incorrect.</p><p>The Amazon OpenSearch service is not a suitable solution for this scenario as it is a service that is used for searching and analyzing data sets. In this case the data simply needs to be transformed (normalized) before loading it to S3 so there is no need to involve OpenSearch.</p><p><strong>INCORRECT:</strong> \"Configure the application to send its logs to Amazon QuickSight, then use the Amazon QuickSight SPICE engine to normalize the logs. Do the analysis directly from Amazon QuickSight\" is incorrect.</p><p>QuickSight is used for analysis but in this scenario we simply need a solution for normalizing the log files before loading to S3.</p><p><strong>INCORRECT:</strong> \"Configure the application to send the log files to an Amazon S3 bucket and use Amazon Redshift Spectrum to normalize the logs in place\" is incorrect.</p><p>To use RedShift Spectrum you must have a RedShift cluster in place and these run-on Amazon EC2 instances. Therefore, this solution is unlikely to be the lowest cost option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html\">https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html",
      "https://digitalcloud.training/amazon-kinesis/"
    ]
  },
  {
    "id": 14,
    "question": "<p>A company uses a single AWS account and Region for development activities with multiple teams working on independent projects. A DevOps engineer must implement a mechanism to notify the operations manager when the creation of resources approaches the service limits for the AWS account.</p><p>Which solution will accomplish this with the LEAST amount of development effort?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an AWS Lambda function that refreshes AWS Personal Health Dashboard checks and use an Amazon EventBridge rule to run the Lambda function regularly. Create another EventBridge rule with an event pattern matching Trusted Advisor checks and configure an Amazon SNS topic that notifies the operations manager.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an AWS Lambda function that evaluates the deployed resources in the account and evaluates them against resource limits on the account. Create an Amazon EventBridge rule to run the Lambda function regularly and configure an Amazon SNS topic that notifies the operations manager.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an AWS Lambda function that refreshes AWS Trusted Advisor checks and use an Amazon EventBridge rule to run the Lambda function regularly. Create another EventBridge rule with an event pattern matching Trusted Advisor checks and configure an Amazon SNS topic that notifies the operations manager.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an AWS Config custom rule that runs regularly and checks the AWS service limit status and sends notifications to an Amazon SNS topic. Deploy an AWS Lambda function that notifies the operations manager and subscribe the Lambda function to the SNS topic.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>AWS Trusted Advisor provides recommendations that help you follow AWS best practices. Trusted Advisor evaluates your account by using checks. Some checks are refreshed automatically whilst others do require you to call the RefreshTrustedAdvisorCheck API action. Checks include evaluating the service limit usage by AWS service in a Region.</p><p>The solution presented here uses AWS Lambda to refresh the service limit checks and EventBridge to trigger the Lambda function execution. Then, EventBridge is used again to look for event patterns that indicate service limits have been reached and then send an SNS notification to the operations manager. This is the simplest solution for meeting these requirements.</p><p><strong>CORRECT: </strong>\"Create an AWS Lambda function that refreshes AWS Trusted Advisor checks and use an Amazon EventBridge rule to run the Lambda function regularly. Create another EventBridge rule with an event pattern matching Trusted Advisor checks and configure an Amazon SNS topic that notifies the operations manager\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that evaluates the deployed resources in the account and evaluates them against resource limits on the account. Create an Amazon EventBridge rule to run the Lambda function regularly and configure an Amazon SNS topic that notifies the operations manager\" is incorrect.</p><p>This would require complex development and is much more difficult compared to using AWS Trusted Advisor which already has all the service limit checks built in.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that refreshes AWS Personal Health Dashboard checks and use an Amazon EventBridge rule to run the Lambda function regularly. Create another EventBridge rule with an event pattern matching Trusted Advisor checks and configure an Amazon SNS topic that notifies the operations manager\" is incorrect.</p><p>Personal Health Dashboard does not monitor service limits, it is a place to learn about the availability and operations of AWS services.</p><p><strong>INCORRECT:</strong> \"Create an AWS Config custom rule that runs regularly and checks the AWS service limit status and sends notifications to an Amazon SNS topic. Deploy an AWS Lambda function that notifies the operations manager and subscribe the Lambda function to the SNS topic\" is incorrect.</p><p>This would be complex to implement and again it would be better to use AWS Trusted Advisor which already has the service limit checks built in.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html\">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-trusted-advisor/\">https://digitalcloud.training/aws-trusted-advisor/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html",
      "https://digitalcloud.training/aws-trusted-advisor/"
    ]
  },
  {
    "id": 15,
    "question": "<p>An eCommerce company has operations in several countries around the world. The company runs an application in co-location facilities that uses Linux servers and a relational database running on MySQL. The application will be migrated to AWS and will include Amazon EC2 instances behind an Application Load Balancer in multiple AWS Regions. The database configuration has not yet been finalized.</p><p>A DevOps engineer has been asked to assist with determining the best solution for the database. The data includes product catalog information which must be served with low latency and customer purchase information which should be kept within each Region for compliance purposes.</p><p>Which database solution should the DevOps engineer recommend to meet these requirements with the LEAST changes to the application?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer purchase data.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer purchase data.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer purchase data.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer purchase data.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>The current database runs on MySQL which is a relational database. Therefore, to meet the requirement to minimize changes to the application the cloud solution for the database should also be a relational database. Otherwise significant changes to the application may be required. This rules out all answers that include DynamoDB as that service is a non-relational database.</p><p>Amazon Aurora provides read replicas for scaling read performance horizontally. These replicas can be within a Region or across Regions. The product catalog data can be provided at low latency within each AWS Region using cross-Region read replicas.</p><p>For the customer purchase data, this can be kept within each Region by implementing local Aurora database instances which can additionally have read replicas within the Region if additional read performance is required.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_22-38-09-545c686c23e14af309338aad4a3b523c.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_22-38-09-545c686c23e14af309338aad4a3b523c.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer purchase data\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer purchase data\" is incorrect.</p><p>RedShift is used for online analytics processing (OLAP) use cases as it is a data warehouse solution. In this case the solution calls for an online transaction processing (OLTP) type of database as it is processing transactions.</p><p><strong>INCORRECT:</strong> \"Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer purchase data\" is incorrect.</p><p>DynamoDB is a non-relational database, and the application is designed to work with a relational database. Using DynamoDB would require significant changes to the application.</p><p><strong>INCORRECT:</strong> \"Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer purchase data\" is incorrect.</p><p>As above, DynamoDB should not be used for this solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html",
      "https://digitalcloud.training/amazon-aurora/"
    ]
  },
  {
    "id": 16,
    "question": "<p>A DevOps engineer must deploy a serverless website on AWS. The website will host content that includes HTML files, images, videos, and JavaScript (client-side scripts). The website must be optimized for global performance and be protected against web exploits.</p><p>Which actions should the engineer take?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>You can use Amazon S3 to host a static website. On a <em>static</em> website, individual webpages include static content. They might also contain client-side scripts.</p><p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.</p><p>To get content closer to users for better performance you can also use Amazon CloudFront in front of the S3 static website.</p><p>AWS Web Application Firewall (WAF) can be used to protect the static website from web exploits. To do this, the engineer should create an AWS WAF web ACL.</p><p><strong>CORRECT: </strong>\"Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield\" is incorrect.</p><p>Both EC2 and ElastiCache use EC2 instances so they are not serverless services. AWS Shield protects against DDoS attacks but AWS WAF should be used for web exploits.</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL\" is incorrect.</p><p>ECS with Fargate is serverless but you cannot use a Redis cluster to cache content served by a task running on Amazon ECS. ElastiCache is used for caching database content.</p><p><strong>INCORRECT:</strong> \"Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty\" is incorrect.</p><p>Lambda with a REST API as the frontend is definitely possible, however the static website on S3 would be easier to deploy and manage. AWS GuardDuty does not protect against web exploits.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/",
      "https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html",
      "https://digitalcloud.training/amazon-cloudfront/",
      "https://digitalcloud.training/aws-waf-shield/"
    ]
  },
  {
    "id": 17,
    "question": "<p>A critical production application running on AWS uses automatic scaling. The operations team must run updates on the application that affect only one instance at a time. The deployment process must ensure all remaining instances continue to serve traffic. The deployment must roll back if the update causes the CPU utilization of the updated instance to exceed 85%.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure AWS Elastic Beanstalk with a load balancer and use AWS Auto Scaling. Create an alarm based on the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure AWS CodeDeploy with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Use the CodeDeployDefault.OneAtAtime configuration for deployment. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>You can monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms. Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS.</p><p>Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period. You can monitor metrics such as instance CPU utilization.</p><p>If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover).</p><p>For this scenario the deployment can use the CodeDeployDefault.OneAtAtime strategy which ensures that only one instance will be taken out of action at a time, and automatic rollbacks if the alarm threshold is exceeded. This meets all requirements.</p><p><strong>CORRECT: </strong>\"Configure AWS CodeDeploy with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Use the CodeDeployDefault.OneAtAtime configuration for deployment. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group\" is incorrect.</p><p>CodeDeploy is a much better solution as it is designed for this purpose. This answer pieces together several services in a more complex solution.</p><p><strong>INCORRECT:</strong> \"Configure AWS Elastic Beanstalk with a load balancer and use AWS Auto Scaling. Create an alarm based on the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded\" is incorrect.</p><p>Elastic Beanstalk uses ELB health checks (basic health), or with enhanced health it monitors application logs and the state of your environment's other resources. It does not use CloudWatch alarms.</p><p><strong>INCORRECT:</strong> \"Configure AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded\" is incorrect.</p><p>Systems Manager cannot perform blue/green updates though it can be used with other AWS Developer Tools as part of a solution for deploying updates in this manner.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html",
      "https://digitalcloud.training/aws-developer-tools/"
    ]
  },
  {
    "id": 18,
    "question": "<p>A law firm is planning to migrate existing applications to AWS. These applications are hosted in an on-premises data center and are complex in nature. The applications could take many months to migrate. While the migration is underway, the application development team implemented a tactical solution using Amazon CloudFront with a custom origin pointing to the SSL endpoint URL for the legacy web application.</p><p>The ad-hoc solution worked for several weeks; however, all browser connections recently began showing an HTTP 502 Bad Gateway error with the header \"X-Cache: Error from CloudFront\". Network monitoring services show that the HTTPS port 443 on the legacy web application is open and responding to requests.</p><p>Which option could be the reason for the error and how can it be solved?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The SSL certificate on the legacy web application server has expired. Install a new self-signed certificate along with the full certificate chain onto the legacy web application server.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The SSL certificate on the legacy web application server has expired. Reissue the SSL certificate on the web server via the AWS Certificate Manager (ACM) in the hosting region.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The SSL certificate on the CloudFront distribution has expired. Reissue the SSL certificate on the CloudFront distribution via the AWS Certificate Manager (ACM) in the hosting region.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The SSL certificate on the legacy web application server has expired. Reissue the SSL certificate on the web server that is signed by a globally recognized certificate authority (CA). Install the full certificate chain onto the legacy web application server.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-04-44-3acbb19734df50fc3c67eb1acc619f2b.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-04-44-3acbb19734df50fc3c67eb1acc619f2b.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"The SSL certificate on the legacy web application server has expired. Reissue the SSL certificate on the web server that is signed by a globally recognized certificate authority (CA). Install the full certificate chain onto the legacy web application server\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The SSL certificate on the legacy web application server has expired. Reissue the SSL certificate on the web server via the AWS Certificate Manager (ACM) in the hosting region\" is incorrect.</p><p>You can't export an Amazon Issued ACM public certificate for use on an EC2 instance or another custom web server because ACM manages the private key.</p><p><strong>INCORRECT:</strong> \"The SSL certificate on the CloudFront distribution has expired. Reissue the SSL certificate on the CloudFront distribution via the AWS Certificate Manager (ACM) in the hosting region\" is incorrect.</p><p>If you’re using certificates that you get from a third-party certificate authority (CA), you must monitor certificate expiration dates and renew the certificates that you import into AWS Certificate Manager (ACM) or upload to the AWS Identity and Access Management certificate store before they expire.</p><p><strong>INCORRECT</strong>: \"The SSL certificate on the legacy web application server has expired. Install a new self-signed certificate along with the full certificate chain onto the legacy web application server\" is incorrect.</p><p>A self-signed certificate is a security certificate that is not signed by a certificate authority (CA). You can't use a self-signed certificate for HTTPS communication between CloudFront and your origin, so this option is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html#https-requirements-cert-expiration\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html#https-requirements-cert-expiration</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html#https-requirements-cert-expiration",
      "https://digitalcloud.training/amazon-cloudfront/"
    ]
  },
  {
    "id": 19,
    "question": "<p>A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team has prohibited unauthenticated requests to S3 buckets for this project.</p><p>How can this issue be resolved in the <strong>MOST secure</strong> manner?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>Removing unauthenticated access from the bucket through a bucket policy ensures that the S3 bucket is protected from unauthorized access. Granting the CodeBuild project's service role explicit permissions to access the S3 bucket aligns with the principle of least privilege. Using the AWS CLI within the build spec ensures secure and seamless access to the bucket.<br><br></p><p><strong>CORRECT: </strong>\"Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script\" is incorrect.</p><p>Adding the bucket name to the AllowedBuckets section does not address the security issue. The CodeBuild project needs proper IAM permissions to access the S3 bucket securely. Relying on unauthenticated access remains insecure and against the security team's requirements.</p><p><strong>INCORRECT:</strong> \"Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script\" is incorrect.</p><p>Enabling HTTPS basic authentication for the S3 bucket is not supported by AWS. Amazon S3 uses IAM policies and bucket policies for access control, not basic authentication. Additionally, using cURL to pass tokens does not align with AWS's recommended best practices.</p><p><strong>INCORRECT:</strong> \"Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key\" is incorrect.</p><p>Using IAM access keys and secret access keys directly is less secure than granting a CodeBuild service role the necessary permissions. Hardcoding access keys in the build spec or environment can lead to security vulnerabilities, such as accidental exposure of credentials.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html"
    ]
  },
  {
    "id": 20,
    "question": "<p>A DevOps engineer is deploying a three-tier application on AWS that includes an Application Load Balancer in front of the Amazon ECS web tier, an Amazon ECS application tier, and an Amazon RDS database. The load balancer should only distribute traffic to the web tier if the web tier can successfully communicate with the application and database tiers.</p><p>How can this validation be implemented?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Register the ECS tasks and RDS database instances in the target group for the load balancer. Create health checks that test the liveness of the ECS tasks and database instances.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Route 53 health checks to detect issues with the web service and use the Application Auto Scaling service to automatically terminate unhealthy ECS and RDS instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a health check endpoint in the web application that tests connectivity to the application and database tiers. Configure the endpoint as the health check URL for the target group.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use an Amazon RDS active connection count and an Amazon CloudWatch ELB metric to alarm on a significant change to the number of open connections. Configure the load balancer to consider targets unhealthy based on the alarm status.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>The requirement is that the web tier must be able to communicate with the application and database tiers and this should be validated by the load balancer. You can configure health checks in the load balancer for registered instances/tasks. However, that would not check each tier and the database.</p><p>A solution is to create an endpoint within the web tier that performs a connectivity/liveness check on the application and database tiers. Then, the load balancer simply needs to be configured with this URL as the endpoint for health checks for the target group associated with the web tier.</p><p>If the connectivity exists the health check returns a 200 (success) status code, and the load balancer considers the instance/task as being healthy.</p><p><strong>CORRECT: </strong>\"Create a health check endpoint in the web application that tests connectivity to the application and database tiers. Configure the endpoint as the health check URL for the target group\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an Amazon RDS active connection count and an Amazon CloudWatch ELB metric to alarm on a significant change to the number of open connections. Configure the load balancer to consider targets unhealthy based on the alarm status\" is incorrect.</p><p>This is not a reliable way to test the connectivity between the tiers and you cannot configure the load balancer to consider targets unhealthy based on CloudWatch alarm status.</p><p><strong>INCORRECT:</strong> \"Use Amazon Route 53 health checks to detect issues with the web service and use the Application Auto Scaling service to automatically terminate unhealthy ECS and RDS instances\" is incorrect.</p><p>You cannot use Application Auto Scaling to terminate RDS instances and you wouldn’t want to do that. The solution does not call for terminating only validating connectivity.</p><p><strong>INCORRECT:</strong> \"Register the ECS tasks and RDS database instances in the target group for the load balancer. Create health checks that test the liveness of the ECS tasks and database instances\" is incorrect.</p><p>You cannot register RDS database instances in a target group.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html",
      "https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/"
    ]
  },
  {
    "id": 21,
    "question": "<p>A DevOps engineer launched an Amazon EC2 instance in an Amazon VPC. The instance must download an object from a restricted Amazon S3 bucket. When trying to download the object, a 403 Access Denied error was received.</p><p>What are two possible causes for this error? (Select TWO.)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>S3 versioning is enabled on the S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>There is an issue with the IAM role configuration.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Default encryption is enabled on the S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The object has been moved to Amazon Glacier.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>The bucket policy does not grant permission.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Storage",
    "explanation": "<p>The most likely cause of this error message is that either the S3 bucket policy or IAM role configuration has an error or simply does not grant the required permissions. The DevOps engineer should review the bucket policy or associated IAM user policies for any statements that might be denying access.</p><p>The engineer should also verify that the requests to the bucket meet any conditions in the bucket policy or IAM policies, checking for any incorrect deny statements, missing actions, or incorrect spacing in the policy.</p><p><strong>CORRECT: </strong>\"The bucket policy does not grant permission\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"There is an issue with the IAM role configuration\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Default encryption is enabled on the S3 bucket\" is incorrect.</p><p>This would not affect access to the object as the object would be automatically decrypted by S3.</p><p><strong>INCORRECT:</strong> \"The object has been moved to Amazon Glacier\" is incorrect.</p><p>With Glacier an AccessDeniedException error would be received if permission is not granted.</p><p><strong>INCORRECT:</strong> \"S3 versioning is enabled on the S3 bucket\" is incorrect.</p><p>Versioning does not affect access permissions to an object.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/",
      "https://digitalcloud.training/amazon-s3-and-glacier/"
    ]
  },
  {
    "id": 22,
    "question": "<p>A DevOps engineer is building a web application that will use federated access to a SAML identity provider (IdP). The web application requires sign up and sign in functionality using a custom webpage with authenticated access to AWS services.</p><p>Which steps should the DevOps engineer take to implement the authentication and access control solution for the web application?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Cognito and create a user pool for federated sign-in. Add a SAML IdP and enter identifiers to map the sign-in email addresses to the relevant provider. Generate access tokens and exchange them for temporary security credentials providing access to the appropriate AWS services.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Directory Service to create a Simple AD and configure a trust relationship with the SAML IdP. Create a custom webpage on an Amazon S3 static website and use Amazon CloudFront for sign up and sign in functionality. Use role-based access control to access AWS services through the web application.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Cognito and create an identity pool for federated sign-in. Configure the SAML IdP to add a relying party trust between the IdP and AWS. Use the AssumeRoleWithWebIdentity API to call the AWS Security Token Service (STS) and generate temporary security credentials providing access to the appropriate AWS services.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use an Amazon API Gateway REST API to provide a web endpoint for the application. Use an AWS Lambda authorizer to control access to the API with a bearer token authentication strategy utilizing the SAML IdP. Authorize access to the backend services using identity-based policies.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>The DevOps engineer should use Amazon Cognito with a user pool to create a custom webpage offering sign in and sign up functionality. The user pool can be integrated with a SAML IdP for federated access.</p><p>The process for configuring this federation is as follows:</p><ol><li><p>Create or select a user pool.</p></li><li><p>Choose the Sign-in experience tab. Locate Federated sign-in and select Add an identity provider.</p></li><li><p>Choose a SAML social identity provider.</p></li><li><p>Enter <strong>Identifiers</strong> separated by commas. An identifier tells Amazon Cognito it should check the email address a user enters when they sign in, and then direct them to the provider that corresponds to their domain.</p></li></ol><p>When a user signs into the app, Amazon Cognito verifies the login information. If the login is successful, Amazon Cognito creates a session and returns an ID, access, and refresh token for the authenticated user. The tokens can be used to grant users access to server-side resources or to the Amazon API Gateway. Or they can be exchanged for temporary AWS credentials to access other AWS services.</p><p><strong>CORRECT: </strong>\"Use Amazon Cognito and create a user pool for federated sign-in. Add a SAML IdP and enter identifiers to map the sign-in email addresses to the relevant provider. Generate access tokens and exchange them for temporary security credentials providing access to the appropriate AWS services\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Cognito and create an identity pool for federated sign-in. Configure the SAML IdP to add a relying party trust between the IdP and AWS. Use the AssumeRoleWithWebIdentity API to call the AWS Security Token Service (STS) and generate temporary security credentials providing access to the appropriate AWS services\" is incorrect.</p><p>Though you can use an identity pool for federated sign in you should use a user pool for creating a custom webpage offering sign in and sign up functionality. Also the API action provided is not suitable for SAML – AssumeRoleWith SAML should be used instead.</p><p><strong>INCORRECT:</strong> \"Use an Amazon API Gateway REST API to provide a web endpoint for the application. Use an AWS Lambda authorizer to control access to the API with a bearer token authentication strategy utilizing the SAML IdP. Authorize access to the backend services using identity-based policies\" is incorrect.</p><p>API Gateway is not a suitable web endpoint for providing a custom webpage with the required sign in and sign up functionality.</p><p><strong>INCORRECT:</strong> \"Use AWS Directory Service to create a Simple AD and configure a trust relationship with the SAML IdP. Create a custom webpage on an Amazon S3 static website and use Amazon CloudFront for sign up and sign in functionality. Use role-based access control to access AWS services through the web application\" is incorrect.</p><p>You cannot configure trust relationships with any other directory when using Simple AD. Amazon CloudFront also does not offer sign up and sign in functionality.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/\">https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-configuring-federation-with-saml-2-0-idp.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-configuring-federation-with-saml-2-0-idp.html</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-configuring-federation-with-saml-2-0-idp.html"
    ]
  },
  {
    "id": 23,
    "question": "<p>An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). Users on the internet have reported performance issues with the application. A DevOps engineer must investigate the reports and identify the processing latencies for requests.</p><p>How can the engineer obtain this information?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable access logs on the load balancer.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Install the Amazon CloudWatch agent.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Review Amazon CloudWatch metrics.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS CIoudTrail to identify API requests.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.</p><p>The access logs include the following entries (relating to processing latency):</p><p>· Request_processing_time - The total time elapsed from the time the load balancer received the request until the time it sent the request to a target.</p><p>· Target_processing_time - The total time elapsed from the time the load balancer sent the request to a target until the target started to send the response headers.</p><p>· Response_processing_time - The total time elapsed from the time the load balancer received the response header from the target until it started to send the response to the client.</p><p><strong>CORRECT: </strong>\"Enable access logs on the load balancer\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CIoudTrail to identify API requests\" is incorrect.</p><p>API requests do not include any application metrics.</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch agent\" is incorrect.</p><p>The CloudWatch agent does capture additional metrics, but this does not include application processing times.</p><p><strong>INCORRECT:</strong> \"Review Amazon CloudWatch metrics\" is incorrect.</p><p>CloudWatch does not capture this information in standard metrics. You would need to create custom metrics.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html#access-log-entry-format\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html#access-log-entry-format</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html#access-log-entry-format",
      "https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/"
    ]
  },
  {
    "id": 24,
    "question": "<p>A company runs many different workloads across hundreds of Amazon EC2 instances. The DevOps team requires that all instances have standard configurations. These configurations include standard logging, metrics, security assessments, and weekly patching.</p><p>Which combination of actions meets these requirements with the most operational efficiency? (Select TWO.)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Systems Manager to install and manage Amazon Inspector, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Inspector to install and manage AWS Systems Manager, AWS OpsWorks, and the Amazon CloudWatch agent on all instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS CloudFormation with change sets to deploy AMIs patched by Systems Manager Patch Manager to existing EC2 instances. Use AWS Config to require regular Amazon Inspector assessment runs.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use Amazon EventBridge to schedule Amazon Inspector assessment runs.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Use AWS OpsWorks and execute custom recipes to patch the EC2 instances. Use AWS Systems Manager Run Command to initiate regular Amazon Inspector assessment runs.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Management & Governance",
    "explanation": "<p>The most operationally efficient solution is to use AWS Systems Manager patch manager for patching and Amazon Inspector for assessing the security status of the instances. The Amazon CloudWatch agent can also be installed on the instances to get enhanced metrics and logging.</p><p>Systems Manager can be used to install and manage the other components. You must have the Systems Manager agent installed on the instances and the instance profile attached must have permissions to Systems Manager.</p><p>The patching process can be implemented by using Systems Manager Run Command to schedule tasks that deploy the updates using Systems Manager Patch Manager. EventBridge is ideal for scheduling the Amazon Inspector assessment runs.</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager to install and manage Amazon Inspector, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use Amazon EventBridge to schedule Amazon Inspector assessment runs\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Inspector to install and manage AWS Systems Manager, AWS OpsWorks, and the Amazon CloudWatch agent on all instances\" is incorrect.</p><p>Inspector is not a service that can be used to install anything, it simply runs assessments against your instances and lets you know if there are any security concerns.</p><p><strong>INCORRECT:</strong> \"Use AWS OpsWorks and execute custom recipes to patch the EC2 instances. Use AWS Systems Manager Run Command to initiate regular Amazon Inspector assessment runs\" is incorrect.</p><p>OpsWorks would be less operationally efficient for this purpose. Systems Manager Patch Manager is a better way to deploy patches as it is designed for this purpose and provides many features to simplify and optimize the patching process.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation with change sets to deploy AMIs patched by Systems Manager Patch Manager to existing EC2 instances. Use AWS Config to require regular Amazon Inspector assessment runs\" is incorrect.</p><p>You cannot deploy patched AMIs to existing instances. AMIs are used to deploy the instance initially but you cannot redeploy the AMI without wiping the instance state. Config also cannot be used to enforce an Amazon Inspector assessment run.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p><p><a href=\"https://digitalcloud.training/amazon-inspector/\">https://digitalcloud.training/amazon-inspector/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html",
      "https://aws.amazon.com/inspector/",
      "https://digitalcloud.training/aws-systems-manager/",
      "https://digitalcloud.training/amazon-inspector/"
    ]
  },
  {
    "id": 25,
    "question": "<p>A DevOps engineer manages an application that stores logs in Amazon CloudWatch Logs. The engineer needs to archive the logs in an Amazon S3 bucket. The log files are rarely accessed after 90 days and for compliance reasons must be retained for 10 years. The solution should run in an automated fashion.</p><p>Which combination of steps should the DevOps engineer take to meet the requirements? (Select TWO.)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a CloudWatch Logs export to save the log files to a logging bucket and then import all logs to the destination S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a CloudWatch Logs subscription filter that uses AWS DataSync to synchronize all logs to an S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an S3 bucket lifecycle policy that transitions the log files to Reduced Redundancy after 90 days and expires the log files after 3,650 days.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an S3 bucket lifecycle policy that transitions the log files to S3 Glacier after 90 days and expires the log files after 3,650 days.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create a CloudWatch Logs subscription filter that uses Amazon Kinesis Data Firehose to stream all logs to an S3 bucket.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Storage",
    "explanation": "<p>The best solution to meet these requirements is to use an CloudWatch Logs subscription filter to automatically stream the log files via Amazon Kinesis Data Firehose to the S3 bucket. You can use a subscription filter with Kinesis, Lambda, or Kinesis Data Firehose.</p><p>The S3 bucket can be configured with a bucket lifecycle policy that transitions the log files to S3 Glacier after 90 days and then expires the log files after 10 years (3,650 days). Glacier is the most cost-effective option for storing objects that are rarely accessed.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Logs subscription filter that uses Amazon Kinesis Data Firehose to stream all logs to an S3 bucket\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an S3 bucket lifecycle policy that transitions the log files to S3 Glacier after 90 days and expires the log files after 3,650 days\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch Logs subscription filter that uses AWS DataSync to synchronize all logs to an S3 bucket\" is incorrect.</p><p>You cannot use DataSync with a CloudWatch Logs subscription filter. You can use a subscription filter with Kinesis, Lambda, or Kinesis Data Firehose.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch Logs export to save the log files to a logging bucket and then import all logs to the destination S3 bucket\" is incorrect.</p><p>This is not an automated solution, and the logging bucket is not required in this case.</p><p><strong>INCORRECT:</strong> \"Create an S3 bucket lifecycle policy that transitions the log files to Reduced Redundancy after 90 days and expires the log files after 3,650 days\" is incorrect.</p><p>The most cost-effective solution for rarely accessed files is Glacier.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html",
      "https://digitalcloud.training/amazon-cloudwatch/"
    ]
  }
]