[
  {
    "id": 1,
    "question": "<p>An e-commerce company has deployed its application on Amazon EC2 instances and uses Amazon RDS for the backend database. The EC2 instances and RDS instance are in the same VPC. The company wants to enforce security best practices to ensure that only the EC2 instances can communicate with the RDS instance on the database port, and no external database connections are allowed.</p><p>Which of the following security group rules should be configured to meet the above security requirement? (Select TWO.)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Assign a security group to the RDS instance that has an inbound rule allowing all IP addresses (0.0.0.0/0) to access the database port for easy management.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Establish a VPC peering connection between the VPC of the EC2 instances and the RDS instance, and configure the relevant security group rules to allow traffic on the database port.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an inbound security group rule for the RDS security group that allows traffic on the database port from the public IP addresses of the EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set an inbound security group rule for the RDS security group that allows traffic on the database port from the associated EC2 instances’ security group.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Configure an outbound security group rule for the EC2 instances’ security group that allows traffic to the RDS instance’s security group on the database port.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>Configuring an outbound security group rule for the EC2 instances’ security group allows the application servers to initiate communication with the RDS instance on the specified database port.</p><p>Setting an inbound security group rule for the RDS security group to allow traffic from the EC2 instances’ security group ensures that only the application servers can access the database, adhering to the principle of least privilege.</p><p><strong>CORRECT: </strong>\"Configure an outbound security group rule for the EC2 instances’ security group that allows traffic to the RDS instance’s security group on the database port\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Set an inbound security group rule for the RDS security group that allows traffic on the database port from the associated EC2 instances’ security group\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an inbound security group rule for the RDS security group that allows traffic on the database port from the public IP addresses of the EC2 instances\" is incorrect.</p><p>Allowing traffic based on public IP addresses is not secure as EC2 instances typically have dynamic IP addresses unless allocated with Elastic IP. Also, it unnecessarily exposes the database to potential external access.</p><p><strong>INCORRECT:</strong> \"Assign a security group to the RDS instance that has an inbound rule allowing all IP addresses (0.0.0.0/0) to access the database port for easy management\" is incorrect.</p><p>Allowing all IP addresses to access the RDS instance is against security best practices and would expose the database to the internet, which is not recommended.</p><p><strong>INCORRECT:</strong> \"Establish a VPC peering connection between the VPC of the EC2 instances and the RDS instance, and configure the relevant security group rules to allow traffic on the database port\" is incorrect.</p><p>VPC peering is not necessary within the same VPC and does not directly relate to the security group configuration. It's used for connecting two separate VPCs, which is not the case here.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 2,
    "question": "<p>A financial services company needs to optimize query costs and performance for their extensive transaction logs stored in Amazon S3. The logs are currently in .csv format but are mostly queried for specific columns using Amazon Athena.</p><p>Which method should be used to store these logs in S3 to improve Athena query efficiency and reduce costs?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Replicate .csv logs to Amazon DynamoDB for querying with Amazon Athena.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store .csv logs in Amazon RDS and query with Athena using a JDBC connection.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Convert .csv files to Apache Parquet format using AWS Glue and store in S3 for efficient columnar querying.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Glue to transform .csv logs into XML format for optimized Amazon Athena querying.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>Apache Parquet is a columnar storage file format optimized for analytics querying, particularly with Amazon Athena.</p><p>It allows efficient storage and retrieval by enabling queries to scan only the necessary columns, rather than entire rows.</p><p>This leads to faster query performance and reduced costs, especially for large datasets where only a subset of columns is frequently accessed. A</p><p>WS Glue can be used to automate the conversion of .csv files to Parquet and store them in S3, making this process efficient and scalable.</p><p><strong>CORRECT: </strong>\"Convert .csv files to Apache Parquet format using AWS Glue and store in S3 for efficient columnar querying\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to transform .csv logs into XML format for optimized Amazon Athena querying\" is incorrect.</p><p>XML is not a columnar format and is generally less efficient for query performance compared to Parquet, especially in analytics scenarios. It typically results in larger file sizes and higher query costs.</p><p><strong>INCORRECT:</strong> \"Store .csv logs in Amazon RDS and query with Athena using a JDBC connection\" is incorrect.</p><p>Amazon Athena does not query data directly from Amazon RDS using JDBC. Athena is designed to query data in S3, and RDS is a relational database service, not a data storage solution for Athena.</p><p><strong>INCORRECT:</strong> \"Replicate .csv logs to Amazon DynamoDB for querying with Amazon Athena\" is incorrect.</p><p>Amazon DynamoDB is a NoSQL database service and is not typically used in conjunction with Athena, which is designed for querying data stored in Amazon S3. This approach would add unnecessary complexity and would not leverage Athena's strengths in querying large datasets in S3.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html\">https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 3,
    "question": "<p>A digital marketing firm needs to analyze clickstream data in real-time. The data is streamed through Amazon Kinesis Data Streams and must be enriched by cross-referencing with a customer database hosted on an external API before being stored in Amazon S3 for future analysis.</p><p>Which combination of AWS services should be used to process and enrich the streaming data efficiently?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Lambda to process and enrich data from Kinesis Data Streams, then store it in Amazon S3.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon EMR to consume the Kinesis stream, apply transformations, and write to S3.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Glue to extract data from Kinesis, enrich it, and load the processed data into S3.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon EC2 instances to pull data from Kinesis, enrich it, and then upload to S3.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>AWS Lambda is an effective serverless computing service that can be directly triggered by Amazon Kinesis Data Streams. It is ideal for real-time data processing tasks such as enriching streaming data by making API calls to external services. After enrichment, Lambda can then seamlessly store the data in Amazon S3.</p><p>This setup provides a scalable, efficient, and cost-effective solution for processing streaming data without the need to manage underlying infrastructure</p><p><strong>CORRECT: </strong>\"Use AWS Lambda to process and enrich data from Kinesis Data Streams, then store it in Amazon S3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 instances to pull data from Kinesis, enrich it, and then upload to S3\" is incorrect.</p><p>Using EC2 instances would require manual setup and scaling based on the data load, making it less efficient than a serverless approach. It involves additional operational overhead in terms of managing and scaling the instances.</p><p><strong>INCORRECT:</strong> \"Use Amazon EMR to consume the Kinesis stream, apply transformations, and write to S3\" is incorrect.</p><p>Amazon EMR is a powerful tool for big data processing, but it is more complex and resource-intensive compared to Lambda for real-time data enrichment tasks. It is more suitable for large-scale batch processing rather than real-time streaming data processing.</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to extract data from Kinesis, enrich it, and load the processed data into S3\" is incorrect.</p><p>AWS Glue is primarily an ETL service for batch processing and may not be the optimal choice for real-time data processing and enrichment from a Kinesis stream. It is generally used for scenarios where data is processed in batches rather than in a streaming manner.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 4,
    "question": "<p>A retail company receives a daily .xls file with customer data, which is uploaded to Amazon S3. The file size is about 2 GB. A data engineer is tasked with combining the customer first name and last name fields and then identifying the total number of unique customer entries in the file.</p><p>Which AWS service or feature should the data engineer use to determine the count of distinct customers with minimal operational effort?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Athena to run a SQL query that concatenates the names and counts distinct entries after converting the file to CSV format.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Utilize AWS Lambda with Amazon S3 trigger to process the file and calculate the number of distinct customer names.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up an AWS Glue DataBrew job to concatenate the fields and use the COUNT_DISTINCT function to find the number of unique customers.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Employ Amazon S3 Select to perform the concatenation and distinct count directly on the S3 file.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>AWS Glue DataBrew is a visual data preparation tool that allows data engineers to clean and normalize data without writing code.</p><p>DataBrew can directly access data stored in S3, and with its recipe feature, the data engineer can easily create and run transformations such as concatenating columns and calculating distinct values.</p><p>This approach does not require provisioning or managing servers, making it a low-effort solution for the given task.</p><p><strong>CORRECT: </strong>\"Set up an AWS Glue DataBrew job to concatenate the fields and use the COUNT_DISTINCT function to find the number of unique customers\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Utilize AWS Lambda with Amazon S3 trigger to process the file and calculate the number of distinct customer names\" is incorrect.</p><p>AWS Lambda can process files stored in S3, but the service has a payload limit that might not be suitable for a 2 GB file. Additionally, Lambda would require custom code to handle the .xls format, which increases operational effort.</p><p><strong>INCORRECT:</strong> \"Use Amazon Athena to run a SQL query that concatenates the names and counts distinct entries after converting the file to CSV format\" is incorrect.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. However, it does not natively support .xls files, so the file would first need to be converted to a supported format like CSV, adding extra steps to the process.</p><p><strong>INCORRECT:</strong> \"Employ Amazon S3 Select to perform the concatenation and distinct count directly on the S3 file\" is incorrect.</p><p>S3 Select allows retrieval of subsets of data from a file in S3, but it has limitations in terms of the size of the file and the complexity of the operations it can perform. It also doesn't support .xls files directly, which would necessitate a format conversion.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/glue/features/databrew/\">https://aws.amazon.com/glue/features/databrew/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/glue/features/databrew/",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 5,
    "question": "<p>An analytics team frequently runs complex Amazon Athena queries on a dataset stored in an Amazon S3 bucket, with AWS Glue Data Catalog serving as the metadata repository. They've identified a slowdown in Athena query plans and attribute it to the excessive number of S3 partitions. The team needs to enhance query performance and reduce the time Athena takes in query planning.</p><p>Which two measures should the team take to meet these performance goals? (Select TWO.)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Merge small S3 objects into larger ones using AWS Glue's data transformation jobs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Employ Athena's partition projection for dynamic partitioning based on common query patterns.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Athena DML statements to reorganize data into fewer, more meaningful partitions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Transform the data that is in the S3 bucket to Apache Parquet format.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Implement AWS Glue Elastic Views to automate partition management.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>By converting to Parquet, the data engineer can reduce the number of bytes that Athena needs to read, which can significantly impact the performance of queries, especially those that do not need to scan entire tables.</p><p>Athena's partition projection can help manage many partitions by projecting them into the query results as if they were there, without the need to perform operations on the actual metadata in the Glue Data Catalog. This also speeds up query execution time because Athena spends less time reading the partition metadata.</p><p><strong>CORRECT: </strong>\"Transform the data that is in the S3 bucket to Apache Parquet format\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Employ Athena's partition projection for dynamic partitioning based on common query patterns\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement AWS Glue Elastic Views to automate partition management\" is incorrect.</p><p>AWS Glue Elastic Views is a service that combines and replicates data across multiple data stores, not directly related to Athena query performance or partition management.</p><p><strong>INCORRECT:</strong> \"Use Athena DML statements to reorganize data into fewer, more meaningful partitions\" is incorrect.</p><p>While Athena supports DML statements to manage data, it does not inherently reorganize data into fewer partitions, which is necessary to address the performance bottleneck described.</p><p><strong>INCORRECT:</strong> \"Merge small S3 objects into larger ones using AWS Glue's data transformation jobs\" is incorrect.</p><p>AWS Glue data transformation jobs can reshape and combine S3 objects, but simply merging files without addressing partitioning strategy might not resolve the performance issues in Athena query planning.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html\">https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html</a></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/partition-projection.html\">https://docs.aws.amazon.com/athena/latest/ug/partition-projection.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/columnar-storage.html",
      "https://docs.aws.amazon.com/athena/latest/ug/partition-projection.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 6,
    "question": "<p>A business analytics team is tasked with configuring a provisioned Amazon EMR cluster optimized for executing Apache Spark jobs to analyze extensive datasets. The team must ensure that the cluster operates cost-effectively without compromising on performance or reliability.</p><p>To achieve this, what two recommendations should the team follow to configure their Amazon EMR resources? (Select TWO.)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement EMR Managed Scaling to automatically resize the cluster based on workload.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Choose Spot Instances for task nodes to take advantage of lower prices for flexible workloads.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Graviton instances for core and task nodes to leverage better price-performance.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Select Reserved Instances for core nodes to reduce costs with long-term commitment.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Configure Amazon EMR to use Amazon S3 as a data lake for durable and cost-effective storage.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Using Amazon S3 as a data lake for Amazon EMR is a common best practice because S3 provides highly durable storage at a lower cost compared to HDFS on EMR. S3 also decouples storage from compute, allowing teams to shut down EMR clusters when not in use and avoid paying for persistent HDFS on EMR nodes, thus optimizing costs.</p><p>AWS Graviton instances, which are powered by Arm-based processors, offer a better price-performance ratio compared to traditional x86-based instances. They are designed to deliver cost savings and are optimized for performance, making them a good choice for running cost-optimized and performance-intensive big data workloads on EMR.</p><p><strong>CORRECT: </strong>\"Configure Amazon EMR to use Amazon S3 as a data lake for durable and cost-effective storage\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use Graviton instances for core and task nodes to leverage better price-performance\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement EMR Managed Scaling to automatically resize the cluster based on workload\" is incorrect.</p><p>While HDFS is the traditional storage system used by Hadoop and EMR, it requires the cluster to be continuously running, which can incur higher costs compared to using Amazon S3 for storage, where compute and storage can be scaled independently.</p><p><strong>INCORRECT:</strong> \"Select Reserved Instances for core nodes to reduce costs with long-term commitment\" is incorrect.</p><p>x86-based instances may not be the most cost-effective option compared to AWS Graviton instances when price-performance is considered, especially for the long-running and resource-intensive workloads typical in big data analysis.</p><p><strong>INCORRECT:</strong> \"Choose Spot Instances for task nodes to take advantage of lower prices for flexible workloads\" is incorrect.</p><p>Spot Instances can offer significant savings for fault-tolerant and flexible workloads. However, using Spot Instances for all primary nodes (which include Master nodes) is not recommended due to the possibility of Spot Instance termination by AWS if there is higher demand for the capacity, which can affect cluster reliability and continuity of long-running jobs. It's more common to use Spot Instances for task nodes and not for core or master nodes which are critical for the cluster's operation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-storage.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-storage.html</a></p><p><a href=\"https://aws.amazon.com/ec2/instance-types/\">https://aws.amazon.com/ec2/instance-types/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-storage.html",
      "https://aws.amazon.com/ec2/instance-types/",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 7,
    "question": "<p>A university's research department is storing large datasets on Amazon S3 for various projects. They have two distinct types of data:</p><ol><li><p>\"Project Alpha\" data is accessed frequently for the first 30 days but will be rarely accessed thereafter. However, when needed, the data must be immediately available.</p></li><li><p>\"Project Beta\" data is archival and will only be accessed for potential audits, with retrieval time being less critical.</p></li></ol><p>What combination of S3 storage classes should the research department use to store \"Project Alpha\" and \"Project Beta\" data most cost-effectively?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use S3 Intelligent-Tiering for \"Project Alpha\" data and S3 Standard-Infrequent Access for \"Project Beta\" data.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store \"Project Alpha\" data in S3 Standard and \"Project Beta\" data in S3 Glacier Deep Archive.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use S3 Standard for both \"Project Alpha\" and \"Project Beta\" data for simplicity and consistent access.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store \"Project Alpha\" data in S3 One Zone-Infrequent Access and \"Project Beta\" data in S3 Glacier.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>S3 Standard is ideal for \"Project Alpha\" data due to its high availability and immediate access, even if the data becomes infrequently accessed after the initial 30 days.</p><p>For \"Project Beta\" archival data, S3 Glacier Deep Archive offers the lowest cost storage for data that is rarely accessed, and retrieval times of several hours are acceptable in this case</p><p><strong>CORRECT: </strong>\"Store \"Project Alpha\" data in S3 Standard and \"Project Beta\" data in S3 Glacier Deep Archive\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use S3 Intelligent-Tiering for \"Project Alpha\" data and S3 Standard-Infrequent Access for \"Project Beta\" data\" is incorrect.</p><p>While S3 Intelligent-Tiering could be suitable for \"Project Alpha\" due to its automatic cost savings as access patterns change, S3 Standard-Infrequent Access is not as cost-effective as Glacier Deep Archive for archival data like \"Project Beta\" where immediate access is not necessary.</p><p><strong>INCORRECT:</strong> \"Store \"Project Alpha\" data in S3 One Zone-Infrequent Access and \"Project Beta\" data in S3 Glacier\" is incorrect.</p><p>S3 One Zone-Infrequent Access offers a lower-cost option for infrequently accessed data but does not provide the same level of availability and resilience as S3 Standard, which might be necessary for the \"Project Alpha\" data. S3 Glacier could be used for \"Project Beta\", but Glacier Deep Archive is more cost-effective for long-term archival data.</p><p><strong>INCORRECT:</strong> \"Use S3 Standard for both \"Project Alpha\" and \"Project Beta\" data for simplicity and consistent access\" is incorrect.</p><p>Using S3 Standard for both projects does not optimize costs. \"Project Beta\" data, which is archival, can be stored more cost-effectively in a lower-cost storage class designed for infrequently accessed data.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 8,
    "question": "<p>A tech company is developing a mobile application that requires a flexible data model to handle user-generated content, which varies greatly in size and structure. The application expects unpredictable traffic and needs to automatically scale to accommodate peak loads. Cost-efficiency and performance are primary concerns.</p><p>Which database solution is the most appropriate for this use case?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Aurora with its relational database features and automatic scaling to provide consistent performance under variable workloads.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A self-managed NoSQL database on Amazon EC2 instances to provide full control over the database's scalability and management.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon DynamoDB for a managed NoSQL database experience with seamless scalability and a flexible schema to handle varied user content.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon RDS with its relational database capabilities to ensure ACID transactions and enable complex queries on user data.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Amazon DynamoDB is a NoSQL database service known for its ability to handle large amounts of unstructured data. It provides seamless scalability which means it can automatically adjust to the incoming traffic and data volume, making it ideal for unpredictable traffic patterns.</p><p>DynamoDBs flexible schema allows for handling a variety of user-generated content without the need for predefined schema constraints. This makes it a highly suitable choice for the mobile application described, considering their need for cost-efficiency and performance.</p><p><strong>CORRECT: </strong>\"Amazon DynamoDB for a managed NoSQL database experience with seamless scalability and a flexible schema to handle varied user content\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon RDS with its relational database capabilities to ensure ACID transactions and enable complex queries on user data\" is incorrect.</p><p>Amazon RDS is a relational database service that is best for structured data requiring complex queries and transactions. While it offers the robustness of ACID transactions, it may not be as cost-effective or as easily scalable as DynamoDB for unpredictable workloads with a flexible schema requirement.</p><p><strong>INCORRECT:</strong> \"Amazon Aurora with its relational database features and automatic scaling to provide consistent performance under variable workloads\" is incorrect.</p><p>Amazon Aurora is a relational database that provides some of the scalability and performance of NoSQL databases but still relies on a fixed schema. While it offers automatic scaling, it doesn't inherently manage unstructured data as effectively as DynamoDB.</p><p><strong>INCORRECT:</strong> \"A self-managed NoSQL database on Amazon EC2 instances to provide full control over the database's scalability and management\" is incorrect.</p><p>Managing a NoSQL database on Amazon EC2 instances gives full control over the database configuration but also adds overhead for scaling and managing the infrastructure. It is not as cost-effective or easy to manage compared to a fully managed service like DynamoDB, especially for a company that prioritizes cost-efficiency and performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 9,
    "question": "<p>An organization is creating a data lake on AWS and requires granular access control. They need to grant specific users access to certain rows and columns within their datasets. The organization's teams will query the data using a combination of Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on Amazon EMR.</p><p>Which AWS service should the organization implement to manage data permissions efficiently?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Lake Formation to define fine-grained data access policies and facilitate queries through supported AWS services.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Redshift security groups and views for row and column-level permissions, querying with Athena and Redshift Spectrum.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy Apache Ranger on Amazon EMR for granular access control and utilize Amazon Redshift for querying.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Manage access through S3 bucket policies and IAM roles for row and column-level security.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 4: Data Security and Governance",
    "explanation": "<p>AWS Lake Formation simplifies and centralizes the setup of a secure data lake in AWS. It provides granular access control to data stored in Amazon S3, allowing organizations to define who has access to specific rows and columns within their datasets.</p><p>Lake Formation integrates seamlessly with Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on Amazon EMR, providing the least operational overhead while meeting the organization's access control requirements.</p><p><strong>CORRECT: </strong>\"Use AWS Lake Formation to define fine-grained data access policies and facilitate queries through supported AWS services\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Manage access through S3 bucket policies and IAM roles for row and column-level security\" is incorrect.</p><p>While S3 bucket policies and IAM roles can provide access control, they do not offer row-level and column-level security natively. This approach would require additional management and does not integrate as seamlessly with the querying services for granular permissions as AWS Lake Formation does</p><p><strong>INCORRECT:</strong> \"Deploy Apache Ranger on Amazon EMR for granular access control and utilize Amazon Redshift for querying\" is incorrect.</p><p>Apache Ranger can provide fine-grained access control, but it is typically used within the Hadoop ecosystem and requires additional setup and management when used on Amazon EMR. This approach would also not be as integrated with Amazon Redshift for querying without further configuration, leading to a higher operational overhead compared to using AWS Lake Formation.</p><p><strong>INCORRECT:</strong> \"Use Redshift security groups and views for row and column-level permissions, querying with Athena and Redshift Spectrum\" is incorrect.</p><p>Amazon Redshift security groups and views can control access to data, but they are specific to the Redshift environment. This method would not provide the same level of granular access control across other services like Amazon Athena and would not apply to data stored in Amazon S3, which is the intended storage for the data lake. Redshift is also not typically used as the primary storage for a data lake due to cost and scalability considerations compared to Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html\">https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 10,
    "question": "<p>An e-commerce company is shifting their extensive product catalog data, historically managed in an on-premises relational database, to the AWS cloud. The product data includes structured and semi-structured formats, with new updates and schemas introduced regularly. The company aims to analyze this data using SQL queries without setting up complex database infrastructure. They need a serverless solution that allows them to query this data directly in Amazon S3, where it will be stored.</p><p>Which AWS service should the company use to perform SQL queries on their product catalog data stored in S3, ensuring flexibility for schema changes and a serverless operational model?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure AWS Lambda functions to process and query data in S3, using Amazon DynamoDB for schema management.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Migrate the data to Amazon RDS and use Amazon Athena to perform SQL queries directly on the RDS database.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Redshift Spectrum to query the data in S3, with AWS Glue Data Catalog managing the evolving schemas.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Implement Amazon Athena, leveraging its integration with AWS Glue Data Catalog for handling schema changes and querying data in S3.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Amazon Athena is a serverless interactive query service that allows SQL querying directly on data stored in Amazon S3. Its integration with AWS Glue Data Catalog is crucial for managing evolving data schemas.</p><p>This setup enables the e-commerce company to handle schema variations in their product catalog data without the need to set up and manage a traditional database infrastructure.</p><p>Athena's serverless nature means that the company can run queries as needed without managing any compute resources, making it an efficient and cost-effective solution for their requirements.</p><p><strong>CORRECT: </strong>\"Implement Amazon Athena, leveraging its integration with AWS Glue Data Catalog for handling schema changes and querying data in S3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the data to Amazon RDS and use Amazon Athena to perform SQL queries directly on the RDS database\" is incorrect.</p><p>Athena is designed to query data in Amazon S3, not in Amazon RDS. This option does not leverage Athena's strengths and introduces unnecessary complexity by involving RDS.</p><p><strong>INCORRECT:</strong> \"Use Amazon Redshift Spectrum to query the data in S3, with AWS Glue Data Catalog managing the evolving schemas\" is incorrect.</p><p>While Amazon Redshift Spectrum allows querying data in S3 and can integrate with AWS Glue Data Catalog, it requires managing a Redshift cluster, which contradicts the requirement for a serverless solution.</p><p><strong>INCORRECT:</strong> \"Configure AWS Lambda functions to process and query data in S3, using Amazon DynamoDB for schema management\" is incorrect.</p><p>This approach would require significant custom development and does not provide a simple, serverless solution for SQL querying of data in S3. DynamoDB is not typically used for schema management in this context, and managing Lambda functions adds operational complexity.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/getting-started.html\">https://docs.aws.amazon.com/athena/latest/ug/getting-started.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/getting-started.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 11,
    "question": "<p>A mobile app development company is creating an application that allows users to upload and retrieve photos. The photos will be stored in Amazon S3. The company needs to decide on the best approach to integrate the S3 service into their application for handling these photo uploads and retrievals.</p><p>Which approach should the company use to integrate Amazon S3 in their mobile application for uploading and retrieving photos?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Directly use the Amazon S3 API in the application code for uploading and retrieving photos.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement Amazon EC2 instances to act as intermediaries for S3 operations.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Lambda functions triggered by Amazon API Gateway to handle photo uploads and retrievals with S3.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up AWS Step Functions to manage the workflow of uploading and retrieving photos from S3.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>This approach is ideal for the mobile app development company. Using Amazon API Gateway in conjunction with AWS Lambda provides several benefits. API Gateway acts as a front door to manage all the API calls, ensuring scalability, security, and efficiency.</p><p>API Gateway allows the company to control access to their S3 operations, manage traffic, authorize requests, and handle different versions of the API.</p><p>AWS Lambda can process the upload and retrieval requests, interfacing with Amazon S3 as needed. This serverless architecture is cost-effective and scales automatically with the number of requests, making it well-suited for a mobile application handling varying loads.</p><p><strong>CORRECT: </strong>\"Use AWS Lambda functions triggered by Amazon API Gateway to handle photo uploads and retrievals with S3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Directly use the Amazon S3 API in the application code for uploading and retrieving photos\" is incorrect.</p><p>While directly using the Amazon S3 API in the application is possible, it is not the best practice for mobile applications. This method exposes the S3 service directly to the client, which could lead to security risks and less control over the traffic and operations.</p><p><strong>INCORRECT:</strong> \"Implement Amazon EC2 instances to act as intermediaries for S3 operations\" is incorrect.</p><p>Using EC2 instances as intermediaries introduces unnecessary complexity and cost. It requires maintaining and scaling EC2 instances, which is more resource-intensive compared to the serverless approach offered by AWS Lambda and API Gateway.</p><p><strong>INCORRECT:</strong> \"Set up AWS Step Functions to manage the workflow of uploading and retrieving photos from S3\" is incorrect.</p><p>AWS Step Functions is used to coordinate multiple AWS services into serverless workflows but is not typically used for direct interactions like file uploads and retrievals. For the given use case, Step Functions would add unnecessary complexity without providing significant benefits over API Gateway and Lambda.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 12,
    "question": "<p>A manufacturing company plans to migrate their on-premises data, currently stored in an SMB (Server Message Block) file share, to AWS for improved scalability and data analysis capabilities. The data migration must be efficient, secure, and minimize downtime.</p><p>Which AWS service should the company use to facilitate the transfer of their SMB file share data to the cloud?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy AWS Snowball Edge for large-scale data transfer from the SMB file share to AWS.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement AWS Storage Gateway with a File Gateway configuration to connect the SMB file share to AWS.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure AWS Direct Connect to create a dedicated network connection for data transfer from the SMB share to Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS DataSync to automate and accelerate the transfer of data from the SMB file share to Amazon S3.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>AWS DataSync is a data transfer service designed to simplify and accelerate moving large amounts of data between on-premises storage systems (like SMB file shares) and AWS storage services (such as Amazon S3).</p><p>It is an ideal choice for the company's requirements as it can handle the transfer securely, efficiently, and with minimal operational overhead. DataSync also offers features like scheduling, data validation, and bandwidth throttling, making it well-suited for a seamless migration process.</p><p><strong>CORRECT: </strong>\"Use AWS DataSync to automate and accelerate the transfer of data from the SMB file share to Amazon S3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement AWS Storage Gateway with a File Gateway configuration to connect the SMB file share to AWS\" is incorrect.</p><p>While AWS Storage Gateway's File Gateway can be used to integrate on-premises file shares with cloud storage, it is more commonly used for hybrid cloud storage scenarios rather than a complete migration. File Gateway allows on-premises applications to use cloud storage through SMB or NFS protocols.</p><p><strong>INCORRECT:</strong> \"Deploy AWS Snowball Edge for large-scale data transfer from the SMB file share to AWS\" is incorrect.</p><p>AWS Snowball Edge is a physical device used for transferring large amounts of data into and out of AWS. It is suitable for situations where network conditions are not ideal for online data transfer. However, it might not be necessary if the network bandwidth is sufficient for online transfer using DataSync.</p><p><strong>INCORRECT:</strong> \"Configure AWS Direct Connect to create a dedicated network connection for data transfer from the SMB share to Amazon S3\" is incorrect.</p><p>AWS Direct Connect provides a dedicated network connection to AWS, which can be beneficial for consistent, high-bandwidth data transfer. However, it does not by itself facilitate the data transfer process and would typically be used in conjunction with a data transfer service like DataSync.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/datasync/\">https://aws.amazon.com/datasync/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/datasync/",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 13,
    "question": "<p>A company plans to migrate its on-premises Oracle database to Amazon Aurora PostgreSQL. They need to ensure a seamless migration of both the database schema and the data with minimal downtime. The company also requires a tool to convert the source database schema to be compatible with Amazon Aurora PostgreSQL.</p><p>Which combination of AWS services should the company use to migrate the database schema and data?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Apply Amazon S3 transfer acceleration for data migration and Amazon Athena for schema conversion.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement AWS Glue for both schema conversion and data migration.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize Amazon RDS for data migration and AWS Lambda for schema conversion.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Database Migration Service (DMS) for data migration and AWS Schema Conversion Tool (SCT) for schema conversion.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>AWS DMS is an ideal service for migrating databases with minimal downtime. It supports various source and target databases, including Oracle and Amazon Aurora PostgreSQL. AWS DMS efficiently migrates data from the existing database to the target database on AWS.</p><p>For the schema conversion, the AWS Schema Conversion Tool (SCT) helps convert the source database schema to be compatible with the target database, in this case, Amazon Aurora PostgreSQL. SCT assesses the source database, automatically converts the schema to the target format, and highlights any elements that require manual conversion, ensuring a comprehensive migration process.</p><p><strong>CORRECT: </strong>\"Use AWS Database Migration Service (DMS) for data migration and AWS Schema Conversion Tool (SCT) for schema conversion\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement AWS Glue for both schema conversion and data migration\" is incorrect.</p><p>AWS Glue is primarily an ETL (extract, transform, load) service used for preparing and transforming data for analytics. It is not specifically designed for database schema conversion or for complex data migrations like DMS and SCT.</p><p><strong>INCORRECT:</strong> \"Utilize Amazon RDS for data migration and AWS Lambda for schema conversion\" is incorrect.</p><p>Amazon RDS is a database service for operation and management of databases, not for migrating data between different database platforms. AWS Lambda is a serverless computing service and is not suitable for database schema conversion tasks.</p><p><strong>INCORRECT:</strong> \"Apply Amazon S3 transfer acceleration for data migration and Amazon Athena for schema conversion\" is incorrect.</p><p>Amazon S3 transfer acceleration is intended to speed up the transfer of files to S3, not for database migrations. Amazon Athena is a query service used to analyze data in Amazon S3 using SQL and is not designed for schema conversion or database migrations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_DMSIntegration.html\">https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_DMSIntegration.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_DMSIntegration.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 14,
    "question": "<p>A data engineer needs to perform a one-time extraction of a specific column from a large dataset in Apache Parquet format stored in an Amazon S3 bucket. The requirement is to accomplish this with minimal setup and execution time.</p><p>Which AWS service or feature should the data engineer use to query the single column from the Parquet files with the least operational overhead?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use S3 Select to run a SQL SELECT statement to extract the required column from the S3 objects.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Utilize Amazon Athena to execute a SQL SELECT statement that retrieves the required column from the Parquet files in the S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy an Amazon EMR cluster with Hive to perform a SELECT query on the column of the Parquet files stored in S3.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure Amazon Redshift Spectrum to query the required column directly from the S3 bucket.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>S3 Select allows the data engineer to retrieve a subset of data from an object using simple SQL expressions. By enabling the querying of just the needed column directly within the S3 service, S3 Select minimizes the operational overhead.</p><p>There is no need for provisioning compute resources or setting up additional services; the data can be filtered directly from the S3 bucket. This service is particularly useful for one-time tasks where a full-fledged ETL (extract, transform, load) process is not required.</p><p><strong>CORRECT: </strong>\"Use S3 Select to run a SQL SELECT statement to extract the required column from the S3 objects\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Utilize Amazon Athena to execute a SQL SELECT statement that retrieves the required column from the Parquet files in the S3 bucket\" is incorrect.</p><p>While Athena is capable of querying data directly in S3, it involves setting up a database, defining a table, and running a crawler if the schema is not already in place. This would be more overhead compared to the simplicity of S3 Select for a one-time task.</p><p><strong>INCORRECT:</strong> \"Configure Amazon Redshift Spectrum to query the required column directly from the S3 bucket\" is incorrect.</p><p>Redshift Spectrum extends Redshift querying capabilities to S3 data, but it requires an active Redshift cluster and external table definitions. This is more suitable for ongoing analytics rather than one-time tasks.</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon EMR cluster with Hive to perform a SELECT query on the column of the Parquet files stored in S3\" is incorrect.</p><p>Setting up an EMR cluster is a significant overhead for a one-time query task. It involves provisioning resources and configuring Hive, which is unnecessary when simpler options like S3 Select are available.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 15,
    "question": "<p>A growing fintech company is leveraging an Amazon Redshift cluster equipped with dense compute (DC2) nodes. To accommodate their expanding user base, the company needs to dynamically adjust both read and write capacities based on varying workloads. The data engineer has been tasked to configure the Redshift cluster to automatically add additional query processing power.</p><p>What action should the data engineer take to enable this functionality?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Activate concurrency scaling in the Redshift cluster's workload management (WLM) configuration.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Implement automatic WLM on the Redshift cluster to dynamically manage query queues.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable elasticity at the Redshift cluster by configuring Elastic IP addresses for each node.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the Redshift cluster to utilize Elastic Resize for adjusting node count based on demand.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Concurrency scaling is a feature specific to Amazon Redshift that automatically adds additional cluster capacity when needed to handle bursts in query load. This feature is particularly useful for unpredictable workloads and can be enabled directly in the Redshift cluster's WLM settings.</p><p>When concurrency scaling is turned on, Redshift automatically and transparently uses new clusters to provide additional compute resources, ensuring that read and write operations are scaled to meet demand without manual intervention.</p><p><strong>CORRECT: </strong>\"Activate concurrency scaling in the Redshift cluster's workload management (WLM) configuration\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the Redshift cluster to utilize Elastic Resize for adjusting node count based on demand\" is incorrect.</p><p>Elastic Resize changes the number of nodes in a Redshift cluster, which can scale the cluster's compute resources. However, it is not the same as concurrency scaling, which specifically addresses the need to scale read and write capacity on-demand without resizing the cluster.</p><p><strong>INCORRECT:</strong> \"Enable elasticity at the Redshift cluster by configuring Elastic IP addresses for each node\" is incorrect.</p><p>Elastic IP addresses provide a static IP address for EC2 instances, which is unrelated to the scaling of a Redshift cluster's read and write capacity. They are used for network-related configurations, not database scaling.</p><p><strong>INCORRECT:</strong> \"Implement automatic WLM on the Redshift cluster to dynamically manage query queues\" is incorrect.</p><p>Automatic Workload Management (WLM) dynamically manages memory and query concurrency to improve query processing. However, it does not by itself provide the additional compute resources for scaling read and write capacity on-demand like concurrency scaling does. Automatic WLM optimizes the processing of queries within the existing capacity of the cluster rather than scaling it out.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling-queues.html\">https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling-queues.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling-queues.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 16,
    "question": "<p>A financial institution stores sensitive customer data files, including personally identifiable information (PII), in Amazon S3 buckets. To enhance data security and compliance, the institution wants to automatically discover, classify, and protect the sensitive data in these S3 buckets.</p><p>Which AWS service should the financial institution use to automatically identify and safeguard sensitive customer data stored in Amazon S3?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure AWS WAF to apply web filtering rules on access to the S3 buckets.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy AWS Shield Advanced for protection against DDoS attacks targeting data in S3 buckets.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Macie to automatically discover and classify sensitive data in Amazon S3.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Implement Amazon GuardDuty for continuous monitoring and threat detection on S3 data.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Amazon Macie is the most suitable service for the financial institution’s needs. Macie is a data security service that uses machine learning and pattern matching to discover and classify sensitive data in Amazon S3.</p><p>Amazon Macie is specifically designed to identify and protect sensitive data, such as PII. Macie can assess, audit, and report on the data it finds, making it a powerful tool for ensuring data security and compliance with various data protection regulations.</p><p><strong>CORRECT: </strong>\"Use Amazon Macie to automatically discover and classify sensitive data in Amazon S3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement Amazon GuardDuty for continuous monitoring and threat detection on S3 data\" is incorrect.</p><p>Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior. While it is useful for security monitoring, it does not specialize in classifying or protecting sensitive data within S3 buckets like Macie does.</p><p><strong>INCORRECT:</strong> \"Configure AWS WAF to apply web filtering rules on access to the S3 buckets\" is incorrect.</p><p>AWS WAF (Web Application Firewall) is used to protect web applications from common web exploits. It is not designed to discover, classify, or protect sensitive data stored in S3 buckets.</p><p><strong>INCORRECT:</strong> \"Deploy AWS Shield Advanced for protection against DDoS attacks targeting data in S3 buckets\" is incorrect.</p><p>AWS Shield Advanced provides protection against DDoS (Distributed Denial of Service) attacks. It is not a tool for data classification or protection of sensitive data in S3 buckets, as it focuses on mitigating network and transport layer DDoS attacks.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/macie/\">https://aws.amazon.com/macie/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/macie/",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 17,
    "question": "<p>A company stores operational data in Amazon S3 using the S3 Standard storage class. Data usage analysis shows that most files are frequently accessed for the first 6 months, occasionally accessed for the next 18 months, and rarely accessed thereafter. The company seeks a cost-effective storage strategy that maintains high availability throughout the data lifecycle.</p><p>Which S3 Lifecycle policy should the company implement for their data storage needs?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months, then to S3 Glacier after 2 years.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Transition objects to S3 Intelligent-Tiering after 6 months, then to S3 Glacier Deep Archive after 2 years.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 6 months, then to S3 Glacier Deep Archive after 2 years.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months, then to S3 Glacier Deep Archive after 2 years.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>S3 Standard-Infrequent Access (S3 Standard-IA) is designed for data that is less frequently accessed but requires rapid access when needed. Transitioning to S3 Standard-IA after 6 months is cost-effective for the occasional access pattern.</p><p>After 2 years, when the data is rarely accessed, moving it to S3 Glacier Deep Archive offers the lowest storage cost for long-term archiving, while still maintaining data availability, albeit with longer retrieval times.</p><p><strong>CORRECT: </strong>\"Transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months, then to S3 Glacier Deep Archive after 2 years\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months, then to S3 Glacier after 2 years\" is incorrect.</p><p>While transitioning to S3 Standard-IA after 6 months is appropriate, moving to S3 Glacier after 2 years is not as cost-effective as S3 Glacier Deep Archive for data that is accessed only once or twice a year. S3 Glacier is better suited for data that might need to be accessed more frequently.</p><p><strong>INCORRECT:</strong> \"Transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 6 months, then to S3 Glacier Deep Archive after 2 years\" is incorrect.</p><p>S3 One Zone-IA stores data in a single Availability Zone and is less expensive than S3 Standard-IA. However, it does not provide the same level of high availability as S3 Standard-IA, which stores data across multiple Availability Zones.</p><p><strong>INCORRECT:</strong> \"Transition objects to S3 Intelligent-Tiering after 6 months, then to S3 Glacier Deep Archive after 2 years\" is incorrect.</p><p>S3 Intelligent-Tiering automatically moves data between access tiers based on usage patterns. While it's a good option for unknown or changing access patterns, in this scenario, where the access pattern is well-defined, the company can more cost-effectively manage storage costs by directly transitioning to S3 Standard-IA and then to S3 Glacier Deep Archive.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/s3/storage-classes/",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 18,
    "question": "<p>A startup is developing a serverless application with a Vue.js frontend, which interacts with a backend via Amazon API Gateway. They need to execute a Python script to run data processing tasks based on API requests and return the results. The script will be executed infrequently, and minimizing management overhead is a priority.</p><p>Which approach should the startup take to meet these requirements most efficiently?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Fargate to run a containerized version of the Python script and configure API Gateway for the Fargate service.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Host the Python script on an Amazon EC2 instance and set up an API Gateway endpoint to trigger the script execution.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an AWS Lambda function using Python, triggered by API Gateway, to run the script and return the response.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Implement the Python script as an AWS Glue job and trigger it with API Gateway using AWS SDK integrations.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Using an AWS Lambda function for the Python script, aligns with the requirement for low operational overhead while ensuring scalability. Lambda's integration with Amazon API Gateway enables the script to be triggered on-demand, running only when needed, which is cost-efficient for the startup's occasional use.</p><p>The infrastructure management is fully handled by AWS, which means no server maintenance worries for the startup. This setup not only simplifies the deployment process but also guarantees that the backend can handle varying loads, making it a suitable choice for applications with unpredictable traffic patterns.</p><p><strong>CORRECT: </strong>\"Create an AWS Lambda function using Python, triggered by API Gateway, to run the script and return the response\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Host the Python script on an Amazon EC2 instance and set up an API Gateway endpoint to trigger the script execution\" is incorrect.</p><p>Hosting on an Amazon EC2 instance requires managing the underlying server, including security patches and scaling, which adds operational overhead</p><p><strong>INCORRECT:</strong> \"Use AWS Fargate to run a containerized version of the Python script and configure API Gateway for the Fargate service\" is incorrect.</p><p>AWS Fargate allows you to run containers without managing servers or clusters but is more suitable for applications that need continuous running or complex orchestration, which is unnecessary for infrequent script execution.</p><p><strong>INCORRECT:</strong> \"Implement the Python script as an AWS Glue job and trigger it with API Gateway using AWS SDK integrations\" is incorrect.</p><p>AWS Glue is a managed ETL service that can be overkill for simple script execution and does not provide real-time responses as efficiently as Lambda when integrated with API Gateway.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 19,
    "question": "<p>A data analytics team needs to generate a summarized table from a large S3 dataset of sales transactions for quick analysis. What AWS service should they use to efficiently create this aggregated table directly from the S3 dataset?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement AWS Glue ETL job to transform and aggregate the data, then store the result back in S3.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Athena to perform a 'CREATE TABLE AS SELECT' operation for aggregating data from the S3 dataset.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Utilize Amazon Redshift Spectrum to query the S3 data and create a summary table in Redshift.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure an Amazon EMR cluster to run a Hive script for data aggregation and store the summarized table in S3.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Amazon Athena's CTAS feature allows the team to create a new table from the results of a SELECT query on an existing table. This approach is efficient for summarizing and aggregating large datasets stored in Amazon S3.</p><p>The CTAS operation in Athena enables the creation of a summarized table based on specific aggregation criteria, such as sales data by region and product category. It's a serverless option that requires no infrastructure management, making it a convenient and cost-effective solution for the team's requirements</p><p><strong>CORRECT: </strong>\"Use Amazon Athena to perform a 'CREATE TABLE AS SELECT' operation for aggregating data from the S3 dataset\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Utilize Amazon Redshift Spectrum to query the S3 data and create a summary table in Redshift\" is incorrect.</p><p>While Redshift Spectrum allows querying data in S3 directly from Redshift, it involves more complexity compared to using Athena, as it requires managing a Redshift cluster. Additionally, Redshift is more suitable for ongoing, complex analytical workloads rather than quick, ad-hoc aggregations.</p><p><strong>INCORRECT:</strong> \"Implement AWS Glue ETL job to transform and aggregate the data, then store the result back in S3\" is incorrect.</p><p>AWS Glue can be used for ETL processes, but it would be an overkill for the team's requirement to simply aggregate data. Glue involves setting up and managing ETL jobs, which is more complex and resource-intensive compared to running a CTAS query in Athena.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon EMR cluster to run a Hive script for data aggregation and store the summarized table in S3\" is incorrect.</p><p>Using Amazon EMR with Hive for data aggregation is a viable solution but requires provisioning and managing an EMR cluster. This adds operational overhead compared to the serverless nature of Athena, which can perform the aggregation without the need to manage any cluster.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/ctas.html\">https://docs.aws.amazon.com/athena/latest/ug/ctas.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/ctas.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 20,
    "question": "<p>A data engineer is using Amazon Athena for querying customer engagement data stored in Amazon S3. The analyst needs to extract information about customer interactions for the year 2023 from a table called interaction_data. The initial query designed to fetch the total interactions per customer for 2023 is not yielding results for some customers known to have interactions in that year. The query needs adjustment to correctly reflect the entire dataset.</p><p>The original query is:</p><p>SELECT customer_id, sum(interactions)</p><p>FROM interaction_data</p><p>WHERE year = 2023</p><p>GROUP BY customer_id</p><p>How should the data engineer modify the Athena query to ensure it includes all relevant customer data for 2023?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add a LEFT JOIN with another table that lists all customers, ensuring inclusion of customers with zero interactions.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Replace WHERE year = 2023 with a more inclusive condition, such as WHERE year &gt;= 2023, to capture late-recorded data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Introduce a CASE statement within the SUM function to handle any null values in the interactions column.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Include a subquery to first select distinct customer IDs from the interaction data, then sum interactions for those customers.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>The correct solution in this context is to add a LEFT JOIN with another table that lists all customers. This approach is particularly effective in scenarios where the primary data table (interaction_data in this case) might not include all relevant entities - for example, customers who had no interactions in the specified time frame.</p><p>By joining the interaction data with a complete list of customers, the query ensures that all customers are accounted for in the results, regardless of whether they had interactions in 2023 or not. This method is especially useful in analytics and reporting to provide a comprehensive view of the data, including showing customers with zero interactions.</p><p>The LEFT JOIN operation in SQL is used to return all records from the left table (the complete customer list), and the matched records from the right table (the interaction data). If there is no match, the result is NULL on the right side.</p><p>This type of join ensures that even if a customer did not have any interactions in 2023, they will still appear in the query output, potentially with NULL or zero in the interactions column. This modification to the original query is critical for comprehensive data analysis, ensuring no customer data is inadvertently omitted due to lack of interactions.</p><p><strong>CORRECT: </strong>\"Add a LEFT JOIN with another table that lists all customers, ensuring inclusion of customers with zero interactions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Replace WHERE year = 2023 with a more inclusive condition, such as WHERE year &gt;= 2023, to capture late-recorded data\" is incorrect.</p><p>This change broadens the query to include data beyond the year 2023. While it might include additional data, it doesn't directly address the issue of missing results for certain customers within the 2023 data set.</p><p><strong>INCORRECT:</strong> \"Include a subquery to first select distinct customer IDs from the interaction data, then sum interactions for those customers\" is incorrect.</p><p>This method involves creating a subquery to identify all distinct customer IDs and then calculating the sum of interactions for these customers. This could potentially resolve issues related to data organization or missing entries but might not be the most direct way to ensure all 2023 data is included.</p><p><strong>INCORRECT:</strong> \"Introduce a CASE statement within the SUM function to handle any null values in the interactions column\" is incorrect.</p><p>This adjustment accounts for potential null values in the interactions column, ensuring they are handled appropriately in the summation process. However, this approach may not solve the issue of missing customer records if the issue is related to the way the data is filtered for the year 2023.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/ddl-sql-reference.html\">https://docs.aws.amazon.com/athena/latest/ug/ddl-sql-reference.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/athena/latest/ug/ddl-sql-reference.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 21,
    "question": "<p>A media company has a workflow where high-resolution videos are uploaded to an Amazon S3 bucket. Once a video is uploaded, it needs to be processed immediately to create lower-resolution versions. The company wants to automate this process so that as soon as a video is uploaded to the S3 bucket, an AWS Lambda function is triggered to start the processing.</p><p>Which AWS service should the company use to automatically trigger the Lambda function upon video upload to the S3 bucket?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Amazon CloudWatch Events to detect the S3 upload event and trigger the Lambda function.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up AWS Step Functions to orchestrate the process, triggering the Lambda function upon S3 object upload.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 Event Notifications to directly invoke the Lambda function when a new video is uploaded.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Implement Amazon Kinesis Data Streams to capture upload events and trigger the Lambda function.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Amazon S3 Event Notifications is the most suitable service for this requirement. It can be configured to trigger a Lambda function automatically when a new object is uploaded to an S3 bucket.</p><p>This direct integration between S3 and Lambda is efficient for real-time processing of new objects, such as the high-resolution videos in this scenario. When a video is uploaded to the specified S3 bucket, the configured event notification will invoke the Lambda function to process the video, making the workflow automated and seamless.</p><p><strong>CORRECT: </strong>\"Use Amazon S3 Event Notifications to directly invoke the Lambda function when a new video is uploaded\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure Amazon CloudWatch Events to detect the S3 upload event and trigger the Lambda function\" is incorrect.</p><p>CloudWatch Events can respond to S3 object-level operations, but it is a more indirect method compared to using S3 Event Notifications. S3 Event Notifications provide a more straightforward and immediate way to trigger Lambda functions upon object upload.</p><p><strong>INCORRECT:</strong> \"Implement Amazon Kinesis Data Streams to capture upload events and trigger the Lambda function\" is incorrect.</p><p>Amazon Kinesis Data Streams is primarily used for real-time data streaming and processing but is not necessary for triggering a Lambda function based on S3 object uploads. This approach would add unnecessary complexity to the solution.</p><p><strong>INCORRECT:</strong> \"Set up AWS Step Functions to orchestrate the process, triggering the Lambda function upon S3 object upload\" is incorrect.</p><p>AWS Step Functions is a workflow service for orchestrating various AWS services. However, for the specific need to trigger a Lambda function upon an S3 object upload, Step Functions would be an overcomplication. S3 Event Notifications provide a more direct and efficient solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 22,
    "question": "<p>An online gaming platform generates large amounts of player activity logs, which are stored in Amazon S3 as JSON files. The platform requires a solution to process these logs nightly, aggregate player statistics, and then store the aggregated data in a format that supports efficient querying for daily reports.</p><p>Which AWS solution should be implemented to process and store the aggregated player statistics for efficient daily reporting?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement an AWS Lambda function to process the logs in S3, aggregate statistics, and write the data to Amazon DynamoDB.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure Amazon EMR to run nightly batch jobs on the S3 logs, aggregate the data, and output to Amazon Aurora.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Data Pipeline to orchestrate the data processing in S3 and store the aggregated results in Amazon RDS.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Glue to transform the JSON logs in S3, aggregate the data, and store the results in Amazon Redshift.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Domain 1: Data Ingestion and Transformation",
    "explanation": "<p>AWS Glue is a serverless data integration service that can be used to prepare and transform data for analytics. It is well-suited for processing large volumes of data, such as player activity logs, and can easily read and write data in Amazon S3.</p><p>After transforming and aggregating the data, storing the results in Amazon Redshift, a data warehousing service, allows for efficient querying and generation of daily reports.</p><p>Redshift is optimized for handling large datasets and complex queries, making it an ideal choice for storing aggregated data for reporting purposes.</p><p><strong>CORRECT: </strong>\"Use AWS Glue to transform the JSON logs in S3, aggregate the data, and store the results in Amazon Redshift\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement an AWS Lambda function to process the logs in S3, aggregate statistics, and write the data to Amazon DynamoDB\" is incorrect.</p><p>While AWS Lambda is capable of processing data, using it for nightly batch jobs on large volumes of logs might be less efficient due to Lambda's execution time limits. Additionally, DynamoDB, being a NoSQL database, is more suited for fast read/write operations but may not be as efficient for complex analytical queries typically required for reporting.</p><p><strong>INCORRECT:</strong> \"Configure Amazon EMR to run nightly batch jobs on the S3 logs, aggregate the data, and output to Amazon Aurora\" is incorrect.</p><p>Amazon EMR is effective for big data processing, but using it alongside Amazon Aurora, a relational database, for nightly aggregation tasks may introduce unnecessary complexity and costs. EMR is more aligned with large-scale data processing and may not be the most cost-effective solution for this use case.</p><p><strong>INCORRECT:</strong> \"Use AWS Data Pipeline to orchestrate the data processing in S3 and store the aggregated results in Amazon RDS\" is incorrect.</p><p>AWS Data Pipeline is a service for processing and moving data between different AWS services. While it can automate these tasks, combining it with Amazon RDS for storage may not provide the same level of performance and query optimization for reporting as using Amazon Redshift, which is specifically designed for data warehousing and analytics</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/glue/",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 23,
    "question": "<p>A company stores large image files in an Amazon S3 bucket. They need a solution to dynamically resize images on-the-fly when accessed by different applications, without storing multiple versions of the same image. The solution should modify the image data during retrieval based on the application's requirements.</p><p>Which AWS service should the company use to achieve this on-the-fly transformation of image data?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Step Functions</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon S3 Object Lambda</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS Lambda@Edge</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Elastic Transcoder</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Amazon S3 Object Lambda allows you to add custom code to process data retrieved from S3 before returning it to an application. This service is ideal for the company's requirement to dynamically resize images on-the-fly.</p><p>When an application requests an image, S3 Object Lambda can invoke a Lambda function to modify the image (e.g., resize it) based on the specific needs of the application.</p><p>This approach eliminates the need to store multiple versions of each image, as transformations are applied in real time during data retrieval.</p><p><strong>CORRECT: </strong>\"Amazon S3 Object Lambda\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Lambda@Edge\" is incorrect.</p><p>AWS Lambda@Edge allows you to run Lambda functions at AWS Edge locations in response to CloudFront events. While it can be used for processing content closer to users, it is not specifically designed for on-the-fly transformations of data stored in S3, unlike S3 Object Lambda which directly integrates with S3 for data processing during retrieval.</p><p><strong>INCORRECT:</strong> \"Amazon Elastic Transcoder\" is incorrect.</p><p>Amazon Elastic Transcoder is a media transcoding service in the cloud. It is primarily used for converting media files from their source format into different formats that can play on various devices. While it can handle image resizing, it is more suited for batch processing rather than dynamic, on-the-fly transformations as required in this scenario.</p><p><strong>INCORRECT:</strong> \"AWS Step Functions\" is incorrect.</p><p>AWS Step Functions is a service for orchestrating workflows across various AWS services. While it can coordinate complex workflows, it is not tailored for modifying S3 object data during retrieval. S3 Object Lambda is a more direct solution for applying real-time transformations to data as it is fetched from S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/features/object-lambda/\">https://aws.amazon.com/s3/features/object-lambda/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/s3/features/object-lambda/",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 24,
    "question": "<p>A logistics company utilizes Amazon S3 to manage shipment tracking information. Each tracking update is stored as individual JSON files. The company requires a solution to efficiently process only the new or updated tracking records each hour and ensure they're reflected in their analytics platform.</p><p>Which AWS service should the data engineer implement to identify and process only the new or modified tracking records in a cost-effective manner?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement Amazon S3 Event Notifications to invoke a processing job in AWS Batch for newly modified files.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Glue crawlers to detect schema changes in the S3 bucket and run ETL jobs for new or updated files.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure AWS DataSync to monitor and synchronize only the changed files to another S3 bucket for dedicated processing.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure AWS Lambda with S3 triggers to parse new and updated JSON files and process the changes hourly.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Domain 3: Data Operations and Support",
    "explanation": "<p>Amazon S3 Event Notifications can be configured to trigger automatically when new files are added or existing files are modified.</p><p>By setting up event notifications to invoke a processing job in AWS Batch, the logistics company can efficiently process only the new or updated tracking records.</p><p>AWS Batch can handle the job scheduling, execution, and resource management, making it a cost-effective and operationally efficient solution for processing data at regular intervals</p><p><strong>CORRECT: </strong>\"Implement Amazon S3 Event Notifications to invoke a processing job in AWS Batch for newly modified files\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS Lambda with S3 triggers to parse new and updated JSON files and process the changes hourly\" is incorrect.</p><p>While AWS Lambda functions can be triggered by S3 events to process new and updated files, managing this at scale, especially with potentially numerous small files, could lead to higher costs and more complex lambda function management compared to batch processing solutions.</p><p><strong>INCORRECT:</strong> \"Use AWS Glue crawlers to detect schema changes in the S3 bucket and run ETL jobs for new or updated files\" is incorrect.</p><p>AWS Glue crawlers are primarily used for schema detection and metadata cataloging, not for processing individual file changes. While they can trigger ETL jobs, using them for hourly data changes could be less efficient and more costly for frequent, small updates.</p><p><strong>INCORRECT:</strong> \"Configure AWS DataSync to monitor and synchronize only the changed files to another S3 bucket for dedicated processing\" is incorrect.</p><p>AWS DataSync is designed for synchronizing datasets between storage services, which could be used to mirror changes to a processing environment. However, it is generally used for data transfer rather than triggering processing workflows, and might not be as cost-effective for the company's needs of hourly processing of tracking updates.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  },
  {
    "id": 25,
    "question": "<p>A retail organization is developing a business intelligence platform. They are utilizing Amazon S3 to store their sales data and Amazon Redshift for warehousing. To efficiently perform queries on their S3-stored data using Amazon Redshift Spectrum,</p><p>Which two of the following practices will yield the FASTEST query performance? (Select TWO.)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Choose file formats that allow for multi-threaded data processing.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store the data using a columnar format like Parquet or ORC.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Partition the data in S3 based on frequently queried attributes such as date or product category.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Divide the data into numerous small files, each smaller than 128 KB.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Compress the data files using bzip2 to ensure they are between 1 GB and 5 GB after compression.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Domain 2: Data Store Management",
    "explanation": "<p>Columnar storage formats like Parquet and ORC are optimized for analytics and can significantly speed up queries because they allow Redshift Spectrum to read only the necessary columns for a query rather than entire rows. This reduces the amount of data scanned and improves performance.</p><p>Partitioning data on commonly used query predicates allows Redshift Spectrum to skip over irrelevant parts of the dataset, which can greatly improve query performance. By scanning only the relevant partitions, less data is processed, leading to faster query execution.</p><p><strong>CORRECT: </strong>\"Store the data using a columnar format like Parquet or ORC\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Partition the data in S3 based on frequently queried attributes such as date or product category\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Compress the data files using bzip2 to ensure they are between 1 GB and 5 GB after compression\" is incorrect.</p><p>While bzip2 is a compression format, it is not as performant as gzip in the context of Redshift Spectrum due to its non-splittable nature. Redshift Spectrum processes data in parallel, and splittable formats like gzip (when files are larger than the block size) can be processed more efficiently.</p><p><strong>INCORRECT:</strong> \"Divide the data into numerous small files, each smaller than 128 KB\" is incorrect.</p><p>Having many small files can actually degrade performance because each file incurs overhead for processing. Redshift Spectrum is optimized for larger file sizes that allow for more efficient processing.</p><p><strong>INCORRECT:</strong> \"Choose file formats that allow for multi-threaded data processing\" is incorrect.</p><p>While multi-threaded processing is beneficial, the prompt asks for file format considerations. All else being equal, file formats themselves do not enable multi-threaded processing; this is dependent on the query engine or service used. In Redshift Spectrum, the columnar formats mentioned in option 2 inherently allow for parallel processing of data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-performance.html\">https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-performance.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/\">https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-performance.html",
      "https://digitalcloud.training/aws-certified-data-engineer-associate-cheat-sheet/"
    ]
  }
]