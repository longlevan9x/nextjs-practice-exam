[
  {
    "id": 1,
    "question": "<p>A Solutions Architect must design a solution to allow many Amazon EC2 instances across multiple subnets to access a shared data store. The data must be accessed by all instances simultaneously and access should use the NFS protocol. The solution must also be highly scalable and easy to implement.</p><p>Which solution best meets these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon S3 bucket and configure a Network ACL. Grant the EC2 instances permission to access the bucket using the NFS protocol.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon EFS file system. Configure a mount target in each Availability Zone. Attach each instance to the appropriate mount target.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure an additional EC2 instance as a file server. Create a role in AWS IAM that grants permissions to the file share and attach the role to the EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon EBS volume and create a resource-based policy that grants an AWS IAM role access to the data. Attach the role to the EC2 instances.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>The Amazon Elastic File System (EFS) is a perfect solution for this requirement. Amazon EFS filesystems are accessed using the NFS protocol and can be mounted by many instances across multiple subnets simultaneously. EFS filesystems are highly scalable and very easy to implement.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-08-11-6d39438584e2d604d169e4d38f77f00e.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-08-11-6d39438584e2d604d169e4d38f77f00e.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Create an Amazon EFS file system. Configure a mount target in each Availability Zone. Attach each instance to the appropriate mount target\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Configure an additional EC2 instance as a file server. Create a role in AWS IAM that grants permissions to the file share and attach the role to the EC2 instances\" is incorrect. You cannot use IAM roles to grant permissions to a file share created within the operating system of an EC2 instance. Also, this solution is not as highly scalable or easy to implement as Amazon EFS.</p><p><strong>INCORRECT:</strong> \"Create an Amazon S3 bucket and configure a Network ACL. Grant the EC2 instances permission to access the bucket using the NFS protocol\" is incorrect. A Network ACL is created to restrict traffic in and out of subnets, it is not used to control access to S3 buckets (use a bucket policy or bucket ACL instead). You cannot grant permission to access an S3 bucket using a protocol, and NFS is not supported for S3 as it is an object-based storage system.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EBS volume and create a resource-based policy that grants an AWS IAM role access to the data. Attach the role to the EC2 instances\" is incorrect. You cannot configure a resource-based policy on an Amazon EBS volume.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html\">https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html",
      "https://digitalcloud.training/amazon-efs/"
    ]
  },
  {
    "id": 2,
    "question": "<p>An application runs on Amazon EC2 Linux instances. The application generates log files which are written using standard API calls. A storage solution is required that can be used to store the files indefinitely and must allow concurrent access to all files.</p><p>Which storage service meets these requirements and is the MOST cost-effective?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon EFS</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon S3</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon EBS</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon EC2 instance store</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>The application is writing the files using API calls which means it will be compatible with Amazon S3 which uses a REST API. S3 is a massively scalable key-based object store that is well-suited to allowing concurrent access to the files from many instances.</p><p>Amazon S3 will also be the most cost-effective choice. A rough calculation using the AWS pricing calculator shows the cost differences between 1TB of storage on EBS, EFS, and S3 Standard.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-55-11-6ae407730f9451342d76fb514c903b8a.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-55-11-6ae407730f9451342d76fb514c903b8a.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Amazon S3\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon EFS\" is incorrect as though this does offer concurrent access from many EC2 Linux instances, it is not the most cost-effective solution.</p><p><strong>INCORRECT:</strong> \"Amazon EBS\" is incorrect. The Elastic Block Store (EBS) is not a good solution for concurrent access from many EC2 instances and is not the most cost-effective option either. EBS volumes are mounted to a single instance except when using multi-attach which is a new feature and has several constraints.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 instance store\" is incorrect as this is an ephemeral storage solution which means the data is lost when powered down. Therefore, this is not an option for long-term data storage.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html",
      "https://digitalcloud.training/amazon-s3-and-glacier/"
    ]
  },
  {
    "id": 3,
    "question": "<p>A company is deploying a solution for sharing media files around the world using Amazon CloudFront with an Amazon S3 origin configured as a static website. The company requires that all traffic for the website must be inspected by AWS WAF.</p><p>Which solution meets these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an Amazon Route 53 Alias record to forward traffic for the website to AWS WAF. Configure AWS WAF to inspect traffic and attach the CloudFront distribution.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy CloudFront with an S3 origin and configure an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the CloudFront distribution.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create an S3 bucket policy with a condition that only allows requests that originate from AWS WAF.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a Network ACL that limits access to the S3 bucket to the CloudFront IP addresses. Attach a WebACL to the CloudFront distribution.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>The AWS Web Application Firewall (WAF) can be attached to an Amazon CloudFront distribution to enable protection from web exploits. In this case the distribution uses an S3 origin, and the question is stating that all traffic must be inspected by AWS WAF. This means we need to ensure that requests cannot circumvent AWS WAF and hit the S3 bucket directly.</p><p>This can be achieved by configuring an origin access identity (OAI) which is a special type of CloudFront user that is created within the distribution and configured in an S3 bucket policy. The policy will only allow requests that come from the OAI which means all requests must come via the distribution and cannot hit S3 directly.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-32-57-0cec77f550d1e2e6100046094949925b.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-32-57-0cec77f550d1e2e6100046094949925b.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Deploy CloudFront with an S3 origin and configure an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the CloudFront distribution\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Network ACL that limits access to the S3 bucket to the CloudFront IP addresses. Attach a WebACL to the CloudFront distribution\" is incorrect. Network ACLs restrict traffic in/out of subnets but S3 is a public service.</p><p><strong>INCORRECT:</strong> \"Use an Amazon Route 53 Alias record to forward traffic for the website to AWS WAF. Configure AWS WAF to inspect traffic and attach the CloudFront distribution\" is incorrect. You cannot direct traffic to AWS WAF using an Alias record.</p><p><strong>INCORRECT:</strong> \"Create an S3 bucket policy with a condition that only allows requests that originate from AWS WAF\" is incorrect. This cannot be done. Instead use an OAI in the bucket policy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html",
      "https://digitalcloud.training/amazon-cloudfront/"
    ]
  },
  {
    "id": 4,
    "question": "<p>A company hosts statistical data in an Amazon S3 bucket that users around the world download from their website using a URL that resolves to a domain name. The company needs to provide low latency access to users and plans to use Amazon Route 53 for hosting DNS records.</p><p>Which solution meets these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a web distribution on Amazon CloudFront pointing to an Amazon S3 origin. Create an ALIAS record in the Amazon Route 53 hosted zone that points to the CloudFront distribution, resolving to the application's URL domain name.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a web distribution on Amazon CloudFront pointing to an Amazon S3 origin. Create a CNAME record in a Route 53 hosted zone that points to the CloudFront distribution, resolving to the application's URL domain name.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an A record in Route 53, use a Route 53 traffic policy for the web application, and configure a geoproximity rule. Configure health checks to check the health of the endpoint and route DNS queries to other endpoints if an endpoint is unhealthy.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an A record in Route 53, use a Route 53 traffic policy for the web application, and configure a geolocation rule. Configure health checks to check the health of the endpoint and route DNS queries to other endpoints if an endpoint is unhealthy.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>This is a simple requirement for low latency access to the contents of an Amazon S3 bucket for global users. The best solution here is to use Amazon CloudFront to cache the content in Edge Locations around the world. This involves creating a web distribution that points to an S3 origin (the bucket) and then create an Alias record in Route 53 that resolves the applications URL to the CloudFront distribution endpoint.</p><p><strong>CORRECT: </strong>\"Create a web distribution on Amazon CloudFront pointing to an Amazon S3 origin. Create an ALIAS record in the Amazon Route 53 hosted zone that points to the CloudFront distribution, resolving to the application's URL domain name\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a web distribution on Amazon CloudFront pointing to an Amazon S3 origin. Create a CNAME record in a Route 53 hosted zone that points to the CloudFront distribution, resolving to the application's URL domain name\" is incorrect. An Alias record should be used to point to an Amazon CloudFront distribution.</p><p><strong>INCORRECT:</strong> \"Create an A record in Route 53, use a Route 53 traffic policy for the web application, and configure a geolocation rule. Configure health checks to check the health of the endpoint and route DNS queries to other endpoints if an endpoint is unhealthy\" is incorrect. There is only a single endpoint (the Amazon S3 bucket) so this strategy would not work. Much better to use CloudFront to cache in multiple locations.</p><p><strong>INCORRECT:</strong> \"Create an A record in Route 53, use a Route 53 traffic policy for the web application, and configure a geoproximity rule. Configure health checks to check the health of the endpoint and route DNS queries to other endpoints if an endpoint is unhealthy\" is incorrect. Again, there is only one endpoint so this strategy will simply not work.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html",
      "https://digitalcloud.training/amazon-cloudfront/",
      "https://digitalcloud.training/amazon-route-53/"
    ]
  },
  {
    "id": 5,
    "question": "<p>A financial services company manages its web application on Amazon EC2 instances. The EC2 instances are registered in an <strong>IP address-type target group</strong> behind an Application Load Balancer (ALB). The company uses AWS Systems Manager for patching and routine maintenance of the instances.</p><p>To meet security compliance requirements, the company must ensure that EC2 instances are temporarily removed from service during patching to prevent serving traffic. During a recent patching attempt, the company experienced application errors and traffic disruptions.</p><p>Which combination of solutions will resolve these issues? (Select TWO.)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Systems Manager State Manager to schedule patching jobs and ensure instances are deregistered and re-registered with the ALB after patching is complete.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Change the target type of the target group from IP address type to instance type and re-register the instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure ALB health checks to automatically remove unhealthy instances during patching. Use Systems Manager Run Command to apply the patches manually.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the Systems Manager Maintenance Windows feature to schedule patching and automatically deregister instances from the ALB during updates.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Implement the <strong>AWSEC2-PatchLoadBalancerInstance</strong> Systems Manager Automation document to manage the patching process for EC2 instances behind the ALB.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Management & Governance",
    "explanation": "<p><strong>Use the Systems Manager Maintenance Windows feature to schedule patching and automatically deregister instances from the ALB during updates:</strong> This is correct because Maintenance Windows coordinate the patching process, including removing instances from the ALB during updates, ensuring compliance and preventing traffic disruptions.</p><p><strong>Implement the AWSEC2-PatchLoadBalancerInstance Systems Manager Automation document to manage the patching process for EC2 instances behind the ALB:</strong> This is correct because this Automation document automates the removal of instances from the ALB, applies patches, and re-registers the instances after patching. This eliminates manual errors and ensures seamless updates without disrupting application traffic.</p><p><strong>Change the target type of the target group from IP address type to instance type and re-register the instances:</strong> This is incorrect because changing the target type does not address the root cause of the errors. The patching process itself requires proper management of instance removal and re-registration.</p><p><strong>Configure ALB health checks to automatically remove unhealthy instances during patching. Use Systems Manager Run Command to apply the patches manually:</strong> This is incorrect because relying on ALB health checks for patching introduces unnecessary delays. Manually applying patches with Run Command is operationally inefficient compared to using Automation documents.</p><p><strong>Use Systems Manager State Manager to schedule patching jobs and ensure instances are deregistered and re-registered with the ALB after patching is complete:</strong> This is incorrect because State Manager is not designed to dynamically handle instance deregistration and re-registration with load balancers. Automation documents like AWSEC2-PatchLoadBalancerInstance are more appropriate.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-awsec2-patch-load-balancer-instance.html\">https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-awsec2-patch-load-balancer-instance.html</a><br><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-maintenance.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-maintenance.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-awsec2-patch-load-balancer-instance.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-maintenance.html",
      "https://digitalcloud.training/aws-systems-manager/"
    ]
  },
  {
    "id": 6,
    "question": "<p>A company has experienced malicious traffic from some suspicious IP addresses. The security team discovered the requests are from different IP addresses under the same CIDR range.</p><p>What should a solutions architect recommend to the team?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add a deny rule in the inbound table of the network ACL with a lower rule number than other rules</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Add a rule in the outbound table of the security group to deny the traffic from that CIDR range</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add a deny rule in the outbound table of the network ACL with a lower rule number than other rules</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Add a rule in the inbound table of the security group to deny the traffic from that CIDR range</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>You can only create deny rules with network ACLs, it is not possible with security groups. Network ACLs process rules in order from the lowest numbered rules to the highest until they reach and allow or deny. The following table describes some of the differences between security groups and network ACLs:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_02-02-30-acd31eb94885375c9562ead5a41a3639.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_02-02-30-acd31eb94885375c9562ead5a41a3639.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the solutions architect should add a deny rule in the inbound table of the network ACL with a lower rule number than other rules.</p><p><strong>CORRECT: </strong>\"Add a deny rule in the inbound table of the network ACL with a lower rule number than other rules\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add a deny rule in the outbound table of the network ACL with a lower rule number than other rules\" is incorrect as this will only block outbound traffic.</p><p><strong>INCORRECT:</strong> \"Add a rule in the inbound table of the security group to deny the traffic from that CIDR range\" is incorrect as you cannot create a deny rule with a security group.</p><p><strong>INCORRECT:</strong> \"Add a rule in the outbound table of the security group to deny the traffic from that CIDR range\" is incorrect as you cannot create a deny rule with a security group.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html",
      "https://digitalcloud.training/amazon-vpc/"
    ]
  },
  {
    "id": 7,
    "question": "<p>A Financial Services company currently stores data in Amazon S3. Each bucket contains items which have different access patterns. The Chief Financial officer of the organization wants to reduce costs, as they have noticed a sharp increase in their S3 bill. The Chief Financial Officer wants to reduce the S3 spend as quickly as possible.</p><p>What is the quickest way to reduce the S3 spend with the LEAST operational overhead?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Place all objects in S3 Glacier Instant Retrieval.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Automate the move of your S3 objects to the best storage class with AWS Trusted Advisor.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a Lambda function to scan your S3 buckets, check which objects are stored in the appropriate buckets, and move them there.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Transition the objects to the appropriate storage class by using an S3 Lifecycle configuration.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:</p><p>● Transition actions – These actions define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after creating them, or archive objects to the S3 Glacier Flexible Retrieval storage class one year after creating them. For more information, see Using Amazon S3 storage classes.</p><p>● Expiration actions – These actions define when objects expire. Amazon S3 deletes expired objects on your behalf.</p><p><strong>CORRECT: </strong>\"Transition the objects to the appropriate storage class by using an S3 Lifecycle configuration” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Automate the move of your S3 objects to the best storage class with AWS Trusted Advisor” is incorrect. Trusted Advisor does not automatically transfer objects into the most appropriate buckets. You can use Trusted Advisor to review cost optimization options, and check for public access to your buckets but you cannot automatically transition objects.</p><p><strong>INCORRECT:</strong> \"Create a Lambda function to scan your S3 buckets, check which objects are stored in the appropriate buckets, and move them there” is incorrect. You could perhaps build a Lambda function to do this, however the easiest way to do this would be to use an S3 Lifecycle configuration.</p><p><strong>INCORRECT:</strong> \"Place all objects in S3 Glacier Instant Retrieval” is incorrect. It states in the question that each bucket contains items which have different access patterns, therefore S3 Glacier is not a suitable use case.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html",
      "https://digitalcloud.training/amazon-s3-and-glacier/"
    ]
  },
  {
    "id": 8,
    "question": "<p>A company needs to migrate a large quantity of data from an on-premises environment to Amazon S3. The company is connected via an AWS Direct Connect (DX) connection. The company requires a fully managed solution that will keep the data private and automate and accelerate the replication of the data to AWS storage services.</p><p>Which solution should a Solutions Architect recommend?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy an AWS Storage Gateway file gateway with a local cache and store the primary data set in Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy an AWS DataSync agent for the on-premises environment. Configure a task to replicate the data and connect it to a public endpoint.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy an AWS Storage Gateway volume gateway in stored volume mode and take point-in-time copies of the volumes using AWS Backup.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy an AWS DataSync agent for the on-premises environment. Configure a task to replicate the data and connect it to a VPC endpoint.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>AWS DataSync can be used to automate and accelerate the replication of data to AWS storage services. Note that Storage Gateway is used for hybrid scenarios where servers need local access to data with various options for storing and synchronizing the data to AWS storage services. Storage Gateway does not accelerate replication of data.</p><p>To deploy DataSync an agent must be installed. Then a task must be configured to replicated data to AWS. The task requires a connection to a service endpoint. To keep the data private and send it across the DX connection, a VPC endpoint should be used.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-04-32-b583aab06b8f4559436be3906fdb3054.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-04-32-b583aab06b8f4559436be3906fdb3054.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Deploy an AWS DataSync agent for the on-premises environment. Configure a task to replicate the data and connect it to a VPC endpoint\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS DataSync agent for the on-premises environment. Configure a task to replicate the data and connect it to a public endpoint\" is incorrect. A public endpoint will send data over the public internet which should be avoided in this scenario.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Storage Gateway volume gateway in stored volume mode and take point-in-time copies of the volumes using AWS Backup\" is incorrect. Storage Gateway will not accelerate replication and a volume gateway will create EBS snapshots (not S3 objects).</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Storage Gateway file gateway with a local cache and store the primary data set in Amazon S3\" is incorrect. Storage Gateway will not accelerate replication and a file gateway should be used for providing NFS or CIFS/SMB access to data locally which is not required.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/datasync/latest/userguide/choose-service-endpoint.html\">https://docs.aws.amazon.com/datasync/latest/userguide/choose-service-endpoint.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/datasync/latest/userguide/choose-service-endpoint.html",
      "https://digitalcloud.training/aws-migration-services/"
    ]
  },
  {
    "id": 9,
    "question": "<p>A company recently performed a lift and shift migration of its on-premises Oracle database workload to run on an Amazon EC2 memory-optimized Linux instance. The EC2 Linux instance uses a 1 TB Provisioned IOPS SSD (io1) EBS volume with 64,000 IOPS. The database storage performance after the migration is slower than the performance of the on-premises database.</p><p>Which solution will improve storage performance?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Increase the size of the Provisioned IOPS SSD (io1) EBS volume to 2 TB.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add more Provisioned IOPS SSD (io1) EBS volumes. Use OS commands to create a Logical Volume Management (LVM) stripe.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Increase the Provisioned IOPS SSD (io1) EBS volume to more than 64,000 IOPS.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Change the EC2 Linux instance to a storage-optimized instance type. Do not change the Provisioned IOPS SSD (io1) EBS volume.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p><strong>Add more Provisioned IOPS SSD (io1) EBS volumes. Use OS commands to create a Logical Volume Management (LVM) stripe:</strong> This is correct because creating a striped volume using multiple io1 EBS volumes allows you to aggregate performance and exceed the performance limits of a single volume, resulting in better storage performance.</p><p><strong>Increase the Provisioned IOPS SSD (io1) EBS volume to more than 64,000 IOPS:</strong> This is incorrect because 64,000 IOPS is the maximum performance for a single io1 volume attached to an EC2 instance. Increasing the IOPS would not be effective.</p><p><strong>Increase the size of the Provisioned IOPS SSD (io1) EBS volume to 2 TB:</strong> This is incorrect because the volume size does not directly affect the IOPS beyond the already configured value of 64,000 IOPS.</p><p><strong>Change the EC2 Linux instance to a storage-optimized instance type. Do not change the Provisioned IOPS SSD (io1) EBS volume:</strong> This is incorrect because changing the instance type does not address the bottleneck at the EBS volume level.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a><br><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-io-characteristics.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-io-characteristics.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-io-characteristics.html",
      "https://digitalcloud.training/amazon-ebs/"
    ]
  },
  {
    "id": 10,
    "question": "<p>A streaming service company runs its video recommendation engine on an Amazon EC2 Auto Scaling group behind an Application Load Balancer (ALB) in a single AWS Region. The service generates personalized recommendations based on user activity and serves dynamic content to millions of users worldwide.</p><p>The company needs a cost-optimized solution to improve performance and scalability while ensuring that users across the globe experience low latency when accessing personalized recommendations.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an Amazon CloudFront distribution and configure the existing ALB as the origin. Use dynamic cache settings to reduce latency for global users.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure AWS Global Accelerator to route traffic to the existing ALB and EC2 instances in the Region closest to each user.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy additional EC2 instances and ALBs in multiple Regions. Use Amazon Route 53 latency-based routing to direct users to the Region with the lowest latency.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Migrate the recommendation engine to Amazon S3 and enable static website hosting. Use an Amazon CloudFront distribution to cache the content globally.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p><strong>Set up an Amazon CloudFront distribution and configure the existing ALB as the origin. Use dynamic cache settings to reduce latency for global users:</strong> This is correct because CloudFront provides a global content delivery network (CDN) that reduces latency by caching content closer to users. For dynamic content, CloudFront can still improve performance by optimizing requests and routing through its edge locations. This solution is cost-effective and requires minimal architectural changes.</p><p><strong>Configure AWS Global Accelerator to route traffic to the existing ALB and EC2 instances in the Region closest to each user:</strong> This is incorrect because Global Accelerator improves performance for static and dynamic content, but it requires additional costs and does not cache dynamic content at the edge like CloudFront. It is better suited for multi-Region deployments.</p><p><strong>Deploy additional EC2 instances and ALBs in multiple Regions. Use Amazon Route 53 latency-based routing to direct users to the Region with the lowest latency:</strong> This is incorrect because deploying and managing resources across multiple Regions increases cost and operational overhead. For a cost-optimized solution, CloudFront is more efficient as it uses its global edge locations without requiring additional infrastructure.</p><p><strong>Migrate the recommendation engine to Amazon S3 and enable static website hosting. Use an Amazon CloudFront distribution to cache the content globally:</strong> This is incorrect because the recommendation engine serves dynamic content, which cannot be hosted on Amazon S3. S3 is designed for static content like images or HTML files and is not appropriate for processing user-specific dynamic data.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a><br><a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
      "https://aws.amazon.com/global-accelerator/",
      "https://digitalcloud.training/amazon-cloudfront/"
    ]
  },
  {
    "id": 11,
    "question": "<p>A retail company runs an on-premises application that uses Java Spring Boot on Windows servers. The application is resource-intensive and handles customer-facing operations. The company wants to modernize the application by migrating it to a containerized environment running on AWS. The new solution must automatically scale based on Amazon CloudWatch metrics and minimize operational overhead for managing infrastructure.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS App2Container to containerize the application. Deploy the containerized application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using an AWS CloudFormation template.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS App2Container to containerize the application. Deploy the containerized application to Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 instances by using an AWS CloudFormation template.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS App Runner to containerize the application. Deploy the containerized application to Amazon Elastic Kubernetes Service (Amazon EKS) on Amazon EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS App Runner to containerize the application. Use App Runner to automatically deploy and manage the application without using ECS or EC2.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p><strong>Use AWS App Runner to containerize the application. Use App Runner to automatically deploy and manage the application without using ECS or EC2:</strong> This is correct because App Runner simplifies deployment and management of containerized applications. It automatically scales based on demand and integrates with CloudWatch, minimizing operational overhead.</p><p><strong>Use AWS App2Container to containerize the application. Deploy the containerized application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using an AWS CloudFormation template:</strong> This is incorrect because while Fargate reduces operational overhead by eliminating the need to manage EC2 instances, using ECS still requires more configuration and maintenance compared to App Runner.</p><p><strong>Use AWS App2Container to containerize the application. Deploy the containerized application to Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 instances by using an AWS CloudFormation template:</strong> This is incorrect because running ECS on EC2 requires managing the underlying EC2 instances, which increases operational overhead.</p><p><strong>Use AWS App Runner to containerize the application. Deploy the containerized application to Amazon Elastic Kubernetes Service (Amazon EKS) on Amazon EC2 instances:</strong> This is incorrect because EKS involves additional complexity in managing Kubernetes clusters and EC2 instances, making it less suitable for reducing operational overhead.</p><p><strong>References:</strong></p><p><br></p><p><a href=\"https://aws.amazon.com/apprunner/\">https://aws.amazon.com/apprunner/</a></p><p><br></p><p><a href=\"https://aws.amazon.com/app2container/\">https://aws.amazon.com/app2container/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/apprunner/",
      "https://aws.amazon.com/app2container/"
    ]
  },
  {
    "id": 12,
    "question": "<p>A gaming company collects real-time data and stores it in an on-premises database system. The company are migrating to AWS and need better performance for the database. A solutions architect has been asked to recommend an in-memory database that supports data replication.</p><p>Which database should a solutions architect recommend?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon ElastiCache for Redis</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon ElastiCache for Memcached</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon RDS for MySQL</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon RDS for PostgreSQL</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Amazon ElastiCache is an in-memory database. With ElastiCache Memcached there is no data replication or high availability. As you can see in the diagram, each node is a separate partition of data:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_02-01-20-6b625ae592ca124e03dd8f3ac8c2a94d.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_02-01-20-6b625ae592ca124e03dd8f3ac8c2a94d.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Therefore, the Redis engine must be used which does support both data replication and clustering. The following diagram shows a Redis architecture with cluster mode enabled:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_02-01-34-c19e5fe437e8f141a5fe0a655e0990bd.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_02-01-34-c19e5fe437e8f141a5fe0a655e0990bd.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Amazon ElastiCache for Redis\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache for Memcached\" is incorrect as Memcached does not support data replication or high availability.</p><p><strong>INCORRECT:</strong> \"Amazon RDS for MySQL\" is incorrect as this is not an in-memory database.</p><p><strong>INCORRECT:</strong> \"Amazon RDS for PostgreSQL\" is incorrect as this is not an in-memory database.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/elasticache/redis/",
      "https://digitalcloud.training/amazon-elasticache/"
    ]
  },
  {
    "id": 13,
    "question": "<p>An application consists of a web tier in a public subnet and a MySQL cluster hosted on Amazon EC2 instances in a private subnet. The MySQL instances must retrieve product data from a third-party provider over the internet. A Solutions Architect must determine a strategy to enable this access with maximum security and minimum operational overhead.</p><p>What should the Solutions Architect do to meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy a NAT instance in the private subnet. Direct all internet traffic to the NAT instance.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an internet gateway and attach it to the VPC. Modify the private subnet route table to direct internet traffic to the internet gateway.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a virtual private gateway and attach it to the VPC. Modify the private subnet route table to direct internet traffic to the virtual private gateway.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy a NAT gateway in the public subnet. Modify the route table in the private subnet to direct all internet traffic to the NAT gateway.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>The MySQL clusters instances need to access a service on the internet. The most secure method of enabling this access with low operational overhead is to create a NAT gateway. When deploying a NAT gateway, the gateway itself should be deployed in a public subnet whilst the route table in the private subnet must be updated to point traffic to the NAT gateway ID.</p><p>The configuration can be seen in the diagram below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-02-45-d46799d3b11819240a38a0c28fcd0171.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-02-45-d46799d3b11819240a38a0c28fcd0171.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Deploy a NAT gateway in the public subnet. Modify the route table in the private subnet to direct all internet traffic to the NAT gateway\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Deploy a NAT instance in the private subnet. Direct all internet traffic to the NAT instance\" is incorrect. NAT instances require more operational overhead and need to be deployed in a public subnet.</p><p><strong>INCORRECT:</strong> \"Create an internet gateway and attach it to the VPC. Modify the private subnet route table to direct internet traffic to the internet gateway\" is incorrect. You cannot point the instances in the private subnet to an internet gateway as they do not have public IP addresses which is required to use an internet gateway.</p><p><strong>INCORRECT:</strong> \"Create a virtual private gateway and attach it to the VPC. Modify the private subnet route table to direct internet traffic to the virtual private gateway\" is incorrect. A virtual private gateway (VGW) is used with a VPN connection, not for connecting instances in private subnets to the internet.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
      "https://digitalcloud.training/amazon-vpc/"
    ]
  },
  {
    "id": 14,
    "question": "<p>A shared services VPC is being setup for use by several AWS accounts. An application needs to be securely shared from the shared services VPC. The solution should not allow consumers to connect to other instances in the VPC.</p><p>How can this be setup with the least administrative effort? (Select TWO.)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS PrivateLink to expose the application as an endpoint service</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure security groups to restrict access</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a Network Load Balancer (NLB)</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS ClassicLink to expose the application as an endpoint service</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Setup VPC peering between each AWS VPC</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>VPCs can be shared among multiple AWS accounts. Resources can then be shared amongst those accounts. However, to restrict access so that consumers cannot connect to other instances in the VPC the best solution is to use PrivateLink to create an endpoint for the application. The endpoint type will be an interface endpoint and it uses an NLB in the shared services VPC.</p>\n<img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2025-01-31_10-01-18-f1abd64d6af0478b6b261bb031d50696.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2025-01-31_10-01-18-f1abd64d6af0478b6b261bb031d50696.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Create a Network Load Balancer (NLB)\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Use AWS PrivateLink to expose the application as an endpoint service\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS ClassicLink to expose the application as an endpoint service\" is incorrect. ClassicLink allows you to link EC2-Classic instances to a VPC in your account, within the same region. This solution does not include EC2-Classic which is now deprecated (replaced by VPC).</p><p><strong>INCORRECT:</strong> \"Setup VPC peering between each AWS VPC\" is incorrect. VPC peering could be used along with security groups to restrict access to the application and other instances in the VPC. However, this would be administratively difficult as you would need to ensure that you maintain the security groups as resources and addresses change.</p><p><strong>INCORRECT:</strong> \"Configure security groups to restrict access\" is incorrect. This could be used in conjunction with VPC peering but better method is to use PrivateLink for this use case.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-virtual-private-clouds-can-now-be-shared-with-other-aws-accounts/\">https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-virtual-private-clouds-can-now-be-shared-with-other-aws-accounts/</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/\">https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/aws-privatelink.pdf\">https://d1.awsstatic.com/whitepapers/aws-privatelink.pdf</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-virtual-private-clouds-can-now-be-shared-with-other-aws-accounts/",
      "https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/",
      "https://d1.awsstatic.com/whitepapers/aws-privatelink.pdf",
      "https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/"
    ]
  },
  {
    "id": 15,
    "question": "<p>A company’s staff connect from home office locations to administer applications using bastion hosts in a single AWS Region. The company requires a resilient bastion host architecture that requires minimal ongoing operational overhead.</p><p>How can a Solutions Architect best meet these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a Network Load Balancer backed by an Auto Scaling group with instances in multiple AWS Regions.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a Network Load Balancer backed by an Auto Scaling group with instances in multiple Availability Zones.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a Network Load Balancer backed by Reserved Instances in a cluster placement group.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a Network Load Balancer backed by the existing servers in different Availability Zones.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>Bastion hosts (aka “jump hosts”) are EC2 instances in public subnets that administrators and operations staff can connect to from the internet. From the bastion host they are then able to connect to other instances and applications within AWS by using internal routing within the VPC.</p><p>All answers use a Network Load Balancer which is acceptable for forwarding incoming connections to targets. The differences are in where the connections are forwarded to. The best option is to create an Auto Scaling group with EC2 instances in multiple Availability Zones. This creates a resilient architecture within a single AWS Region which is exactly what the question asks for.</p><p><strong>CORRECT: </strong>\"Create a Network Load Balancer backed by an Auto Scaling group with instances in multiple Availability Zones\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer backed by an Auto Scaling group with instances in multiple AWS Regions\" is incorrect. You cannot have instances in an ASG across multiple Regions and you can’t have an NLB distribute connections across multiple Regions.</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer backed by Reserved Instances in a cluster placement group\" is incorrect. A cluster placement group packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly coupled node-to-node communication that is typical of HPC applications.</p><p><strong>INCORRECT:</strong> \"Create a Network Load Balancer backed by the existing servers in different Availability Zones\" is incorrect. An Auto Scaling group is required to maintain instances in different AZs for resilience.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html",
      "https://digitalcloud.training/amazon-ec2-auto-scaling/"
    ]
  },
  {
    "id": 16,
    "question": "<p>A company wants to use Amazon Elastic Container Service (Amazon ECS) to run its containerized application in a hybrid environment. The company needs to ensure that the application can scale across both on-premises and AWS environments. It also requires a load balancer to handle HTTP traffic for the new containers that will run in the AWS Cloud.</p><p>Which combination of actions will meet these requirements? (Select TWO.)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an ECS cluster that uses the Amazon EC2 launch type for the cloud application containers. Use Amazon ECS Anywhere with an AWS Fargate launch type for the on-premises application containers.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an Application Load Balancer for cloud ECS services.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up an ECS cluster that uses the AWS Fargate launch type. Use Fargate for the cloud application containers and the on-premises application containers.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up an ECS cluster that uses the AWS Fargate launch type for the cloud application containers. Use an Amazon ECS Anywhere external launch type for the on-premises application containers.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Set up a Network Load Balancer for cloud ECS services.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Compute",
    "explanation": "<p><strong>Set up an ECS cluster that uses the AWS Fargate launch type for the cloud application containers. Use an Amazon ECS Anywhere external launch type for the on-premises application containers</strong>: This meets the requirement of a hybrid environment by running containers in AWS Fargate for cloud workloads and Amazon ECS Anywhere for on-premises workloads. ECS Anywhere enables on-premises servers to join ECS clusters.</p><p><strong>Set up an Application Load Balancer for cloud ECS services</strong>: This is the appropriate load balancer for handling HTTP traffic, ensuring proper routing and scalability for the cloud-based application containers.</p><p><strong>Set up a Network Load Balancer for cloud ECS services</strong>: This is incorrect because a Network Load Balancer is primarily used for TCP or UDP traffic, not HTTP traffic.</p><p><strong>Set up an ECS cluster that uses the AWS Fargate launch type. Use Fargate for the cloud application containers and the on-premises application containers</strong>: This is incorrect because Fargate cannot be used to run containers in an on-premises environment.</p><p><strong>Set up an ECS cluster that uses the Amazon EC2 launch type for the cloud application containers. Use Amazon ECS Anywhere with an AWS Fargate launch type for the on-premises application containers</strong>: This is incorrect because ECS Anywhere uses external instances, not AWS Fargate, for on-premises environments.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/what-is-ecs.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/what-is-ecs.html</a><br><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-anywhere.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-anywhere.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/what-is-ecs.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-anywhere.html",
      "https://digitalcloud.training/amazon-ecs-and-eks/"
    ]
  },
  {
    "id": 17,
    "question": "<p>A security officer requires that access to company financial reports is logged. The reports are stored in an Amazon S3 bucket. Additionally, any modifications to the log files must be detected.</p><p>Which actions should a solutions architect take?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use S3 server access logging on the bucket that houses the reports with the read and write data events and the log file validation options enabled</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS CloudTrail to create a new trail. Configure the trail to log read and write management events on the S3 bucket that houses the reports. Log these events to a new bucket, and enable log file validation</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS CloudTrail to create a new trail. Configure the trail to log read and write data events on the S3 bucket that houses the reports. Log these events to a new bucket, and enable log file validation</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use S3 server access logging on the bucket that houses the reports with the read and write management events and log file validation options enabled</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>Amazon CloudTrail can be used to log activity on the reports. The key difference between the two answers that include CloudTrail is that one references data events whereas the other references management events.</p><p>Data events provide visibility into the resource operations performed on or within a resource. These are also known as data plane operations. Data events are often high-volume activities.</p><p>Example data events include:</p><p> • Amazon S3 object-level API activity (for example, GetObject, DeleteObject, and PutObject API operations).</p><p> • AWS Lambda function execution activity (the Invoke API).</p><p>Management events provide visibility into management operations that are performed on resources in your AWS account. These are also known as control plane operations. Example management events include:</p><p> • Configuring security (for example, IAM AttachRolePolicy API operations)</p><p> • Registering devices (for example, Amazon EC2 CreateDefaultVpc API operations).</p><p>Therefore, to log data about access to the S3 objects the solutions architect should log read and write data events.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-53-27-925a2b271e999a9b3ac4717b47ed69a3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-53-27-925a2b271e999a9b3ac4717b47ed69a3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>Log file validation can also be enabled on the destination bucket:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-53-38-ab1c8f32e2aec2ce99d13ff85ff2a881.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-53-38-ab1c8f32e2aec2ce99d13ff85ff2a881.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Use AWS CloudTrail to create a new trail. Configure the trail to log read and write data events on the S3 bucket that houses the reports. Log these events to a new bucket, and enable log file validation\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail to create a new trail. Configure the trail to log read and write management events on the S3 bucket that houses the reports. Log these events to a new bucket, and enable log file validation\" is incorrect as data events should be logged rather than management events.</p><p><strong>INCORRECT:</strong> \"Use S3 server access logging on the bucket that houses the reports with the read and write data events and the log file validation options enabled\" is incorrect as server access logging does not have an option for choosing data events or log file validation.</p><p><strong>INCORRECT:</strong> \"Use S3 server access logging on the bucket that houses the reports with the read and write management events and log file validation options enabled\" is incorrect as server access logging does not have an option for choosing management events or log file validation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudtrail/\">https://digitalcloud.training/aws-cloudtrail/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html",
      "https://digitalcloud.training/aws-cloudtrail/"
    ]
  },
  {
    "id": 18,
    "question": "<p>A company runs a business-critical application in the us-east-1 Region. The application uses an Amazon Aurora MySQL database cluster which is 2 TB in size. A Solutions Architect needs to determine a disaster recovery strategy for failover to the us-west-2 Region. The strategy must provide a recovery time objective (RTO) of 10 minutes and a recovery point objective (RPO) of 5 minutes.</p><p>Which strategy will meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a cross-Region Aurora MySQL read replica in us-west-2 Region. Configure an Amazon EventBridge rule that invokes an AWS Lambda function that promotes the read replica in us-west-2 when failure is detected.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Recreate the database as an Aurora multi master cluster across the us-east-1 and us-west-2 Regions with multiple writers to allow read/write capabilities from all database instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a multi-Region Aurora MySQL DB cluster in us-east-1 and us-west-2. Use an Amazon Route 53 health check to monitor us-east-1 and fail over to us-west-2 upon failure.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Recreate the database as an Aurora global database with the primary DB cluster in us-east-1 and a secondary DB cluster in us-west-2. Use an Amazon EventBridge rule that invokes an AWS Lambda function to promote the DB cluster in us-west-2 when failure is detected.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages</p><p>If your primary region suffers a performance degradation or outage, you can promote one of the secondary regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute even in the event of a complete regional outage.</p><p>This provides your application with an effective Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-22-11-2d4e0f2ec825327cf3bcbdbe7905c475.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-22-11-2d4e0f2ec825327cf3bcbdbe7905c475.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Recreate the database as an Aurora global database with the primary DB cluster in us-east-1 and a secondary DB cluster in us-west-2. Use an Amazon EventBridge rule that invokes an AWS Lambda function to promote the DB cluster in us-west-2 when failure is detected\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a multi-Region Aurora MySQL DB cluster in us-east-1 and us-west-2. Use an Amazon Route 53 health check to monitor us-east-1 and fail over to us-west-2 upon failure\" is incorrect. You cannot create a multi-Region Aurora MySQL DB cluster. Options are to create MySQL Replicas (may meet the RTO objectives), or to use global database.</p><p><strong>INCORRECT:</strong> \"Create a cross-Region Aurora MySQL read replica in us-west-2 Region. Configure an Amazon EventBridge rule that invokes an AWS Lambda function that promotes the read replica in us-west-2 when failure is detected\" is incorrect. This may not meet the RTO objectives as large databases may well take more than 10 minutes to promote.</p><p><strong>INCORRECT:</strong> \"Recreate the database as an Aurora multi master cluster across the us-east-1 and us-west-2 Regions with multiple writers to allow read/write capabilities from all database instances\" is incorrect. Multi master only works within a Region it does not work across Regions.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/rds/aurora/global-database/",
      "https://digitalcloud.training/amazon-aurora/"
    ]
  },
  {
    "id": 19,
    "question": "<p>A company requires a high-performance file system that can be mounted on Amazon EC2 Windows instances and Amazon EC2 Linux instances. Applications running on the EC2 instances perform separate processing of the same files and the solution must provide a file system that can be mounted by all instances simultaneously.</p><p>Which solution meets these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Elastic File System (Amazon EFS) with General Purpose performance mode for the Windows instances and the Linux instances.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon FSx for Windows File Server for the Windows instances and the Linux instances.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon FSx for Windows File Server for the Windows instances. Use Amazon FSx for Lustre for the Linux instances. Link both Amazon FSx file systems to the same Amazon S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon FSx for Windows File Server for the Windows instances. Use Amazon Elastic File System (Amazon EFS) with Max I/O performance mode for the Linux instances.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>Amazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system so you can easily move your Windows-based applications that require shared file storage to AWS. You can easily connect Linux instances to the file system by installing the cifs-utils package. The Linux instances can then mount an SMB/CIFS file system.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-13-16-ac3ea6d1bde80de577cab4ac0352a586.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-13-16-ac3ea6d1bde80de577cab4ac0352a586.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Use Amazon FSx for Windows File Server for the Windows instances and the Linux instances\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon FSx for Windows File Server for the Windows instances. Use Amazon Elastic File System (Amazon EFS) with Max I/O performance mode for the Linux instances\" is incorrect. This solution results in two separate file systems and a shared file system is required.</p><p><strong>INCORRECT:</strong> \"Use Amazon Elastic File System (Amazon EFS) with General Purpose performance mode for the Windows instances and the Linux instances\" is incorrect. You cannot use Amazon EFS for Windows instances as this is not supported.</p><p><strong>INCORRECT:</strong> \"Use Amazon FSx for Windows File Server for the Windows instances. Use Amazon FSx for Lustre for the Linux instances. Link both Amazon FSx file systems to the same Amazon S3 bucket\" is incorrect. Amazon FSx for Windows File Server does not use Amazon S3 buckets, so this is another solution that results in separate file systems.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/using-file-shares.html#map-shares-linux\">https://docs.aws.amazon.com/fsx/latest/WindowsGuide/using-file-shares.html#map-shares-linux</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/using-file-shares.html#map-shares-linux",
      "https://digitalcloud.training/amazon-fsx/"
    ]
  },
  {
    "id": 20,
    "question": "<p>A Solutions Architect is designing a solution for an application that requires very low latency between the client and the backend. The application uses the UDP protocol, and the backend is hosted on Amazon EC2 instances. The solution must be highly available across multiple Regions and users around the world should be directed to the most appropriate Region based on performance.</p><p>How can the Solutions Architect meet these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy an Application Load Balancer in front of the EC2 instances in each Region. Use AWS WAF to direct traffic to the most optimal Regional endpoint.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy a Network Load Balancer in front of the EC2 instances in each Region. Use AWS Global Accelerator to route traffic to the most optimal Regional endpoint.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Deploy an Amazon CloudFront distribution with a custom origin pointing to Amazon EC2 instances in multiple Regions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy Amazon EC2 instances in multiple Regions. Create a multivalue answer routing record in Amazon Route 53 that includes all EC2 endpoints.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>An NLB is ideal for latency-sensitive applications and can listen on UDP for incoming requests. As Elastic Load Balancers are region-specific it is necessary to have an NLB in each Region in front of the EC2 instances.</p><p>To direct traffic based on optimal performance, AWS Global Accelerator can be used. GA will ensure traffic is routed across the AWS global network to the most optimal endpoint based on performance.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-37-03-06957cedd24132e515e0704262ea64fe.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-37-03-06957cedd24132e515e0704262ea64fe.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Deploy a Network Load Balancer in front of the EC2 instances in each Region. Use AWS Global Accelerator to route traffic to the most optimal Regional endpoint\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Deploy an Application Load Balancer in front of the EC2 instances in each Region. Use AWS WAF to direct traffic to the most optimal Regional endpoint\" is incorrect. You cannot use WAF to direct traffic to endpoints based on performance.</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon CloudFront distribution with a custom origin pointing to Amazon EC2 instances in multiple Regions\" is incorrect. CloudFront cannot listen on UDP, it is used for HTTP/HTTPS.</p><p><strong>INCORRECT:</strong> \"Deploy Amazon EC2 instances in multiple Regions. Create a multivalue answer routing record in Amazon Route 53 that includes all EC2 endpoints\" is incorrect. This configuration would not route incoming requests to the most optimal endpoint based on performance, it would provide multiple records in answers and traffic would be distributed across multiple Regions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-how-it-works.html\">https://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-how-it-works.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-global-accelerator/\">https://digitalcloud.training/aws-global-accelerator/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-how-it-works.html",
      "https://digitalcloud.training/aws-global-accelerator/"
    ]
  },
  {
    "id": 21,
    "question": "<p>A media streaming company stores user activity logs in an Amazon S3 bucket. The logs are accessed frequently for real-time analytics and reporting. The company enforces strict encryption requirements for data stored in S3 and currently uses AWS Key Management Service (AWS KMS) for encryption.</p><p>The company wants to reduce costs related to encrypting objects in the S3 bucket while maintaining compliance with its encryption requirements and minimizing the number of AWS KMS calls.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use server-side encryption with customer-provided encryption keys (SSE-C) and store the keys in AWS Secrets Manager.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use client-side encryption with AWS KMS customer-managed keys to encrypt the data before uploading it to S3.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use server-side encryption with Amazon S3 managed keys (SSE-S3) to eliminate AWS KMS usage.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable S3 Bucket Key for server-side encryption with AWS KMS keys (SSE-KMS) on the objects to reduce the cost of KMS requests.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p><strong>Enable S3 Bucket Key for server-side encryption with AWS KMS keys (SSE-KMS) on the objects to reduce the cost of KMS requests:</strong> This is correct because enabling S3 Bucket Key reduces the number of AWS KMS calls required for object encryption. S3 Bucket Key caches the encryption keys at the bucket level, significantly lowering the cost of encryption for frequently accessed objects while meeting encryption requirements.</p><p><strong>Use server-side encryption with Amazon S3 managed keys (SSE-S3) to eliminate AWS KMS usage:</strong> This is incorrect because SSE-S3 does not use AWS KMS, but it does not meet the strict encryption requirements specified by the company, which mandate the use of AWS KMS.</p><p><strong>Use client-side encryption with AWS KMS customer-managed keys to encrypt the data before uploading it to S3:</strong> This is incorrect because client-side encryption increases operational complexity and does not optimize costs associated with AWS KMS calls.</p><p><strong>Use server-side encryption with customer-provided encryption keys (SSE-C) and store the keys in AWS Secrets Manager:</strong> This is incorrect because SSE-C requires the company to manage its own encryption keys, introducing operational overhead. Additionally, this approach does not optimize costs related to AWS KMS.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html</a><br><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html",
      "https://digitalcloud.training/aws-kms/"
    ]
  },
  {
    "id": 22,
    "question": "<p>A company has created a duplicate of its environment in another AWS Region. The application is running in warm standby mode. There is an Application Load Balancer (ALB) in front of the application. Currently, failover is manual and requires updating a DNS alias record to point to the secondary ALB.</p><p>How can a solutions architect automate the failover process?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a CNAME record on Amazon Route 53 pointing to the ALB endpoint</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable an Amazon Route 53 health check</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a latency based routing policy on Amazon Route 53</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable an ALB health check</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>You can use Route 53 to check the health of your resources and only return healthy resources in response to DNS queries. There are three types of DNS failover configurations:</p><p>1. Active-passive: Route 53 actively returns a primary resource. In case of failure, Route 53 returns the backup resource. Configured using a failover policy.</p><p>2. Active-active: Route 53 actively returns more than one resource. In case of failure, Route 53 fails back to the healthy resource. Configured using any routing policy besides failover.</p><p>3. Combination: Multiple routing policies (such as latency-based, weighted, etc.) are combined into a tree to configure more complex DNS failover.</p><p>In this case an alias already exists for the secondary ALB. Therefore, the solutions architect just needs to enable a failover configuration with an Amazon Route 53 health check.</p><p>The configuration would look something like this:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_02-03-25-80669d24056e76a9d92f8db887b73ccc.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_02-03-25-80669d24056e76a9d92f8db887b73ccc.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Enable an Amazon Route 53 health check\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable an ALB health check\" is incorrect. The point of an ALB health check is to identify the health of targets (EC2 instances). It cannot redirect clients to another Region.</p><p><strong>INCORRECT:</strong> \"Create a CNAME record on Amazon Route 53 pointing to the ALB endpoint\" is incorrect as an Alias record already exists and is better for mapping to an ALB.</p><p><strong>INCORRECT:</strong> \"Create a latency based routing policy on Amazon Route 53\" is incorrect as this will only take into account latency, it is not used for failover.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/route-53-dns-health-checks/\">https://aws.amazon.com/premiumsupport/knowledge-center/route-53-dns-health-checks/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/route-53-dns-health-checks/",
      "https://digitalcloud.training/amazon-route-53/"
    ]
  },
  {
    "id": 23,
    "question": "<p>A financial services company needs to set up an Amazon RDS Multi-AZ database to store customer transaction records. The database will serve as the backend for an on-premises financial analysis application. The company requires the on-premises application to connect directly to the RDS database when employees are working from the office.</p><p>The company must ensure the connection is established securely and efficiently.</p><p>Which solution provides the required connectivity MOST securely?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a VPC with two public subnets. Deploy the RDS database in the public subnets. Configure an AWS Direct Connect connection between the on-premises office and the VPC for low-latency access.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a VPC with two private subnets. Deploy the RDS database in the private subnets. Establish connectivity between the on-premises office and AWS using AWS Site-to-Site VPN with a customer gateway.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a VPC with two private subnets. Deploy the RDS database in the private subnets. Configure RDS security groups to allow the on-premises office IP ranges to access the database directly over the internet.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a VPC with two public subnets. Deploy the RDS database in the public subnets. Use AWS Client VPN to establish secure connectivity between employees’ desktops and the database.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p><strong>Create a VPC with two private subnets. Deploy the RDS database in the private subnets. Establish connectivity between the on-premises office and AWS using AWS Site-to-Site VPN with a customer gateway:</strong></p><p>This is correct because placing the RDS database in private subnets ensures it is not publicly accessible. Using AWS Site-to-Site VPN securely connects the on-premises office to the VPC, allowing direct connectivity to the database while maintaining security.</p><p><strong>Create a VPC with two public subnets. Deploy the RDS database in the public subnets. Configure an AWS Direct Connect connection between the on-premises office and the VPC for low-latency access:</strong></p><p>This is incorrect because placing the RDS database in public subnets exposes it to the internet, violating security best practices. While Direct Connect provides low latency, public subnets do not meet the company's security requirements.</p><p><strong>Create a VPC with two private subnets. Deploy the RDS database in the private subnets. Configure RDS security groups to allow the on-premises office IP ranges to access the database directly over the internet:</strong></p><p>This is incorrect because private subnets do not have direct internet access, and accessing the database over the internet introduces security vulnerabilities, even with security group rules.</p><p><strong>Create a VPC with two public subnets. Deploy the RDS database in the public subnets. Use AWS Client VPN to establish secure connectivity between employees’ desktops and the database:</strong></p><p>This is incorrect because deploying the database in public subnets unnecessarily exposes it to potential threats. Client VPN is a good solution for employee access but does not address the requirement to connect securely from an on-premises office.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_Overview.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_Overview.html</a><br><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_Overview.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.html",
      "https://digitalcloud.training/amazon-vpc/"
    ]
  },
  {
    "id": 24,
    "question": "<p>A company has deployed an API in a VPC behind an internal Network Load Balancer (NLB). An application that consumes the API as a client is deployed in a second account in private subnets.</p><p>Which architectural configurations will allow the API to be consumed without using the public Internet? (Select TWO.)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure a VPC peering connection between the two VPCs. Access the API using the private address</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure an AWS Direct Connect connection between the two VPCs. Access the API using the private address</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure a ClassicLink connection for the API into the client VPC. Access the API using the ClassicLink address</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a PrivateLink connection for the API into the client VPC. Access the API using the PrivateLink address</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Configure an AWS Resource Access Manager connection between the two accounts. Access the API using the private address</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>You can create your own application in your VPC and configure it as an AWS PrivateLink-powered service (referred to as an <em>endpoint service</em>). Other AWS principals can create a connection from their VPC to your endpoint service using an <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html\">interface VPC endpoint</a>. You are the <em>service provider</em>, and the AWS principals that create connections to your service are <em>service consumers</em>.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-18_13-30-56-6e12fed09086b40d3a54c6810873d3c2.JPG\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-18_13-30-56-6e12fed09086b40d3a54c6810873d3c2.JPG\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>This configuration is powered by AWS PrivateLink and clients do not need to use an internet gateway, NAT device, VPN connection or AWS Direct Connect connection, nor do they require public IP addresses.</p><p>Another option is to use a VPC Peering connection. A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account.</p><p><strong>CORRECT: </strong>\"Configure a VPC peering connection between the two VPCs. Access the API using the private address\" is a correct answer.</p><p><strong>CORRECT: </strong>\"Configure a PrivateLink connection for the API into the client VPC. Access the API using the PrivateLink address\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Configure an AWS Direct Connect connection between the two VPCs. Access the API using the private address\" is incorrect. Direct Connect is used for connecting from on-premises data centers into AWS. It is not used from one VPC to another.</p><p><strong>INCORRECT:</strong> \"Configure a ClassicLink connection for the API into the client VPC. Access the API using the ClassicLink address\" is incorrect. ClassicLink allows you to link EC2-Classic instances to a VPC in your account, within the same Region. This is not relevant to sending data between two VPCs.</p><p><strong>INCORRECT:</strong> \"Configure an AWS Resource Access Manager connection between the two accounts. Access the API using the private address\" is incorrect. AWS RAM lets you share resources that are provisioned and managed in other AWS services. However, APIs are not shareable resources with AWS RAM.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service.html\">https://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service.html",
      "https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
      "https://digitalcloud.training/amazon-vpc/"
    ]
  },
  {
    "id": 25,
    "question": "<p>An application has multiple components for receiving requests that must be processed and subsequently processing the requests. The company requires a solution for decoupling the application components. The application receives around 10,000 requests per day and requests can take up to 2 days to process. Requests that fail to process must be retained.</p><p>Which solution meets these requirements most efficiently?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon DynamoDB table and enable DynamoDB streams. Configure the processing component to process requests from the stream.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use an Amazon Kinesis data stream to decouple application components and integrate the processing component with the Kinesis Client Library (KCL).</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Decouple the application components with an Amazon SQS queue. Configure a dead-letter queue to collect the requests that failed to process.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Decouple the application components with an Amazon SQS Topic. Configure the receiving component to subscribe to the SNS Topic.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>The Amazon Simple Queue Service (SQS) is ideal for decoupling the application components. Standard queues can support up to 120,000 in flight messages and messages can be retained for up to 14 days in the queue.</p><p>To ensure the retention of requests (messages) that fail to process, a dead-letter queue can be configured. Messages that fail to process are sent to the dead-letter queue (based on the redrive policy) and can be subsequently dealt with.</p><p><strong>CORRECT: </strong>\"Decouple the application components with an Amazon SQS queue. Configure a dead-letter queue to collect the requests that failed to process\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Decouple the application components with an Amazon SQS Topic. Configure the receiving component to subscribe to the SNS Topic\" is incorrect. SNS does not store requests, it immediately forwards all notifications to subscribers.</p><p><strong>INCORRECT:</strong> \"Use an Amazon Kinesis data stream to decouple application components and integrate the processing component with the Kinesis Client Library (KCL)\" is incorrect. This is a less efficient solution and will likely be less cost-effective compared to using Amazon SQS. There is also no option for retention of requests that fail to process.</p><p><strong>INCORRECT:</strong> \"Create an Amazon DynamoDB table and enable DynamoDB streams. Configure the processing component to process requests from the stream\" is incorrect. This solution does not offer any way of retaining requests that fail to process of removal of items from the table and is therefore less efficient.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/sqs/faqs/",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 26,
    "question": "<p>A healthcare startup is building a cloud-based patient management system on AWS. The system processes sensitive health data and uses Amazon RDS for the database, Amazon S3 for storing medical reports, and AWS Lambda for processing event-driven workflows triggered by S3 Event Notifications.</p><p>The startup uses AWS IAM Identity Center to manage user authentication. The development, testing, and operations teams need secure access to RDS and S3 while ensuring compliance with healthcare regulations that mandate least privilege access and centralized access control.</p><p>Which solution meets these requirements with the LEAST operational overhead?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Organizations to create separate accounts for development, testing, and operations teams. Apply Service Control Policies (SCPs) to restrict access at the account level. Use cross-account IAM roles to grant granular permissions for RDS and S3 based on team needs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure an Amazon Cognito user pool to authenticate team members. Use a custom Lambda function to generate temporary credentials for RDS and S3 access. Implement role-based access controls within the Lambda function to enforce least privilege.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS IAM Identity Center integrated with the startup's existing Active Directory. Create permission sets with fine-grained permissions for RDS and S3. Assign team members to appropriate groups in Active Directory, which map to Identity Center permission sets.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create separate IAM users for all team members. Assign each user predefined managed policies with RDS and S3 permissions. Use IAM Access Analyzer to review permissions periodically to ensure compliance with least privilege principles.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p><strong>Use AWS IAM Identity Center integrated with the startup's existing Active Directory. Create permission sets with fine-grained permissions for RDS and S3. Assign team members to appropriate groups in Active Directory, which map to Identity Center permission sets:</strong> This is correct because IAM Identity Center enables centralized user management and integrates seamlessly with Active Directory. It allows the creation of permission sets that enforce least privilege while minimizing operational overhead by using existing group structures.</p><p><strong>Configure an Amazon Cognito user pool to authenticate team members. Use a custom Lambda function to generate temporary credentials for RDS and S3 access. Implement role-based access controls within the Lambda function to enforce least privilege:</strong> This is incorrect because Cognito is primarily designed for managing end-user authentication, not for managing internal team access to AWS resources. Using Cognito and a custom Lambda function would add unnecessary complexity and operational overhead.</p><p><strong>Create separate IAM users for all team members. Assign each user predefined managed policies with RDS and S3 permissions. Use IAM Access Analyzer to review permissions periodically to ensure compliance with least privilege principles:</strong> This is incorrect because managing individual IAM users introduces significant administrative overhead. IAM Identity Center is a more scalable and centralized solution.</p><p><strong>Use AWS Organizations to create separate accounts for development, testing, and operations teams. Apply Service Control Policies (SCPs) to restrict access at the account level. Use cross-account IAM roles to grant granular permissions for RDS and S3 based on team needs:</strong> This is incorrect because creating separate accounts and managing cross-account IAM roles adds unnecessary complexity. IAM Identity Center provides a more efficient solution for centralized access control within a single account.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html</a><br><a href=\"https://aws.amazon.com/cognito/\">https://aws.amazon.com/cognito/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html",
      "https://aws.amazon.com/cognito/",
      "https://digitalcloud.training/aws-iam/"
    ]
  },
  {
    "id": 27,
    "question": "<p>A research organization runs its photo analysis application on AWS. The application processes images uploaded by field scientists and stores them temporarily on an Amazon EC2 instance's locally attached Amazon Elastic Block Store (Amazon EBS) volume. Every evening, the processed images are uploaded to an Amazon S3 bucket for long-term storage.</p><p>The solutions architect has discovered that the images are being uploaded to S3 through the public internet. The organization wants to ensure that the upload traffic to Amazon S3 remains private and does not use the public internet.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy a NAT gateway in the VPC. Configure the EC2 instance's security group to allow outbound traffic to the NAT gateway, which will route traffic to the S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use an Amazon S3 access point for the EC2 instance. Configure the photo analysis application to upload files to the bucket through the access point.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a gateway VPC endpoint for the S3 bucket. Update the VPC's route table to route all S3 traffic through the gateway endpoint.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure a VPC peering connection between the VPC containing the EC2 instance and Amazon S3. Update the route table to use the peering connection for traffic to S3.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p><strong>Create a gateway VPC endpoint for the S3 bucket. Update the VPC's route table to route all S3 traffic through the gateway endpoint:</strong> This is correct because a gateway VPC endpoint establishes a private connection to Amazon S3 without using the public internet. Updating the route table ensures all traffic to S3 is routed through this private endpoint. This solution is secure and cost-effective.<br><strong>Use an Amazon S3 access point for the EC2 instance. Configure the photo analysis application to upload files to the bucket through the access point:</strong> This is incorrect because while S3 access points simplify access management for specific applications, they do not provide private connectivity. A gateway VPC endpoint is required to route traffic privately.<br><strong>Configure a VPC peering connection between the VPC containing the EC2 instance and Amazon S3. Update the route table to use the peering connection for traffic to S3:</strong> This is incorrect because VPC peering does not provide connectivity to AWS services like Amazon S3. Gateway VPC endpoints are the correct mechanism for private S3 connectivity.<br><strong>Deploy a NAT gateway in the VPC. Configure the EC2 instance's security group to allow outbound traffic to the NAT gateway, which will route traffic to the S3 bucket:</strong> This is incorrect because a NAT gateway routes traffic to the internet, and S3 traffic would still use public endpoints. This does not meet the requirement of avoiding the public internet.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a><br><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html",
      "https://digitalcloud.training/amazon-vpc/"
    ]
  },
  {
    "id": 28,
    "question": "<p>A Solutions Architect working for a large financial institution is building an application to manage their customers financial information and their sensitive personal information. The Solutions Architect requires that the storage layer can store immutable data out of the box, with the ability to encrypt the data at rest and requires that the storage layer provides ACID properties. They also want to use a containerized solution to manage the compute layer.</p><p>Which solution will meet these requirements with the LEAST amount of operational overhead?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure an ECS cluster on EC2 behind an Application Load Balancer within an Auto Scaling Group. Store data using Amazon DynamoDB.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a cluster of ECS instances on AWS Fargate within an Auto Scaling Group behind an Application Load Balancer. To manage the storage layer, use Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up an ECS cluster behind an Application Load Balancer on AWS Fargate. Use Amazon Quantum Ledger Database (QLDB) to manage the storage layer.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Auto Scaling Group with EC2 instances behind an Application Load Balancer. To manage the storage layer, use Amazon S3.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>The solution requires that the storage layer be immutable. This immutability can only be delivered by Amazon Quantum Ledger Database (QLDB), as Amazon QLDB has a built-in immutable journal that stores an accurate and sequenced entry of every data change. The journal is append-only, meaning that data can only be added to a journal, and it cannot be overwritten or deleted.<br>Secondly the compute layer needs to not only be containerized, and implemented with the least possible operational overhead. The option that best fits these requirements is Amazon ECS on AWS Fargate, as AWS Fargate is a Serverless, containerized deployment option.</p><p><strong>CORRECT: </strong>\"Set up an ECS cluster behind an Application Load Balancer on AWS Fargate. Use Amazon Quantum Ledger Database (QLDB) to manage the storage layer” is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling Group with EC2 instances behind an Application Load Balancer. To manage the storage layer, use Amazon S3” is incorrect. EC2 instances are virtual machines, not a container product and Amazon S3 is an object storage service which does not act as an immutable storage layer.</p><p><strong>INCORRECT:</strong> \"Configure an ECS cluster on EC2 behind an Application Load Balancer within an Auto Scaling Group. Store data using Amazon DynamoDB” is incorrect. ECS on EC2 provides a higher level of operational overhead than using AWS Fargate, as Fargate is a Serverless service.</p><p><strong>INCORRECT:</strong> \"Create a cluster of ECS instances on AWS Fargate within an Auto Scaling Group behind an Application Load Balancer. To manage the storage layer, use Amazon S3” is incorrect. Although Fargate would be a suitable deployment option, Amazon S3 is not suitable for the storage layer as it is not immutable by default.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/qldb/features/\">https://aws.amazon.com/qldb/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-associate/aws-database-saa/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-associate/aws-database-saa/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/qldb/features/",
      "https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-associate/aws-database-saa/"
    ]
  },
  {
    "id": 29,
    "question": "<p>A financial services company runs a credit evaluation system in a private subnet behind an Application Load Balancer (ALB) in a VPC. The VPC includes a NAT gateway and an internet gateway. The system analyzes customer credit data and uploads the results to Amazon S3 for reporting.</p><p>The company has strict regulatory requirements stating that all data traffic must remain within AWS’s private network and must not traverse the public internet. Additionally, the company wants to implement a cost-effective solution while ensuring compliance.</p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable S3 Transfer Acceleration for faster uploads and downloads while restricting access to trusted IP addresses.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a VPN connection between the VPC and Amazon S3 to ensure secure communication without public internet traffic.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure an S3 gateway endpoint. Update the route table of the private subnet to direct S3 traffic through the endpoint.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure an S3 interface endpoint. Attach a security group to the endpoint that allows the application to send traffic to Amazon S3 securely.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p><strong>Configure an S3 gateway endpoint. Update the route table of the private subnet to direct S3 traffic through the endpoint:</strong> This is correct because an S3 gateway endpoint enables private and secure communication between the VPC and Amazon S3, ensuring no traffic leaves the AWS network. It is also a cost-effective solution because gateway endpoints do not require additional infrastructure like NAT gateways.</p><p><strong>Configure an S3 interface endpoint. Attach a security group to the endpoint that allows the application to send traffic to Amazon S3 securely:</strong> This is incorrect because S3 interface endpoints are designed for use cases involving VPC security group control but are typically more expensive than gateway endpoints. Gateway endpoints are purpose-built for S3 and DynamoDB, offering a simpler and more cost-effective solution.</p><p><strong>Enable S3 Transfer Acceleration for faster uploads and downloads while restricting access to trusted IP addresses:</strong> This is incorrect because S3 Transfer Acceleration is designed to improve performance for global uploads and downloads but does not ensure that traffic avoids the public internet. This does not meet the regulatory requirement of keeping traffic private.</p><p><strong>Create a VPN connection between the VPC and Amazon S3 to ensure secure communication without public internet traffic:</strong> This is incorrect because Amazon S3 does not support direct VPN connections. Gateway endpoints are the proper mechanism for secure, private communication with S3.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html",
      "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html",
      "https://digitalcloud.training/amazon-vpc/"
    ]
  },
  {
    "id": 30,
    "question": "<p>A company has created an application that stores sales performance data in an Amazon DynamoDB table. A web application is being created to display the data. A Solutions Architect must design the web application using managed services that require minimal operational maintenance.</p><p>Which architectures meet these requirements? (Select TWO.)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>An Amazon Route 53 hosted zone routes requests to an AWS Lambda endpoint to invoke a Lambda function that reads data from the DynamoDB table.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>An Elastic Load Balancer forwards requests to a target group of Amazon EC2 instances. The EC2 instances run an application that reads data from the DynamoDB table.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>An Amazon API Gateway REST API invokes an AWS Lambda function. The Lambda function reads data from the DynamoDB table.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>An Elastic Load Balancer forwards requests to a target group with the DynamoDB table configured as the target.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>An Amazon API Gateway REST API directly accesses the sales performance data in the DynamoDB table.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>There are two architectures here that fulfill the requirement to create a web application that displays the data from the DynamoDB table.</p><p>The first one is to use an API Gateway REST API that invokes an AWS Lambda function. A Lambda proxy integration can be used, and this will proxy the API requests to the Lambda function which processes the request and accesses the DynamoDB table.</p><p>The second option is to use an API Gateway REST API to directly access the sales performance data. In this case a proxy for the DynamoDB query API can be created using a method in the REST API.</p><p><strong>CORRECT: </strong>\"An Amazon API Gateway REST API invokes an AWS Lambda function. The Lambda function reads data from the DynamoDB table\" is a correct answer.</p><p><strong>CORRECT: </strong>\"An Amazon API Gateway REST API directly accesses the sales performance data in the DynamoDB table\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"An Amazon Route 53 hosted zone routes requests to an AWS Lambda endpoint to invoke a Lambda function that reads data from the DynamoDB table\" is incorrect. An Alias record could be created in a hosted zone but a hosted zone itself does not route to a Lambda endpoint. Using an Alias, it is possible to route to a VPC endpoint that uses a Lambda function however there would not be a web front end so a REST API would be preferable.</p><p><strong>INCORRECT:</strong> \"An Elastic Load Balancer forwards requests to a target group with the DynamoDB table configured as the target\" is incorrect. You cannot configure DynamoDB as a target in a target group.</p><p><strong>INCORRECT:</strong> \"An Elastic Load Balancer forwards requests to a target group of Amazon EC2 instances. The EC2 instances run an application that reads data from the DynamoDB table\" is incorrect. This would not offer low operational maintenance as you must manage the EC2 instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/using-amazon-api-gateway-as-a-proxy-for-dynamodb/\">https://aws.amazon.com/blogs/compute/using-amazon-api-gateway-as-a-proxy-for-dynamodb/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html",
      "https://aws.amazon.com/blogs/compute/using-amazon-api-gateway-as-a-proxy-for-dynamodb/",
      "https://digitalcloud.training/amazon-dynamodb/",
      "https://digitalcloud.training/aws-lambda/"
    ]
  },
  {
    "id": 31,
    "question": "<p>A company has a Production VPC and a Pre-Production VPC. The Production VPC uses VPNs through a customer gateway to connect to a single device in an on-premises data center. The Pre-Production VPC uses a virtual private gateway attached to two AWS Direct Connect (DX) connections. Both VPCs are connected using a single VPC peering connection.</p><p>How can a Solutions Architect improve this architecture to remove any single point of failure?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add a second virtual private gateway and attach it to the Production VPC.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add an additional VPC peering connection between the two VPCs.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add additional VPNs to the Production VPC from a second customer gateway device.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Add a set of VPNs between the Production and Pre-Production VPCs.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>The only single point of failure in this architecture is the customer gateway device in the on-premises data center. A customer gateway device is the on-premises (client) side of the connection into the VPC. The customer gateway configuration is created within AWS, but the actual device is a physical or virtual device running in the on-premises data center. If this device is a single device, then if it fails the VPN connections will fail. The AWS side of the VPN link is the virtual private gateway, and this is a redundant device.</p><p><strong>CORRECT: </strong>\"Add additional VPNs to the Production VPC from a second customer gateway device\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Add an additional VPC peering connection between the two VPCs\" is incorrect. VPC peering connections are already redundant, you do not need multiple connections.</p><p><strong>INCORRECT:</strong> \"Add a set of VPNs between the Production and Pre-Production VPCs\" is incorrect. You cannot create VPN connections between VPCs (using AWS VPNs).</p><p><strong>INCORRECT:</strong> \"Add a second virtual private gateway and attach it to the Production VPC\" is incorrect. Virtual private gateways (VGWs) are redundant devices so a second one is not necessary.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html",
      "https://digitalcloud.training/amazon-vpc/"
    ]
  },
  {
    "id": 32,
    "question": "<p>A video editing company processes high-resolution footage for its clients. Each video file is several terabytes in size and needs to undergo intensive editing, such as applying filters and color grading, before delivery. Processing each video takes up to 25 minutes.</p><p>The company needs a solution that can scale to handle increased demand during peak periods while remaining cost-effective. The processed videos must be accessible for a minimum of 90 days.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an on-premises video processing server connected to AWS Storage Gateway to store and retrieve video files from Amazon S3. Use Amazon RDS for metadata and configure Storage Gateway for caching frequently accessed data.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). Use Amazon Simple Queue Service (Amazon SQS) to queue incoming video processing jobs. Store metadata in Amazon RDS, and store processed videos in Amazon S3 Glacier Flexible Retrieval for long-term storage.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to run containerized video editing tasks. Store metadata in Amazon DynamoDB and processed video files in Amazon S3 Standard-IA for reduced costs.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Batch to orchestrate video editing jobs on Spot Instances. Store metadata in Amazon ElastiCache for Redis and processed video files in Amazon S3 Intelligent-Tiering.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p><strong>Use AWS Batch to orchestrate video editing jobs on Spot Instances. Store metadata in Amazon ElastiCache for Redis and processed video files in Amazon S3 Intelligent-Tiering:</strong> This is correct because AWS Batch handles batch processing workloads efficiently, and Spot Instances reduce compute costs. ElastiCache ensures low-latency metadata access during processing, and S3 Intelligent-Tiering reduces storage costs while keeping processed videos accessible.</p><p><strong>Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to run containerized video editing tasks. Store metadata in Amazon DynamoDB and processed video files in Amazon S3 Standard-IA for reduced costs:</strong> This is incorrect because while ECS with Fargate is scalable and serverless, AWS Batch is better suited for batch processing workloads like video editing.</p><p><strong>Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). Use Amazon Simple Queue Service (Amazon SQS) to queue incoming video processing jobs. Store metadata in Amazon RDS, and store processed videos in Amazon S3 Glacier Flexible Retrieval for long-term storage:</strong> This is incorrect because S3 Glacier Flexible Retrieval is designed for archival data and is not suitable for videos that need to be accessed within 90 days. Additionally, managing EC2 instances increases operational overhead compared to serverless options.</p><p><strong>Use an on-premises video processing server connected to AWS Storage Gateway to store and retrieve video files from Amazon S3. Use Amazon RDS for metadata and configure Storage Gateway for caching frequently accessed data:</strong> This is incorrect because managing on-premises infrastructure introduces significant complexity and overhead. AWS-native services like Batch provide a more scalable and cost-effective solution for this workload.</p><p><strong>References:</strong></p><p><br></p><p><a href=\"https://aws.amazon.com/batch/\">https://aws.amazon.com/batch/</a></p><p><br></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/batch/",
      "https://aws.amazon.com/s3/storage-classes/"
    ]
  },
  {
    "id": 33,
    "question": "<p>A scientific research organization runs an on-premises simulation application that processes large datasets. The organization has migrated all simulation data to Amazon S3 to reduce costs. The simulation application requires low-latency storage access for seamless performance during processing tasks.</p><p>The organization needs to design a storage solution that minimizes costs while maintaining the performance requirements of the application.</p><p>Which storage solution will meet these requirements in the MOST cost-effective way?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS DataSync to copy frequently accessed data from Amazon S3 to an on-premises storage system. Configure the application to use the local storage for low-latency access.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy a high-speed internet connection and configure the on-premises application to access the data directly from Amazon S3 using the S3 API for storage operations.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Copy the data from Amazon S3 to Amazon FSx for Lustre. Use an Amazon FSx File Gateway to provide low-latency access for the on-premises application.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon S3 File Gateway to provide low-latency storage for the on-premises application. The File Gateway will cache frequently accessed data locally.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p><strong>Use Amazon S3 File Gateway to provide low-latency storage for the on-premises application. The File Gateway will cache frequently accessed data locally:</strong> This is correct because S3 File Gateway provides a seamless and cost-effective way to access data stored in Amazon S3. It locally caches frequently accessed data, reducing latency while still leveraging the cost benefits of S3 storage.</p><p><strong>Use AWS DataSync to copy frequently accessed data from Amazon S3 to an on-premises storage system. Configure the application to use the local storage for low-latency access:</strong> This is incorrect because DataSync is designed for batch data transfer and is not suitable for providing continuous, low-latency access to S3 data. It introduces additional complexity and costs for managing on-premises storage systems.</p><p><strong>Copy the data from Amazon S3 to Amazon FSx for Lustre. Use an Amazon FSx File Gateway to provide low-latency access for the on-premises application:</strong> This is incorrect because FSx for Lustre is optimized for high-performance workloads but is more expensive and introduces unnecessary complexity compared to S3 File Gateway. Additionally, copying all data to FSx increases storage costs.</p><p><strong>Deploy a high-speed internet connection and configure the on-premises application to access the data directly from Amazon S3 using the S3 API for storage operations:</strong> This is incorrect because accessing data directly from S3 over the internet does not meet the low-latency requirements for the application. While cost-effective, it will degrade performance for latency-sensitive workloads.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html</a><br><a href=\"https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html\">https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html",
      "https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html",
      "https://digitalcloud.training/aws-storage-gateway/"
    ]
  },
  {
    "id": 34,
    "question": "<p>A company has on-premises file servers that include both Windows SMB and Linux NFS protocols. The company plans to migrate to AWS and consolidate these file servers into a managed cloud solution. The chosen solution must support both NFS and SMB access, provide protocol sharing, and offer redundancy at the Availability Zone level.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon FSx for NetApp ONTAP to consolidate storage and enable multi-protocol access for both SMB and NFS.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create two Amazon EC2 instances with locally attached storage: one instance for SMB access and the other instance for NFS access.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy Amazon FSx for Windows File Server for SMB access and Amazon FSx for OpenZFS for NFS access.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon S3 for storage and deploy an Amazon S3 File Gateway for on-premises access to both SMB and NFS clients.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p><strong>Use Amazon FSx for NetApp ONTAP to consolidate storage and enable multi-protocol access for both SMB and NFS:</strong> This is correct because Amazon FSx for NetApp ONTAP supports both SMB and NFS protocols with multi-protocol sharing and redundancy across Availability Zones.</p><p><strong>Deploy Amazon FSx for Windows File Server for SMB access and Amazon FSx for OpenZFS for NFS access:</strong> This is incorrect because this approach separates SMB and NFS access, does not provide protocol sharing, and introduces additional complexity.</p><p><strong>Use Amazon S3 for storage and deploy an Amazon S3 File Gateway for on-premises access to both SMB and NFS clients:</strong> This is incorrect because Amazon S3 File Gateway does not natively support SMB or NFS protocols for simultaneous access.</p><p><strong>Create two Amazon EC2 instances with locally attached storage: one instance for SMB access and the other instance for NFS access:</strong> This is incorrect because this solution is not a managed service, lacks redundancy at the Availability Zone level, and increases operational overhead.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html\">https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html</a><br><a href=\"https://aws.amazon.com/fsx/netapp-ontap/\">https://aws.amazon.com/fsx/netapp-ontap/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html",
      "https://aws.amazon.com/fsx/netapp-ontap/",
      "https://digitalcloud.training/amazon-fsx/"
    ]
  },
  {
    "id": 35,
    "question": "<p>An organization is extending a secure development environment into AWS. They have already secured the VPC including removing the Internet Gateway and setting up a Direct Connect connection. What else needs to be done to add encryption?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable IPSec encryption on the Direct Connect connection</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Setup the Border Gateway Protocol (BGP) with encryption</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure an AWS Direct Connect Gateway</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Setup a Virtual Private Gateway (VPG)</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>A VPG is used to setup an AWS VPN which you can use in combination with Direct Connect to encrypt all data that traverses the Direct Connect link. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections.</p><p><strong>CORRECT: </strong>\"Setup a Virtual Private Gateway (VPG)\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable IPSec encryption on the Direct Connect connection\" is incorrect. There is no option to enable IPSec encryption on the Direct Connect connection.</p><p><strong>INCORRECT:</strong> \"Setup the Border Gateway Protocol (BGP) with encryption\" is incorrect. The BGP protocol is not used to enable encryption for Direct Connect, it is used for routing.</p><p><strong>INCORRECT:</strong> \"Configure an AWS Direct Connect Gateway\" is incorrect. An AWS Direct Connect Gateway is used to connect to VPCs across multiple AWS regions. It is not involved with encryption.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html</a></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-vpc/\">https://digitalcloud.training/amazon-vpc/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html",
      "https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html",
      "https://digitalcloud.training/amazon-vpc/"
    ]
  },
  {
    "id": 36,
    "question": "<p>An online education company is launching a new e-learning platform on AWS. The platform will run on Amazon EC2 instances deployed across multiple Availability Zones in multiple AWS Regions. Students worldwide will access the platform through the internet to stream educational content. The company wants to ensure that each student is directed to the EC2 instances in the Region that is geographically closest to their location. The solution must provide high availability and efficient traffic routing.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Route 53 geoproximity routing policy to route students to the geographically closest Region. Configure an internet-facing Network Load Balancer to distribute traffic across the EC2 instances within each Availability Zone.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Route 53 weighted routing policy to balance traffic across Regions. Use an internet-facing Application Load Balancer to distribute traffic across the EC2 instances within each Availability Zone.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Route 53 latency routing policy to direct students to the Region with the lowest network latency. Use an internet-facing Application Load Balancer to distribute traffic across the EC2 instances within each Region.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Route 53 geolocation routing policy to direct students to the closest Region. Use an internet-facing Application Load Balancer to distribute traffic across the EC2 instances within each Region.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p><strong>Use Amazon Route 53 latency routing policy to direct students to the Region with the lowest network latency. Use an internet-facing Application Load Balancer to distribute traffic across the EC2 instances within each Region:</strong> This is correct because the latency routing policy dynamically routes users to the Region with the lowest latency. The Application Load Balancer ensures traffic is evenly distributed across all EC2 instances within the Region.</p><p><strong>Use Amazon Route 53 geolocation routing policy to direct students to the closest Region. Use an internet-facing Application Load Balancer to distribute traffic across the EC2 instances within each Region:</strong> This is incorrect because the geolocation routing policy routes users based on their geographic location, which may not always align with the Region with the lowest latency. For example, users in a region geographically close to a Region with high latency may not experience the best performance.</p><p><strong>Use Amazon Route 53 geoproximity routing policy to route students to the geographically closest Region. Configure an internet-facing Network Load Balancer to distribute traffic across the EC2 instances within each Availability Zone:</strong> This is incorrect because while geoproximity routing directs users based on proximity, it requires manual adjustments for bias, which increases operational complexity. Additionally, a Network Load Balancer is less suitable for HTTP/HTTPS workloads compared to an Application Load Balancer.</p><p><strong>Use Amazon Route 53 weighted routing policy to balance traffic across Regions. Use an internet-facing Application Load Balancer to distribute traffic across the EC2 instances within each Availability Zone:</strong> This is incorrect because a weighted routing policy does not guarantee that users are directed to the closest Region or the one with the lowest latency.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a><br><a href=\"https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\">https://aws.amazon.com/elasticloadbalancing/application-load-balancer/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
      "https://aws.amazon.com/elasticloadbalancing/application-load-balancer/",
      "https://digitalcloud.training/amazon-route-53/"
    ]
  },
  {
    "id": 37,
    "question": "<p>A Solutions Architect has been tasked with migrating 30 TB of data from an on-premises data center within 20 days. The company has an internet connection that is limited to 25 Mbps and the data transfer cannot use more than 50% of the connection speed.</p><p>What should a Solutions Architect do to meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a site-to-site VPN.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS DataSync.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Storage Gateway.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Snowball.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>This is a simple case of working out roughly how long it will take to migrate the data using the 12.5 Mbps of bandwidth that is available for transfer and seeing which options are feasible. Transferring 30 TB of data across a 25 Mbps connection could take upwards of 200 days.</p><p>Therefore, we know that using the Internet connection will not meet the requirements and we can rule out any solution that will use the internet (all options except for Snowball). AWS Snowball is a physical device that is shipped to your office or data center. You can then load data onto it and ship it back to AWS where the data is uploaded to Amazon S3.</p><p>Snowball is the only solution that will achieve the data migration requirements within the 20-day period.</p><p><strong>CORRECT: </strong>\"Use AWS Snowball\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS DataSync\" is incorrect. This uses the internet which will not meet the 20-day deadline.</p><p><strong>INCORRECT:</strong> \"Use AWS Storage Gateway\" is incorrect. This uses the internet which will not meet the 20-day deadline.</p><p><strong>INCORRECT:</strong> \"Use a site-to-site VPN\" is incorrect. This uses the internet which will not meet the 20-day deadline.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/snowball/",
      "https://digitalcloud.training/amazon-s3-and-glacier/"
    ]
  },
  {
    "id": 38,
    "question": "<p>A company runs its critical payment processing application on an Amazon Aurora MySQL cluster in the ap-southeast-1 Region. As part of its disaster recovery (DR) strategy, the company has selected the ap-northeast-1 Region for failover capabilities.</p><p>The company requires a recovery point objective (RPO) of less than 5 minutes and a recovery time objective (RTO) of no more than 15 minutes. The company also wants to minimize operational overhead and ensure failover happens with minimal downtime and configuration.</p><p>Which solution will meet these requirements with the MOST operational efficiency?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon S3 Cross-Region Replication to replicate database backups from ap-southeast-1 to ap-northeast-1. Restore the backups to a new Aurora cluster during failover.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a new Aurora MySQL cluster in ap-northeast-1 and use AWS Database Migration Service (AWS DMS) to replicate data between clusters.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Convert the Aurora cluster to an Aurora global database. Configure cross-Region replication and managed failover.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Aurora read replica in ap-northeast-1 to replicate data from the primary Aurora cluster. Promote the read replica manually in the event of a failover.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p><strong>Convert the Aurora cluster to an Aurora global database. Configure cross-Region replication and managed failover</strong>: This is correct because Aurora global databases are purpose-built for disaster recovery and can achieve an RPO of under 5 seconds with automated failover, meeting the company’s stringent RPO and RTO requirements.</p><p><strong>Create an Aurora read replica in ap-northeast-1 to replicate data from the primary Aurora cluster. Promote the read replica manually in the event of a failover</strong>: While this approach provides DR capabilities, it involves manual intervention during failover, which increases downtime and operational complexity, making it less efficient for meeting the RTO goal.</p><p><strong>Create a new Aurora MySQL cluster in ap-northeast-1 and use AWS Database Migration Service (AWS DMS) to replicate data between clusters</strong>: AWS DMS is not ideal for low-latency replication of high-throughput transactional databases, and it does not natively provide automated failover capabilities.</p><p><strong>Use Amazon S3 Cross-Region Replication to replicate database backups from ap-southeast-1 to ap-northeast-1. Restore the backups to a new Aurora cluster during failover</strong>: Restoring backups to create a new Aurora cluster involves significant downtime and is not suitable for the given RPO and RTO requirements.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html</a><br><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html",
      "https://aws.amazon.com/rds/aurora/global-database/",
      "https://digitalcloud.training/amazon-aurora/"
    ]
  },
  {
    "id": 39,
    "question": "<p>A solutions architect is designing a microservices architecture. AWS Lambda will store data in an Amazon DynamoDB table named Orders. The solutions architect needs to apply an IAM policy to the Lambda function’s execution role to allow it to put, update, and delete items in the Orders table. No other actions should be allowed.</p><p>Which of the following code snippets should be included in the IAM policy to fulfill this requirement whilst providing the LEAST privileged access?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"str\">\"dynamodb:PutItem\"</span><span class=\"pun\">,</span></li><li class=\"L4\"><span class=\"str\">\"dynamodb:UpdateItem\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"dynamodb:DeleteItem\"</span></li><li class=\"L6\"><span class=\"pun\">],</span></li><li class=\"L7\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/Orders\"</span></li></ol></pre></div></div>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"str\">\"dynamodb:PutItem\"</span><span class=\"pun\">,</span></li><li class=\"L4\"><span class=\"str\">\"dynamodb:UpdateItem\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"dynamodb:DeleteItem\"</span></li><li class=\"L6\"><span class=\"pun\">],</span></li><li class=\"L7\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/*\"</span></li></ol></pre></div></div>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Deny\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"dynamodb:* \"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/Orders\"</span></li></ol></pre></div></div>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"dynamodb:* \"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/Orders\"</span></li></ol></pre></div></div>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>The key requirements are to allow the Lambda function the put, update, and delete actions on a single table. Using the principle of least privilege the answer should not allow any other access.</p><p><strong>CORRECT: </strong>The following answer is correct:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"str\">\"dynamodb:PutItem\"</span><span class=\"pun\">,</span></li><li class=\"L4\"><span class=\"str\">\"dynamodb:UpdateItem\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"dynamodb:DeleteItem\"</span></li><li class=\"L6\"><span class=\"pun\">],</span></li><li class=\"L7\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/Orders\"</span></li></ol></pre></div></div><p>This code snippet specifies the exact actions to allow and also specified the resource to apply those permissions to.</p><p><strong>INCORRECT:</strong> the following answer is incorrect:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"str\">\"dynamodb:PutItem\"</span><span class=\"pun\">,</span></li><li class=\"L4\"><span class=\"str\">\"dynamodb:UpdateItem\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"str\">\"dynamodb:DeleteItem\"</span></li><li class=\"L6\"><span class=\"pun\">],</span></li><li class=\"L7\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/*\"</span></li></ol></pre></div></div><p>This code snippet specifies the correct list of actions but it provides a wildcard “*” instead of specifying the exact resource. Therefore, the function will be able to put, update, and delete items on any table in the account.</p><p><strong>INCORRECT:</strong> the following answer is incorrect:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"dynamodb:* \"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/Orders\"</span></li></ol></pre></div></div><p>This code snippet allows any action on DynamoDB by using a wildcard “dynamodb:*”. This does not follow the principle of least privilege.</p><p><strong>INCORRECT:</strong> the following answer is incorrect:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"str\">\"Sid\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"PutUpdateDeleteOnOrders\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Deny\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"dynamodb:* \"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"arn:aws:dynamodb:us-east-1:227392126428:table/Orders\"</span></li></ol></pre></div></div><p>This code snippet denies any action on the table. This does not have the desired effect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html",
      "https://digitalcloud.training/aws-iam/"
    ]
  },
  {
    "id": 40,
    "question": "<p>An educational content provider has accumulated several terabytes of learning resources in an Amazon S3 bucket located in a specific AWS Region. A partner organization, based in a different AWS Region, has been granted access to the S3 bucket to retrieve the resources for integration into its own platform. The content provider wants to minimize its data transfer costs when the partner organization accesses the S3 bucket.</p><p><strong>Which solution will meet these requirements?</strong></p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the bucket to use S3 Standard-IA storage to reduce access costs for the partner organization.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable the Requester Pays feature on the content provider’s S3 bucket.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up S3 Cross-Region Replication (CRR) to copy the learning resources to the partner organization’s S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use S3 Transfer Acceleration to allow the partner organization to retrieve data from the bucket.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p><strong>Enable the Requester Pays feature on the content provider’s S3 bucket</strong>: This is correct because the Requester Pays feature shifts the data transfer costs from the content provider to the partner organization, thereby minimizing the content provider’s expenses.</p><p><strong>Use S3 Transfer Acceleration to allow the partner organization to retrieve data from the bucket</strong>: This solution optimizes data transfer speeds but does not specifically address minimizing the content provider's data transfer costs.</p><p><strong>Set up S3 Cross-Region Replication (CRR) to copy the learning resources to the partner organization’s S3 bucket</strong>: While this enables data availability in the partner organization’s Region, it introduces additional costs for cross-Region replication, which the content provider would incur.</p><p><strong>Configure the bucket to use S3 Standard-IA storage to reduce access costs for the partner organization</strong>: Changing the storage class to Standard-IA focuses on storage cost optimization and does not directly address minimizing data transfer costs for the content provider.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html</a><br><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html",
      "https://digitalcloud.training/amazon-s3-and-glacier/"
    ]
  },
  {
    "id": 41,
    "question": "<p>A company needs to implement a new data retention policy for regulatory compliance. As part of this policy, sensitive documents that are stored in an Amazon S3 bucket must be protected from deletion or modification for a fixed period of time.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Activate S3 Object Lock in compliance mode on the bucket. Configure a WORM (Write Once, Read Many) policy.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon S3 bucket with versioning enabled. Use a lifecycle rule to automatically delete older versions after the retention period.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Backup to create immutable backups of the S3 objects and enforce a retention policy.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable S3 Object Lock on the required objects and set compliance mode.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p><strong>Enable S3 Object Lock on the required objects and set compliance mode:</strong> This is correct because compliance mode ensures that no user, including the root user, can delete or modify objects during the retention period, making it suitable for regulatory requirements.</p><p><strong>Activate S3 Object Lock in compliance mode on the bucket. Configure a WORM (Write Once, Read Many) policy:</strong> This is incorrect because S3 Object Lock applies to specific objects, not the entire bucket, and there is no WORM-specific configuration required beyond compliance mode.</p><p><strong>Create an Amazon S3 bucket with versioning enabled. Use a lifecycle rule to automatically delete older versions after the retention period:</strong> This is incorrect because versioning and lifecycle policies do not provide protection against modification or deletion during the retention period.</p><p><strong>Use AWS Backup to create immutable backups of the S3 objects and enforce a retention policy:</strong> This is incorrect because AWS Backup does not integrate directly with S3 Object Lock to meet regulatory compliance needs for immutability.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html</a><br><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-object-lock.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-object-lock.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-object-lock.html",
      "https://digitalcloud.training/amazon-s3-and-glacier/"
    ]
  },
  {
    "id": 42,
    "question": "<p>A company runs a containerized application on an Amazon Elastic Kubernetes Service (EKS) using a microservices architecture. The company requires a solution to collect, aggregate, and summarize metrics and logs. The solution should provide a centralized dashboard for viewing information including CPU and memory utilization for EKS namespaces, services, and pods.</p><p>Which solution meets these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Run the Amazon CloudWatch agent in the existing EKS cluster. View the metrics and logs in the CloudWatch console.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Migrate the containers to Amazon ECS and enable Amazon CloudWatch Container Insights. View the metrics and logs in the CloudWatch console.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure Amazon CloudWatch Container Insights in the existing EKS cluster. View the metrics and logs in the CloudWatch console.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure AWS X-Ray to enable tracing for the EKS microservices. Query the trace data using Amazon Elasticsearch.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>Use CloudWatch Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Container Insights is available for Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), and Kubernetes platforms on Amazon EC2.</p><p>With Container Insights for EKS you can see the top contributors by memory or CPU, or the most recently active resources. This is available when you select any of the following dashboards in the drop-down box near the top of the page:</p><p>&nbsp; •&nbsp; ECS Services</p><p>&nbsp; •&nbsp; ECS Tasks</p><p>&nbsp; •&nbsp; EKS Namespaces</p><p>&nbsp; •&nbsp; EKS Services</p><p>&nbsp; •&nbsp; EKS Pods</p><p><strong>CORRECT: </strong>\"Configure Amazon CloudWatch Container Insights in the existing EKS cluster. View the metrics and logs in the CloudWatch console\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Run the Amazon CloudWatch agent in the existing EKS cluster. View the metrics and logs in the CloudWatch console\" is incorrect. Container Insights is the best way to view the required data.</p><p><strong>INCORRECT:</strong> \"Migrate the containers to Amazon ECS and enable Amazon CloudWatch Container Insights. View the metrics and logs in the CloudWatch console\" is incorrect. There is no need to migrate containers to ECS as EKS is supported for Container Insights.</p><p><strong>INCORRECT:</strong> \"Configure AWS X-Ray to enable tracing for the EKS microservices. Query the trace data using Amazon Elasticsearch\" is incorrect. X-Ray will not deliver the required statistics to a centralized dashboard.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights.html",
      "https://digitalcloud.training/amazon-ecs-and-eks/"
    ]
  },
  {
    "id": 43,
    "question": "<p>A company operates a production environment on Amazon EC2 instances. The instances are required to run continuously from Tuesday to Sunday without interruptions. On Mondays, the instances are needed for only 8 hours, and they also cannot tolerate interruptions. The company wants to implement a cost-effective solution to optimize EC2 usage while meeting these requirements.</p><p>Which solution will provide the MOST cost-effective results?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Purchase Standard Reserved Instances for the EC2 instances that operate continuously from Tuesday to Sunday. Use Scheduled Reserved Instances for the EC2 instances that run for 8 hours on Mondays.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Spot Instances for the EC2 instances that run for 8 hours on Mondays. Purchase Standard Reserved Instances for the EC2 instances that operate continuously from Tuesday to Sunday.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Purchase Standard Reserved Instances for the EC2 instances that operate continuously from Tuesday to Sunday. Use Convertible Reserved Instances for the EC2 instances that run for 8 hours on Mondays.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Purchase Convertible Reserved Instances for the EC2 instances that operate continuously from Tuesday to Sunday. Use Spot Instances for the EC2 instances that run for 8 hours on Mondays.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p><strong>Purchase Standard Reserved Instances for the EC2 instances that operate continuously from Tuesday to Sunday. Use Scheduled Reserved Instances for the EC2 instances that run for 8 hours on Mondays:</strong> This is correct because Standard Reserved Instances provide cost savings for long-term, predictable workloads, like the continuous operation from Tuesday to Sunday. Scheduled Reserved Instances are ideal for predictable workloads with fixed schedules, like the 8-hour workload on Mondays, offering savings while ensuring uninterrupted operation.</p><p><strong>Use Spot Instances for the EC2 instances that run for 8 hours on Mondays. Purchase Standard Reserved Instances for the EC2 instances that operate continuously from Tuesday to Sunday:</strong> This is incorrect because Spot Instances, while cost-effective, are not suitable for workloads that cannot tolerate interruptions. Spot Instances may be terminated unexpectedly, which does not meet the requirement for uninterrupted operation.</p><p><strong>Purchase Convertible Reserved Instances for the EC2 instances that operate continuously from Tuesday to Sunday. Use Spot Instances for the EC2 instances that run for 8 hours on Mondays:</strong> This is incorrect because Convertible Reserved Instances, while flexible, are not the most cost-effective option for predictable and consistent workloads like continuous operation from Tuesday to Sunday. Additionally, Spot Instances are unsuitable for workloads requiring uninterrupted operation.</p><p><strong>Purchase Standard Reserved Instances for the EC2 instances that operate continuously from Tuesday to Sunday. Use Convertible Reserved Instances for the EC2 instances that run for 8 hours on Mondays:</strong> This is incorrect because Convertible Reserved Instances are not as cost-effective as Scheduled Reserved Instances for fixed and predictable schedules, such as the 8-hour Monday workload.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html",
      "https://digitalcloud.training/amazon-ec2/"
    ]
  },
  {
    "id": 44,
    "question": "<p>An application runs on-premises and produces data that must be stored in a locally accessible file system that servers can mount using the NFS protocol. The data must be subsequently analyzed by Amazon EC2 instances in the AWS Cloud.</p><p>How can these requirements be met?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an AWS Storage Gateway volume gateway in stored mode to regularly take snapshots of the local data, then copy the data to AWS.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use an AWS Storage Gateway file gateway to provide a locally accessible file system that replicates data to the cloud, then analyze the data in the AWS Cloud.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use an AWS Storage Gateway volume gateway in cached mode to back up all the local storage in the AWS Cloud, then perform analytics on this data in the cloud.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use an AWS Storage Gateway tape gateway to take a backup of the local data and store it on AWS, then perform analytics on this data in the AWS Cloud.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>The best solution for this requirement is to use an AWS Storage Gateway file gateway. This will provide a local NFS mount point for the data and a local cache. The data is then replicated to Amazon S3 where it can be analyzed by the Amazon EC2 instances in the AWS Cloud.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-19-32-45d0ef2489b4ea2fb6fae5b15c844aa0.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-19-32-45d0ef2489b4ea2fb6fae5b15c844aa0.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Use an AWS Storage Gateway file gateway to provide a locally accessible file system that replicates data to the cloud, then analyze the data in the AWS Cloud\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an AWS Storage Gateway tape gateway to take a backup of the local data and store it on AWS, then perform analytics on this data in the AWS Cloud\" is incorrect. A tape gateway does not provide a local NFS mount point, it is simply a backup solution not a file system.</p><p><strong>INCORRECT:</strong> \"Use an AWS Storage Gateway volume gateway in stored mode to regularly take snapshots of the local data, then copy the data to AWS\" is incorrect. Volume gateways use block-based protocols not NFS.</p><p><strong>INCORRECT:</strong> \"Use an AWS Storage Gateway volume gateway in cached mode to back up all the local storage in the AWS Cloud, then perform analytics on this data in the cloud\" is incorrect. Volume gateways use block-based protocols not NFS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/storagegateway/file/",
      "https://digitalcloud.training/amazon-efs/"
    ]
  },
  {
    "id": 45,
    "question": "<p>A company requires a fully managed replacement for an on-premises storage service. The company’s employees often work remotely from various locations. The solution should also be easily accessible to systems connected to the on-premises environment.</p><p>Which solution meets these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon FSx to create an SMB file share. Connect remote clients to the file share over a client VPN.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Transfer Acceleration to replicate files to Amazon S3 and enable public access.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Storage Gateway to create a volume gateway to store and transfer files to Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS DataSync to synchronize data between the on-premises service and Amazon S3.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>Amazon FSx for Windows File Server (Amazon FSx) is a fully managed, highly available, and scalable file storage solution built on Windows Server that uses the Server Message Block (SMB) protocol. It allows for Microsoft Active Directory integration, data deduplication, and fully managed backups, among other critical enterprise features.</p><p>An Amazon FSx file system can be created to host the file shares. Clients can then be connected to an AWS Client VPN endpoint and gateway to enable remote access. The protocol used in this solution will be SMB.</p><p><strong>CORRECT: </strong>\"Use Amazon FSx to create an SMB file share. Connect remote clients to the file share over a client VPN\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use AWS Transfer Acceleration to replicate files to Amazon S3 and enable public access\" is incorrect. This is simply a way of improving upload speeds to S3, it is not suitable for enabling internal and external access to a file system.</p><p><strong>INCORRECT:</strong> \"Use AWS DataSync to synchronize data between the on-premises service and Amazon S3\" is incorrect. The on-premises solution is to be replaced so this is not a satisfactory solution. Also, DataSync syncs one way, it is not bidirectional.</p><p><strong>INCORRECT:</strong> \"Use AWS Storage Gateway to create a volume gateway to store and transfer files to Amazon S3\" is incorrect. Storage Gateway volume gateways are mounted using block-based protocols (iSCSI), so this would not be workable as block-based protocols cannot be used over long distances such as by the workers in remote locations.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/storage/accessing-smb-file-shares-remotely-with-amazon-fsx-for-windows-file-server/\">https://aws.amazon.com/blogs/storage/accessing-smb-file-shares-remotely-with-amazon-fsx-for-windows-file-server/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/storage/accessing-smb-file-shares-remotely-with-amazon-fsx-for-windows-file-server/",
      "https://digitalcloud.training/amazon-fsx/"
    ]
  },
  {
    "id": 46,
    "question": "<p>A company manages several applications that run in different AWS accounts within an AWS Organizations setup. The company has outsourced the management of certain applications to external contractors. The contractors require secure access to the AWS Management Console and operating system access to Amazon Linux-based Amazon EC2 instances in private subnets for troubleshooting. The company must ensure all activities are logged and minimize the risk of unauthorized access.</p><p>Which solution will meet these requirements MOST securely?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy AWS Systems Manager Agent (SSM Agent) to all instances. Assign an instance profile to the instances with the required Systems Manager policies. Grant contractors access to the AWS Management Console by configuring permission sets in AWS IAM Identity Center. Use Systems Manager Session Manager for secure instance access without requiring open network ports.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Systems Manager Agent (SSM Agent) with an attached instance profile to manage EC2 access. Provide contractors with temporary local IAM user credentials in each AWS account for console access. Require contractors to use Systems Manager Session Manager for instance access.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up AWS VPN or Direct Connect to create a private network connection to the contractors’ office. Allow access to the AWS Management Console by creating IAM user credentials in each AWS account. Use security groups to allow SSH access from the contractors' office to the private EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a bastion host in a public subnet. Restrict SSH access to the bastion host by using security groups to allow connections only from the contractors' IP address ranges. Provide contractors with IAM user credentials for Management Console access and SSH key pairs for accessing private instances via the bastion host.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p><strong>Deploy AWS Systems Manager Agent (SSM Agent) to all instances. Assign an instance profile to the instances with the required Systems Manager policies. Grant contractors access to the AWS Management Console by configuring permission sets in AWS IAM Identity Center. Use Systems Manager Session Manager for secure instance access without requiring open network ports:</strong></p><p>This is correct because Systems Manager Session Manager provides secure, auditable access to instances without requiring SSH keys or open ports. IAM Identity Center enables centralized management of console access for contractors.</p><p><strong>Configure a bastion host in a public subnet. Restrict SSH access to the bastion host by using security groups to allow connections only from the contractors' IP address ranges. Provide contractors with IAM user credentials for Management Console access and SSH key pairs for accessing private instances via the bastion host:</strong></p><p>This is incorrect because it increases operational overhead and requires managing SSH key pairs while leaving an exposed attack surface with the bastion host.</p><p><strong>Use AWS Systems Manager Agent (SSM Agent) with an attached instance profile to manage EC2 access. Provide contractors with temporary local IAM user credentials in each AWS account for console access. Require contractors to use Systems Manager Session Manager for instance access:</strong></p><p>This is incorrect because creating local IAM users for console access increases the risk of credential management issues and does not align with centralized access management best practices.</p><p><strong>Set up AWS VPN or Direct Connect to create a private network connection to the contractors’ office. Allow access to the AWS Management Console by creating IAM user credentials in each AWS account. Use security groups to allow SSH access from the contractors' office to the private EC2 instances:</strong></p><p>This is incorrect because using VPN or Direct Connect introduces unnecessary complexity and cost. SSH access also increases the risk of mismanagement and security vulnerabilities.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html</a><br><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html",
      "https://digitalcloud.training/aws-systems-manager/"
    ]
  },
  {
    "id": 47,
    "question": "<p>A research organization is planning to migrate its simulation analysis platform to AWS. The platform stores simulation results and logs on an on-premises NFS server. The platform's codebase is legacy and cannot be modified to use any protocol other than NFS to store and retrieve data. The organization needs a storage solution on AWS that supports NFS and is highly available and scalable.</p><p>Which storage solution should a solutions architect recommend for use after the migration?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Elastic File System (Amazon EFS) to provide an NFS-compatible shared file system that integrates with AWS services.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Storage Gateway File Gateway to provide an NFS interface backed by Amazon S3 for storing and retrieving data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Elastic Block Store (Amazon EBS) volumes attached to each EC2 instance for storage. Use NFS software on the EC2 instances to create a shared file system.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon FSx for Windows File Server to create a shared file system for data storage and access through the SMB protocol.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p><strong>Use Amazon Elastic File System (Amazon EFS) to provide an NFS-compatible shared file system that integrates with AWS services:</strong> This is correct because Amazon EFS is a fully managed, scalable, and highly available file storage service that supports NFS. It is designed to work seamlessly with applications requiring NFS without additional setup or modifications.</p><p><strong>Use AWS Storage Gateway File Gateway to provide an NFS interface backed by Amazon S3 for storing and retrieving data:</strong> This is incorrect because File Gateway is typically used to integrate on-premises environments with AWS. It is not as suitable for applications entirely migrated to AWS, especially when a native NFS solution like EFS is available.</p><p><strong>Use Amazon Elastic Block Store (Amazon EBS) volumes attached to each EC2 instance for storage. Use NFS software on the EC2 instances to create a shared file system:</strong> This is incorrect because using EBS requires additional configuration and management to implement an NFS-based shared file system. EFS provides a simpler, fully managed solution.</p><p><strong>Use Amazon FSx for Windows File Server to create a shared file system for data storage and access through the SMB protocol:</strong> This is incorrect because FSx for Windows File Server supports the SMB protocol, not NFS. The application explicitly requires NFS, making FSx for Windows File Server unsuitable.</p><p><strong>References:</strong><br><a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a><br><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-efs/\">https://digitalcloud.training/amazon-efs/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/efs/",
      "https://aws.amazon.com/storagegateway/file/",
      "https://digitalcloud.training/amazon-efs/"
    ]
  },
  {
    "id": 48,
    "question": "<p>A company is migrating its legacy customer support applications from an on-premises data center to AWS. Each application runs on a dedicated virtual machine and relies on proprietary software that cannot be modified. The applications must remain highly available and continue to operate in the event of a single Availability Zone failure. The company wants to minimize changes to its architecture and operational overhead.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon Machine Image (AMI) for each application. Launch two EC2 instances for each application in different Availability Zones. Use an Application Load Balancer to distribute traffic evenly between the instances.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Elastic Disaster Recovery (AWS DRS) to replicate the on-premises virtual machines to AWS. Launch the virtual machines in an Auto Scaling group configured to span multiple Availability Zones.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Refactor the applications into microservices and deploy them on Amazon ECS with Fargate. Use a Network Load Balancer to route traffic to the Fargate tasks.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Backup to configure hourly backups of each EC2 instance. Store backups in Amazon S3 Glacier. In case of failure, restore the latest backup to a new EC2 instance in another Availability Zone.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p><strong>Create an Amazon Machine Image (AMI) for each application. Launch two EC2 instances for each application in different Availability Zones. Use an Application Load Balancer to distribute traffic evenly between the instances:</strong> This is the correct solution as it provides high availability by deploying instances across Availability Zones and distributes traffic using a load balancer.</p><p><strong>Use AWS Backup to configure hourly backups of each EC2 instance. Store backups in Amazon S3 Glacier. In case of failure, restore the latest backup to a new EC2 instance in another Availability Zone:</strong> While backups help in disaster recovery, this solution does not provide high availability or fault tolerance in real-time.</p><p><strong>Use AWS Elastic Disaster Recovery (AWS DRS) to replicate the on-premises virtual machines to AWS. Launch the virtual machines in an Auto Scaling group configured to span multiple Availability Zones:</strong> While AWS DRS is useful for disaster recovery, this approach introduces unnecessary complexity for an already migrated application.</p><p><strong>Refactor the applications into microservices and deploy them on Amazon ECS with Fargate. Use a Network Load Balancer to route traffic to the Fargate tasks:</strong> This is incorrect because the requirement explicitly states that the proprietary applications cannot be modified.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a><br><a href=\"https://docs.aws.amazon.com/auto-scaling/index.html\">https://docs.aws.amazon.com/auto-scaling/index.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2/\">https://digitalcloud.training/amazon-ec2/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://docs.aws.amazon.com/auto-scaling/index.html",
      "https://digitalcloud.training/amazon-ec2/"
    ]
  },
  {
    "id": 49,
    "question": "<p>A high-performance file system is required for a financial modelling application. The data set will be stored on Amazon S3 and the storage solution must have seamless integration so objects can be accessed as files.</p><p>Which storage solution should be used?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon FSx for Windows File Server</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon FSx for Lustre</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon Elastic File System (EFS)</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Elastic Block Store (EBS)</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>Amazon FSx for Lustre provides a high-performance file system optimized for fast processing of workloads such as machine learning, high performance computing (HPC), video processing, financial modeling, and electronic design automation (EDA). Amazon FSx works natively with Amazon S3, letting you transparently access your S3 objects as files on Amazon FSx to run analyses for hours to months.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-06-30_17-36-07-2dee8736108866a3aac56736557b7717.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-06-30_17-36-07-2dee8736108866a3aac56736557b7717.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p><strong>CORRECT: </strong>\"Amazon FSx for Lustre\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Amazon FSx for Windows File Server\" is incorrect. Amazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system so you can easily move your Windows-based applications that require shared file storage to AWS. This solution integrates with Windows file shares, not with Amazon S3.</p><p><strong>INCORRECT:</strong> \"Amazon Elastic File System (EFS)\" is incorrect. EFS and EBS are not good use cases for this solution. Neither storage solution is capable of presenting Amazon S3 objects as files to the application.</p><p><strong>INCORRECT:</strong> \"Amazon Elastic Block Store (EBS)\" is incorrect. EFS and EBS are not good use cases for this solution. Neither storage solution is capable of presenting Amazon S3 objects as files to the application.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/\">https://aws.amazon.com/fsx/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-fsx/\">https://digitalcloud.training/amazon-fsx/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/fsx/",
      "https://digitalcloud.training/amazon-fsx/"
    ]
  },
  {
    "id": 50,
    "question": "<p>A company has created a disaster recovery solution for an application that runs behind an Application Load Balancer (ALB). The DR solution consists of a second copy of the application running behind a second ALB in another Region. The Solutions Architect requires a method of automatically updating the DNS record to point to the ALB in the second Region.</p><p>What action should the Solutions Architect take?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable an ALB health check.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable an Amazon Route 53 health check.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure an alarm on a CloudTrail trail.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon EventBridge to cluster the ALBs.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>Amazon Route 53 health checks monitor the health and performance of your web applications, web servers, and other resources. Each health check that you create can monitor one of the following:</p><p>&nbsp; •&nbsp; The health of a specified resource, such as a web server</p><p>&nbsp; •&nbsp; The status of other health checks</p><p>&nbsp; •&nbsp; The status of an Amazon CloudWatch alarm</p><p>Health checks can be used with other configurations such as a failover routing policy. In this case a failover routing policy will direct traffic to the ALB of the primary Region unless health checks fail at which time it will direct traffic to the secondary record for the DR ALB.</p><p><strong>CORRECT: </strong>\"Enable an Amazon Route 53 health check\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Enable an ALB health check\" is incorrect. This will simply perform health checks of the instances behind the ALB, rather than the ALB itself. This could be used in combination with Route 53 health checks.</p><p><strong>INCORRECT:</strong> \"Use Amazon EventBridge to cluster the ALBs\" is incorrect. You cannot cluster ALBs in any way.</p><p><strong>INCORRECT:</strong> \"Configure an alarm on a CloudTrail trail\" is incorrect. CloudTrail records API activity so this does not help.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html",
      "https://digitalcloud.training/amazon-route-53/"
    ]
  },
  {
    "id": 51,
    "question": "<p>An Architect needs to find a way to automatically and repeatably create many member accounts within an AWS Organization. The accounts also need to be moved into an OU and have VPCs and subnets created.</p><p>What is the best way to achieve this?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AWS CLI</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the AWS Organizations API</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the AWS Management Console</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use CloudFormation with scripts</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>The best solution is to use a combination of scripts and AWS CloudFormation. You will also leverage the AWS Organizations API. This solution can provide all of the requirements.</p><p><strong>CORRECT: </strong>\"Use CloudFormation with scripts\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use the AWS Organizations API\" is incorrect. You can create member accounts with the AWS Organizations API. However, you cannot use that API to configure the account and create VPCs and subnets.</p><p><strong>INCORRECT:</strong> \"Use the AWS Management Console\" is incorrect. Using the AWS Management Console is not a method of automatically creating the resources.</p><p><strong>INCORRECT:</strong> \"Use the AWS CLI\" is incorrect. You can do all tasks using the AWS CLI but it is better to automate the process using AWS CloudFormation.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-aws-organizations-to-automate-end-to-end-account-creation/\">https://aws.amazon.com/blogs/security/how-to-use-aws-organizations-to-automate-end-to-end-account-creation/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/security/how-to-use-aws-organizations-to-automate-end-to-end-account-creation/",
      "https://digitalcloud.training/aws-organizations/"
    ]
  },
  {
    "id": 52,
    "question": "<p>A Solutions Architect works for a company looking to centralize its Machine Learning Operations. Currently they have a large amount of existing cloud storage to store their operational data which is used for machine learning analysis. There is some data which exists within an Amazon RDS MySQL database, and they need a solution which can easily retrieve data from the database.</p><p>Which service can be used to build a centralized data repository to be used for Machine Learning purposes?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Neptune</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon Quantum Ledger Database (QLDB)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Lake Formation</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Storage",
    "explanation": "<p>AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis. With AWS Lake Formation, you can import data from MySQL, PostgreSQL, SQL Server, MariaDB, and Oracle databases running in Amazon Relational Database Service (RDS) or hosted in Amazon Elastic Compute Cloud (EC2). Both bulk and incremental data loading are supported.</p><p><strong>CORRECT: </strong>\"AWS Lake Formation\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon S3\" is incorrect. Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. It is not however suitable for directly retrieving data from MySQL on RDS and using the data for a Machine learning use case.</p><p><strong>INCORRECT:</strong> \"Amazon Quantum Ledger Database\" is incorrect. Amazon Quantum Ledger Database (QLDB) is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log. It is not suitable for directly retrieving data from MySQL on RDS and using the data for a Machine learning use case.</p><p><strong>INCORRECT:</strong> \"Amazon Neptune\" is incorrect. Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications. It is not suitable for directly retrieving data from MySQL on RDS and using the data for a Machine learning use case.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lake-formation/features/\">https://aws.amazon.com/lake-formation/features/</a></p><p><a href=\"https://aws.amazon.com/lake-formation/features/\"><br></a><strong>Save time with our AWS cheat sheets:</strong></p><p><strong><br></strong><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-associate/aws-storage-saa/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-associate/aws-storage-saa/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/lake-formation/features/",
      "https://digitalcloud.training/category/aws-cheat-sheets/aws-solutions-architect-associate/aws-storage-saa/"
    ]
  },
  {
    "id": 53,
    "question": "<p>An application is deployed on multiple AWS regions and accessed from around the world. The application exposes static public IP addresses. Some users are experiencing poor performance when accessing the application over the Internet.</p><p>What should a solutions architect recommend to reduce internet latency?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an Amazon Route 53 geoproximity routing policy to route traffic</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an Amazon CloudFront distribution to access an application</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up AWS Global Accelerator and add endpoints</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up AWS Direct Connect locations in multiple Regions</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p>AWS Global Accelerator is a service in which you create <em>accelerators</em> to improve availability and performance of your applications for local and global users. Global Accelerator directs traffic to optimal endpoints over the AWS global network. This improves the availability and performance of your internet applications that are used by a global audience. Global Accelerator is a global service that supports endpoints in multiple AWS Regions, which are listed in the <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/\">AWS Region Table</a>.</p><p>By default, Global Accelerator provides you with two static IP addresses that you associate with your accelerator. (Or, instead of using the IP addresses that Global Accelerator provides, you can configure these entry points to be IPv4 addresses from your own IP address ranges that you bring to Global Accelerator.)</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-57-03-6258b515dd5b68a3f0c87a51702006a3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-57-03-6258b515dd5b68a3f0c87a51702006a3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>The static IP addresses are anycast from the AWS edge network and distribute incoming application traffic across multiple endpoint resources in multiple AWS Regions, which increases the availability of your applications. Endpoints can be Network Load Balancers, Application Load Balancers, EC2 instances, or Elastic IP addresses that are located in one AWS Region or multiple Regions.</p><p><strong>CORRECT: </strong>\"Set up AWS Global Accelerator and add endpoints\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Set up AWS Direct Connect locations in multiple Regions\" is incorrect as this is used to connect from an on-premises data center to AWS. It does not improve performance for users who are not connected to the on-premises data center.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon CloudFront distribution to access an application\" is incorrect as CloudFront cannot expose static public IP addresses.</p><p><strong>INCORRECT:</strong> \"Set up an Amazon Route 53 geoproximity routing policy to route traffic\" is incorrect as this does not reduce internet latency as well as using Global Accelerator. GA will direct users to the closest edge location and then use the AWS global network.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html\">https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-global-accelerator/\">https://digitalcloud.training/aws-global-accelerator/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/",
      "https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html",
      "https://digitalcloud.training/aws-global-accelerator/"
    ]
  },
  {
    "id": 54,
    "question": "<p>A government agency is moving its document management system to AWS. The application will store classified documents in Amazon S3. The agency must encrypt the documents before storing them in S3 to ensure compliance with strict data security regulations.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Encrypt the documents by using client-side encryption with Amazon S3 managed keys and upload the encrypted files to S3.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Encrypt the documents by using server-side encryption with AWS KMS keys (SSE-KMS) configured with custom key policies for access control.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Encrypt the documents by using client-side encryption with customer managed keys and upload the encrypted files to S3.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Encrypt the documents by using server-side encryption with customer-provided keys (SSE-C).</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p><strong>Encrypt the documents by using client-side encryption with Amazon S3 managed keys and upload the encrypted files to S3:</strong></p><p>This is correct because client-side encryption means the data is encrypted before it ever leaves the client system, giving maximum control over the encryption process. Using customer managed keys (CMKs) from AWS Key Management Service (KMS), the agency retains full ownership and control over key policies, rotation, and access, which is often a requirement for handling classified or sensitive government data. This approach ensures compliance with strict regulations by minimizing trust in the cloud provider to protect unencrypted data.</p><p><strong>Encrypt the documents by using server-side encryption with AWS KMS keys (SSE-KMS) configured with custom key policies for access control:</strong> This is incorrect because while secure and easy to manage, SSE-KMS means AWS handles the encryption after data reaches S3</p><p><strong>Encrypt the documents by using client-side encryption with customer managed keys and upload the encrypted files to S3:</strong> This is incorrect because while client-side encryption can ensure security before data reaches S3, it increases operational complexity since the customer must manage key creation, storage, and rotation independently.</p><p><strong>Encrypt the documents by using server-side encryption with customer-provided keys (SSE-C):</strong> This is incorrect because SSE-C requires the customer to provide the encryption keys for every S3 operation, which can increase operational overhead and risks if key management is not handled correctly.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html</a><br><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-kms/\">https://digitalcloud.training/aws-kms/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html",
      "https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html",
      "https://digitalcloud.training/aws-kms/"
    ]
  },
  {
    "id": 55,
    "question": "<p>A research institute uses an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to run machine learning workloads. The institute must ensure that Kubernetes service accounts within the EKS cluster have secure, fine-grained access to specific AWS resources for model training and data processing. The solution must use IAM roles for service accounts (IRSA) to meet these requirements.</p><p>Which combination of solutions will meet these requirements? (Select TWO.)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Define an IAM role that includes the required permissions. Annotate the Kubernetes service accounts with the Amazon Resource Name (ARN) of the IAM role.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create an IAM policy that defines the necessary permissions for AWS resources. Attach the policy directly to the IAM role of the EKS worker nodes.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure a trust relationship between the IAM roles for the service accounts and an OpenID Connect (OIDC) identity provider associated with the EKS cluster.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Modify the EKS cluster's worker node IAM role to include permissions for Kubernetes service accounts. Ensure all service accounts map to a single IAM role.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Implement pod security policies in the EKS cluster to restrict pods from accessing unauthorized AWS resources.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p><strong>Define an IAM role that includes the required permissions. Annotate the Kubernetes service accounts with the Amazon Resource Name (ARN) of the IAM role:</strong> This is correct because IRSA enables Kubernetes service accounts to securely assume IAM roles with the necessary permissions for accessing specific AWS resources.</p><p><strong>Configure a trust relationship between the IAM roles for the service accounts and an OpenID Connect (OIDC) identity provider associated with the EKS cluster:</strong> This is correct because OIDC integration is required for IRSA to allow the EKS service accounts to assume the associated IAM roles securely.</p><p><strong>Create an IAM policy that defines the necessary permissions for AWS resources. Attach the policy directly to the IAM role of the EKS worker nodes:</strong> This is incorrect because attaching the policy to the worker node role gives broad permissions to all pods running on those nodes, violating the principle of least privilege.</p><p><strong>Implement pod security policies in the EKS cluster to restrict pods from accessing unauthorized AWS resources:</strong> This is incorrect because pod security policies manage pod-level security but do not directly control access to AWS resources via IAM.</p><p><strong>Modify the EKS cluster's worker node IAM role to include permissions for Kubernetes service accounts. Ensure all service accounts map to a single IAM role:</strong> This is incorrect because mapping all service accounts to a single IAM role does not provide the granularity required for secure access.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html\">https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html</a><br><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html",
      "https://digitalcloud.training/aws-iam/"
    ]
  },
  {
    "id": 56,
    "question": "<p>An online store uses an Amazon Aurora database. The database is deployed as a Multi-AZ deployment. Recently, metrics have shown that database read requests are high and causing performance issues which result in latency for write requests.</p><p>What should the solutions architect do to separate the read requests from the write requests?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable read through caching on the Amazon Aurora database</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a read replica and modify the application to use the appropriate endpoint</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Update the application to read from the Aurora Replica</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create a second Amazon Aurora database and link it to the primary database as a read replica</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Database",
    "explanation": "<p>Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region.</p><p>The DB cluster volume is made up of multiple copies of the data for the DB cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary instance and to Aurora Replicas in the DB cluster.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-56-02-d92494e987b1b8e80c6782e078540b97.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-56-02-d92494e987b1b8e80c6782e078540b97.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>As well as providing scaling for reads, Aurora Replicas are also targets for multi-AZ. In this case the solutions architect can update the application to read from the Aurora Replica</p><p><strong>CORRECT: </strong>\"Update the application to read from the Aurora Replica\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create a read replica and modify the application to use the appropriate endpoint\" is incorrect. An Aurora Replica is both a standby in a Multi-AZ configuration and a target for read traffic. The architect simply needs to direct traffic to the Aurora Replica.</p><p><strong>INCORRECT:</strong> \"Enable read through caching on the Amazon Aurora database.\" is incorrect as this is not a feature of Amazon Aurora.</p><p><strong>INCORRECT:</strong> \"Create a second Amazon Aurora database and link it to the primary database as a read replica\" is incorrect as an Aurora Replica already exists as this is a Multi-AZ configuration and the standby is an Aurora Replica that can be used for read traffic.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html",
      "https://digitalcloud.training/amazon-aurora/"
    ]
  },
  {
    "id": 57,
    "question": "<p>A Solutions Architect created the following policy and associated to an AWS IAM group containing several administrative users:</p><p>{</p><p>&nbsp; &nbsp;\"Version\": \"2012-10-17\",</p><p>&nbsp; &nbsp; \"Statement\": [</p><p>&nbsp; &nbsp; {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Effect\": \"Allow\",</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Action\": \"ec2:TerminateInstances\",</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Resource\": \"*\",</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Condition\": {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"IpAddress\": {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"aws:SourceIp\": \"10.1.2.0/24\"</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</p><p>&nbsp; &nbsp; },</p><p>&nbsp; &nbsp;{</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Effect\": \"Deny\",</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Action\": \"ec2:*\",</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Resource\": \"*\",</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Condition\": {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"StringNotEquals\": {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"ec2:Region\": \"us-east-1\"</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]</p><p>&nbsp; &nbsp;}</p><p>What is the effect of this policy?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Administrators cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.1.2.28.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Administrators can terminate an EC2 instance in any AWS Region except us-east-1.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Administrators can terminate an EC2 instance with the IP address 10.1.2.5 in the us-east-1 Region.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Administrators can terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.1.2.28.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Security, Identity, & Compliance",
    "explanation": "<p>The Condition element (or Condition <em>block</em>) lets you specify conditions for when a policy is in effect. The Condition element is optional. In the Condition element, you build expressions in which you use condition operators (equal, less than, etc.) to match the condition keys and values in the policy against keys and values in the request context.</p><p>In this policy statement the first block allows the \"ec2:TerminateInstances\" API action only if the IP address of the requester is within the \"10.1.2.0/24\" range. This is specified using the \"aws:SourceIp\" condition.</p><p>The second block denies all EC2 API actions with a conditional operator (StringNotEquals) that checks the Region the request is being made in (\"ec2:Region\"). If the Region is any value other than us-east-1 the request will be denied. If the Region the request is being made in is us-east-1 the request will not be denied.</p><p><strong>CORRECT: </strong>\"Administrators can terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.1.2.28\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Administrators cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.1.2.28\" is incorrect. This is not true; the conditions allow this action.</p><p><strong>INCORRECT:</strong> \"Administrators can terminate an EC2 instance in any AWS Region except us-east-1\" is incorrect. The API action to terminate instances only has a condition of the source IP. If the source IP is in the range it will allow. The second block only denies API actions if the Region is NOT us-east-1. Therefore, the user can terminate instances in us-east-1</p><p><strong>INCORRECT:</strong> \"Administrators can terminate an EC2 instance with the IP address 10.1.2.5 in the us-east-1 Region\" is incorrect. The aws:SourceIp condition is checking the IP address of the requester (where you’re making the call from), not the resource you want to terminate.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html",
      "https://digitalcloud.training/aws-iam/"
    ]
  },
  {
    "id": 58,
    "question": "<p>An application runs on a fleet of Amazon EC2 instances in an Amazon EC2 Auto Scaling group behind an Elastic Load Balancer. The operations team has determined that the application performs best when the CPU utilization of the EC2 instances is at or near 60%.</p><p>Which scaling configuration should a Solutions Architect use to optimize the applications performance?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a target tracking policy to dynamically scale the Auto Scaling group.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use a scheduled scaling policy to dynamically the Auto Scaling group.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use a simple scaling policy to dynamically scale the Auto Scaling group.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a step scaling policy to dynamically scale the Auto Scaling group.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value.</p><p>The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to changes in the metric due to a changing load pattern.</p><p>The following diagram shows a target tracking policy set to keep the CPU utilization of the EC2 instances at or close to 60%.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-24-12-01662811c941eb690ffe2f963ddac8c5.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-24-12-01662811c941eb690ffe2f963ddac8c5.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Use a target tracking policy to dynamically scale the Auto Scaling group\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use a simple scaling policy to dynamically scale the Auto Scaling group\" is incorrect. Simple scaling is not used for maintaining a target utilization. It is used for making simple adjustments up or down based on a threshold value.</p><p><strong>INCORRECT:</strong> \"Use a step scaling policy to dynamically scale the Auto Scaling group\" is incorrect. Step scaling is not used for maintaining a target utilization. It is used for making step adjustments that vary based on the size of the alarm breach.</p><p><strong>INCORRECT:</strong> \"Use a scheduled scaling policy to dynamically the Auto Scaling group\" is incorrect. Scheduled scaling is not used for maintaining a target utilization. It is used for scheduling changes at specific dates and times.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html",
      "https://digitalcloud.training/amazon-ec2-auto-scaling/"
    ]
  },
  {
    "id": 59,
    "question": "<p>A company is deploying an application that produces data that must be processed in the order it is received. The company requires a solution for decoupling the event data from the processing layer. The solution must minimize operational overhead.</p><p>How can a Solutions Architect meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon SNS topic to decouple the application. Configure an AWS Lambda function as a subscriber.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon SNS topic to decouple the application. Configure an Amazon SQS queue as a subscriber.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon SQS FIFO queue to decouple the application. Configure an AWS Lambda function to process messages from the queue.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon SQS standard queue to decouple the application. Set up an AWS Lambda function to process messages from the queue independently.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>Amazon SQS can be used to decouple this application using a FIFO queue. With a FIFO queue the order in which messages are sent and received is strictly preserved. You can configure an AWS Lambda function to poll the queue, or you can configure a Lambda function as a destination to asynchronously process messages from the queue.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-17-48-037e094b09540ca2dd15db19aec25f8a.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/test_question_description/2021-05-18_05-17-48-037e094b09540ca2dd15db19aec25f8a.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p><strong>CORRECT: </strong>\"Create an Amazon SQS FIFO queue to decouple the application. Configure an AWS Lambda function to process messages from the queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SQS standard queue to decouple the application. Set up an AWS Lambda function to process messages from the queue independently\" is incorrect. A standard queue only offers best-effort ordering so it may not preserve the order of the data.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic to decouple the application. Configure an AWS Lambda function as a subscriber\" is incorrect. Amazon SQS is better for this use case as there are a sequence of events for which the order must be maintained, and these events can be queued for processing whereas SNS delivers them for immediate processing.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS topic to decouple the application. Configure an Amazon SQS queue as a subscriber\" is incorrect. As above an SQS queue would be preferred for queuing the messages.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-lambda-function-trigger.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-lambda-function-trigger.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-lambda-function-trigger.html",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 60,
    "question": "<p>A logistics company needs to replicate ongoing data changes from an on-premises Microsoft SQL Server database to Amazon RDS for SQL Server. The volume of data to replicate varies throughout the day due to periodic spikes in activity. The company plans to use AWS Database Migration Service (AWS DMS) for this task. The solution must dynamically allocate capacity based on workload demand while keeping operational overhead low.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure AWS DMS Serverless to create a replication task that scales its capacity automatically based on workload demand.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon EC2 Spot Instances to host the AWS DMS replication instance and manually scale up or down based on replication needs.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an AWS DMS replication instance with provisioned capacity in a Multi-AZ deployment to improve availability and fault tolerance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy AWS DMS in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster and use an autoscaler to adjust compute capacity during data spikes.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Migration & Transfer",
    "explanation": "<p><strong>Configure AWS DMS Serverless to create a replication task that scales its capacity automatically based on workload demand:</strong> This is correct because AWS DMS Serverless dynamically adjusts replication capacity in response to data volume changes, providing cost efficiency and reducing manual management.</p><p><strong>Deploy AWS DMS in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster and use an autoscaler to adjust compute capacity during data spikes:</strong> This is incorrect because AWS DMS does not support deployment on Amazon EKS. Additionally, this solution introduces unnecessary complexity.</p><p><strong>Use Amazon EC2 Spot Instances to host the AWS DMS replication instance and manually scale up or down based on replication needs:</strong> This is incorrect because manually scaling replication tasks increases operational overhead and Spot Instances could be interrupted, affecting replication stability.</p><p><strong>Create an AWS DMS replication instance with provisioned capacity in a Multi-AZ deployment to improve availability and fault tolerance:</strong> This is incorrect because while Multi-AZ deployment enhances availability, it does not dynamically adjust capacity to match workload demand.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/what-is-dms.html\">https://docs.aws.amazon.com/dms/latest/userguide/what-is-dms.html</a><br><a href=\"https://aws.amazon.com/dms/serverless/\">https://aws.amazon.com/dms/serverless/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/dms/latest/userguide/what-is-dms.html",
      "https://aws.amazon.com/dms/serverless/",
      "https://digitalcloud.training/aws-migration-services/"
    ]
  },
  {
    "id": 61,
    "question": "<p>A company has deployed an application that consists of several microservices running on Amazon EC2 instances behind an Amazon API Gateway API. A Solutions Architect is concerned that the microservices are not designed to elastically scale when large increases in demand occur.</p><p>Which solution addresses this concern?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon SQS queue to store incoming requests. Configure the microservices to retrieve the requests from the queue for processing.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Spread the microservices across multiple Availability Zones and configure Amazon Data Lifecycle Manager to take regular snapshots.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an Elastic Load Balancer to distribute the traffic between the microservices. Configure Amazon CloudWatch metrics to monitor traffic to the microservices.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon CloudWatch alarms to notify operations staff when the microservices are suffering high CPU utilization.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Compute",
    "explanation": "<p>The individual microservices are not designed to scale. Therefore, the best way to ensure they are not overwhelmed by requests is to decouple the requests from the microservices. An Amazon SQS queue can be created, and the API Gateway can be configured to add incoming requests to the queue. The microservices can then pick up the requests from the queue when they are ready to process them.</p><p><strong>CORRECT: </strong>\"Create an Amazon SQS queue to store incoming requests. Configure the microservices to retrieve the requests from the queue for processing\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch alarms to notify operations staff when the microservices are suffering high CPU utilization\" is incorrect. This solution requires manual intervention and does not help the application to elastically scale.</p><p><strong>INCORRECT:</strong> \"Spread the microservices across multiple Availability Zones and configure Amazon Data Lifecycle Manager to take regular snapshots\" is incorrect. This does not automate the elasticity of the application.</p><p><strong>INCORRECT:</strong> \"Use an Elastic Load Balancer to distribute the traffic between the microservices. Configure Amazon CloudWatch metrics to monitor traffic to the microservices\" is incorrect. You cannot use an ELB spread traffic across many different individual microservices as the requests must be directed to individual microservices. Therefore, you would need a target group per microservice, and you would need Auto Scaling to scale the microservices.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/understanding-asynchronous-messaging-for-microservices/\">https://aws.amazon.com/blogs/compute/understanding-asynchronous-messaging-for-microservices/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/blogs/compute/understanding-asynchronous-messaging-for-microservices/",
      "https://digitalcloud.training/aws-application-integration-services/"
    ]
  },
  {
    "id": 62,
    "question": "<p>A media company is building a video content distribution platform on AWS. The platform uses an REST API hosted on Amazon API Gateway to serve metadata about the videos, such as titles and descriptions. The metadata is confidential and must be accessible only from a specific set of trusted IP addresses belonging to the company’s office network.</p><p>Which solution will meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure an API Gateway resource policy that denies access to any IP address that is not explicitly allowed.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Modify the API Gateway security group to allow inbound requests only from the trusted IP addresses.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up API Gateway with a private integration and restrict access to the trusted IP addresses using a VPC endpoint policy.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the API Gateway in a private subnet and configure a network ACL to permit traffic only from the trusted IP addresses.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "AWS Networking & Content Delivery",
    "explanation": "<p><strong>Configure an API Gateway resource policy that denies access to any IP address that is not explicitly allowed:</strong> This is correct because resource policies in API Gateway allow you to restrict access to APIs by specifying conditions, such as IP addresses. By creating a resource policy with a condition that permits traffic only from the trusted IP range, you can ensure that the API is accessible only from the company’s internal network.</p><p><strong>Deploy the API Gateway in a private subnet and configure a network ACL to permit traffic only from the trusted IP addresses:</strong> This is incorrect because API Gateway cannot be directly deployed within a private subnet. Instead, it is a managed service that operates outside of VPCs. To use a private network, you would need a VPC endpoint, but this does not fully meet the requirements without additional configuration.</p><p><strong>Set up API Gateway with a private integration and restrict access to the trusted IP addresses using a VPC endpoint policy:</strong> This is incorrect because private integrations in API Gateway are used to route traffic to resources within a VPC, such as an application running on Amazon EC2. While you can use a VPC endpoint policy for restricting access, this setup is more complex than using a resource policy and does not directly fulfill the requirement of limiting IP addresses for the API.</p><p><strong>Modify the API Gateway security group to allow inbound requests only from the trusted IP addresses:</strong> This is incorrect because API Gateway is not associated with security groups. Security groups are used for resources like EC2 instances or load balancers, not for API Gateway endpoints.</p><p><strong>References:</strong><br><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html</a><br><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-privatelink.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-privatelink.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-privatelink.html",
      "https://digitalcloud.training/amazon-api-gateway/"
    ]
  },
  {
    "id": 63,
    "question": "<p>A gaming company recently launched a multiplayer gaming platform for its users. The platform runs on multiple Amazon EC2 instances across two Availability Zones. Players use TCP to communicate with the platform in real time. The platform must be highly available and automatically scale as the number of players increases, while remaining cost-effective.</p><p>Which combination of steps will meet these requirements MOST cost-effectively? (Select TWO.)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add a Network Load Balancer in front of the EC2 instances to manage TCP traffic.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use an Application Load Balancer to distribute TCP traffic to the EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure Amazon Route 53 to implement latency-based routing across multiple EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure an Auto Scaling group to add or remove EC2 instances based on player traffic.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Deploy an Amazon ECS cluster to replace the EC2 instances and handle player traffic.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "AWS Compute",
    "explanation": "<p><strong>Configure an Auto Scaling group to add or remove EC2 instances based on player traffic:</strong> This is correct because an Auto Scaling group automatically adjusts the number of EC2 instances in response to traffic changes. This ensures cost efficiency by scaling out during high demand and scaling in during low demand.</p><p><strong>Add a Network Load Balancer in front of the EC2 instances to manage TCP traffic:</strong> This is correct because a Network Load Balancer is optimized for handling TCP traffic with low latency. It provides the required scalability and high availability across multiple Availability Zones.</p><p><strong>Use an Application Load Balancer to distribute TCP traffic to the EC2 instances:</strong> This is incorrect because Application Load Balancers are optimized for HTTP/HTTPS traffic, not TCP. A Network Load Balancer is more cost-effective and appropriate for this use case.</p><p><strong>Deploy an Amazon ECS cluster to replace the EC2 instances and handle player traffic:</strong> This is incorrect because switching to Amazon ECS introduces operational complexity and higher costs. The question specifies using EC2 instances, so Auto Scaling and a Network Load Balancer are more cost-effective and aligned with the scenario.</p><p><strong>Configure Amazon Route 53 to implement latency-based routing across multiple EC2 instances:</strong> This is incorrect because Route 53 latency-based routing is not designed to provide automatic scaling or load balancing. Instead, it is used for directing traffic across endpoints in different Regions or locations.</p><p><strong>References:</strong><br><a href=\"https://aws.amazon.com/elasticloadbalancing/network-load-balancer/\">https://aws.amazon.com/elasticloadbalancing/network-load-balancer/</a><br><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://aws.amazon.com/elasticloadbalancing/network-load-balancer/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html",
      "https://digitalcloud.training/amazon-ec2-auto-scaling/",
      "https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/"
    ]
  },
  {
    "id": 64,
    "question": "<p>A web app allows users to upload images for viewing online. The compute layer that processes the images is behind an Auto Scaling group. The processing layer should be decoupled from the front end and the ASG needs to dynamically adjust based on the number of images being uploaded.</p><p>How can this be achieved?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon SNS Topic to generate a notification each time a message is uploaded. Have the ASG scale based on the number of SNS messages</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a target tracking policy that keeps the ASG at 70% CPU utilization</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a scheduled policy that scales the ASG at times of expected peak load</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon SQS queue and custom CloudWatch metric to measure the number of messages in the queue. Configure the ASG to scale based on the number of messages in the queue</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Application Integration",
    "explanation": "<p>The best solution is to use Amazon SQS to decouple the front end from the processing compute layer. To do this you can create a custom CloudWatch metric that measures the number of messages in the queue and then configure the ASG to scale using a target tracking policy that tracks a certain value.</p><p><strong>CORRECT: </strong>\"Create an Amazon SQS queue and custom CloudWatch metric to measure the number of messages in the queue. Configure the ASG to scale based on the number of messages in the queue\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an Amazon SNS Topic to generate a notification each time a message is uploaded. Have the ASG scale based on the number of SNS messages\" is incorrect. The Amazon Simple Notification Service (SNS) is used for sending notifications using topics. Amazon SQS is a better solution for this scenario as it provides a decoupling mechanism where the actual images can be stored for processing. SNS does not provide somewhere for the images to be stored.</p><p><strong>INCORRECT:</strong> \"Create a target tracking policy that keeps the ASG at 70% CPU utilization\" is incorrect. Using a target tracking policy with the ASG that tracks CPU utilization does not allow scaling based on the number of images being uploaded.</p><p><strong>INCORRECT:</strong> \"Create a scheduled policy that scales the ASG at times of expected peak load\" is incorrect. Using a scheduled policy is less dynamic as though you may be able to predict usage patterns, it would be better to adjust dynamically based on actual usage.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html",
      "https://digitalcloud.training/amazon-ec2-auto-scaling/"
    ]
  },
  {
    "id": 65,
    "question": "<p>A security team wants to limit access to specific services or actions in all of the team's AWS accounts. All accounts belong to a large organization in AWS Organizations. The solution must be scalable and there must be a single point where permissions can be maintained.</p><p>What should a solutions architect do to accomplish this?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a security group to allow accounts and attach it to user groups</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an ACL to provide access to the services or actions</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create cross-account roles in each account to deny access to the services or actions</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a service control policy in the root organizational unit to deny access to the services or actions</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "AWS Management & Governance",
    "explanation": "<p>Service control policies (SCPs) offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-57-51-f9ea69c694961184b290662661f770a4.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-05-21_01-57-51-f9ea69c694961184b290662661f770a4.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p><p>SCPs alone are not sufficient for allowing access in the accounts in your organization. Attaching an SCP to an AWS Organizations entity (root, OU, or account) defines a guardrail for what actions the principals can perform. You still need to attach <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html\">identity-based or resource-based policies</a> to principals or resources in your organization's accounts to actually grant permissions to them.</p><p><strong>CORRECT: </strong>\"Create a service control policy in the root organizational unit to deny access to the services or actions\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Create an ACL to provide access to the services or actions\" is incorrect as access control lists are not used for permissions associated with IAM. Permissions policies are used with IAM.</p><p><strong>INCORRECT:</strong> \"Create a security group to allow accounts and attach it to user groups\" is incorrect as security groups are instance level firewalls. They do not limit service actions.</p><p><strong>INCORRECT:</strong> \"Create cross-account roles in each account to deny access to the services or actions\" is incorrect as this is a complex solution and does not provide centralized control</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html",
      "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html",
      "https://digitalcloud.training/aws-organizations/"
    ]
  }
]