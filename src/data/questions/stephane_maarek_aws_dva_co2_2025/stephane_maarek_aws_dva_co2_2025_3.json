[
  {
    "id": 1,
    "question": "<p>You are working for a shipping company that is automating the creation of ECS clusters with an Auto Scaling Group using an AWS CloudFormation template that accepts cluster name as its parameters.  Initially, you launch the template with input value 'MainCluster', which deployed five instances across two availability zones. The second time, you launch the template with an input value 'SecondCluster'. However, the instances created in the second run were also launched in 'MainCluster' even after specifying a different cluster name.</p>\n\n<p>What is the root cause of this issue?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>The ECS agent Docker image must be re-built to connect to the other clusters</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The EC2 instance is missing IAM permissions to join the other clusters</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The security groups on the EC2 instance are pointing to the wrong ECS cluster</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap</strong> - In the ecs.config file you have to configure the parameter ECS_CLUSTER='your_cluster_name' to register the container instance with a cluster named 'your_cluster_name'.</p>\n\n<p>Sample config for ECS Container Agent:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q35-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q35-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EC2 instance is missing IAM permissions to join the other clusters</strong> - EC2 instances are getting registered to the first cluster, so permissions are not an issue here and hence this statement is an incorrect choice for the current use case.</p>\n\n<p><strong>The ECS agent Docker image must be re-built to connect to the other clusters</strong> -  Since the first set of instances got created from the template without any issues, there is no issue with the ECS agent here.</p>\n\n<p><strong>The security groups on the EC2 instance are pointing to the wrong ECS cluster</strong> - Security groups govern the rules about the incoming network traffic to your ECS containers. The issue here is not about user access and hence is a wrong choice for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap</strong> - In the ecs.config file you have to configure the parameter ECS_CLUSTER='your_cluster_name' to register the container instance with a cluster named 'your_cluster_name'."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q35-i1.jpg",
        "answer": "",
        "explanation": "Sample config for ECS Container Agent:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The EC2 instance is missing IAM permissions to join the other clusters</strong> - EC2 instances are getting registered to the first cluster, so permissions are not an issue here and hence this statement is an incorrect choice for the current use case."
      },
      {
        "answer": "",
        "explanation": "<strong>The ECS agent Docker image must be re-built to connect to the other clusters</strong> -  Since the first set of instances got created from the template without any issues, there is no issue with the ECS agent here."
      },
      {
        "answer": "",
        "explanation": "<strong>The security groups on the EC2 instance are pointing to the wrong ECS cluster</strong> - Security groups govern the rules about the incoming network traffic to your ECS containers. The issue here is not about user access and hence is a wrong choice for the current use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html"
    ]
  },
  {
    "id": 2,
    "question": "<p>A company uses Amazon RDS as its database. For improved user experience, it has been decided that a highly reliable fully-managed caching layer has to be configured in front of RDS.</p>\n\n<p>Which of the following is the right choice, keeping in mind that cache content regeneration is a costly activity?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Install Redis on an Amazon EC2 instance</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Migrate the database to Amazon Redshift</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Implement Amazon ElastiCache Memcached</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Implement Amazon ElastiCache Redis in Cluster Mode</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement Amazon ElastiCache Redis in Cluster-Mode</strong> - One can leverage ElastiCache for Redis with cluster mode enabled to enhance reliability and availability with little change to your existing workload. Cluster mode comes with the primary benefit of horizontal scaling of your Redis cluster, with almost zero impact on the performance of the cluster.</p>\n\n<p>When building production workloads, you should consider using a configuration with replication, unless you can easily recreate your data. Enabling Cluster-Mode provides a number of additional benefits in scaling your cluster. In short, it allows you to scale in or out the number of shards (horizontal scaling) versus scaling up or down the node type (vertical scaling). This means that Cluster-Mode can scale to very large amounts of storage (potentially 100s of terabytes) across up to 90 shards, whereas a single node can only store as much data in memory as the instance type has capacity for.</p>\n\n<p>Redis Cluster config:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q14-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q14-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/\">https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Install Redis on an Amazon EC2 instance</strong> - It is possible to install Redis directly onto Amazon EC2 instance. But, unlike ElastiCache for Redis, which is a managed service, you will need to maintain and manage your Redis installation.</p>\n\n<p><strong>Implement Amazon ElastiCache Memcached</strong> - Redis and Memcached are popular, open-source, in-memory data stores. Although they are both easy to use and offer high performance, there are important differences to consider when choosing an engine. Memcached is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases. Redis offers snapshots facility, replication, and supports transactions, which Memcached cannot and hence ElastiCache Redis is the right choice for our use case.</p>\n\n<p><strong>Migrate the database to Amazon Redshift</strong> - Amazon Redshift belongs to \"Big Data as a Service\" cloud facility, while Redis can be primarily classified under \"In-Memory Databases\". \"Data Warehousing\" is the primary reason why developers consider Amazon Redshift over the competitors, whereas \"Performance\" is the key factor in picking Redis.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/\">https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Implement Amazon ElastiCache Redis in Cluster-Mode</strong> - One can leverage ElastiCache for Redis with cluster mode enabled to enhance reliability and availability with little change to your existing workload. Cluster mode comes with the primary benefit of horizontal scaling of your Redis cluster, with almost zero impact on the performance of the cluster."
      },
      {
        "answer": "",
        "explanation": "When building production workloads, you should consider using a configuration with replication, unless you can easily recreate your data. Enabling Cluster-Mode provides a number of additional benefits in scaling your cluster. In short, it allows you to scale in or out the number of shards (horizontal scaling) versus scaling up or down the node type (vertical scaling). This means that Cluster-Mode can scale to very large amounts of storage (potentially 100s of terabytes) across up to 90 shards, whereas a single node can only store as much data in memory as the instance type has capacity for."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q14-i1.jpg",
        "answer": "",
        "explanation": "Redis Cluster config:"
      },
      {
        "link": "https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Install Redis on an Amazon EC2 instance</strong> - It is possible to install Redis directly onto Amazon EC2 instance. But, unlike ElastiCache for Redis, which is a managed service, you will need to maintain and manage your Redis installation."
      },
      {
        "answer": "",
        "explanation": "<strong>Implement Amazon ElastiCache Memcached</strong> - Redis and Memcached are popular, open-source, in-memory data stores. Although they are both easy to use and offer high performance, there are important differences to consider when choosing an engine. Memcached is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases. Redis offers snapshots facility, replication, and supports transactions, which Memcached cannot and hence ElastiCache Redis is the right choice for our use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Migrate the database to Amazon Redshift</strong> - Amazon Redshift belongs to \"Big Data as a Service\" cloud facility, while Redis can be primarily classified under \"In-Memory Databases\". \"Data Warehousing\" is the primary reason why developers consider Amazon Redshift over the competitors, whereas \"Performance\" is the key factor in picking Redis."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/",
      "https://aws.amazon.com/elasticache/redis-vs-memcached/"
    ]
  },
  {
    "id": 3,
    "question": "<p>An organization is moving its on-premises resources to the cloud. Source code will be moved to AWS CodeCommit and AWS CodeBuild will be used for compiling the source code using Apache Maven as a build tool. The organization wants the build environment should allow for scaling and running builds in parallel.</p>\n\n<p>Which of the following options should the organization choose for their requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable CodeBuild Auto Scaling</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Choose a high-performance instance type for your CodeBuild instances</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Run CodeBuild in an Auto Scaling group</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>CodeBuild scales automatically, the organization does not have to do anything for scaling or for parallel builds</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>CodeBuild scales automatically, the organization does not have to do anything for scaling or for parallel builds</strong> - AWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more. You can also customize build environments in CodeBuild to use your own build tools. CodeBuild scales automatically to meet peak build requests.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Choose a high-performance instance type for your CodeBuild instances</strong> - For the current requirement, this is will not make any difference.</p>\n\n<p><strong>Run CodeBuild in an Auto Scaling Group</strong> - AWS CodeBuild is a managed service and scales automatically, does not need Auto Scaling Group to scale it up.</p>\n\n<p><strong>Enable CodeBuild Auto Scaling</strong> - This has been added as a distractor. CodeBuild scales automatically to meet peak build requests.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-compute-types.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-compute-types.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>CodeBuild scales automatically, the organization does not have to do anything for scaling or for parallel builds</strong> - AWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more. You can also customize build environments in CodeBuild to use your own build tools. CodeBuild scales automatically to meet peak build requests."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Choose a high-performance instance type for your CodeBuild instances</strong> - For the current requirement, this is will not make any difference."
      },
      {
        "answer": "",
        "explanation": "<strong>Run CodeBuild in an Auto Scaling Group</strong> - AWS CodeBuild is a managed service and scales automatically, does not need Auto Scaling Group to scale it up."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable CodeBuild Auto Scaling</strong> - This has been added as a distractor. CodeBuild scales automatically to meet peak build requests."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html",
      "https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-compute-types.html"
    ]
  },
  {
    "id": 4,
    "question": "<p>As part of their on-boarding, the employees at an IT company need to upload their profile photos in a private S3 bucket. The company wants to build an in-house web application hosted on an EC2 instance that should display the profile photos in a secure way when the employees mark their attendance.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest to address this use-case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Save the S3 key for each user's profile photo in a DynamoDB table and use a lambda function to dynamically generate a pre-signed URL. Reference this URL for display via the web application</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Make the S3 bucket public so that the application can reference the image URL for display</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Keep each user's profile image encoded in base64 format in a DynamoDB table and reference it from the application for display</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Keep each user's profile image encoded in base64 format in an RDS table and reference it from the application for display</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>\"Save the S3 key for each user's profile photo in a DynamoDB table and use a lambda function to dynamically generate a pre-signed URL. Reference this URL for display via the web application\"</p>\n\n<p>On Amazon S3, all objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.</p>\n\n<p>You can also use an IAM instance profile to create a pre-signed URL. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The pre-signed URLs are valid only for the specified duration. So for the given use-case, the object key can be retrieved from the DynamoDB table, and then the application can generate the pre-signed URL using the IAM instance profile.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q5-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q5-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Make the S3 bucket public so that the application can reference the image URL for display\" - Making the S3 bucket public would violate the security and privacy requirements for the use-case, so this option is incorrect.</p>\n\n<p>\"Keep each user's profile image encoded in base64 format in a DynamoDB table and reference it from the application for display\"</p>\n\n<p>\"Keep each user's profile image encoded in base64 format in an RDS table and reference it from the application for display\"</p>\n\n<p>It's a bad practice to keep the raw image data in the database itself. Also, it would not be possible to create a secure access URL for the image without a significant development effort. Hence both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Save the S3 key for each user's profile photo in a DynamoDB table and use a lambda function to dynamically generate a pre-signed URL. Reference this URL for display via the web application</strong>"
      },
      {
        "answer": "",
        "explanation": "On Amazon S3, all objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects."
      },
      {
        "answer": "",
        "explanation": "You can also use an IAM instance profile to create a pre-signed URL. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The pre-signed URLs are valid only for the specified duration. So for the given use-case, the object key can be retrieved from the DynamoDB table, and then the application can generate the pre-signed URL using the IAM instance profile."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q5-i1.jpg",
        "answer": "",
        "explanation": "Please see this note for more details:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Make the S3 bucket public so that the application can reference the image URL for display</strong> - Making the S3 bucket public would violate the security and privacy requirements for the use-case, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Keep each user's profile image encoded in base64 format in a DynamoDB table and reference it from the application for display</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Keep each user's profile image encoded in base64 format in an RDS table and reference it from the application for display</strong>"
      },
      {
        "answer": "",
        "explanation": "It's a bad practice to keep the raw image data in the database itself. Also, it would not be possible to create a secure access URL for the image without a significant development effort. Hence both these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html"
    ]
  },
  {
    "id": 5,
    "question": "<p>Your company uses an Application Load Balancer to route incoming end-user traffic to applications hosted on Amazon EC2 instances. The applications capture incoming request information and store it in the Amazon Relational Database Service (RDS) running on Microsoft SQL Server DB engines.</p>\n\n<p>As part of new compliance rules, you need to capture the client's IP address. How will you achieve this?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>You can get the Client IP addresses from Elastic Load Balancing logs</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>You can get the Client IP addresses from server access logs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the header X-Forwarded-From</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the header X-Forwarded-For</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the header X-Forwarded-For</strong> - The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can get the Client IP addresses from server access logs</strong> - As discussed above, Load Balancers intercept traffic between clients and servers, so server access logs will contain only the IP address of the load balancer.</p>\n\n<p><strong>Use the header X-Forwarded-From</strong> - This is a made-up option and given as a distractor.</p>\n\n<p><strong>You can get the Client IP addresses from Elastic Load Balancing logs</strong> - Elastic Load Balancing logs requests sent to the load balancer, including requests that never made it to the targets. For example, if a client sends a malformed request, or there are no healthy targets to respond to the request, the request is still logged. So, this is not the right option if we wish to collect the IP addresses of the clients that have access to the instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the header X-Forwarded-For</strong> - The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You can get the Client IP addresses from server access logs</strong> - As discussed above, Load Balancers intercept traffic between clients and servers, so server access logs will contain only the IP address of the load balancer."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the header X-Forwarded-From</strong> - This is a made-up option and given as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>You can get the Client IP addresses from Elastic Load Balancing logs</strong> - Elastic Load Balancing logs requests sent to the load balancer, including requests that never made it to the targets. For example, if a client sends a malformed request, or there are no healthy targets to respond to the request, the request is still logged. So, this is not the right option if we wish to collect the IP addresses of the clients that have access to the instances."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html"
    ]
  },
  {
    "id": 6,
    "question": "<p>You have been asked by your Team Lead to enable detailed monitoring of the Amazon EC2 instances your team uses. As a Developer working on AWS CLI, which of the below command will you run?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>aws ec2 monitor-instances --instance-ids i-1234567890abcdef0</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>aws ec2 monitor-instances --instance-id i-1234567890abcdef0</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>aws ec2 run-instances --image-id ami-09092360 --monitoring State=enabled</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>aws ec2 run-instances --image-id ami-09092360 --monitoring Enabled=true</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong><code>aws ec2 monitor-instances --instance-ids i-1234567890abcdef0</code></strong> - This enables detailed monitoring for a running instance.</p>\n\n<p>EC2 detailed monitoring:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q19-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q19-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/cli/latest/reference/ec2/monitor-instances.html\">https://docs.aws.amazon.com/cli/latest/reference/ec2/monitor-instances.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>aws ec2 run-instances --image-id ami-09092360 --monitoring Enabled=true</code></strong> - This syntax is used to enable detailed monitoring when launching an instance from AWS CLI.</p>\n\n<p><strong><code>aws ec2 run-instances --image-id ami-09092360 --monitoring State=enabled</code></strong> - This is an invalid syntax</p>\n\n<p><strong><code>aws ec2 monitor-instances --instance-id i-1234567890abcdef0</code></strong> - This is an invalid syntax</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/ec2/run-instances.html\">https://docs.aws.amazon.com/cli/latest/reference/ec2/run-instances.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong><code>aws ec2 monitor-instances --instance-ids i-1234567890abcdef0</code></strong> - This enables detailed monitoring for a running instance."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q19-i1.jpg",
        "answer": "",
        "explanation": "EC2 detailed monitoring:"
      },
      {
        "link": "https://docs.aws.amazon.com/cli/latest/reference/ec2/monitor-instances.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong><code>aws ec2 run-instances --image-id ami-09092360 --monitoring Enabled=true</code></strong> - This syntax is used to enable detailed monitoring when launching an instance from AWS CLI."
      },
      {
        "answer": "",
        "explanation": "<strong><code>aws ec2 run-instances --image-id ami-09092360 --monitoring State=enabled</code></strong> - This is an invalid syntax"
      },
      {
        "answer": "",
        "explanation": "<strong><code>aws ec2 monitor-instances --instance-id i-1234567890abcdef0</code></strong> - This is an invalid syntax"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/cli/latest/reference/ec2/monitor-instances.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html",
      "https://docs.aws.amazon.com/cli/latest/reference/ec2/run-instances.html"
    ]
  },
  {
    "id": 7,
    "question": "<p>A development team has deployed a REST API in Amazon API Gateway to two different stages - a test stage and a prod stage. The test stage is used as a test build and the prod stage as a stable build. After the updates have passed the test, the team wishes to promote the test stage to the prod stage.</p>\n\n<p>Which of the following represents the optimal solution for this use-case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Update stage variable value from the stage name of test to that of prod</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the API to the prod stage</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Update stage variable value from the stage name of test to that of prod</strong></p>\n\n<p>After creating your API, you must deploy it to make it callable by your users. To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. They're included in the URL that you use to invoke the API. Each stage is a named reference to a deployment of the API and is made available for client applications to call.</p>\n\n<p>Stages enable robust version control of your API. In our current use-case, after the updates pass the test, you can promote the test stage to the prod stage. The promotion can be done by redeploying the API to the prod stage or updating a stage variable value from the stage name of test to that of prod.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages</strong> - An API can only be deployed to a stage. Hence, it is not possible to deploy an API without choosing a stage.</p>\n\n<p><em>Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage</em>* - This is possible, but not an optimal way of deploying a change. Also, as prod refers to real production system, this option will result in downtime.</p>\n\n<p><strong>API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the API to the prod stage</strong> - For each stage, you can optimize API performance by adjusting the default account-level request throttling limits and enabling API caching. And these settings can be changed/updated at any time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Update stage variable value from the stage name of test to that of prod</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "After creating your API, you must deploy it to make it callable by your users. To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. They're included in the URL that you use to invoke the API. Each stage is a named reference to a deployment of the API and is made available for client applications to call."
      },
      {
        "answer": "",
        "explanation": "Stages enable robust version control of your API. In our current use-case, after the updates pass the test, you can promote the test stage to the prod stage. The promotion can be done by redeploying the API to the prod stage or updating a stage variable value from the stage name of test to that of prod."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages</strong> - An API can only be deployed to a stage. Hence, it is not possible to deploy an API without choosing a stage."
      },
      {
        "answer": "",
        "explanation": "<em>Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage</em>* - This is possible, but not an optimal way of deploying a change. Also, as prod refers to real production system, this option will result in downtime."
      },
      {
        "answer": "",
        "explanation": "<strong>API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the API to the prod stage</strong> - For each stage, you can optimize API performance by adjusting the default account-level request throttling limits and enabling API caching. And these settings can be changed/updated at any time."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html"
    ]
  },
  {
    "id": 8,
    "question": "<p>You have a three-tier web application consisting of a web layer using AngularJS, an application layer using an AWS API Gateway and a data layer in an Amazon Relational Database Service (RDS) database. Your web application allows visitors to look up popular movies from the past. The company is looking at reducing the number of calls made to endpoint and improve latency to the API.</p>\n\n<p>What can you do to improve performance?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Stage Variables</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Mapping Templates</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Kinesis Data Streams to stream incoming data and reduce the burden on Gateway APIs</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable API Gateway Caching</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable API Gateway Caching</strong> - You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Mapping Templates</strong> - A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates do not help in latency issues of the APIs.</p>\n\n<p><strong>Use Stage Variables</strong> - Stage variables act like environment variables and can be used to change the behavior of your API Gateway methods for each deployment stage; for example, making it possible to reach a different back end depending on which stage the API is running on. Stage variables do not help in latency issues.</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to stream incoming data and reduce the burden on Gateway APIs</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable API Gateway Caching</strong> - You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Mapping Templates</strong> - A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates do not help in latency issues of the APIs."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Stage Variables</strong> - Stage variables act like environment variables and can be used to change the behavior of your API Gateway methods for each deployment stage; for example, making it possible to reach a different back end depending on which stage the API is running on. Stage variables do not help in latency issues."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Data Streams to stream incoming data and reduce the burden on Gateway APIs</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html"
    ]
  },
  {
    "id": 9,
    "question": "<p>As a developer, you are looking at creating a custom configuration for Amazon EC2 instances running in an Auto Scaling group. The solution should allow the group to auto-scale based on the metric of 'average RAM usage' for your Amazon EC2 instances.</p>\n\n<p>Which option provides the best solution?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a custom alarm for your ASG and make your instances trigger the alarm using PutAlarmData API</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Migrate your application to AWS Lambda</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable detailed monitoring for EC2 and ASG to get the RAM usage data and create a CloudWatch Alarm on top of it</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a custom metric in CloudWatch and make your instances send data to it using PutMetricData. Then, create an alarm based on this metric</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a custom metric in CloudWatch and make your instances send data to it using PutMetricData. Then, create an alarm based on this metric</strong> - You can create a custom CloudWatch metric for your EC2 Linux instance statistics by creating a script through the AWS Command Line Interface (AWS CLI). Then, you can monitor that metric by pushing it to CloudWatch.</p>\n\n<p>You can publish your own metrics to CloudWatch using the AWS CLI or an API. Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p>\n\n<p>High-resolution metrics can give you more immediate insight into your application's sub-minute activity. But, every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a high-resolution metric can lead to higher charges.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a custom alarm for your ASG and make your instances trigger the alarm using PutAlarmData API</strong> - This solution will not work, your instances must be aware of each other's RAM utilization status, to know when the average RAM would be too high to trigger the alarm.</p>\n\n<p><strong>Enable detailed monitoring for EC2 and ASG to get the RAM usage data and create a CloudWatch Alarm on top of it</strong> - By enabling detailed monitoring you define the frequency at which the metric data has to be sent to CloudWatch, from 5 minutes to 1-minute frequency window. But, you still need to create and collect the custom metric you wish to track.</p>\n\n<p><strong>Migrate your application to AWS Lambda</strong> - This option has been added as a distractor. You cannot use Lambda for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html#CloudWatchAlarm\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html#CloudWatchAlarm</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a custom metric in CloudWatch and make your instances send data to it using PutMetricData. Then, create an alarm based on this metric</strong> - You can create a custom CloudWatch metric for your EC2 Linux instance statistics by creating a script through the AWS Command Line Interface (AWS CLI). Then, you can monitor that metric by pushing it to CloudWatch."
      },
      {
        "answer": "",
        "explanation": "You can publish your own metrics to CloudWatch using the AWS CLI or an API. Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds."
      },
      {
        "answer": "",
        "explanation": "High-resolution metrics can give you more immediate insight into your application's sub-minute activity. But, every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a high-resolution metric can lead to higher charges."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a custom alarm for your ASG and make your instances trigger the alarm using PutAlarmData API</strong> - This solution will not work, your instances must be aware of each other's RAM utilization status, to know when the average RAM would be too high to trigger the alarm."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable detailed monitoring for EC2 and ASG to get the RAM usage data and create a CloudWatch Alarm on top of it</strong> - By enabling detailed monitoring you define the frequency at which the metric data has to be sent to CloudWatch, from 5 minutes to 1-minute frequency window. But, you still need to create and collect the custom metric you wish to track."
      },
      {
        "answer": "",
        "explanation": "<strong>Migrate your application to AWS Lambda</strong> - This option has been added as a distractor. You cannot use Lambda for the given use-case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html#CloudWatchAlarm"
    ]
  },
  {
    "id": 10,
    "question": "<p>You company runs business logic on smaller software components that perform various functions. Some functions process information in a few seconds while others seem to take a long time to complete. Your manager asked you to decouple components that take a long time to ensure software applications stay responsive under load. You decide to configure Amazon Simple Queue Service (SQS) to work with your Elastic Beanstalk configuration.</p>\n\n<p>Which of the following Elastic Beanstalk environment should you choose to meet this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Single Instance Worker node</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Single Instance with Elastic IP</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Load-balancing, Autoscaling environment</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Dedicated worker environment</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>Elastic BeanStalk Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q59-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q59-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a><p></p>\n\n<p><strong>Dedicated worker environment</strong> - If your AWS Elastic Beanstalk application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load.</p>\n\n<p>A long-running task is anything that substantially increases the time it takes to complete a request, such as processing images or videos, sending emails, or generating a ZIP archive. These operations can take only a second or two to complete, but a delay of a few seconds is a lot for a web request that would otherwise complete in less than 500 ms.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q59-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q59-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Single Instance Worker node</strong> - Worker machines in Kubernetes are called nodes. Amazon EKS worker nodes are standard Amazon EC2 instances. EKS worker nodes are not to be confused with the Elastic Beanstalk worker environment. Since we are talking about the Elastic Beanstalk environment, this is not the correct choice.</p>\n\n<p><strong>Load-balancing, Autoscaling environment</strong> - A load-balancing and autoscaling environment uses the Elastic Load Balancing and Amazon EC2 Auto Scaling services to provision the Amazon EC2 instances that are required for your deployed application. Amazon EC2 Auto Scaling automatically starts additional instances to accommodate increasing load on your application. If your application requires scalability with the option of running in multiple Availability Zones, use a load-balancing, autoscaling environment. This is not the right environment for the given use-case since it will add costs to the overall solution.</p>\n\n<p><strong>Single Instance with Elastic IP</strong> - A single-instance environment contains one Amazon EC2 instance with an Elastic IP address. A single-instance environment doesn't have a load balancer, which can help you reduce costs compared to a load-balancing, autoscaling environment. This is not a highly available architecture, because if that one instance goes down then your application is down. This is not recommended for production environments.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q59-i1.jpg",
        "answer": "",
        "explanation": "Elastic BeanStalk Key Concepts:"
      },
      {
        "link": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Dedicated worker environment</strong> - If your AWS Elastic Beanstalk application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load."
      },
      {
        "answer": "",
        "explanation": "A long-running task is anything that substantially increases the time it takes to complete a request, such as processing images or videos, sending emails, or generating a ZIP archive. These operations can take only a second or two to complete, but a delay of a few seconds is a lot for a web request that would otherwise complete in less than 500 ms."
      },
      {
        "link": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Single Instance Worker node</strong> - Worker machines in Kubernetes are called nodes. Amazon EKS worker nodes are standard Amazon EC2 instances. EKS worker nodes are not to be confused with the Elastic Beanstalk worker environment. Since we are talking about the Elastic Beanstalk environment, this is not the correct choice."
      },
      {
        "answer": "",
        "explanation": "<strong>Load-balancing, Autoscaling environment</strong> - A load-balancing and autoscaling environment uses the Elastic Load Balancing and Amazon EC2 Auto Scaling services to provision the Amazon EC2 instances that are required for your deployed application. Amazon EC2 Auto Scaling automatically starts additional instances to accommodate increasing load on your application. If your application requires scalability with the option of running in multiple Availability Zones, use a load-balancing, autoscaling environment. This is not the right environment for the given use-case since it will add costs to the overall solution."
      },
      {
        "answer": "",
        "explanation": "<strong>Single Instance with Elastic IP</strong> - A single-instance environment contains one Amazon EC2 instance with an Elastic IP address. A single-instance environment doesn't have a load balancer, which can help you reduce costs compared to a load-balancing, autoscaling environment. This is not a highly available architecture, because if that one instance goes down then your application is down. This is not recommended for production environments."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html"
    ]
  },
  {
    "id": 11,
    "question": "<p>An application runs on an EC2 instance and processes orders on a nightly basis. This EC2 instance needs to access the orders that are stored in S3.</p>\n\n<p>How would you recommend the EC2 instance access the orders securely?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use EC2 User Data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an S3 bucket policy that authorises public access</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an IAM role</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an IAM programmatic user and store the access key and secret access key on the EC2 <code>~/.aws/credentials</code> file.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use an IAM role</strong></p>\n\n<p>IAM roles have been incorporated so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.</p>\n\n<p>Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the IAM console, the console creates an instance profile automatically and gives it the same name as the role to which it corresponds.</p>\n\n<p>This is the most secure option as the role assigned to EC2 can be used to access S3 without storing any credentials onto the EC2 instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM programmatic user and store the access key and secret access key on the EC2 <code>~/.aws/credentials</code> file.</strong> - While this would work, this is highly insecure as anyone gaining access to the EC2 instance would be able to steal the credentials stored in that file.</p>\n\n<p><strong>Use EC2 User Data</strong> - EC2 User Data is used to run bootstrap scripts at an instance's first launch. This option is not the right fit for the given use-case.</p>\n\n<p><strong>Create an S3 bucket policy that authorizes public access</strong> - While this would work, it would allow anyone to access your S3 bucket files. So this option is ruled out.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use an IAM role</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "IAM roles have been incorporated so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles."
      },
      {
        "answer": "",
        "explanation": "Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the IAM console, the console creates an instance profile automatically and gives it the same name as the role to which it corresponds."
      },
      {
        "answer": "",
        "explanation": "This is the most secure option as the role assigned to EC2 can be used to access S3 without storing any credentials onto the EC2 instance."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an IAM programmatic user and store the access key and secret access key on the EC2 <code>~/.aws/credentials</code> file.</strong> - While this would work, this is highly insecure as anyone gaining access to the EC2 instance would be able to steal the credentials stored in that file."
      },
      {
        "answer": "",
        "explanation": "<strong>Use EC2 User Data</strong> - EC2 User Data is used to run bootstrap scripts at an instance's first launch. This option is not the right fit for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an S3 bucket policy that authorizes public access</strong> - While this would work, it would allow anyone to access your S3 bucket files. So this option is ruled out."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html"
    ]
  },
  {
    "id": 12,
    "question": "<p>Your company hosts a static website on Amazon Simple Storage Service (S3) written in HTML5. The website targets aviation enthusiasts and it has grown a worldwide audience with hundreds of thousands of visitors accessing the website now on a monthly basis. While users in the United States have a great user experience, users from other parts of the world are experiencing slow responses and lag.</p>\n\n<p>Which service can mitigate this issue?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon ElastiCache for Redis</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon S3 Transfer Acceleration</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 Caching</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon CloudFront</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon CloudFront</strong></p>\n\n<p>Storing your static content with S3 provides a lot of advantages. But to help optimize your applications performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your S3 bucket to serve and protect the content. CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>By caching your content in Edge Locations, CloudFront reduces the load on your S3 bucket and helps ensure a faster response for your users when they request content. In addition, data transfer out for content by using CloudFront is often more cost-effective than serving files directly from S3, and there is no data transfer fee from S3 to CloudFront.</p>\n\n<p>A security feature of CloudFront is Origin Access Identity (OAI), which restricts access to an S3 bucket and its content to only CloudFront and operations it performs.</p>\n\n<p>CloudFront Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q16-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q16-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon ElastiCache for Redis</strong> - Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, and Q&amp;A portals). ElastiCache is often used with databases that need millisecond latency. For the current scenario, we do not need a caching layer since the data load is not that heavy.</p>\n\n<p><strong>Use Amazon S3 Caching</strong> - This is a made-up option, given as a distractor.</p>\n\n<p><strong>Use Amazon S3 Transfer Acceleration</strong> - Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. However, S3 Transfer Acceleration leverages Amazon CloudFronts globally distributed AWS Edge Locations. Each time S3 Transfer Acceleration is used to upload an object, AWS checks whether S3 Transfer Acceleration is likely to be faster than a regular Amazon S3 transfer. If it finds that S3 Transfer Acceleration might not be significantly faster, AWS shifts back to normal Amazon S3 transfer mode. So, this is not the right option for our use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use Amazon CloudFront</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Storing your static content with S3 provides a lot of advantages. But to help optimize your applications performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your S3 bucket to serve and protect the content. CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users."
      },
      {
        "answer": "",
        "explanation": "By caching your content in Edge Locations, CloudFront reduces the load on your S3 bucket and helps ensure a faster response for your users when they request content. In addition, data transfer out for content by using CloudFront is often more cost-effective than serving files directly from S3, and there is no data transfer fee from S3 to CloudFront."
      },
      {
        "answer": "",
        "explanation": "A security feature of CloudFront is Origin Access Identity (OAI), which restricts access to an S3 bucket and its content to only CloudFront and operations it performs."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q16-i1.jpg",
        "answer": "",
        "explanation": "CloudFront Overview:"
      },
      {
        "link": "https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon ElastiCache for Redis</strong> - Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, and Q&amp;A portals). ElastiCache is often used with databases that need millisecond latency. For the current scenario, we do not need a caching layer since the data load is not that heavy."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Caching</strong> - This is a made-up option, given as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon S3 Transfer Acceleration</strong> - Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. However, S3 Transfer Acceleration leverages Amazon CloudFronts globally distributed AWS Edge Locations. Each time S3 Transfer Acceleration is used to upload an object, AWS checks whether S3 Transfer Acceleration is likely to be faster than a regular Amazon S3 transfer. If it finds that S3 Transfer Acceleration might not be significantly faster, AWS shifts back to normal Amazon S3 transfer mode. So, this is not the right option for our use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/",
      "https://aws.amazon.com/elasticache/"
    ]
  },
  {
    "id": 13,
    "question": "<p>You are a developer working with the AWS CLI to create Lambda functions that contain environment variables. Your functions will require over 50 environment variables consisting of sensitive information of database table names.</p>\n\n<p>What is the total set size/number of environment variables you can create for AWS Lambda?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The total size of all environment variables shouldn't exceed 8 KB. There is no limit on the number of variables</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The total size of all environment variables shouldn't exceed 4 KB. There is no limit on the number of variables</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The total size of all environment variables shouldn't exceed 8 KB. The maximum number of variables that can be created is 50</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The total size of all environment variables shouldn't exceed 4 KB. The maximum number of variables that can be created is 35</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The total size of all environment variables shouldn't exceed 4 KB. There is no limit on the number of variables</strong></p>\n\n<p>An environment variable is a pair of strings that are stored in a function's version-specific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. The total size of all environment variables doesn't exceed 4 KB. There is no limit defined on the number of variables that can be used.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The total size of all environment variables shouldn't exceed 8 KB. The maximum number of variables that can be created is 50</strong> - Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.</p>\n\n<p><strong>The total size of all environment variables shouldn't exceed 8 KB. There is no limit on the number of variables</strong> - Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.</p>\n\n<p><strong>The total size of all environment variables shouldn't exceed 4 KB. The maximum number of variables that can be created is 35</strong> - Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>The total size of all environment variables shouldn't exceed 4 KB. There is no limit on the number of variables</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An environment variable is a pair of strings that are stored in a function's version-specific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. The total size of all environment variables doesn't exceed 4 KB. There is no limit defined on the number of variables that can be used."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The total size of all environment variables shouldn't exceed 8 KB. The maximum number of variables that can be created is 50</strong> - Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables."
      },
      {
        "answer": "",
        "explanation": "<strong>The total size of all environment variables shouldn't exceed 8 KB. There is no limit on the number of variables</strong> - Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables."
      },
      {
        "answer": "",
        "explanation": "<strong>The total size of all environment variables shouldn't exceed 4 KB. The maximum number of variables that can be created is 35</strong> - Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html"
    ]
  },
  {
    "id": 14,
    "question": "<p>A media company uses Amazon Simple Queue Service (SQS) queue to manage their transactions. With changing business needs, the payload size of the messages is increasing. The Team Lead of the project is worried about the 256 KB message size limit that SQS has.</p>\n\n<p>What can be done to make the queue accept messages of a larger size?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the MultiPart API</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the SQS Extended Client</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use gzip compression</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Get a service limit increase from AWS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the SQS Extended Client</strong> - To manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the Amazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for storing your data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the MultiPart API</strong> - This is an incorrect statement. There is no multi-part API for Amazon Simple Queue Service.</p>\n\n<p><strong>Get a service limit increase from AWS</strong> - While it is possible to get service limits extended for certain AWS services, AWS already offers Extended Client to deal with queues that have larger messages.</p>\n\n<p><strong>Use gzip compression</strong> - You can compress the messages before sending them to the queue. The messages also need to be encoded after this to cater to SQS message standards. This adds bulk to the messages and will not be an optimal solution for the current scenario.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the SQS Extended Client</strong> - To manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the Amazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for storing your data."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the MultiPart API</strong> - This is an incorrect statement. There is no multi-part API for Amazon Simple Queue Service."
      },
      {
        "answer": "",
        "explanation": "<strong>Get a service limit increase from AWS</strong> - While it is possible to get service limits extended for certain AWS services, AWS already offers Extended Client to deal with queues that have larger messages."
      },
      {
        "answer": "",
        "explanation": "<strong>Use gzip compression</strong> - You can compress the messages before sending them to the queue. The messages also need to be encoded after this to cater to SQS message standards. This adds bulk to the messages and will not be an optimal solution for the current scenario."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html"
    ]
  },
  {
    "id": 15,
    "question": "<p>You have migrated an on-premise SQL Server database to an Amazon Relational Database Service (RDS) database attached to a VPC inside a private subnet. Also, the related Java application, hosted on-premise, has been moved to an Amazon Lambda function.</p>\n\n<p>Which of the following should you implement to connect AWS Lambda function to its RDS instance?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Lambda layers to connect to the internet and RDS separately</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Environment variables to pass in the RDS connection string</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure lambda to connect to the public subnet that will give internet access and use Security Group to access RDS inside the private subnet</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS</strong> - You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your lambda function to the VPC to access private resources during execution. When you connect a function to a VPC, Lambda creates an elastic network interface for each combination of the security group and subnet in your function's VPC configuration. This is the right way of giving RDS access to Lambda.</p>\n\n<p>Lambda VPC Config:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q39-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q39-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Lambda layers to connect to the internet and RDS separately</strong> - You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. Layers will not help in configuring access to RDS instance and hence is an incorrect choice.</p>\n\n<p><strong>Configure lambda to connect to the public subnet that will give internet access and use the Security Group to access RDS inside the private subnet</strong> - This is an incorrect statement. Connecting a Lambda function to a public subnet does not give it internet access or a public IP address. To grant internet access to your function, its associated VPC must have a NAT gateway (or NAT instance) in a public subnet.</p>\n\n<p><strong>Use Environment variables to pass in the RDS connection string</strong> - You can use environment variables to store secrets securely and adjust your function's behavior without updating code. You can use environment variables to exchange data with RDS, but you will still need access to RDS, which is not possible with just environment variables.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/\">https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS</strong> - You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your lambda function to the VPC to access private resources during execution. When you connect a function to a VPC, Lambda creates an elastic network interface for each combination of the security group and subnet in your function's VPC configuration. This is the right way of giving RDS access to Lambda."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q39-i1.jpg",
        "answer": "",
        "explanation": "Lambda VPC Config:"
      },
      {
        "link": "https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Lambda layers to connect to the internet and RDS separately</strong> - You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. Layers will not help in configuring access to RDS instance and hence is an incorrect choice."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure lambda to connect to the public subnet that will give internet access and use the Security Group to access RDS inside the private subnet</strong> - This is an incorrect statement. Connecting a Lambda function to a public subnet does not give it internet access or a public IP address. To grant internet access to your function, its associated VPC must have a NAT gateway (or NAT instance) in a public subnet."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Environment variables to pass in the RDS connection string</strong> - You can use environment variables to store secrets securely and adjust your function's behavior without updating code. You can use environment variables to exchange data with RDS, but you will still need access to RDS, which is not possible with just environment variables."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/"
    ]
  },
  {
    "id": 16,
    "question": "<p>As part of employee skills upgrade, the developers of your team have been delegated few responsibilities of DevOps engineers. Developers now have full control over modeling the entire software delivery process, from coding to deployment. As the team lead, you are now responsible for any manual approvals needed in the process.</p>\n\n<p>Which of the following approaches supports the given workflow?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use CodePipeline with Amazon Virtual Private Cloud</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create one CodePipeline for your entire flow and add a manual approval step</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create multiple CodePipelines for each environment and link them using AWS Lambda</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create deeply integrated AWS CodePipelines for each environment</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create one CodePipeline for your entire flow and add a manual approval step</strong> - You can add an approval action to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop so someone can manually approve or reject the action. Approval actions can't be added to Source stages. Source stages can contain only source actions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create multiple CodePipelines for each environment and link them using AWS Lambda</strong> - You can create Lambda functions and add them as actions in your pipelines but the approval step is confined to a workflow process and cannot be outsourced to any other AWS service.</p>\n\n<p><strong>Create deeply integrated AWS CodePipelines for each environment</strong> - You can use an AWS CloudFormation template in conjunction with AWS CodePipeline and AWS CodeCommit to create a test environment that deploys to your production environment when the changes to your application are approved, helping you automate a continuous delivery workflow. This is a possible answer but not an optimized way of achieving what the client needs.</p>\n\n<p><strong>Use CodePipeline with Amazon Virtual Private Cloud</strong> - AWS CodePipeline supports Amazon Virtual Private Cloud (Amazon VPC) endpoints powered by AWS PrivateLink. This means you can connect directly to CodePipeline through a private endpoint in your VPC, keeping all traffic inside your VPC and the AWS network. This is a robust security feature but is of no value for our current use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/vpc-support.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/vpc-support.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create one CodePipeline for your entire flow and add a manual approval step</strong> - You can add an approval action to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop so someone can manually approve or reject the action. Approval actions can't be added to Source stages. Source stages can contain only source actions."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create multiple CodePipelines for each environment and link them using AWS Lambda</strong> - You can create Lambda functions and add them as actions in your pipelines but the approval step is confined to a workflow process and cannot be outsourced to any other AWS service."
      },
      {
        "answer": "",
        "explanation": "<strong>Create deeply integrated AWS CodePipelines for each environment</strong> - You can use an AWS CloudFormation template in conjunction with AWS CodePipeline and AWS CodeCommit to create a test environment that deploys to your production environment when the changes to your application are approved, helping you automate a continuous delivery workflow. This is a possible answer but not an optimized way of achieving what the client needs."
      },
      {
        "answer": "",
        "explanation": "<strong>Use CodePipeline with Amazon Virtual Private Cloud</strong> - AWS CodePipeline supports Amazon Virtual Private Cloud (Amazon VPC) endpoints powered by AWS PrivateLink. This means you can connect directly to CodePipeline through a private endpoint in your VPC, keeping all traffic inside your VPC and the AWS network. This is a robust security feature but is of no value for our current use-case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html",
      "https://docs.aws.amazon.com/codepipeline/latest/userguide/vpc-support.html"
    ]
  },
  {
    "id": 17,
    "question": "<p>Your company is planning to move away from reserving EC2 instances and would like to adopt a more agile form of serverless architecture.</p>\n\n<p>Which of the following is the simplest and the least effort way of deploying the Docker containers on this serverless architecture?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Elastic Container Service (Amazon ECS) on EC2</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon Elastic Container Service (Amazon ECS) on Fargate</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS Elastic Beanstalk</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Elastic Kubernetes Service (Amazon EKS) on Fargate</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Elastic Container Service (Amazon ECS) on Fargate</strong> - Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type.</p>\n\n<p>AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.</p>\n\n<p>ECS Fargate Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q15-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q15-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Elastic Container Service (Amazon ECS) on EC2</strong> - Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. ECS uses EC2 instances and hence cannot be called a serverless solution.</p>\n\n<p><strong>Amazon Elastic Kubernetes Service (Amazon EKS) on Fargate</strong> - Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service. You can choose to run your EKS clusters using AWS Fargate, which is a serverless compute for containers. Since the use-case talks about the simplest and the least effort way to deploy Docker containers, EKS is not the best fit as you can use ECS Fargate to build a much easier solution. EKS is better suited to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane or worker nodes.</p>\n\n<p><strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. Beanstalk uses EC2 instances for its deployment, hence cannot be called a serverless architecture.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/eks/\">https://aws.amazon.com/eks/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Elastic Container Service (Amazon ECS) on Fargate</strong> - Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type."
      },
      {
        "answer": "",
        "explanation": "AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q15-i1.jpg",
        "answer": "",
        "explanation": "ECS Fargate Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Elastic Container Service (Amazon ECS) on EC2</strong> - Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. ECS uses EC2 instances and hence cannot be called a serverless solution."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Elastic Kubernetes Service (Amazon EKS) on Fargate</strong> - Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service. You can choose to run your EKS clusters using AWS Fargate, which is a serverless compute for containers. Since the use-case talks about the simplest and the least effort way to deploy Docker containers, EKS is not the best fit as you can use ECS Fargate to build a much easier solution. EKS is better suited to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane or worker nodes."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. Beanstalk uses EC2 instances for its deployment, hence cannot be called a serverless architecture."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html",
      "https://aws.amazon.com/eks/",
      "https://aws.amazon.com/elasticbeanstalk/"
    ]
  },
  {
    "id": 18,
    "question": "<p>Your application is deployed automatically using AWS Elastic Beanstalk. Your YAML configuration files are stored in the folder .ebextensions and new files are added or updated often. The DevOps team does not want to re-deploy the application every time there are configuration changes, instead, they would rather manage configuration externally, securely, and have it load dynamically into the application at runtime.</p>\n\n<p>What option allows you to do this?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use S3</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SSM Parameter Store</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Stage Variables</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Environment variables</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use SSM Parameter Store</strong></p>\n\n<p>AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. For the given use-case, as the DevOps team does not want to re-deploy the application every time there are configuration changes, so they can use the SSM Parameter Store to store the configuration externally.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Environment variables</strong> - Environment variables provide another way to specify configuration options and credentials, and can be useful for scripting or temporarily setting a named profile as the default. Your application is not running AWS CLI. Since the use-case requires the configuration to be stored securely, so using Environment variables is ruled out, as these are not encrypted at rest and these are visible in clear text in the AWS Console as well as in the response of some actions of the Elastic BeanStalk API.</p>\n\n<p><strong>Use Stage Variables</strong> - You can use stage variables for managing multiple release stages for API Gateway, this is not what you are looking for here.</p>\n\n<p><strong>Use S3</strong> - S3 offers the same benefit as the SSM Parameter Store where there are no servers to manage. With S3 you have to set encryption and choose other security options and there are more chances of misconfiguring security if you share your S3 bucket with other objects. You would have to create a custom setup to come close to the parameter store. Use Parameter Store and let AWS handle the rest.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use SSM Parameter Store</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. For the given use-case, as the DevOps team does not want to re-deploy the application every time there are configuration changes, so they can use the SSM Parameter Store to store the configuration externally."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Environment variables</strong> - Environment variables provide another way to specify configuration options and credentials, and can be useful for scripting or temporarily setting a named profile as the default. Your application is not running AWS CLI. Since the use-case requires the configuration to be stored securely, so using Environment variables is ruled out, as these are not encrypted at rest and these are visible in clear text in the AWS Console as well as in the response of some actions of the Elastic BeanStalk API."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Stage Variables</strong> - You can use stage variables for managing multiple release stages for API Gateway, this is not what you are looking for here."
      },
      {
        "answer": "",
        "explanation": "<strong>Use S3</strong> - S3 offers the same benefit as the SSM Parameter Store where there are no servers to manage. With S3 you have to set encryption and choose other security options and there are more chances of misconfiguring security if you share your S3 bucket with other objects. You would have to create a custom setup to come close to the parameter store. Use Parameter Store and let AWS handle the rest."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html"
    ]
  },
  {
    "id": 19,
    "question": "<p>An IT company has migrated to a serverless application stack on the AWS Cloud with the compute layer being implemented via Lambda functions. The engineering managers would like to actively troubleshoot any failures in the Lambda functions.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest for this use-case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The developers should insert logging statements in the Lambda function code which are then available via CloudWatch logs</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use CodeCommit to identify and notify any failures in the Lambda code</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use CodeDeploy to identify and notify any failures in the Lambda code</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use CloudWatch Events to identify and notify any failures in the Lambda code</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>\"The developers should insert logging statements in the Lambda function code which are then available via CloudWatch logs\"</p>\n\n<p>When you invoke a Lambda function, two types of error can occur. Invocation errors occur when the invocation request is rejected before your function receives it. Function errors occur when your function's code or runtime returns an error. Depending on the type of error, the type of invocation, and the client or service that invokes the function, the retry behavior, and the strategy for managing errors varies.</p>\n\n<p>Lambda function failures are commonly caused by:</p>\n\n<p>Permissions issues\nCode issues\nNetwork issues\nThrottling\nInvoke API 500 and 502 errors</p>\n\n<p>You can insert logging statements into your code to help you validate that your code is working as expected. Lambda automatically integrates with CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function, which is named <code>/aws/lambda/&lt;function name&gt;</code>.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q9-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q9-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html\">https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Use CloudWatch Events to identify and notify any failures in the Lambda code\" - Typically Lambda functions are triggered as a response to a CloudWatch Event. CloudWatch Events cannot identify and notify failures in the Lambda code.</p>\n\n<p>\"Use CodeCommit to identify and notify any failures in the Lambda code\"</p>\n\n<p>\"Use CodeDeploy to identify and notify any failures in the Lambda code\"</p>\n\n<p>AWS CodeCommit is a fully-managed source control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem.</p>\n\n<p>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.</p>\n\n<p>Neither CodeCommit nor CodeDeploy can identify and notify failures in the Lambda code.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html\">https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The developers should insert logging statements in the Lambda function code which are then available via CloudWatch logs</strong>"
      },
      {
        "answer": "",
        "explanation": "When you invoke a Lambda function, two types of error can occur. Invocation errors occur when the invocation request is rejected before your function receives it. Function errors occur when your function's code or runtime returns an error. Depending on the type of error, the type of invocation, and the client or service that invokes the function, the retry behavior, and the strategy for managing errors varies."
      },
      {
        "answer": "",
        "explanation": "Lambda function failures are commonly caused by:"
      },
      {
        "answer": "",
        "explanation": "Permissions issues\nCode issues\nNetwork issues\nThrottling\nInvoke API 500 and 502 errors"
      },
      {
        "answer": "",
        "explanation": "You can insert logging statements into your code to help you validate that your code is working as expected. Lambda automatically integrates with CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function, which is named <code>/aws/lambda/&lt;function name&gt;</code>."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q9-i1.jpg",
        "answer": "",
        "explanation": "Please see this note for more details:"
      },
      {
        "link": "https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use CloudWatch Events to identify and notify any failures in the Lambda code</strong> - Typically Lambda functions are triggered as a response to a CloudWatch Event. CloudWatch Events cannot identify and notify failures in the Lambda code."
      },
      {
        "answer": "",
        "explanation": "<strong>Use CodeCommit to identify and notify any failures in the Lambda code</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Use CodeDeploy to identify and notify any failures in the Lambda code</strong>"
      },
      {
        "answer": "",
        "explanation": "AWS CodeCommit is a fully-managed source control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem."
      },
      {
        "answer": "",
        "explanation": "AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers."
      },
      {
        "answer": "",
        "explanation": "Neither CodeCommit nor CodeDeploy can identify and notify failures in the Lambda code."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html"
    ]
  },
  {
    "id": 20,
    "question": "<p>A development team uses shared Amazon S3 buckets to upload files. Due to this shared access, objects in S3 buckets have different owners making it difficult to manage the objects.</p>\n\n<p>As a developer associate, which of the following would you suggest to automatically make the S3 bucket owner, also the owner of all objects in the bucket, irrespective of the AWS account used for uploading the objects?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Bucket Access Control Lists (ACLs) to control access on S3 bucket and then define its owner</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use S3 CORS to make the S3 bucket owner, the owner of all objects in the bucket</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use S3 Access Analyzer to identify the owners of all objects and change the ownership to the bucket owner</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucket</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucket</strong></p>\n\n<p>S3 Object Ownership is an Amazon S3 bucket setting that you can use to control ownership of new objects that are uploaded to your buckets. By default, when other AWS accounts upload objects to your bucket, the objects remain owned by the uploading account. With S3 Object Ownership, any new objects that are written by other accounts with the bucket-owner-full-control canned access control list (ACL) automatically become owned by the bucket owner, who then has full control of the objects.</p>\n\n<p>S3 Object Ownership has two settings:\n1. Object writer  The uploading account will own the object.\n2. Bucket owner preferred  The bucket owner will own the object if the object is uploaded with the <code>bucket-owner-full-control</code> canned ACL. Without this setting and canned ACL, the object is uploaded and remains owned by the uploading account.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use S3 CORS to make the S3 bucket owner, the owner of all objects in the bucket</strong> - Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.</p>\n\n<p><strong>Use S3 Access Analyzer to identify the owners of all objects and change the ownership to the bucket owner</strong> - Access Analyzer for S3 helps review all buckets that have bucket access control lists (ACLs), bucket policies, or access point policies that grant public or shared access. Access Analyzer for S3 alerts you to buckets that are configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization.</p>\n\n<p><strong>Use Bucket Access Control Lists (ACLs) to control access on S3 bucket and then define its owner</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. A bucket ACLs allow you to control access at bucket level.</p>\n\n<p>None of the above features are useful for the current scenario and hence are incorrect options.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucket</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "S3 Object Ownership is an Amazon S3 bucket setting that you can use to control ownership of new objects that are uploaded to your buckets. By default, when other AWS accounts upload objects to your bucket, the objects remain owned by the uploading account. With S3 Object Ownership, any new objects that are written by other accounts with the bucket-owner-full-control canned access control list (ACL) automatically become owned by the bucket owner, who then has full control of the objects."
      },
      {
        "answer": "",
        "explanation": "S3 Object Ownership has two settings:\n1. Object writer  The uploading account will own the object.\n2. Bucket owner preferred  The bucket owner will own the object if the object is uploaded with the <code>bucket-owner-full-control</code> canned ACL. Without this setting and canned ACL, the object is uploaded and remains owned by the uploading account."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use S3 CORS to make the S3 bucket owner, the owner of all objects in the bucket</strong> - Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain."
      },
      {
        "answer": "",
        "explanation": "<strong>Use S3 Access Analyzer to identify the owners of all objects and change the ownership to the bucket owner</strong> - Access Analyzer for S3 helps review all buckets that have bucket access control lists (ACLs), bucket policies, or access point policies that grant public or shared access. Access Analyzer for S3 alerts you to buckets that are configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Bucket Access Control Lists (ACLs) to control access on S3 bucket and then define its owner</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. A bucket ACLs allow you to control access at bucket level."
      },
      {
        "answer": "",
        "explanation": "None of the above features are useful for the current scenario and hence are incorrect options."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html"
    ]
  },
  {
    "id": 21,
    "question": "<p>The development team at a company wants to encrypt a 111 GB object using AWS KMS.</p>\n\n<p>Which of the following represents the best solution?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Make a <code>GenerateDataKeyWithPlaintext</code> API call that returns an encrypted copy of a data key. Use a plaintext key to encrypt the data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Make a <code>GenerateDataKey</code> API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Make an <code>Encrypt</code> API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Make a <code>GenerateDataKeyWithoutPlaintext</code> API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Make a <code>GenerateDataKey</code> API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data</strong> - <code>GenerateDataKey</code> API, generates a unique symmetric data key for client-side encryption. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.</p>\n\n<p><code>GenerateDataKey</code> returns a unique data key for each request. The bytes in the plaintext key are not related to the caller or the CMK.</p>\n\n<p>To encrypt data outside of AWS KMS:</p>\n\n<ol>\n<li><p>Use the <code>GenerateDataKey</code> operation to get a data key.</p></li>\n<li><p>Use the plaintext data key (in the Plaintext field of the response) to encrypt your data outside of AWS KMS. Then erase the plaintext data key from memory.</p></li>\n<li><p>Store the encrypted data key (in the CiphertextBlob field of the response) with the encrypted data.</p></li>\n</ol>\n\n<p>To decrypt data outside of AWS KMS:</p>\n\n<ol>\n<li><p>Use the Decrypt operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key.</p></li>\n<li><p>Use the plaintext data key to decrypt data outside of AWS KMS, then erase the plaintext data key from memory.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Make a <code>GenerateDataKeyWithPlaintext</code> API call that returns an encrypted copy of a data key. Use a plaintext key to encrypt the data</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p><strong>Make an <code>Encrypt</code> API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material</strong> - <code>Encrypt</code> API is used to encrypt plaintext into ciphertext by using a customer master key (CMK). The Encrypt operation has two primary use cases:</p>\n\n<ol>\n<li><p>To encrypt small amounts of arbitrary data, such as a personal identifier or database password, or other sensitive information.</p></li>\n<li><p>To move encrypted data from one AWS Region to another.</p></li>\n</ol>\n\n<p>Neither of the two is useful for the given scenario.</p>\n\n<p><strong>Make a <code>GenerateDataKeyWithoutPlaintext</code> API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data</strong> - <code>GenerateDataKeyWithoutPlaintext</code> API, generates a unique symmetric data key. This operation returns a data key that is encrypted under a customer master key (CMK) that you specify.</p>\n\n<p><code>GenerateDataKeyWithoutPlaintext</code> is identical to the <code>GenerateDataKey</code> operation except that returns only the encrypted copy of the data key. This operation is useful for systems that need to encrypt data at some point, but not immediately. When you need to encrypt the data, you call the Decrypt operation on the encrypted copy of the key.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_Encrypt.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_Encrypt.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Make a <code>GenerateDataKey</code> API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data</strong> - <code>GenerateDataKey</code> API, generates a unique symmetric data key for client-side encryption. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data."
      },
      {
        "answer": "",
        "explanation": "<code>GenerateDataKey</code> returns a unique data key for each request. The bytes in the plaintext key are not related to the caller or the CMK."
      },
      {
        "answer": "",
        "explanation": "To encrypt data outside of AWS KMS:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>Use the <code>GenerateDataKey</code> operation to get a data key.</p></li>\n<li><p>Use the plaintext data key (in the Plaintext field of the response) to encrypt your data outside of AWS KMS. Then erase the plaintext data key from memory.</p></li>\n<li><p>Store the encrypted data key (in the CiphertextBlob field of the response) with the encrypted data.</p></li>\n</ol>"
      },
      {
        "answer": "",
        "explanation": "To decrypt data outside of AWS KMS:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>Use the Decrypt operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key.</p></li>\n<li><p>Use the plaintext data key to decrypt data outside of AWS KMS, then erase the plaintext data key from memory.</p></li>\n</ol>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Make a <code>GenerateDataKeyWithPlaintext</code> API call that returns an encrypted copy of a data key. Use a plaintext key to encrypt the data</strong> - This is a made-up option, given only as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Make an <code>Encrypt</code> API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material</strong> - <code>Encrypt</code> API is used to encrypt plaintext into ciphertext by using a customer master key (CMK). The Encrypt operation has two primary use cases:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>To encrypt small amounts of arbitrary data, such as a personal identifier or database password, or other sensitive information.</p></li>\n<li><p>To move encrypted data from one AWS Region to another.</p></li>\n</ol>"
      },
      {
        "answer": "",
        "explanation": "Neither of the two is useful for the given scenario."
      },
      {
        "answer": "",
        "explanation": "<strong>Make a <code>GenerateDataKeyWithoutPlaintext</code> API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data</strong> - <code>GenerateDataKeyWithoutPlaintext</code> API, generates a unique symmetric data key. This operation returns a data key that is encrypted under a customer master key (CMK) that you specify."
      },
      {
        "answer": "",
        "explanation": "<code>GenerateDataKeyWithoutPlaintext</code> is identical to the <code>GenerateDataKey</code> operation except that returns only the encrypted copy of the data key. This operation is useful for systems that need to encrypt data at some point, but not immediately. When you need to encrypt the data, you call the Decrypt operation on the encrypted copy of the key."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html",
      "https://docs.aws.amazon.com/kms/latest/APIReference/API_Encrypt.html",
      "https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html"
    ]
  },
  {
    "id": 22,
    "question": "<p>A cybersecurity company is running a serverless backend with several compute-heavy workflows running on Lambda functions. The development team has noticed a performance lag after analyzing the performance metrics for the Lambda functions.</p>\n\n<p>As a Developer Associate, which of the following options would you suggest as the BEST solution to address the compute-heavy workloads?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use reserved concurrency to account for the compute-heavy workflows</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use provisioned concurrency to account for the compute-heavy workflows</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Increase the amount of memory available to the Lambda functions</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Invoke the Lambda functions asynchronously to process the compute-heavy workflows</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Increase the amount of memory available to the Lambda functions</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>In the AWS Lambda resource model, you choose the amount of memory you want for your function which allocates proportional CPU power and other resources. This means you will have access to more compute power when you choose one of the new larger settings. You can set your memory in 64MB increments from 128MB to 3008MB. You access these settings when you create a function or update its configuration. The settings are available using the AWS Management Console, AWS CLI, or SDKs.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q2-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q2-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a><p></p>\n\n<p>Therefore, by increasing the amount of memory available to the Lambda functions, you can run the compute-heavy workflows.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Invoke the Lambda functions asynchronously to process the compute-heavy workflows</strong> - When you invoke a function asynchronously, you don't wait for a response from the function code. You hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to a downstream resource to chain together components of your application. The method of invocation has no bearing on the Lambda function's ability to process the compute-heavy workflows.</p>\n\n<p><strong>Use reserved concurrency to account for the compute-heavy workflows</strong></p>\n\n<p><strong>Use provisioned concurrency to account for the compute-heavy workflows</strong></p>\n\n<p>Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency. The type of concurrency has no bearing on the Lambda function's ability to process the compute-heavy workflows. So both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Increase the amount of memory available to the Lambda functions</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume."
      },
      {
        "answer": "",
        "explanation": "In the AWS Lambda resource model, you choose the amount of memory you want for your function which allocates proportional CPU power and other resources. This means you will have access to more compute power when you choose one of the new larger settings. You can set your memory in 64MB increments from 128MB to 3008MB. You access these settings when you create a function or update its configuration. The settings are available using the AWS Management Console, AWS CLI, or SDKs."
      },
      {
        "link": "https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html"
      },
      {
        "answer": "",
        "explanation": "Therefore, by increasing the amount of memory available to the Lambda functions, you can run the compute-heavy workflows."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Invoke the Lambda functions asynchronously to process the compute-heavy workflows</strong> - When you invoke a function asynchronously, you don't wait for a response from the function code. You hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to a downstream resource to chain together components of your application. The method of invocation has no bearing on the Lambda function's ability to process the compute-heavy workflows."
      },
      {
        "answer": "<strong>Use reserved concurrency to account for the compute-heavy workflows</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Use provisioned concurrency to account for the compute-heavy workflows</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency. The type of concurrency has no bearing on the Lambda function's ability to process the compute-heavy workflows. So both these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html"
    ]
  },
  {
    "id": 23,
    "question": "<p>A developer is designing an AWS CloudFormation template for deploying Amazon EC2 instances in numerous AWS accounts. The developer needs to select EC2 instances from a list of pre-approved instance types.</p>\n\n<p>What measures could the developer take to integrate the list of authorized instance types into the CloudFormation template?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure a pseudo parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure separate parameters for each EC2 instance type in the CloudFormation template</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure a parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure a mapping having a list of EC2 instance types as parameters in the CloudFormation template</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template</strong></p>\n\n<p>You can use the Parameters section to customize your templates. Parameters enable you to input custom values to your template each time you create or update a stack.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q57-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q57-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a><p></p>\n\n<p>AllowedValues refers to an array containing the list of values allowed for the parameter. When applied to a parameter of type String, the parameter value must be one of the allowed values. When applied to a parameter of type CommaDelimitedList, each value in the list must be one of the specified allowed values.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure separate parameters for each EC2 instance type in the CloudFormation template</strong> - Creating separate parameters for each instance type is semantically incorrect as the underlying value will point to the same resource but have multiple inputs.</p>\n\n<p><strong>Configure a mapping having a list of EC2 instance types as parameters in the CloudFormation template</strong> - The Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function to retrieve values in a map. A mapping is not a list, rather, it consists of key value pairs. You can't include parameters, pseudo parameters, or intrinsic functions in the Mappings section. So, this option is incorrect.</p>\n\n<p><strong>Configure a pseudo parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template</strong> - Pseudo parameters are parameters that are predefined by AWS CloudFormation. You don't declare them in your template.  So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Configure a parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can use the Parameters section to customize your templates. Parameters enable you to input custom values to your template each time you create or update a stack."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html"
      },
      {
        "answer": "",
        "explanation": "AllowedValues refers to an array containing the list of values allowed for the parameter. When applied to a parameter of type String, the parameter value must be one of the allowed values. When applied to a parameter of type CommaDelimitedList, each value in the list must be one of the specified allowed values."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure separate parameters for each EC2 instance type in the CloudFormation template</strong> - Creating separate parameters for each instance type is semantically incorrect as the underlying value will point to the same resource but have multiple inputs."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a mapping having a list of EC2 instance types as parameters in the CloudFormation template</strong> - The Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function to retrieve values in a map. A mapping is not a list, rather, it consists of key value pairs. You can't include parameters, pseudo parameters, or intrinsic functions in the Mappings section. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a pseudo parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template</strong> - Pseudo parameters are parameters that are predefined by AWS CloudFormation. You don't declare them in your template.  So, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html"
    ]
  },
  {
    "id": 24,
    "question": "<p>A development team has created a new IAM user that has <code>s3:putObject</code> permission to write to an S3 bucket. This S3 bucket uses server-side encryption with AWS KMS managed keys (SSE-KMS) as the default encryption. Using the access key ID and the secret access key of the IAM user, the application received an access denied error when calling the <code>PutObject</code> API.</p>\n\n<p>As a Developer Associate, how would you resolve this issue?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Correct the policy of the IAM user to allow the <code>kms:GenerateDataKey</code> action</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Correct the bucket policy of the S3 bucket to allow the IAM user to upload encrypted objects</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Correct the ACL of the S3 bucket to allow the IAM user to upload encrypted objects</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Correct the policy of the IAM user to allow the <code>s3:Encrypt</code> action</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Correct the policy of the IAM user to allow the <code>kms:GenerateDataKey</code> action</strong> - You can protect data at rest in Amazon S3 by using three different modes of server-side encryption: SSE-S3, SSE-C, or SSE-KMS. SSE-KMS requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS. You can choose a customer managed CMK or the AWS managed CMK for Amazon S3 in your account. If you choose to encrypt your data using the standard features, AWS KMS and Amazon S3 perform the following actions:</p>\n\n<ol>\n<li><p>Amazon S3 requests a plaintext data key and a copy of the key encrypted under the specified CMK.</p></li>\n<li><p>AWS KMS generates a data key, encrypts it under the CMK, and sends both the plaintext data key and the encrypted data key to Amazon S3.</p></li>\n<li><p>Amazon S3 encrypts the data using the data key and removes the plaintext key from memory as soon as possible after use.</p></li>\n<li><p>Amazon S3 stores the encrypted data key as metadata with the encrypted data.</p></li>\n</ol>\n\n<p>The error message indicates that your IAM user or role needs permission for the <code>kms:GenerateDataKey</code> action. This permission is required for buckets that use default encryption with a custom AWS KMS key.</p>\n\n<p>In the JSON policy documents, look for policies related to AWS KMS access. Review statements with \"Effect\": \"Allow\" to check if the user or role has permissions for the kms:GenerateDataKey action on the bucket's AWS KMS key. If this permission is missing, then add the permission to the appropriate policy.</p>\n\n<p>In the JSON policy documents, look for statements with \"Effect\": \"Deny\". Then, confirm that those statements don't deny the s3:PutObject action on the bucket. The statements must also not deny the IAM user or role access to the kms:GenerateDataKey action on the key used to encrypt the bucket. Additionally, make sure the necessary KMS and S3 permissions are not restricted using a VPC endpoint policy, service control policy, permissions boundary, or session policy.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Correct the policy of the IAM user to allow the <code>s3:Encrypt</code> action</strong> - This is an invalid action given only as a distractor.</p>\n\n<p><strong>Correct the bucket policy of the S3 bucket to allow the IAM user to upload encrypted objects</strong> - The user already has access to the bucket. What the user lacks is access to generate a KMS key, which is mandatory when a bucket is enabled for default encryption.</p>\n\n<p><strong>Correct the ACL of the S3 bucket to allow the IAM user to upload encrypted objects</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which AWS accounts or groups are granted access and the type of access. ACL is another way of giving access to S3 bucket objects. Permissions to use KMS keys will still be needed.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-error-kms/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-error-kms/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Correct the policy of the IAM user to allow the <code>kms:GenerateDataKey</code> action</strong> - You can protect data at rest in Amazon S3 by using three different modes of server-side encryption: SSE-S3, SSE-C, or SSE-KMS. SSE-KMS requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS. You can choose a customer managed CMK or the AWS managed CMK for Amazon S3 in your account. If you choose to encrypt your data using the standard features, AWS KMS and Amazon S3 perform the following actions:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>Amazon S3 requests a plaintext data key and a copy of the key encrypted under the specified CMK.</p></li>\n<li><p>AWS KMS generates a data key, encrypts it under the CMK, and sends both the plaintext data key and the encrypted data key to Amazon S3.</p></li>\n<li><p>Amazon S3 encrypts the data using the data key and removes the plaintext key from memory as soon as possible after use.</p></li>\n<li><p>Amazon S3 stores the encrypted data key as metadata with the encrypted data.</p></li>\n</ol>"
      },
      {
        "answer": "",
        "explanation": "The error message indicates that your IAM user or role needs permission for the <code>kms:GenerateDataKey</code> action. This permission is required for buckets that use default encryption with a custom AWS KMS key."
      },
      {
        "answer": "",
        "explanation": "In the JSON policy documents, look for policies related to AWS KMS access. Review statements with \"Effect\": \"Allow\" to check if the user or role has permissions for the kms:GenerateDataKey action on the bucket's AWS KMS key. If this permission is missing, then add the permission to the appropriate policy."
      },
      {
        "answer": "",
        "explanation": "In the JSON policy documents, look for statements with \"Effect\": \"Deny\". Then, confirm that those statements don't deny the s3:PutObject action on the bucket. The statements must also not deny the IAM user or role access to the kms:GenerateDataKey action on the key used to encrypt the bucket. Additionally, make sure the necessary KMS and S3 permissions are not restricted using a VPC endpoint policy, service control policy, permissions boundary, or session policy."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Correct the policy of the IAM user to allow the <code>s3:Encrypt</code> action</strong> - This is an invalid action given only as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Correct the bucket policy of the S3 bucket to allow the IAM user to upload encrypted objects</strong> - The user already has access to the bucket. What the user lacks is access to generate a KMS key, which is mandatory when a bucket is enabled for default encryption."
      },
      {
        "answer": "",
        "explanation": "<strong>Correct the ACL of the S3 bucket to allow the IAM user to upload encrypted objects</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which AWS accounts or groups are granted access and the type of access. ACL is another way of giving access to S3 bucket objects. Permissions to use KMS keys will still be needed."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-error-kms/",
      "https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html"
    ]
  },
  {
    "id": 25,
    "question": "<p>A leading financial services company offers data aggregation services for Wall Street trading firms. The company bills its clients based on per unit of clickstream data provided to the clients. As the company operates in a regulated industry, it needs to have the same ordered clickstream data available for auditing within a window of 7 days.</p>\n\n<p>As a Developer Associate, which of the following AWS services do you think provides the ability to run the billing process and auditing process on the given clickstream data in the same order?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Kinesis Data Firehose</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Kinesis Data Analytics</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon SQS</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Kinesis Data Streams</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\nAmazon Kinesis Data Streams is recommended when you need the ability to consume records in the same order a few hours later.</p>\n\n<p>For example, you have a billing application and an audit application that runs a few hours behind the billing application. By default, records of a stream are accessible for up to 24 hours from the time they are added to the stream. You can raise this limit to a maximum of 365 days. For the given use-case, Amazon Kinesis Data Streams can be configured to store data for up to 7 days and you can run the audit application up to 7 days behind the billing application.</p>\n\n<p>KDS provides the ability to consume records in the same order a few hours later\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q6-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q6-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards youre already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.</p>\n\n<p><strong>AWS Kinesis Data Analytics</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.</p>\n\n<p><strong>Amazon SQS</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers in the same order a few hours later, therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>AWS Kinesis Data Streams</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\nAmazon Kinesis Data Streams is recommended when you need the ability to consume records in the same order a few hours later."
      },
      {
        "answer": "",
        "explanation": "For example, you have a billing application and an audit application that runs a few hours behind the billing application. By default, records of a stream are accessible for up to 24 hours from the time they are added to the stream. You can raise this limit to a maximum of 365 days. For the given use-case, Amazon Kinesis Data Streams can be configured to store data for up to 7 days and you can run the audit application up to 7 days behind the billing application."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q6-i1.jpg",
        "answer": "",
        "explanation": "KDS provides the ability to consume records in the same order a few hours later"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/faqs/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards youre already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Kinesis Data Analytics</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon SQS</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers in the same order a few hours later, therefore this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/faqs/",
      "https://aws.amazon.com/kinesis/data-firehose/faqs/",
      "https://aws.amazon.com/kinesis/data-analytics/faqs/"
    ]
  },
  {
    "id": 26,
    "question": "<p>A banking application needs to send real-time alerts and notifications based on any updates from the backend services. The company wants to avoid implementing complex polling mechanisms for these notifications.</p>\n\n<p>Which of the following types of APIs supported by the Amazon API Gateway is the right fit?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>HTTP APIs</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>REST or HTTP APIs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>WebSocket APIs</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>REST APIs</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>WebSocket APIs</strong></p>\n\n<p>In a WebSocket API, the client and the server can both send messages to each other at any time. Backend servers can easily push data to connected users and devices, avoiding the need to implement complex polling mechanisms.</p>\n\n<p>For example, you could build a serverless application using an API Gateway WebSocket API and AWS Lambda to send and receive messages to and from individual users or groups of users in a chat room. Or you could invoke backend services such as AWS Lambda, Amazon Kinesis, or an HTTP endpoint based on message content.</p>\n\n<p>You can use API Gateway WebSocket APIs to build secure, real-time communication applications without having to provision or manage any servers to manage connections or large-scale data exchanges. Targeted use cases include real-time applications such as the following:</p>\n\n<ol>\n<li>Chat applications</li>\n<li>Real-time dashboards such as stock tickers</li>\n<li>Real-time alerts and notifications</li>\n</ol>\n\n<p>API Gateway provides WebSocket API management functionality such as the following:</p>\n\n<ol>\n<li>Monitoring and throttling of connections and messages</li>\n<li>Using AWS X-Ray to trace messages as they travel through the APIs to backend services</li>\n<li>Easy integration with HTTP/HTTPS endpoints</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>REST or HTTP APIs</strong></p>\n\n<p><strong>REST APIs</strong> - An API Gateway REST API is made up of resources and methods. A resource is a logical entity that an app can access through a resource path. A method corresponds to a REST API request that is submitted by the user of your API and the response returned to the user.</p>\n\n<p>For example, /incomes could be the path of a resource representing the income of the app user. A resource can have one or more operations that are defined by appropriate HTTP verbs such as GET, POST, PUT, PATCH, and DELETE. A combination of a resource path and an operation identifies a method of the API. For example, a POST /incomes method could add an income earned by the caller, and a GET /expenses method could query the reported expenses incurred by the caller.</p>\n\n<p><strong>HTTP APIs</strong> - HTTP APIs enable you to create RESTful APIs with lower latency and lower cost than REST APIs. You can use HTTP APIs to send requests to AWS Lambda functions or to any publicly routable HTTP endpoint.</p>\n\n<p>For example, you can create an HTTP API that integrates with a Lambda function on the backend. When a client calls your API, API Gateway sends the request to the Lambda function and returns the function's response to the client.</p>\n\n<p>Server push mechanism is not possible in REST and HTTP APIs.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>WebSocket APIs</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "In a WebSocket API, the client and the server can both send messages to each other at any time. Backend servers can easily push data to connected users and devices, avoiding the need to implement complex polling mechanisms."
      },
      {
        "answer": "",
        "explanation": "For example, you could build a serverless application using an API Gateway WebSocket API and AWS Lambda to send and receive messages to and from individual users or groups of users in a chat room. Or you could invoke backend services such as AWS Lambda, Amazon Kinesis, or an HTTP endpoint based on message content."
      },
      {
        "answer": "",
        "explanation": "You can use API Gateway WebSocket APIs to build secure, real-time communication applications without having to provision or manage any servers to manage connections or large-scale data exchanges. Targeted use cases include real-time applications such as the following:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li>Chat applications</li>\n<li>Real-time dashboards such as stock tickers</li>\n<li>Real-time alerts and notifications</li>\n</ol>"
      },
      {
        "answer": "",
        "explanation": "API Gateway provides WebSocket API management functionality such as the following:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li>Monitoring and throttling of connections and messages</li>\n<li>Using AWS X-Ray to trace messages as they travel through the APIs to backend services</li>\n<li>Easy integration with HTTP/HTTPS endpoints</li>\n</ol>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>REST or HTTP APIs</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "<strong>REST APIs</strong> - An API Gateway REST API is made up of resources and methods. A resource is a logical entity that an app can access through a resource path. A method corresponds to a REST API request that is submitted by the user of your API and the response returned to the user."
      },
      {
        "answer": "",
        "explanation": "For example, /incomes could be the path of a resource representing the income of the app user. A resource can have one or more operations that are defined by appropriate HTTP verbs such as GET, POST, PUT, PATCH, and DELETE. A combination of a resource path and an operation identifies a method of the API. For example, a POST /incomes method could add an income earned by the caller, and a GET /expenses method could query the reported expenses incurred by the caller."
      },
      {
        "answer": "",
        "explanation": "<strong>HTTP APIs</strong> - HTTP APIs enable you to create RESTful APIs with lower latency and lower cost than REST APIs. You can use HTTP APIs to send requests to AWS Lambda functions or to any publicly routable HTTP endpoint."
      },
      {
        "answer": "",
        "explanation": "For example, you can create an HTTP API that integrates with a Lambda function on the backend. When a client calls your API, API Gateway sends the request to the Lambda function and returns the function's response to the client."
      },
      {
        "answer": "",
        "explanation": "Server push mechanism is not possible in REST and HTTP APIs."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.html"
    ]
  },
  {
    "id": 27,
    "question": "<p>A new member of your team is working on creating Dead Letter Queue (DLQ) for AWS Lambda functions.</p>\n\n<p>As a Developer Associate, can you help him identify the use cases, wherein AWS Lambda will add a message into a DLQ after being processed? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The Lambda function invocation is synchronous</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The event fails all processing attempts</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The Lambda function invocation failed only once but succeeded thereafter</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The event has been processed successfully</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>The Lambda function invocation is asynchronous</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The Lambda function invocation is asynchronous</strong> - When an asynchronous invocation event exceeds the maximum age or fails all retry attempts, Lambda discards it. Or sends it to dead-letter queue if you have configured one.</p>\n\n<p><strong>The event fails all processing attempt</strong> - A dead-letter queue acts the same as an on-failure destination in that it is used when an event fails all processing attempts or expires without being processed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Lambda function invocation is synchronous</strong> - When you invoke a function synchronously, Lambda runs the function and waits for a response. Queues are generally used with asynchronous invocations since queues implement the decoupling feature of various connected systems. It does not make sense to use queues when the calling code will wait on it for a response.</p>\n\n<p><strong>The event has been processed successfully</strong> - A successfully processed event is not sent to the dead-letter queue.</p>\n\n<p><strong>The event processing failed only once but succeeded thereafter</strong> - A successfully processed event is not sent to the dead-letter queue.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The Lambda function invocation is asynchronous</strong> - When an asynchronous invocation event exceeds the maximum age or fails all retry attempts, Lambda discards it. Or sends it to dead-letter queue if you have configured one."
      },
      {
        "answer": "",
        "explanation": "<strong>The event fails all processing attempt</strong> - A dead-letter queue acts the same as an on-failure destination in that it is used when an event fails all processing attempts or expires without being processed."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The Lambda function invocation is synchronous</strong> - When you invoke a function synchronously, Lambda runs the function and waits for a response. Queues are generally used with asynchronous invocations since queues implement the decoupling feature of various connected systems. It does not make sense to use queues when the calling code will wait on it for a response."
      },
      {
        "answer": "",
        "explanation": "<strong>The event has been processed successfully</strong> - A successfully processed event is not sent to the dead-letter queue."
      },
      {
        "answer": "",
        "explanation": "<strong>The event processing failed only once but succeeded thereafter</strong> - A successfully processed event is not sent to the dead-letter queue."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html"
    ]
  },
  {
    "id": 28,
    "question": "<p>Your web application architecture consists of multiple Amazon EC2 instances running behind an Elastic Load Balancer with an Auto Scaling group having the desired capacity of 5 EC2 instances. You would like to integrate AWS CodeDeploy for automating application deployment. The deployment should re-route traffic from your application's original environment to the new environment.</p>\n\n<p>Which of the following options will meet your deployment criteria?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Opt for Immutable deployment</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Opt for In-place deployment</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Opt for Blue/Green deployment</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Opt for Rolling deployment</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Opt for Blue/Green deployment</strong> - A Blue/Green deployment is used to update your applications while minimizing interruptions caused by the changes of a new application version. CodeDeploy provisions your new application version alongside the old version before rerouting your production traffic. The behavior of your deployment depends on which compute platform you use:</p>\n\n<ol>\n<li>AWS Lambda: Traffic is shifted from one version of a Lambda function to a new version of the same Lambda function.</li>\n<li>Amazon ECS: Traffic is shifted from a task set in your Amazon ECS service to an updated, replacement task set in the same Amazon ECS service.</li>\n<li>EC2/On-Premises: Traffic is shifted from one set of instances in the original environment to a replacement set of instances.</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for Rolling deployment</strong> - This deployment type is present for AWS Elastic Beanstalk and not for EC2 instances directly.</p>\n\n<p><strong>Opt for Immutable deployment</strong> - This deployment type is present for AWS Elastic Beanstalk and not for EC2 instances directly.</p>\n\n<p><strong>Opt for In-place deployment</strong> - Under this deployment type, the application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview-blue-green\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview-blue-green</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Opt for Blue/Green deployment</strong> - A Blue/Green deployment is used to update your applications while minimizing interruptions caused by the changes of a new application version. CodeDeploy provisions your new application version alongside the old version before rerouting your production traffic. The behavior of your deployment depends on which compute platform you use:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li>AWS Lambda: Traffic is shifted from one version of a Lambda function to a new version of the same Lambda function.</li>\n<li>Amazon ECS: Traffic is shifted from a task set in your Amazon ECS service to an updated, replacement task set in the same Amazon ECS service.</li>\n<li>EC2/On-Premises: Traffic is shifted from one set of instances in the original environment to a replacement set of instances.</li>\n</ol>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Opt for Rolling deployment</strong> - This deployment type is present for AWS Elastic Beanstalk and not for EC2 instances directly."
      },
      {
        "answer": "",
        "explanation": "<strong>Opt for Immutable deployment</strong> - This deployment type is present for AWS Elastic Beanstalk and not for EC2 instances directly."
      },
      {
        "answer": "",
        "explanation": "<strong>Opt for In-place deployment</strong> - Under this deployment type, the application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview-blue-green",
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html"
    ]
  },
  {
    "id": 29,
    "question": "<p>AWS CloudFormation helps model and provision all the cloud infrastructure resources needed for your business.</p>\n\n<p>Which of the following services rely on CloudFormation to provision resources (Select two)?</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Autoscaling</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS CodeBuild</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Serverless Application Model (AWS SAM)</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>AWS Elastic Beanstalk</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>AWS Lambda</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. Elastic Beanstalk uses AWS CloudFormation to launch the resources in your environment and propagate configuration changes.</p>\n\n<p><strong>AWS Serverless Application Model (AWS SAM)</strong> - You use the AWS SAM specification to define your serverless application. AWS SAM templates are an extension of AWS CloudFormation templates, with some additional components that make them easier to work with. AWS SAM needs CloudFormation templates as a basis for its configuration.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Hence, Lamda does not need CloudFormation to run its services.</p>\n\n<p><strong>AWS Autoscaling</strong> - AWS Auto Scaling monitors your applications and automatically adjusts the capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, its easy to setup application scaling for multiple resources across multiple services in minutes. Auto Scaling used CloudFormation but is not a mandatory requirement.</p>\n\n<p><strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you dont need to provision, manage, and scale your own build servers. AWS CodePipeline uses AWS CloudFormation as a deployment action but is not a mandatory service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. Elastic Beanstalk uses AWS CloudFormation to launch the resources in your environment and propagate configuration changes."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Serverless Application Model (AWS SAM)</strong> - You use the AWS SAM specification to define your serverless application. AWS SAM templates are an extension of AWS CloudFormation templates, with some additional components that make them easier to work with. AWS SAM needs CloudFormation templates as a basis for its configuration."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Hence, Lamda does not need CloudFormation to run its services."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Autoscaling</strong> - AWS Auto Scaling monitors your applications and automatically adjusts the capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, its easy to setup application scaling for multiple resources across multiple services in minutes. Auto Scaling used CloudFormation but is not a mandatory requirement."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you dont need to provision, manage, and scale your own build servers. AWS CodePipeline uses AWS CloudFormation as a deployment action but is not a mandatory service."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification.html",
      "https://aws.amazon.com/elasticbeanstalk/"
    ]
  },
  {
    "id": 30,
    "question": "<p>A developer from your team has configured the load balancer to route traffic equally between instances or across Availability Zones. However, Elastic Load Balancing (ELB) routes more traffic to one instance or Availability Zone than the others.</p>\n\n<p>Why is this happening and how can it be fixed? (Select two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>There could be short-lived TCP connections between clients and instances</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Instances of a specific capacity type arent equally distributed across Availability Zones</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Sticky sessions are enabled for the load balancer</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>For Application Load Balancers, cross-zone load balancing is disabled by default</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Sticky sessions are enabled for the load balancer</strong> - This can be the reason for potential unequal traffic routing by the load balancer. Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies.</p>\n\n<p>When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target.</p>\n\n<p>If you use duration-based session stickiness, configure an appropriate cookie expiration time for your specific use case. If you set session stickiness from individual applications, use session cookies instead of persistent cookies where possible.</p>\n\n<p><strong>Instances of a specific capacity type arent equally distributed across Availability Zones</strong> - A Classic Load Balancer with HTTP or HTTPS listeners might route more traffic to higher-capacity instance types. This distribution aims to prevent lower-capacity instance types from having too many outstanding requests. Its a best practice to use similar instance types and configurations to reduce the likelihood of capacity gaps and traffic imbalances.</p>\n\n<p>A traffic imbalance might also occur if you have instances of similar capacities running on different Amazon Machine Images (AMIs). In this scenario, the imbalance of the traffic in favor of higher-capacity instance types is desirable.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>There could be short-lived TCP connections between clients and instances</strong> - This is an incorrect statement. Long-lived TCP connections between clients and instances can potentially lead to unequal distribution of traffic by the load balancer. Long-lived TCP connections between clients and instances cause uneven traffic load distribution by design. As a result, new instances take longer to reach connection equilibrium. Be sure to check your metrics for long-lived TCP connections that might be causing routing issues in the load balancer.</p>\n\n<p><strong>For Application Load Balancers, cross-zone load balancing is disabled by default</strong> - This is an incorrect statement. With Application Load Balancers, cross-zone load balancing is always enabled.</p>\n\n<p><strong>After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic</strong> - This is an incorrect statement. After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer. However, even though they remain registered, the load balancer does not route traffic to them.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elb-fix-unequal-traffic-routing/\">https://aws.amazon.com/premiumsupport/knowledge-center/elb-fix-unequal-traffic-routing/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#availability-zones\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#availability-zones</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Sticky sessions are enabled for the load balancer</strong> - This can be the reason for potential unequal traffic routing by the load balancer. Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies."
      },
      {
        "answer": "",
        "explanation": "When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target."
      },
      {
        "answer": "",
        "explanation": "If you use duration-based session stickiness, configure an appropriate cookie expiration time for your specific use case. If you set session stickiness from individual applications, use session cookies instead of persistent cookies where possible."
      },
      {
        "answer": "",
        "explanation": "<strong>Instances of a specific capacity type arent equally distributed across Availability Zones</strong> - A Classic Load Balancer with HTTP or HTTPS listeners might route more traffic to higher-capacity instance types. This distribution aims to prevent lower-capacity instance types from having too many outstanding requests. Its a best practice to use similar instance types and configurations to reduce the likelihood of capacity gaps and traffic imbalances."
      },
      {
        "answer": "",
        "explanation": "A traffic imbalance might also occur if you have instances of similar capacities running on different Amazon Machine Images (AMIs). In this scenario, the imbalance of the traffic in favor of higher-capacity instance types is desirable."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>There could be short-lived TCP connections between clients and instances</strong> - This is an incorrect statement. Long-lived TCP connections between clients and instances can potentially lead to unequal distribution of traffic by the load balancer. Long-lived TCP connections between clients and instances cause uneven traffic load distribution by design. As a result, new instances take longer to reach connection equilibrium. Be sure to check your metrics for long-lived TCP connections that might be causing routing issues in the load balancer."
      },
      {
        "answer": "",
        "explanation": "<strong>For Application Load Balancers, cross-zone load balancing is disabled by default</strong> - This is an incorrect statement. With Application Load Balancers, cross-zone load balancing is always enabled."
      },
      {
        "answer": "",
        "explanation": "<strong>After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic</strong> - This is an incorrect statement. After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer. However, even though they remain registered, the load balancer does not route traffic to them."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions",
      "https://aws.amazon.com/premiumsupport/knowledge-center/elb-fix-unequal-traffic-routing/",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#availability-zones"
    ]
  },
  {
    "id": 31,
    "question": "<p>To meet compliance guidelines, a company needs to ensure replication of any data stored in its S3 buckets.</p>\n\n<p>Which of the following characteristics are correct while configuring an S3 bucket for replication? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Replicated objects do not retain metadata</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Same-Region Replication (SRR) and Cross-Region Replication (CRR) can be configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Object tags cannot be replicated across AWS Regions using Cross-Region Replication</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Once replication is enabled on a bucket, all old and new objects will be replicated</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>S3 lifecycle actions are not replicated with S3 replication</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Same-Region Replication (SRR) and Cross-Region Replication (CRR) can be configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags</strong> - Amazon S3 Replication (CRR and SRR) is configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags. You add a replication configuration on your source bucket by specifying a destination bucket in the same or different AWS region for replication.</p>\n\n<p><strong>S3 lifecycle actions are not replicated with S3 replication</strong> - With S3 Replication (CRR and SRR), you can establish replication rules to make copies of your objects into another storage class, in the same or a different region. Lifecycle actions are not replicated, and if you want the same lifecycle configuration applied to both source and destination buckets, enable the same lifecycle configuration on both.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Object tags cannot be replicated across AWS Regions using Cross-Region Replication</strong> - Object tags can be replicated across AWS Regions using Cross-Region Replication. For customers with Cross-Region Replication already enabled, new permissions are required for tags to replicate.</p>\n\n<p><strong>Once replication is enabled on a bucket, all old and new objects will be replicated</strong> - Replication only replicates the objects added to the bucket after replication is enabled on the bucket. Any objects present in the bucket before enabling replication are not replicated.</p>\n\n<p><strong>Replicated objects do not retain metadata</strong> - You can use replication to make copies of your objects that retain all metadata, such as the original object creation time and version IDs. This capability is important if you need to ensure that your replica is identical to the source object.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/replication.html\">https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/replication.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Same-Region Replication (SRR) and Cross-Region Replication (CRR) can be configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags</strong> - Amazon S3 Replication (CRR and SRR) is configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags. You add a replication configuration on your source bucket by specifying a destination bucket in the same or different AWS region for replication."
      },
      {
        "answer": "",
        "explanation": "<strong>S3 lifecycle actions are not replicated with S3 replication</strong> - With S3 Replication (CRR and SRR), you can establish replication rules to make copies of your objects into another storage class, in the same or a different region. Lifecycle actions are not replicated, and if you want the same lifecycle configuration applied to both source and destination buckets, enable the same lifecycle configuration on both."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Object tags cannot be replicated across AWS Regions using Cross-Region Replication</strong> - Object tags can be replicated across AWS Regions using Cross-Region Replication. For customers with Cross-Region Replication already enabled, new permissions are required for tags to replicate."
      },
      {
        "answer": "",
        "explanation": "<strong>Once replication is enabled on a bucket, all old and new objects will be replicated</strong> - Replication only replicates the objects added to the bucket after replication is enabled on the bucket. Any objects present in the bucket before enabling replication are not replicated."
      },
      {
        "answer": "",
        "explanation": "<strong>Replicated objects do not retain metadata</strong> - You can use replication to make copies of your objects that retain all metadata, such as the original object creation time and version IDs. This capability is important if you need to ensure that your replica is identical to the source object."
      }
    ],
    "references": [
      "https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/replication.html"
    ]
  },
  {
    "id": 32,
    "question": "<p>A digital marketing company has its website hosted on an Amazon S3 bucket A. The development team notices that the web fonts that are hosted on another S3 bucket B are not loading on the website.</p>\n\n<p>Which of the following solutions can be used to address this issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Update bucket policies on both bucket A and bucket B to allow successful loading of the web fonts on the website</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure CORS on the bucket B that is hosting the web fonts to allow Bucket A origin to make the requests</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure CORS on the bucket A that is hosting the website to allow any origin to respond to requests</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable versioning on both the buckets to facilitate correct functioning of the website</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure CORS on the bucket B that is hosting the web fonts to allow Bucket A origin to make the requests</strong></p>\n\n<p>Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.</p>\n\n<p>To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operation-specific information.</p>\n\n<p>For the given use-case, you would create a <code>&lt;CORSRule&gt;</code> in <code>&lt;CORSConfiguration&gt;</code> for bucket B to allow access from the S3 website origin hosted on bucket A.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q4-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q4-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable versioning on both the buckets to facilitate the correct functioning of the website</strong> - This option is a distractor and versioning will not help to address the web fonts loading issue on the website.</p>\n\n<p><strong>Update bucket policies on both bucket A and bucket B to allow successful loading of the web fonts on the website</strong> - It's not the bucket policies but the CORS configuration on bucket B that needs to be updated to allow web fonts to be loaded on the website. Updating bucket policies will not help to address the web fonts loading issue on the website.</p>\n\n<p><strong>Configure CORS on the bucket A that is hosting the website to allow any origin to respond to requests</strong> - The CORS configuration needs to be updated on bucket B to allow web fonts to be loaded on the website hosted on bucket A. So this option in incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Configure CORS on the bucket B that is hosting the web fonts to allow Bucket A origin to make the requests</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain."
      },
      {
        "answer": "",
        "explanation": "To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operation-specific information."
      },
      {
        "answer": "",
        "explanation": "For the given use-case, you would create a <code>&lt;CORSRule&gt;</code> in <code>&lt;CORSConfiguration&gt;</code> for bucket B to allow access from the S3 website origin hosted on bucket A."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q4-i1.jpg",
        "answer": "",
        "explanation": "Please see this note for more details:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable versioning on both the buckets to facilitate the correct functioning of the website</strong> - This option is a distractor and versioning will not help to address the web fonts loading issue on the website."
      },
      {
        "answer": "",
        "explanation": "<strong>Update bucket policies on both bucket A and bucket B to allow successful loading of the web fonts on the website</strong> - It's not the bucket policies but the CORS configuration on bucket B that needs to be updated to allow web fonts to be loaded on the website. Updating bucket policies will not help to address the web fonts loading issue on the website."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure CORS on the bucket A that is hosting the website to allow any origin to respond to requests</strong> - The CORS configuration needs to be updated on bucket B to allow web fonts to be loaded on the website hosted on bucket A. So this option in incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html"
    ]
  },
  {
    "id": 33,
    "question": "<p>A developer wants to securely store an access token that allows a transaction-processing application running on Amazon EC2 instances to authenticate and send a chat message (via the chat API) to the company's support team when an invalid transaction is detected. While minimizing management overhead, the chat API access token must be encrypted both at rest and in transit, and also be accessible from other AWS accounts.</p>\n\n<p>What is the most efficient solution to address this scenario?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage AWS Secrets Manager with an AWS KMS customer-managed key to store the access token as a secret and configure a resource-based policy for the secret to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Secrets Manager. Fetch the token from Secrets Manager and then use the decrypted access token to send the message to the chat</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Leverage AWS Systems Manager Parameter Store with an AWS KMS customer-managed key to store the access token as a SecureString parameter and configure a resource-based policy for the parameter to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Parameter Store. Fetch the token from Parameter Store using the <code>with decryption</code> flag and then use the decrypted access token to send the message to the chat</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Store AWS KMS encrypted access token in a DynamoDB table and configure a resource-based policy for the DynamoDB table to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the DynamoDB table. Fetch the token from the Dynamodb table and then use the decrypted access token to send the message to the chat</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage SSE-KMS to store the access token as an encrypted object on S3 and configure a resource-based policy for the S3 bucket to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the S3 object. Fetch the token from S3 and then use the decrypted access token to send the message to the chat</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage AWS Secrets Manager with an AWS KMS customer-managed key to store the access token as a secret and configure a resource-based policy for the secret to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Secrets Manager. Fetch the token from Secrets Manager and then use the decrypted access token to send the message to the chat</strong></p>\n\n<p>AWS Secrets Manager is an AWS service that encrypts and stores your secrets, and transparently decrypts and returns them to you in plaintext. It's designed especially to store application secrets, such as login credentials, that change periodically and should not be hard-coded or stored in plaintext in the application. In place of hard-coded credentials or table lookups, your application calls Secrets Manager.</p>\n\n<p>Secrets Manager also supports features that periodically rotate the secrets associated with commonly used databases. It always encrypts newly rotated secrets before they are stored.</p>\n\n<p>Secrets Manager integrates with AWS Key Management Service (AWS KMS) to encrypt every version of every secret value with a unique data key that is protected by an AWS KMS key. This integration protects your secrets under encryption keys that never leave AWS KMS unencrypted. It also enables you to set custom permissions on the KMS key and audit the operations that generate, encrypt, and decrypt the data keys that protect your secrets.</p>\n\n<p>To grant permission to retrieve secret values, you can attach policies to secrets or identities.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q46-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q46-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_examples.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_examples.html</a><p></p>\n\n<p>For the given use case, you can use the resource-based policy to the secret to allow access from other accounts. Then you need to update the IAM role of the EC2 instances with permissions to access Secrets Manager which will retrieve the token from Secrets Manager and use the decrypted access token to send the message to the support team via the chat API.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage AWS Systems Manager Parameter Store with an AWS KMS customer-managed key to store the access token as a SecureString parameter and configure a resource-based policy for the parameter to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Parameter Store. Fetch the token from Parameter Store using the <code>with decryption</code> flag and then use the decrypted access token to send the message to the chat</strong> - You cannot use a resource-based policy with a parameter in the Parameter Store. Parameter Store supports parameter policies that are available for parameters that use the advanced parameters tier. Parameter policies help you manage a growing set of parameters by allowing you to assign specific criteria to a parameter such as an expiration date or time to live. Parameter policies are especially helpful in forcing you to update or delete passwords and configuration data stored in Parameter Store, a capability of AWS Systems Manager. So this option is incorrect.</p>\n\n<p><strong>Store AWS KMS encrypted access token in a DynamoDB table and configure a resource-based policy for the DynamoDB table to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the DynamoDB table. Fetch the token from the Dynamodb table and then use the decrypted access token to send the message to the chat</strong> - You should note that DynamoDB does not support resource-based policies. Moreover, it's a security bad practice to keep sensitive access credentials in code, database or a flat file on a file system or object storage. Therefore, this option is incorrect.</p>\n\n<p><strong>Leverage SSE-KMS to store the access token as an encrypted object on S3 and configure a resource-based policy for the S3 bucket to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the S3 object. Fetch the token from S3 and then use the decrypted access token to send the message to the chat</strong> - It is considered a security bad practice to keep sensitive access credentials in code, database, or a flat file on a file system or object storage. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_examples.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_examples.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-policies.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-policies.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Leverage AWS Secrets Manager with an AWS KMS customer-managed key to store the access token as a secret and configure a resource-based policy for the secret to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Secrets Manager. Fetch the token from Secrets Manager and then use the decrypted access token to send the message to the chat</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Secrets Manager is an AWS service that encrypts and stores your secrets, and transparently decrypts and returns them to you in plaintext. It's designed especially to store application secrets, such as login credentials, that change periodically and should not be hard-coded or stored in plaintext in the application. In place of hard-coded credentials or table lookups, your application calls Secrets Manager."
      },
      {
        "answer": "",
        "explanation": "Secrets Manager also supports features that periodically rotate the secrets associated with commonly used databases. It always encrypts newly rotated secrets before they are stored."
      },
      {
        "answer": "",
        "explanation": "Secrets Manager integrates with AWS Key Management Service (AWS KMS) to encrypt every version of every secret value with a unique data key that is protected by an AWS KMS key. This integration protects your secrets under encryption keys that never leave AWS KMS unencrypted. It also enables you to set custom permissions on the KMS key and audit the operations that generate, encrypt, and decrypt the data keys that protect your secrets."
      },
      {
        "answer": "",
        "explanation": "To grant permission to retrieve secret values, you can attach policies to secrets or identities."
      },
      {
        "link": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_examples.html"
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can use the resource-based policy to the secret to allow access from other accounts. Then you need to update the IAM role of the EC2 instances with permissions to access Secrets Manager which will retrieve the token from Secrets Manager and use the decrypted access token to send the message to the support team via the chat API."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage AWS Systems Manager Parameter Store with an AWS KMS customer-managed key to store the access token as a SecureString parameter and configure a resource-based policy for the parameter to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Parameter Store. Fetch the token from Parameter Store using the <code>with decryption</code> flag and then use the decrypted access token to send the message to the chat</strong> - You cannot use a resource-based policy with a parameter in the Parameter Store. Parameter Store supports parameter policies that are available for parameters that use the advanced parameters tier. Parameter policies help you manage a growing set of parameters by allowing you to assign specific criteria to a parameter such as an expiration date or time to live. Parameter policies are especially helpful in forcing you to update or delete passwords and configuration data stored in Parameter Store, a capability of AWS Systems Manager. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Store AWS KMS encrypted access token in a DynamoDB table and configure a resource-based policy for the DynamoDB table to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the DynamoDB table. Fetch the token from the Dynamodb table and then use the decrypted access token to send the message to the chat</strong> - You should note that DynamoDB does not support resource-based policies. Moreover, it's a security bad practice to keep sensitive access credentials in code, database or a flat file on a file system or object storage. Therefore, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage SSE-KMS to store the access token as an encrypted object on S3 and configure a resource-based policy for the S3 bucket to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the S3 object. Fetch the token from S3 and then use the decrypted access token to send the message to the chat</strong> - It is considered a security bad practice to keep sensitive access credentials in code, database, or a flat file on a file system or object storage. Therefore, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_examples.html",
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-policies.html"
    ]
  },
  {
    "id": 34,
    "question": "<p>A high-frequency stock trading firm is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon SQS. The development team at the company wants to minimize the costs of using SQS.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend to address the given use-case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SQS message timer to retrieve messages from your Amazon SQS queues</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SQS long polling to retrieve messages from your Amazon SQS queues</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use SQS visibility timeout to retrieve messages from your Amazon SQS queues</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SQS short polling to retrieve messages from your Amazon SQS queues</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use SQS long polling to retrieve messages from your Amazon SQS queues</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.</p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.</p>\n\n<p>Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the differences between Short Polling vs Long Polling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q1-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q1-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q1-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q1-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use SQS short polling to retrieve messages from your Amazon SQS queues</strong> - With short polling, Amazon SQS sends the response right away, even if the query found no messages. You end up paying more because of the increased number of empty receives.</p>\n\n<p><strong>Use SQS visibility timeout to retrieve messages from your Amazon SQS queues</strong> - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.</p>\n\n<p><strong>Use SQS message timer to retrieve messages from your Amazon SQS queues</strong> - You can use message timers to set an initial invisibility period for a message added to a queue. So, if you send a message with a 60-second timer, the message isn't visible to consumers for its first 60 seconds in the queue. The default (minimum) delay for a message is 0 seconds. The maximum is 15 minutes. You cannot use message timer to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use SQS long polling to retrieve messages from your Amazon SQS queues</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications."
      },
      {
        "answer": "",
        "explanation": "Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires."
      },
      {
        "answer": "",
        "explanation": "Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q1-i1.jpg",
        "answer": "",
        "explanation": "Please review the differences between Short Polling vs Long Polling:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use SQS short polling to retrieve messages from your Amazon SQS queues</strong> - With short polling, Amazon SQS sends the response right away, even if the query found no messages. You end up paying more because of the increased number of empty receives."
      },
      {
        "answer": "",
        "explanation": "<strong>Use SQS visibility timeout to retrieve messages from your Amazon SQS queues</strong> - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to retrieve messages from your Amazon SQS queues. This option has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Use SQS message timer to retrieve messages from your Amazon SQS queues</strong> - You can use message timers to set an initial invisibility period for a message added to a queue. So, if you send a message with a 60-second timer, the message isn't visible to consumers for its first 60 seconds in the queue. The default (minimum) delay for a message is 0 seconds. The maximum is 15 minutes. You cannot use message timer to retrieve messages from your Amazon SQS queues. This option has been added as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html"
    ]
  },
  {
    "id": 35,
    "question": "<p>A company follows collaborative development practices. The engineering manager wants to isolate the development effort by setting up simulations of API components owned by various development teams.</p>\n\n<p>Which API integration type is best suited for this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>HTTP</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS_PROXY</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>HTTP_PROXY</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>MOCK</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>MOCK</strong></p>\n\n<p>This type of integration lets API Gateway return a response without sending the request further to the backend. This is useful for API testing because it can be used to test the integration setup without incurring charges for using the backend and to enable collaborative development of an API.</p>\n\n<p>In collaborative development, a team can isolate their development effort by setting up simulations of API components owned by other teams by using the MOCK integrations. It is also used to return CORS-related headers to ensure that the API method permits CORS access. In fact, the API Gateway console integrates the OPTIONS method to support CORS with a mock integration.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS_PROXY</strong> - This type of integration lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup. This integration relies on direct interactions between the client and the integrated Lambda function.</p>\n\n<p><strong>HTTP_PROXY</strong> - The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on single API method. You do not set the integration request or the integration response. API Gateway passes the incoming request from the client to the HTTP endpoint and passes the outgoing response from the HTTP endpoint to the client.</p>\n\n<p><strong>HTTP</strong> - This type of integration lets an API expose HTTP endpoints in the backend. With the HTTP integration, you must configure both the integration request and integration response. You must set up necessary data mappings from the method request to the integration request, and from the integration response to the method response.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>MOCK</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This type of integration lets API Gateway return a response without sending the request further to the backend. This is useful for API testing because it can be used to test the integration setup without incurring charges for using the backend and to enable collaborative development of an API."
      },
      {
        "answer": "",
        "explanation": "In collaborative development, a team can isolate their development effort by setting up simulations of API components owned by other teams by using the MOCK integrations. It is also used to return CORS-related headers to ensure that the API method permits CORS access. In fact, the API Gateway console integrates the OPTIONS method to support CORS with a mock integration."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS_PROXY</strong> - This type of integration lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup. This integration relies on direct interactions between the client and the integrated Lambda function."
      },
      {
        "answer": "",
        "explanation": "<strong>HTTP_PROXY</strong> - The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on single API method. You do not set the integration request or the integration response. API Gateway passes the incoming request from the client to the HTTP endpoint and passes the outgoing response from the HTTP endpoint to the client."
      },
      {
        "answer": "",
        "explanation": "<strong>HTTP</strong> - This type of integration lets an API expose HTTP endpoints in the backend. With the HTTP integration, you must configure both the integration request and integration response. You must set up necessary data mappings from the method request to the integration request, and from the integration response to the method response."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html"
    ]
  },
  {
    "id": 36,
    "question": "<p>An intern at an IT company is getting started with AWS Cloud and wants to understand the following Amazon S3 bucket access policy:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListAllS3Buckets\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:ListAllMyBuckets\"],\n            \"Resource\": \"arn:aws:s3:::*\"\n        },\n        {\n            \"Sid\": \"AllowBucketLevelActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::*\"\n        },\n        {\n            \"Sid\": \"AllowBucketObjectActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:PutObjectAcl\",\n                \"s3:GetObject\",\n                \"s3:GetObjectAcl\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": \"arn:aws:s3:::*/*\"\n        },\n        {\n            \"Sid\": \"RequireMFAForProductionBucket\",\n            \"Effect\": \"Deny\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::Production/*\",\n                \"arn:aws:s3:::Production\"\n            ],\n            \"Condition\": {\n                \"NumericGreaterThanIfExists\": {\"aws:MultiFactorAuthAge\": \"1800\"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>As a Developer Associate, can you help him identify what the policy is for?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Allows full S3 access, but explicitly denies access to the Production bucket if the user has not signed in using MFA within the last thirty minutes</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Allows a user to manage a single Amazon S3 bucket and denies every other AWS action and resource if the user is not signed in using MFA within last thirty minutes</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Allows full S3 access to an Amazon Cognito user, but explicitly denies access to the Production bucket if the Cognito user is not authenticated</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Allows IAM users to access their own home directory in Amazon S3, programmatically and in the console</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Allows full S3 access, but explicitly denies access to the Production bucket if the user has not signed in using MFA within the last thirty minutes</strong> - This example shows how you might create a policy that allows an Amazon S3 user to access any bucket, including updating, adding, and deleting objects. However, it explicitly denies access to the Production bucket if the user has not signed in using multi-factor authentication (MFA) within the last thirty minutes. This policy grants the permissions necessary to perform this action in the console or programmatically using the AWS CLI or AWS API.</p>\n\n<p>This policy never allows programmatic access to the Production bucket using long-term user access keys. This is accomplished using the aws:MultiFactorAuthAge condition key with the NumericGreaterThanIfExists condition operator. This policy condition returns true if MFA is not present or if the age of the MFA is greater than 30 minutes. In those situations, access is denied. To access the Production bucket programmatically, the S3 user must use temporary credentials that were generated in the last 30 minutes using the GetSessionToken API operation.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Allows a user to manage a single Amazon S3 bucket and denies every other AWS action and resource if the user is not signed in using MFA within last thirty minutes</strong></p>\n\n<p><strong>Allows full S3 access to an Amazon Cognito user, but explicitly denies access to the Production bucket if the Cognito user is not authenticated</strong></p>\n\n<p><strong>Allows IAM users to access their own home directory in Amazon S3, programmatically and in the console</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_full-access-except-production.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_full-access-except-production.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Allows full S3 access, but explicitly denies access to the Production bucket if the user has not signed in using MFA within the last thirty minutes</strong> - This example shows how you might create a policy that allows an Amazon S3 user to access any bucket, including updating, adding, and deleting objects. However, it explicitly denies access to the Production bucket if the user has not signed in using multi-factor authentication (MFA) within the last thirty minutes. This policy grants the permissions necessary to perform this action in the console or programmatically using the AWS CLI or AWS API."
      },
      {
        "answer": "",
        "explanation": "This policy never allows programmatic access to the Production bucket using long-term user access keys. This is accomplished using the aws:MultiFactorAuthAge condition key with the NumericGreaterThanIfExists condition operator. This policy condition returns true if MFA is not present or if the age of the MFA is greater than 30 minutes. In those situations, access is denied. To access the Production bucket programmatically, the S3 user must use temporary credentials that were generated in the last 30 minutes using the GetSessionToken API operation."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Allows a user to manage a single Amazon S3 bucket and denies every other AWS action and resource if the user is not signed in using MFA within last thirty minutes</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Allows full S3 access to an Amazon Cognito user, but explicitly denies access to the Production bucket if the Cognito user is not authenticated</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Allows IAM users to access their own home directory in Amazon S3, programmatically and in the console</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_full-access-except-production.html"
    ]
  },
  {
    "id": 37,
    "question": "<p>A junior developer working on ECS instances terminated a container instance in Amazon Elastic Container Service (Amazon ECS) as per instructions from the team lead. But the container instance continues to appear as a resource in the ECS cluster.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to fix this behavior?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues</strong> - If you terminate a container instance while it is in the STOPPED state, that container instance isn't automatically removed from the cluster. You will need to deregister your container instance in the STOPPED state by using the Amazon ECS console or AWS Command Line Interface. Once deregistered, the container instance will no longer appear as a resource in your Amazon ECS cluster.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues</strong> -  This is an incorrect statement. If you terminate a container instance in the RUNNING state, that container instance is automatically removed, or deregistered, from the cluster.</p>\n\n<p><strong>The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues</strong> - This is incorrect and has been added as a distractor.</p>\n\n<p><strong>A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again</strong> - This is an incorrect statement. It is already mentioned in the question that the developer has terminated the instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/deregister-ecs-instance/\">https://aws.amazon.com/premiumsupport/knowledge-center/deregister-ecs-instance/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues</strong> - If you terminate a container instance while it is in the STOPPED state, that container instance isn't automatically removed from the cluster. You will need to deregister your container instance in the STOPPED state by using the Amazon ECS console or AWS Command Line Interface. Once deregistered, the container instance will no longer appear as a resource in your Amazon ECS cluster."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues</strong> -  This is an incorrect statement. If you terminate a container instance in the RUNNING state, that container instance is automatically removed, or deregistered, from the cluster."
      },
      {
        "answer": "",
        "explanation": "<strong>The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues</strong> - This is incorrect and has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again</strong> - This is an incorrect statement. It is already mentioned in the question that the developer has terminated the instance."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/deregister-ecs-instance/",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html"
    ]
  },
  {
    "id": 38,
    "question": "<p>You team maintains a public API Gateway that is accessed by clients from another domain. Usage has been consistent for the last few months but recently it has more than doubled. As a result, your costs have gone up and would like to prevent other unauthorized domains from accessing your API.</p>\n\n<p>Which of the following actions should you take?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Mapping Templates</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Assign a Security Group to your API Gateway</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Restrict access by using CORS</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Account-level throttling</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Restrict access by using CORS</strong> - Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. When your API's resources receive requests from a domain other than the API's own domain and you want to restrict servicing these requests, you must disable cross-origin resource sharing (CORS) for selected methods on the resource.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Account-level throttling</strong> - To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API. By default, API Gateway limits the steady-state request rate to 10,000 requests per second (rps). It limits the burst (that is, the maximum bucket size) to 5,000 requests across all APIs within an AWS account. This is Account-level throttling. As you see, this is about limit on the number of requests and is not a suitable answer for the current scenario.</p>\n\n<p><strong>Use Mapping Templates</strong> - A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates have nothing to do with access and are not useful for the current scenario.</p>\n\n<p><strong>Assign a Security Group to your API Gateway</strong> - API Gateway does not use security groups but uses resource policies, which are JSON policy documents that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. You can restrict IP address using this, the downside being, an IP address can be changed by the accessing user. So, this is not an optimal solution for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-protect.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-protect.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Restrict access by using CORS</strong> - Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. When your API's resources receive requests from a domain other than the API's own domain and you want to restrict servicing these requests, you must disable cross-origin resource sharing (CORS) for selected methods on the resource."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Account-level throttling</strong> - To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API. By default, API Gateway limits the steady-state request rate to 10,000 requests per second (rps). It limits the burst (that is, the maximum bucket size) to 5,000 requests across all APIs within an AWS account. This is Account-level throttling. As you see, this is about limit on the number of requests and is not a suitable answer for the current scenario."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Mapping Templates</strong> - A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates have nothing to do with access and are not useful for the current scenario."
      },
      {
        "answer": "",
        "explanation": "<strong>Assign a Security Group to your API Gateway</strong> - API Gateway does not use security groups but uses resource policies, which are JSON policy documents that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. You can restrict IP address using this, the downside being, an IP address can be changed by the accessing user. So, this is not an optimal solution for the current use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-protect.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html"
    ]
  },
  {
    "id": 39,
    "question": "<p>Recently, you started an online learning platform using AWS Lambda and AWS Gateway API. Your first version was successful, and you began developing new features for the second version. You would like to gradually introduce the second version by routing only 10% of the incoming traffic to the new Lambda version.</p>\n\n<p>Which solution should you opt for?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Lambda aliases</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Tags to distinguish the different versions</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy your Lambda in a VPC</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use environment variables</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Lambda aliases</strong> - A Lambda alias is like a pointer to a specific Lambda function version. You can create one or more aliases for your AWS Lambda function. Users can access the function version using the alias ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function.\nEvent sources such as Amazon S3 invoke your Lambda function. These event sources maintain a mapping that identifies the function to invoke when events occur. If you specify a Lambda function alias in the mapping configuration, you don't need to update the mapping when the function version changes. This is the right choice for the current requirement.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Tags to distinguish the different versions</strong> - You can tag Lambda functions to organize them by owner, project or department. Tags are freeform key-value pairs that are supported across AWS services for use in filtering resources and adding detail to billing reports. This does not address the given use-case.</p>\n\n<p><strong>Use environment variables</strong> - You can use environment variables to store secrets securely and adjust your function's behavior without updating code. An environment variable is a pair of strings that are stored in a function's version-specific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. For example, you can use environment variables to point to test, development or production databases by passing it as an environment variable during runtime. This option does not address the given use-case.</p>\n\n<p><strong>Deploy your Lambda in a VPC</strong> - Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined. This adds another layer of security for your entire architecture. Not the right choice for the given scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-tags.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-tags.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Lambda aliases</strong> - A Lambda alias is like a pointer to a specific Lambda function version. You can create one or more aliases for your AWS Lambda function. Users can access the function version using the alias ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function.\nEvent sources such as Amazon S3 invoke your Lambda function. These event sources maintain a mapping that identifies the function to invoke when events occur. If you specify a Lambda function alias in the mapping configuration, you don't need to update the mapping when the function version changes. This is the right choice for the current requirement."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Tags to distinguish the different versions</strong> - You can tag Lambda functions to organize them by owner, project or department. Tags are freeform key-value pairs that are supported across AWS services for use in filtering resources and adding detail to billing reports. This does not address the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use environment variables</strong> - You can use environment variables to store secrets securely and adjust your function's behavior without updating code. An environment variable is a pair of strings that are stored in a function's version-specific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. For example, you can use environment variables to point to test, development or production databases by passing it as an environment variable during runtime. This option does not address the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy your Lambda in a VPC</strong> - Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined. This adds another layer of security for your entire architecture. Not the right choice for the given scenario."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-tags.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html"
    ]
  },
  {
    "id": 40,
    "question": "<p>A companys e-commerce website is expecting hundreds of thousands of visitors on Black Friday. The marketing department is concerned that high volumes of orders might stress SQS leading to message failures. The company has approached you for the steps to be taken as a precautionary measure against the high volumes.</p>\n\n<p>What step will you suggest as a Developer Associate?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Convert the queue into FIFO ordered queue, since messages to the down system will be processed faster once they are ordered</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Pre-configure the SQS queue to increase the capacity when messages hit a certain threshold</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable auto-scaling in the SQS queue</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon SQS is highly scalable and does not need any intervention to handle the expected high volumes</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon SQS is highly scalable and does not need any intervention to handle the expected high volumes</strong></p>\n\n<p>Amazon SQS leverages the AWS cloud to dynamically scale, based on demand. SQS scales elastically with your application so you don't have to worry about capacity planning and pre-provisioning. For most standard queues (depending on queue traffic and message backlog), there can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue).</p>\n\n<p>Info on Queue Quotas:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q53-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q53-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Pre-configure the SQS queue to increase the capacity when messages hit a certain threshold</strong> - This is an incorrect statement. Amazon SQS scales dynamically, automatically provisioning the needed capacity.</p>\n\n<p><strong>Enable auto-scaling in the SQS queue</strong> - SQS queues are, by definition, auto-scalable and do not need any configuration changes for auto-scaling.</p>\n\n<p><strong>Convert the queue into FIFO ordered queue, since messages to the down system will be processed faster once they are ordered</strong> - This is a wrong statement. You cannot convert an existing standard queue to FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.</p>\n\n<p>Standard to FIFO queue conversion:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q53-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q53-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a><p></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Amazon SQS is highly scalable and does not need any intervention to handle the expected high volumes</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon SQS leverages the AWS cloud to dynamically scale, based on demand. SQS scales elastically with your application so you don't have to worry about capacity planning and pre-provisioning. For most standard queues (depending on queue traffic and message backlog), there can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue)."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q53-i1.jpg",
        "answer": "",
        "explanation": "Info on Queue Quotas:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Pre-configure the SQS queue to increase the capacity when messages hit a certain threshold</strong> - This is an incorrect statement. Amazon SQS scales dynamically, automatically provisioning the needed capacity."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable auto-scaling in the SQS queue</strong> - SQS queues are, by definition, auto-scalable and do not need any configuration changes for auto-scaling."
      },
      {
        "answer": "",
        "explanation": "<strong>Convert the queue into FIFO ordered queue, since messages to the down system will be processed faster once they are ordered</strong> - This is a wrong statement. You cannot convert an existing standard queue to FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q53-i2.jpg",
        "answer": "",
        "explanation": "Standard to FIFO queue conversion:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"
    ]
  },
  {
    "id": 41,
    "question": "<p>You are creating a mobile application that needs access to the AWS API Gateway. Users will need to register first before they can access your API and you would like the user management to be fully managed.</p>\n\n<p>Which authentication option should you use for your API Gateway layer?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Cognito User Pools</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Lambda Authorizer</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use IAM permissions with sigv4</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use API Gateway User Pools</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito User Pools</strong> - As an alternative to using IAM roles and policies or Lambda authorizers, you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway.\nTo use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Lambda Authorizer</strong>- A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity. This won't be a fully managed user management solution but it would allow you to check for access at the AWS API Gateway level.</p>\n\n<p><strong>Use IAM permissions with sigv4</strong> - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. For security, most requests to AWS must be signed with an access key, which consists of an access key ID and secret access key. These two keys are commonly referred to as your security credentials. But, we cannot possibly create an IAM user for every visitor of the site, so this is where social identity providers come in to help.</p>\n\n<p><strong>Use API Gateway User Pools</strong> - This is a made-up option.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html\">https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Cognito User Pools</strong> - As an alternative to using IAM roles and policies or Lambda authorizers, you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway.\nTo use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Lambda Authorizer</strong>- A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity. This won't be a fully managed user management solution but it would allow you to check for access at the AWS API Gateway level."
      },
      {
        "answer": "",
        "explanation": "<strong>Use IAM permissions with sigv4</strong> - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. For security, most requests to AWS must be signed with an access key, which consists of an access key ID and secret access key. These two keys are commonly referred to as your security credentials. But, we cannot possibly create an IAM user for every visitor of the site, so this is where social identity providers come in to help."
      },
      {
        "answer": "",
        "explanation": "<strong>Use API Gateway User Pools</strong> - This is a made-up option."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html",
      "https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html"
    ]
  },
  {
    "id": 42,
    "question": "<p>The Development team at a media company is working on securing their databases.</p>\n\n<p>Which of the following AWS database engines can be configured with IAM Database Authentication? (Select two)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>RDS MySQL</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>RDS SQL Server</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>RDS PostGreSQL</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>RDS Db2</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>RDS Oracle</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Security",
    "explanation": "<p>Correct options:</p>\n\n<p>You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication.  With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM.</p>\n\n<p><strong>RDS MySQL</strong> - IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS.</p>\n\n<p><strong>RDS PostGreSQL</strong> - IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>RDS Oracle</strong></p>\n\n<p><strong>RDS SQL Server</strong></p>\n\n<p>These two options contradict the details in the explanation above, so these are incorrect.</p>\n\n<p><strong>RDS Db2</strong> - This option has been added as a distractor. Db2 is a family of data management products, including database servers, developed by IBM. RDS does not support Db2 database engine.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication.  With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM."
      },
      {
        "answer": "",
        "explanation": "<strong>RDS MySQL</strong> - IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS."
      },
      {
        "answer": "",
        "explanation": "<strong>RDS PostGreSQL</strong> - IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>RDS Oracle</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>RDS SQL Server</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These two options contradict the details in the explanation above, so these are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>RDS Db2</strong> - This option has been added as a distractor. Db2 is a family of data management products, including database servers, developed by IBM. RDS does not support Db2 database engine."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html"
    ]
  },
  {
    "id": 43,
    "question": "<p>You have a workflow process that pulls code from AWS CodeCommit and deploys to EC2 instances associated with tag group ProdBuilders. You would like to configure the instances to archive no more than two application revisions to conserve disk space.</p>\n\n<p>Which of the following will allow you to implement this?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Have a load balancer in front of your instances</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Integrate with AWS CodePipeline</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>CodeDeploy Agent</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>AWS CloudWatch Log Agent</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p>\"CodeDeploy Agent\"</p>\n\n<p>The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent archives revisions and log files on instances. The CodeDeploy agent cleans up these artifacts to conserve disk space. You can use the :max_revisions: option in the agent configuration file to specify the number of application revisions to the archive by entering any positive integer. CodeDeploy also archives the log files for those revisions. All others are deleted, except for the log file of the last successful deployment.</p>\n\n<p>More info here:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q48-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q48-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS CloudWatch Log Agent</strong> - The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. This is an incorrect choice for the current use case.</p>\n\n<p><strong>Integrate with AWS CodePipeline</strong> - AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodeCommit and CodePipeline are already integrated services. CodePipeline cannot help in version control and management of archives on an EC2 instance.</p>\n\n<p><strong>Have a load balancer in front of your instances</strong> - Load Balancer helps balance incoming traffic across different EC2 instances. It is an incorrect choice for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>CodeDeploy Agent</strong>"
      },
      {
        "answer": "",
        "explanation": "The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent archives revisions and log files on instances. The CodeDeploy agent cleans up these artifacts to conserve disk space. You can use the :max_revisions: option in the agent configuration file to specify the number of application revisions to the archive by entering any positive integer. CodeDeploy also archives the log files for those revisions. All others are deleted, except for the log file of the last successful deployment."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q48-i1.jpg",
        "answer": "",
        "explanation": "More info here:"
      },
      {
        "link": "https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS CloudWatch Log Agent</strong> - The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. This is an incorrect choice for the current use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Integrate with AWS CodePipeline</strong> - AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodeCommit and CodePipeline are already integrated services. CodePipeline cannot help in version control and management of archives on an EC2 instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Have a load balancer in front of your instances</strong> - Load Balancer helps balance incoming traffic across different EC2 instances. It is an incorrect choice for the current use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html"
    ]
  },
  {
    "id": 44,
    "question": "<p>Two policies are attached to an IAM user. The first policy states that the user has explicitly been denied all access to EC2 instances. The second policy states that the user has been allowed permission for EC2:Describe action.</p>\n\n<p>When the user tries to use 'Describe' action on an EC2 instance using the CLI, what will be the output?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The user will be denied access because one of the policies has an explicit deny on it</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The user will get access because it has an explicit allow</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The IAM user stands in an invalid state, because of conflicting policies</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The user will be denied access because the policy has an explicit deny on it</strong> - User will be denied access because any explicit deny overrides the allow.</p>\n\n<p>Policy Evaluation explained:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q43-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q43-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The IAM user stands in an invalid state, because of conflicting policies</strong> - This is an incorrect statement. Access policies can have allow and deny permissions on them and based on policy rules they are evaluated. A user account does not get invalid because of policies.</p>\n\n<p><strong>The user will get access because it has an explicit allow</strong> - As discussed above, explicit deny overrides all other permissions and hence the user will be denied access.</p>\n\n<p><strong>The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access</strong> - If policies that apply to a request include an Allow statement and a Deny statement, the Deny statement trumps the Allow statement. The request is explicitly denied.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The user will be denied access because the policy has an explicit deny on it</strong> - User will be denied access because any explicit deny overrides the allow."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q43-i1.jpg",
        "answer": "",
        "explanation": "Policy Evaluation explained:"
      },
      {
        "link": "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The IAM user stands in an invalid state, because of conflicting policies</strong> - This is an incorrect statement. Access policies can have allow and deny permissions on them and based on policy rules they are evaluated. A user account does not get invalid because of policies."
      },
      {
        "answer": "",
        "explanation": "<strong>The user will get access because it has an explicit allow</strong> - As discussed above, explicit deny overrides all other permissions and hence the user will be denied access."
      },
      {
        "answer": "",
        "explanation": "<strong>The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access</strong> - If policies that apply to a request include an Allow statement and a Deny statement, the Deny statement trumps the Allow statement. The request is explicitly denied."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html"
    ]
  },
  {
    "id": 45,
    "question": "<p>A telecommunications company that provides internet service for mobile device users maintains over 100 c4.large instances in the us-east-1 region. The EC2 instances run complex algorithms. The manager would like to track CPU utilization of the EC2 instances as frequently as every 10 seconds.</p>\n\n<p>Which of the following represents the BEST solution for the given use-case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a high-resolution custom metric and push the data using a script triggered every 10 seconds</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Simply get it from the CloudWatch Metrics</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable EC2 detailed monitoring</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Open a support ticket with AWS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a high-resolution custom metric and push the data using a script triggered every 10 seconds</strong></p>\n\n<p>Using high-resolution custom metric, your applications can publish metrics to CloudWatch with 1-second resolution. You can watch the metrics scroll across your screen seconds after they are published and you can set up high-resolution CloudWatch Alarms that evaluate as frequently as every 10 seconds. You can alert with High-Resolution Alarms, as frequently as 10-second periods. High-Resolution Alarms allow you to react and take actions faster and support the same actions available today with standard 1-minute alarms.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q11-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q11-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/\">https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable EC2 detailed monitoring</strong> - As part of basic monitoring, Amazon EC2 sends metric data to CloudWatch in 5-minute periods. To send metric data for your instance to CloudWatch in 1-minute periods, you can enable detailed monitoring on the instance, however, this comes at an additional cost.</p>\n\n<p><strong>Simply get it from the CloudWatch Metrics</strong> - You can get data from metrics. The basic monitoring data is available automatically in a 5-minute interval and detailed monitoring data is available in a 1-minute interval.</p>\n\n<p><strong>Open a support ticket with AWS</strong> - This option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/\">https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Create a high-resolution custom metric and push the data using a script triggered every 10 seconds</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Using high-resolution custom metric, your applications can publish metrics to CloudWatch with 1-second resolution. You can watch the metrics scroll across your screen seconds after they are published and you can set up high-resolution CloudWatch Alarms that evaluate as frequently as every 10 seconds. You can alert with High-Resolution Alarms, as frequently as 10-second periods. High-Resolution Alarms allow you to react and take actions faster and support the same actions available today with standard 1-minute alarms."
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable EC2 detailed monitoring</strong> - As part of basic monitoring, Amazon EC2 sends metric data to CloudWatch in 5-minute periods. To send metric data for your instance to CloudWatch in 1-minute periods, you can enable detailed monitoring on the instance, however, this comes at an additional cost."
      },
      {
        "answer": "",
        "explanation": "<strong>Simply get it from the CloudWatch Metrics</strong> - You can get data from metrics. The basic monitoring data is available automatically in a 5-minute interval and detailed monitoring data is available in a 1-minute interval."
      },
      {
        "answer": "",
        "explanation": "<strong>Open a support ticket with AWS</strong> - This option has been added as a distractor."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/"
    ]
  },
  {
    "id": 46,
    "question": "<p>A HealthCare mobile app uses proprietary Machine Learning algorithms to provide early diagnosis using patient health metrics. To protect this sensitive data, the development team wants to transition to a scalable user management system with log-in/sign-up functionality that also supports Multi-Factor Authentication (MFA)</p>\n\n<p>Which of the following options can be used to implement a solution with the LEAST amount of development effort? (Select two)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Cognito to enable Multi-Factor Authentication (MFA) when users log-in</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon SNS to send Multi-Factor Authentication (MFA) code via SMS to mobile app users</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Lambda functions and RDS to create a custom solution for user management</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Cognito for user-management and facilitating the log-in/sign-up process</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Use Lambda functions and DynamoDB to create a custom solution for user management</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Security",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Use Amazon Cognito for user-management and facilitating the log-in/sign-up process</strong></p>\n\n<p><strong>Use Amazon Cognito to enable Multi-Factor Authentication (MFA) when users log-in</strong></p>\n\n<p>Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0.</p>\n\n<p>A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.</p>\n\n<p>Cognito user pools provide support for sign-up and sign-in services as well as security features such as multi-factor authentication (MFA).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q10-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q10-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a><p></p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q10-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q10-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Lambda functions and DynamoDB to create a custom solution for user management</strong></p>\n\n<p><strong>Use Lambda functions and RDS to create a custom solution for user management</strong></p>\n\n<p>As the problem statement mentions that the solution should require the least amount of development effort, so you cannot use Lambda functions with DynamoDB or RDS to create a custom solution. So both these options are incorrect.</p>\n\n<p><strong>Use Amazon SNS to send Multi-Factor Authentication (MFA) code via SMS to mobile app users</strong> - Amazon SNS cannot be used to send MFA codes via SMS to the user's mobile devices as this functionality is only meant to be used for IAM users. An SMS (short message service) MFA device can be any mobile device with a phone number that can receive standard SMS text messages. AWS will soon end support for SMS multi-factor authentication (MFA).</p>\n\n<p>Please see this for more details:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use Amazon Cognito for user-management and facilitating the log-in/sign-up process</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Use Amazon Cognito to enable Multi-Factor Authentication (MFA) when users log-in</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0."
      },
      {
        "answer": "",
        "explanation": "A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK."
      },
      {
        "answer": "",
        "explanation": "Cognito user pools provide support for sign-up and sign-in services as well as security features such as multi-factor authentication (MFA)."
      },
      {
        "link": "https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html"
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q10-i2.jpg",
        "answer": "",
        "explanation": "Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:"
      },
      {
        "link": "https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Use Lambda functions and DynamoDB to create a custom solution for user management</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Use Lambda functions and RDS to create a custom solution for user management</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As the problem statement mentions that the solution should require the least amount of development effort, so you cannot use Lambda functions with DynamoDB or RDS to create a custom solution. So both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SNS to send Multi-Factor Authentication (MFA) code via SMS to mobile app users</strong> - Amazon SNS cannot be used to send MFA codes via SMS to the user's mobile devices as this functionality is only meant to be used for IAM users. An SMS (short message service) MFA device can be any mobile device with a phone number that can receive standard SMS text messages. AWS will soon end support for SMS multi-factor authentication (MFA)."
      },
      {
        "answer": "",
        "explanation": "Please see this for more details:"
      },
      {
        "link": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html",
        "answer": "",
        "explanation": "<a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html</a>"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html"
    ]
  },
  {
    "id": 47,
    "question": "<p>You have an Auto Scaling group configured to a minimum capacity of 1 and a maximum capacity of 5, designed to launch EC2 instances across 3 Availability Zones. During a low utilization period, an entire Availability Zone went down and your application experienced downtime.</p>\n\n<p>What can you do to ensure that your application remains highly available?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Increase the minimum instance capacity of the Auto Scaling Group to 2</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Change the scaling metric of auto-scaling policy to network bytes</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable RDS Multi-AZ</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure ASG fast failover</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Increase the minimum instance capacity of the Auto Scaling Group to 2</strong> -</p>\n\n<p>You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity.</p>\n\n<p>Since a minimum capacity of 1 was defined, an instance was launched in only one AZ. This AZ went down, taking the application with it. If the minimum capacity is set to 2. As per Auto Scale AZ configuration, it would have launched 2 instances- one in each AZ, making the architecture disaster-proof and hence highly available.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q65-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q65-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the scaling metric of auto-scaling policy to network bytes</strong> - With target tracking scaling policies, you select a scaling metric and set a target value. You can use predefined customized metrics. Setting the metric to network bytes will not help in this context since the instances have to be spread across different AZs for high availability. The optimized way of doing it, is by defining minimum and maximum instance capacities, as discussed above.</p>\n\n<p><strong>Configure ASG fast failover</strong> - This is a made-up option, given as a distractor.</p>\n\n<p><strong>Enable RDS Multi-AZ</strong> - This configuration will make your database highly available. But for the current scenario, you will need to have more than 1 instance in separate availability zones to keep the application highly available.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Increase the minimum instance capacity of the Auto Scaling Group to 2</strong> -"
      },
      {
        "answer": "",
        "explanation": "You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity."
      },
      {
        "answer": "",
        "explanation": "Since a minimum capacity of 1 was defined, an instance was launched in only one AZ. This AZ went down, taking the application with it. If the minimum capacity is set to 2. As per Auto Scale AZ configuration, it would have launched 2 instances- one in each AZ, making the architecture disaster-proof and hence highly available."
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Change the scaling metric of auto-scaling policy to network bytes</strong> - With target tracking scaling policies, you select a scaling metric and set a target value. You can use predefined customized metrics. Setting the metric to network bytes will not help in this context since the instances have to be spread across different AZs for high availability. The optimized way of doing it, is by defining minimum and maximum instance capacities, as discussed above."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure ASG fast failover</strong> - This is a made-up option, given as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable RDS Multi-AZ</strong> - This configuration will make your database highly available. But for the current scenario, you will need to have more than 1 instance in separate availability zones to keep the application highly available."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html"
    ]
  },
  {
    "id": 48,
    "question": "<p>An IT company has its serverless stack integrated with AWS X-Ray. The developer at the company has noticed a high volume of data going into X-Ray and the AWS monthly usage charges have skyrocketed as a result. The developer has requested changes to mitigate the issue.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to obtain tracing trends while reducing costs with minimal disruption?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Custom configuration for the X-Ray agents</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement a network sampling rule</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Filter Expressions in the X-Ray console</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable X-Ray sampling</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your applications underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a><p></p>\n\n<p><strong>Enable X-Ray sampling</strong></p>\n\n<p>To ensure efficient tracing and provide a representative sample of the requests that your application serves, the X-Ray SDK applies a sampling algorithm to determine which requests get traced. By default, the X-Ray SDK records the first request each second, and five percent of any additional requests. X-Ray sampling is enabled directly from the AWS console, hence your application code does not need to change.</p>\n\n<p>You can also customize the X-Ray sampling rules:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q13-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q13-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Filter Expressions in the X-Ray console</strong> - When you choose a time period of traces to view in the X-Ray console, you might get more results than the console can display. You can narrow the results to just the traces that you want to find by using a filter expression. This option is not correct because it does not reduce the volume of data sent into the X-Ray console.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q13-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q13-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html</a><p></p>\n\n<p><strong>Custom configuration for the X-Ray agents</strong> - You cannot do a custom configuration, instead you can do custom sampling rules. So this option is incorrect.</p>\n\n<p><strong>Implement a network sampling rule</strong> - This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your applications underlying components."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png",
        "answer": "",
        "explanation": "How X-Ray Works:"
      },
      {
        "link": "https://aws.amazon.com/xray/"
      },
      {
        "answer": "<strong>Enable X-Ray sampling</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "To ensure efficient tracing and provide a representative sample of the requests that your application serves, the X-Ray SDK applies a sampling algorithm to determine which requests get traced. By default, the X-Ray SDK records the first request each second, and five percent of any additional requests. X-Ray sampling is enabled directly from the AWS console, hence your application code does not need to change."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q13-i1.jpg",
        "answer": "",
        "explanation": "You can also customize the X-Ray sampling rules:"
      },
      {
        "link": "https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Filter Expressions in the X-Ray console</strong> - When you choose a time period of traces to view in the X-Ray console, you might get more results than the console can display. You can narrow the results to just the traces that you want to find by using a filter expression. This option is not correct because it does not reduce the volume of data sent into the X-Ray console."
      },
      {
        "link": "https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Custom configuration for the X-Ray agents</strong> - You cannot do a custom configuration, instead you can do custom sampling rules. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Implement a network sampling rule</strong> - This option has been added as a distractor."
      }
    ],
    "references": [
      "https://aws.amazon.com/xray/",
      "https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html",
      "https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html"
    ]
  },
  {
    "id": 49,
    "question": "<p>A company has a workload that requires 14,000 consistent IOPS for data that must be durable and secure. The compliance standards of the company state that the data should be secure at every stage of its lifecycle on all of the EBS volumes they use.</p>\n\n<p>Which of the following statements are true regarding data security on EBS?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>EBS volumes support both in-flight encryption and encryption at rest using KMS</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>EBS volumes support in-flight encryption but does not support encryption at rest</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>EBS volumes do not support in-flight encryption but do support encryption at rest using KMS</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>EBS volumes don't support any encryption</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p>Amazon EBS works with AWS KMS to encrypt and decrypt your EBS volume. You can encrypt both the boot and data volumes of an EC2 instance. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:</p>\n\n<ol>\n<li><p>Data at rest inside the volume</p></li>\n<li><p>All data moving between the volume and the instance</p></li>\n<li><p>All snapshots created from the volume</p></li>\n<li><p>All volumes created from those snapshots</p></li>\n</ol>\n\n<p><strong>EBS volumes support both in-flight encryption and encryption at rest using KMS</strong> - This is a correct statement. Encryption operations occur on the servers that host EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EBS volumes support in-flight encryption but do not support encryption at rest</strong> - This is an incorrect statement. As discussed above, all data moving between the volume and the instance is encrypted.</p>\n\n<p><strong>EBS volumes do not support in-flight encryption but do support encryption at rest using KMS</strong> - This is an incorrect statement. As discussed above, data at rest is also encrypted.</p>\n\n<p><strong>EBS volumes don't support any encryption</strong> - This is an incorrect statement. Amazon EBS encryption offers a straight-forward encryption solution for your EBS resources associated with your EC2 instances. With Amazon EBS encryption, you aren't required to build, maintain, and secure your own key management infrastructure.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon EBS works with AWS KMS to encrypt and decrypt your EBS volume. You can encrypt both the boot and data volumes of an EC2 instance. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>Data at rest inside the volume</p></li>\n<li><p>All data moving between the volume and the instance</p></li>\n<li><p>All snapshots created from the volume</p></li>\n<li><p>All volumes created from those snapshots</p></li>\n</ol>"
      },
      {
        "answer": "",
        "explanation": "<strong>EBS volumes support both in-flight encryption and encryption at rest using KMS</strong> - This is a correct statement. Encryption operations occur on the servers that host EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>EBS volumes support in-flight encryption but do not support encryption at rest</strong> - This is an incorrect statement. As discussed above, all data moving between the volume and the instance is encrypted."
      },
      {
        "answer": "",
        "explanation": "<strong>EBS volumes do not support in-flight encryption but do support encryption at rest using KMS</strong> - This is an incorrect statement. As discussed above, data at rest is also encrypted."
      },
      {
        "answer": "",
        "explanation": "<strong>EBS volumes don't support any encryption</strong> - This is an incorrect statement. Amazon EBS encryption offers a straight-forward encryption solution for your EBS resources associated with your EC2 instances. With Amazon EBS encryption, you aren't required to build, maintain, and secure your own key management infrastructure."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html"
    ]
  },
  {
    "id": 50,
    "question": "<p>A large firm stores its static data assets on Amazon S3 buckets. Each service line of the firm has its own AWS account. For a business use case, the Finance department needs to give access to their S3 bucket's data to the Human Resources department.</p>\n\n<p>Which of the below options is NOT feasible for cross-account access of S3 bucket objects?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Resource-based policies and AWS Identity and Access Management (IAM) policies for programmatic-only access to S3 bucket objects</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Access Control List (ACL) and IAM policies for programmatic-only access to S3 bucket objects</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Cross-account IAM roles for programmatic and console access to S3 bucket objects</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use IAM roles and resource-based policies delegate access across accounts within different partitions via programmatic access only</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use IAM roles and resource-based policies delegate access across accounts within different partitions via programmatic access only</strong> - This statement is incorrect and hence the right choice for this question. IAM roles and resource-based policies delegate access across accounts only within a single partition. For example, assume that you have an account in US West (N. California) in the standard <code>aws</code> partition. You also have an account in China (Beijing) in the <code>aws-cn</code> partition. You can't use an Amazon S3 resource-based policy in your account in China (Beijing) to allow access for users in your standard AWS account.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Resource-based policies and AWS Identity and Access Management (IAM) policies for programmatic-only access to S3 bucket objects</strong> - Use bucket policies to manage cross-account control and audit the S3 object's permissions. If you apply a bucket policy at the bucket level, you can define who can access (Principal element), which objects they can access (Resource element), and how they can access (Action element). Applying a bucket policy at the bucket level allows you to define granular access to different objects inside the bucket by using multiple policies to control access. You can also review the bucket policy to see who can access objects in an S3 bucket.</p>\n\n<p><strong>Use Access Control List (ACL) and IAM policies for programmatic-only access to S3 bucket objects</strong> - Use object ACLs to manage permissions only for specific scenarios and only if ACLs meet your needs better than IAM and S3 bucket policies. Amazon S3 ACLs allow users to define only the following permissions sets: READ, WRITE, READ_ACP, WRITE_ACP, and FULL_CONTROL. You can use only an AWS account or one of the predefined Amazon S3 groups as a grantee for the Amazon S3 ACL.</p>\n\n<p><strong>Use Cross-account IAM roles for programmatic and console access to S3 bucket objects</strong> - Not all AWS services support resource-based policies. This means that you can use cross-account IAM roles to centralize permission management when providing cross-account access to multiple services. Using cross-account IAM roles simplifies provisioning cross-account access to S3 objects that are stored in multiple S3 buckets, removing the need to manage multiple policies for S3 buckets. This method allows cross-account access to objects that are owned or uploaded by another AWS account or AWS services. If you don't use cross-account IAM roles, the object ACL must be modified.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example3.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use IAM roles and resource-based policies delegate access across accounts within different partitions via programmatic access only</strong> - This statement is incorrect and hence the right choice for this question. IAM roles and resource-based policies delegate access across accounts only within a single partition. For example, assume that you have an account in US West (N. California) in the standard <code>aws</code> partition. You also have an account in China (Beijing) in the <code>aws-cn</code> partition. You can't use an Amazon S3 resource-based policy in your account in China (Beijing) to allow access for users in your standard AWS account."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Resource-based policies and AWS Identity and Access Management (IAM) policies for programmatic-only access to S3 bucket objects</strong> - Use bucket policies to manage cross-account control and audit the S3 object's permissions. If you apply a bucket policy at the bucket level, you can define who can access (Principal element), which objects they can access (Resource element), and how they can access (Action element). Applying a bucket policy at the bucket level allows you to define granular access to different objects inside the bucket by using multiple policies to control access. You can also review the bucket policy to see who can access objects in an S3 bucket."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Access Control List (ACL) and IAM policies for programmatic-only access to S3 bucket objects</strong> - Use object ACLs to manage permissions only for specific scenarios and only if ACLs meet your needs better than IAM and S3 bucket policies. Amazon S3 ACLs allow users to define only the following permissions sets: READ, WRITE, READ_ACP, WRITE_ACP, and FULL_CONTROL. You can use only an AWS account or one of the predefined Amazon S3 groups as a grantee for the Amazon S3 ACL."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Cross-account IAM roles for programmatic and console access to S3 bucket objects</strong> - Not all AWS services support resource-based policies. This means that you can use cross-account IAM roles to centralize permission management when providing cross-account access to multiple services. Using cross-account IAM roles simplifies provisioning cross-account access to S3 objects that are stored in multiple S3 buckets, removing the need to manage multiple policies for S3 buckets. This method allows cross-account access to objects that are owned or uploaded by another AWS account or AWS services. If you don't use cross-account IAM roles, the object ACL must be modified."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example3.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html",
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/"
    ]
  },
  {
    "id": 51,
    "question": "<p>The development team at a social media company is considering using Amazon ElastiCache to boost the performance of their existing databases.</p>\n\n<p>As a Developer Associate, which of the following use-cases would you recommend as the BEST fit for ElastiCache? (Select two)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use ElastiCache to improve latency and throughput for write-heavy application workloads</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use ElastiCache to improve latency and throughput for read-heavy application workloads</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use ElastiCache to improve performance of Extract-Transform-Load (ETL) workloads</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use ElastiCache to improve performance of compute-intensive workloads</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Use ElastiCache to run highly complex JOIN queries</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use ElastiCache to improve latency and throughput for read-heavy application workloads</strong></p>\n\n<p><strong>Use ElastiCache to improve performance of compute-intensive workloads</strong></p>\n\n<p>Amazon ElastiCache allows you to run in-memory data stores in the AWS cloud. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/images/ElastiCache-Caching.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/images/ElastiCache-Caching.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a><p></p>\n\n<p>Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, and Q&amp;A portals) or compute-intensive workloads (such as a recommendation engine) by allowing you to store the objects that are often read in the cache.</p>\n\n<p>Overview of Amazon ElastiCache features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q3-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q3-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/elasticache/features/\">https://aws.amazon.com/elasticache/features/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use ElastiCache to improve latency and throughput for write-heavy application workloads</strong> - As mentioned earlier in the explanation, Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads. Caching is not a good fit for write-heavy applications as the cache goes stale at a very fast rate.</p>\n\n<p><strong>Use ElastiCache to improve performance of Extract-Transform-Load (ETL) workloads</strong> - ETL workloads involve reading and transforming high volume data which is not a good fit for caching. You should use AWS Glue or Amazon EMR to facilitate ETL workloads.</p>\n\n<p><strong>Use ElastiCache to run highly complex JOIN queries</strong> - Complex JSON queries can be run on relational databases such as RDS or Aurora. ElastiCache is not a good fit for this use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/features/\">https://aws.amazon.com/elasticache/features/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use ElastiCache to improve latency and throughput for read-heavy application workloads</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Use ElastiCache to improve performance of compute-intensive workloads</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache allows you to run in-memory data stores in the AWS cloud. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html"
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, and Q&amp;A portals) or compute-intensive workloads (such as a recommendation engine) by allowing you to store the objects that are often read in the cache."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q3-i1.jpg",
        "answer": "",
        "explanation": "Overview of Amazon ElastiCache features:"
      },
      {
        "link": "https://aws.amazon.com/elasticache/features/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use ElastiCache to improve latency and throughput for write-heavy application workloads</strong> - As mentioned earlier in the explanation, Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads. Caching is not a good fit for write-heavy applications as the cache goes stale at a very fast rate."
      },
      {
        "answer": "",
        "explanation": "<strong>Use ElastiCache to improve performance of Extract-Transform-Load (ETL) workloads</strong> - ETL workloads involve reading and transforming high volume data which is not a good fit for caching. You should use AWS Glue or Amazon EMR to facilitate ETL workloads."
      },
      {
        "answer": "",
        "explanation": "<strong>Use ElastiCache to run highly complex JOIN queries</strong> - Complex JSON queries can be run on relational databases such as RDS or Aurora. ElastiCache is not a good fit for this use-case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html",
      "https://aws.amazon.com/elasticache/features/"
    ]
  },
  {
    "id": 52,
    "question": "<p>A developer is working on an AWS Lambda function that reads data from Amazon S3 objects and writes the data to an Amazon DynamoDB table. Although the function triggers successfully from an S3 event notification upon object creation, it encounters a failure while attempting to write data to the DynamoDB table.</p>\n\n<p>What is the probable reason for the failure?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The Lambda function's provisioned concurrency limit has been exceeded</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The Lambda function's reserved concurrency limit has been exceeded</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The Lambda function does not have IAM permissions to write to DynamoDB</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>DynamoDB table does not have a Gateway VPC Endpoint, which is required by the Lambda function for a successful write</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The Lambda function does not have IAM permissions to write to DynamoDB</strong></p>\n\n<p>You need to use an identity-based policy that allows read and write access to a specific Amazon DynamoDB table. To use this policy, attach the policy to a Lambda service role. A service role is a role that you create in your account to allow a service to perform actions on your behalf. That service role must include AWS Lambda as the principal in the trust policy. The role is then used to grant a Lambda function access to a DynamoDB table. By using an IAM policy and role to control access, you dont need to embed credentials in code and can tightly control which services the Lambda function can access.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Lambda function's provisioned concurrency limit has been exceeded</strong></p>\n\n<p><strong>The Lambda function's reserved concurrency limit has been exceeded</strong></p>\n\n<p>Reserved concurrency  Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function.</p>\n\n<p>Provisioned concurrency  Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations. Note that configuring provisioned concurrency incurs charges to your AWS account.</p>\n\n<p>Neither reserved concurrency nor provisioned concurrency has any relevance to the given use case. Both options have been added as distractors.</p>\n\n<p><strong>DynamoDB table does not have a Gateway VPC Endpoint, which is required by the Lambda function for a successful write</strong> - Gateway endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC. Gateway endpoints do not enable AWS PrivateLink. This option acts as a distractor since the Lambda function is not provisioned within a VPC by default, so there is no need of a Gateway VPC Endpoint to access DynamoDB.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_lambda-access-dynamodb.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_lambda-access-dynamodb.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-create-an-aws-iam-policy-to-grant-aws-lambda-access-to-an-amazon-dynamodb-table/\">https://aws.amazon.com/blogs/security/how-to-create-an-aws-iam-policy-to-grant-aws-lambda-access-to-an-amazon-dynamodb-table/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>The Lambda function does not have IAM permissions to write to DynamoDB</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You need to use an identity-based policy that allows read and write access to a specific Amazon DynamoDB table. To use this policy, attach the policy to a Lambda service role. A service role is a role that you create in your account to allow a service to perform actions on your behalf. That service role must include AWS Lambda as the principal in the trust policy. The role is then used to grant a Lambda function access to a DynamoDB table. By using an IAM policy and role to control access, you dont need to embed credentials in code and can tightly control which services the Lambda function can access."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>The Lambda function's provisioned concurrency limit has been exceeded</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>The Lambda function's reserved concurrency limit has been exceeded</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Reserved concurrency  Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function."
      },
      {
        "answer": "",
        "explanation": "Provisioned concurrency  Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations. Note that configuring provisioned concurrency incurs charges to your AWS account."
      },
      {
        "answer": "",
        "explanation": "Neither reserved concurrency nor provisioned concurrency has any relevance to the given use case. Both options have been added as distractors."
      },
      {
        "answer": "",
        "explanation": "<strong>DynamoDB table does not have a Gateway VPC Endpoint, which is required by the Lambda function for a successful write</strong> - Gateway endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC. Gateway endpoints do not enable AWS PrivateLink. This option acts as a distractor since the Lambda function is not provisioned within a VPC by default, so there is no need of a Gateway VPC Endpoint to access DynamoDB."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_lambda-access-dynamodb.html",
      "https://aws.amazon.com/blogs/security/how-to-create-an-aws-iam-policy-to-grant-aws-lambda-access-to-an-amazon-dynamodb-table/"
    ]
  },
  {
    "id": 53,
    "question": "<p>Your team lead has requested code review of your code for Lambda functions. Your code is written in Python and makes use of the Amazon Simple Storage Service (S3) to upload logs to an S3 bucket. After the review, your team lead has recommended reuse of execution context to improve the Lambda performance.</p>\n\n<p>Which of the following actions will help you implement the recommendation?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Move the Amazon S3 client initialization, out of your function handler</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Assign more RAM to the function</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use environment variables to pass operational parameters</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable X-Ray integration</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Move the Amazon S3 client initialization, out of your function handler</strong> - AWS best practices for Lambda suggest taking advantage of execution context reuse to improve the performance of your functions. Initialize SDK clients and database connections outside of the function handler, and cache static assets locally in the /tmp directory. Subsequent invocations processed by the same instance of your function can reuse these resources. This saves execution time and cost. To avoid potential data leaks across invocations, dont use the execution context to store user data, events, or other information with security implications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use environment variables to pass operational parameters</strong> - This is one of the suggested best practices for Lambda. By using environment variables to pass operational parameters you can avoid hard-coding useful information. But, this is not the right answer for the current use-case, since it talks about reusing context.</p>\n\n<p><strong>Assign more RAM to the function</strong> - Increasing RAM will help speed up the process. But, in the current question, the reviewer has specifically mentioned about reusing context. Hence, this is not the right answer.</p>\n\n<p><strong>Enable X-Ray integration</strong> - You can use AWS X-Ray to visualize the components of your application, identify performance bottlenecks, and troubleshoot requests that resulted in an error. Your Lambda functions send trace data to X-Ray, and X-Ray processes the data to generate a service map and searchable trace summaries. This is a useful tool for troubleshooting. But, for the current use-case, we already know the bottleneck that needs to be fixed and that is the context reuse.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\">https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Move the Amazon S3 client initialization, out of your function handler</strong> - AWS best practices for Lambda suggest taking advantage of execution context reuse to improve the performance of your functions. Initialize SDK clients and database connections outside of the function handler, and cache static assets locally in the /tmp directory. Subsequent invocations processed by the same instance of your function can reuse these resources. This saves execution time and cost. To avoid potential data leaks across invocations, dont use the execution context to store user data, events, or other information with security implications."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use environment variables to pass operational parameters</strong> - This is one of the suggested best practices for Lambda. By using environment variables to pass operational parameters you can avoid hard-coding useful information. But, this is not the right answer for the current use-case, since it talks about reusing context."
      },
      {
        "answer": "",
        "explanation": "<strong>Assign more RAM to the function</strong> - Increasing RAM will help speed up the process. But, in the current question, the reviewer has specifically mentioned about reusing context. Hence, this is not the right answer."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable X-Ray integration</strong> - You can use AWS X-Ray to visualize the components of your application, identify performance bottlenecks, and troubleshoot requests that resulted in an error. Your Lambda functions send trace data to X-Ray, and X-Ray processes the data to generate a service map and searchable trace summaries. This is a useful tool for troubleshooting. But, for the current use-case, we already know the bottleneck that needs to be fixed and that is the context reuse."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html"
    ]
  },
  {
    "id": 54,
    "question": "<p>Your company has a three-year contract with a healthcare provider. The contract states that monthly database backups must be retained for the duration of the contract for compliance purposes. Currently, the limit on backup retention for automated backups, on Amazon Relational Database Service (RDS), does not meet your requirements.</p>\n\n<p>Which of the following solutions can help you meet your requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable RDS automatic backups</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable RDS Read replicas</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable RDS Multi-AZ</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a cron event in CloudWatch, which triggers an AWS Lambda function that triggers the database snapshot</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a cron event in CloudWatch, which triggers an AWS Lambda function that triggers the database snapshot</strong> - There are multiple ways to run periodic jobs in AWS. CloudWatch Events with Lambda is the simplest of all solutions. To do this, create a CloudWatch Rule and select Schedule as the Event Source. You can either use a cron expression or provide a fixed rate (such as every 5 minutes). Next, select Lambda Function as the Target. Your Lambda will have the necessary code for snapshot functionality.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable RDS automatic backups</strong> - You can enable automatic backups but as of 2020, the retention period is 0 to 35 days.</p>\n\n<p><strong>Enable RDS Read replicas</strong> - Amazon RDS server's built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the read replica. Read replicas are useful for heavy read-only data workloads. These are not suitable for the given use-case.</p>\n\n<p><strong>Enable RDS Multi-AZ</strong> - Multi-AZ allows you to create a highly available application with RDS. It does not directly help in database backups or retention periods.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a cron event in CloudWatch, which triggers an AWS Lambda function that triggers the database snapshot</strong> - There are multiple ways to run periodic jobs in AWS. CloudWatch Events with Lambda is the simplest of all solutions. To do this, create a CloudWatch Rule and select Schedule as the Event Source. You can either use a cron expression or provide a fixed rate (such as every 5 minutes). Next, select Lambda Function as the Target. Your Lambda will have the necessary code for snapshot functionality."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable RDS automatic backups</strong> - You can enable automatic backups but as of 2020, the retention period is 0 to 35 days."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable RDS Read replicas</strong> - Amazon RDS server's built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the read replica. Read replicas are useful for heavy read-only data workloads. These are not suitable for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable RDS Multi-AZ</strong> - Multi-AZ allows you to create a highly available application with RDS. It does not directly help in database backups or retention periods."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html"
    ]
  },
  {
    "id": 55,
    "question": "<p>As a Senior Developer, you manage 10 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for PostgreSQL. You need to make this architecture resilient for disaster recovery.</p>\n\n<p>Which of the following features will help you prepare for database disaster recovery? (Select two)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable the automated backup feature of Amazon RDS  in a multi-AZ deployment that creates backups across multiple Regions</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use cross-Region Read Replicas</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Use database cloning feature of the RDS DB cluster</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use cross-Region Read Replicas</strong></p>\n\n<p>In addition to using Read Replicas to reduce the load on your source DB instance, you can also use Read Replicas to implement a DR solution for your production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in a different Region than the source database. Using a cross-Region Read Replica can help ensure that you get back up and running if you experience a regional availability issue.</p>\n\n<p><strong>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region</strong></p>\n\n<p>Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology.</p>\n\n<p>The automated backup feature of Amazon RDS enables point-in-time recovery for your database instance. Amazon RDS will backup your database and transaction logs and store both for a user-specified retention period. If its a Multi-AZ configuration, backups occur on the standby to reduce I/O impact on the primary. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable the automated backup feature of Amazon RDS  in a multi-AZ deployment that creates backups across multiple Regions</strong> - This is an incorrect statement. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions.</p>\n\n<p><strong>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</strong> - Amazon RDS Provisioned IOPS Storage is an SSD-backed storage option designed to deliver fast, predictable, and consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a disaster recovery option.</p>\n\n<p><strong>Use database cloning feature of the RDS DB cluster</strong> - This option has been added as a distractor. Database cloning is only available for Aurora and not for RDS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/\">https://aws.amazon.com/rds/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use cross-Region Read Replicas</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "In addition to using Read Replicas to reduce the load on your source DB instance, you can also use Read Replicas to implement a DR solution for your production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in a different Region than the source database. Using a cross-Region Read Replica can help ensure that you get back up and running if you experience a regional availability issue."
      },
      {
        "answer": "<strong>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology."
      },
      {
        "answer": "",
        "explanation": "The automated backup feature of Amazon RDS enables point-in-time recovery for your database instance. Amazon RDS will backup your database and transaction logs and store both for a user-specified retention period. If its a Multi-AZ configuration, backups occur on the standby to reduce I/O impact on the primary. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable the automated backup feature of Amazon RDS  in a multi-AZ deployment that creates backups across multiple Regions</strong> - This is an incorrect statement. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions."
      },
      {
        "answer": "",
        "explanation": "<strong>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</strong> - Amazon RDS Provisioned IOPS Storage is an SSD-backed storage option designed to deliver fast, predictable, and consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a disaster recovery option."
      },
      {
        "answer": "",
        "explanation": "<strong>Use database cloning feature of the RDS DB cluster</strong> - This option has been added as a distractor. Database cloning is only available for Aurora and not for RDS."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/features/",
      "https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/"
    ]
  },
  {
    "id": 56,
    "question": "<p>You are running a cloud file storage website with an Internet-facing Application Load Balancer, which routes requests from users over the internet to 10 registered Amazon EC2 instances. Users are complaining that your website always asks them to re-authenticate when they switch pages. You are puzzled because this behavior is not seen in your local machine or dev environment.</p>\n\n<p>What could be the reason?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Application Load Balancer is in slow-start mode, which gives ALB a little more time to read and write session data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The EC2 instances are logging out the users because the instances never have access to the client IPs because of the Load Balancer</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The Load Balancer does not have TLS enabled</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The Load Balancer does not have stickiness enabled</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The Load Balancer does not have stickiness enabled</strong> - Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies.</p>\n\n<p>When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target.</p>\n\n<p>More info here:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q58-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q58-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Application Load Balancer is in slow-start mode, which gives ALB a little more time to read and write session data</strong> - This is an invalid statement. The load balancer serves as a single point of contact for clients and distributes incoming traffic across its healthy registered targets. By default, a target starts to receive its full share of requests as soon as it is registered with a target group and passes an initial health check. Using slow start mode gives targets time to warm up before the load balancer sends them a full share of requests. This does not help in session management.</p>\n\n<p><strong>The EC2 instances are logging out the users because the instances never have access to the client IPs because of the Load Balancer</strong> - This is an incorrect statement. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to the server. If needed, the server can read IP addresses from this data.</p>\n\n<p><strong>The Load Balancer does not have TLS enabled</strong> - To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. The load balancer uses a server certificate to terminate the front-end connection and then decrypt requests from clients before sending them to the targets. This does not help in session management.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#slow-start-mode\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#slow-start-mode</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The Load Balancer does not have stickiness enabled</strong> - Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies."
      },
      {
        "answer": "",
        "explanation": "When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q58-i1.jpg",
        "answer": "",
        "explanation": "More info here:"
      },
      {
        "link": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Application Load Balancer is in slow-start mode, which gives ALB a little more time to read and write session data</strong> - This is an invalid statement. The load balancer serves as a single point of contact for clients and distributes incoming traffic across its healthy registered targets. By default, a target starts to receive its full share of requests as soon as it is registered with a target group and passes an initial health check. Using slow start mode gives targets time to warm up before the load balancer sends them a full share of requests. This does not help in session management."
      },
      {
        "answer": "",
        "explanation": "<strong>The EC2 instances are logging out the users because the instances never have access to the client IPs because of the Load Balancer</strong> - This is an incorrect statement. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to the server. If needed, the server can read IP addresses from this data."
      },
      {
        "answer": "",
        "explanation": "<strong>The Load Balancer does not have TLS enabled</strong> - To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. The load balancer uses a server certificate to terminate the front-end connection and then decrypt requests from clients before sending them to the targets. This does not help in session management."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#slow-start-mode"
    ]
  },
  {
    "id": 57,
    "question": "<p>A team is checking the viability of using AWS Step Functions for creating a banking workflow for loan approvals. The web application will also have human approval as one of the steps in the workflow.</p>\n\n<p>As a developer associate, which of the following would you identify as the key characteristics for AWS Step Function? (Select two)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of 180 days or 6 months</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that do not support any human approval steps</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Both Standard and Express Workflows support all service integrations, activities, and design patterns</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that can also support any human approval steps</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>You should use Express Workflows for workloads with high event rates and short duration</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that can also support any human approval steps</strong> - Standard Workflows on AWS Step Functions are more suitable for long-running, durable, and auditable workflows where repeating workflow steps is expensive (e.g., restarting a long-running media transcode) or harmful (e.g., charging a credit card twice). Example workloads include training and deploying machine learning models, report generation, billing, credit card processing, and ordering and fulfillment processes. Step functions also support any human approval steps.</p>\n\n<p><em>You should use Express Workflows for workloads with high event rates and short duration</em>* - You should use Express Workflows for workloads with high event rates and short durations. Express Workflows support event rates of more than 100,000 per second.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that do not support any human approval steps</strong> - As Step functions support any human approval steps, so this option is incorrect.</p>\n\n<p><strong>Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of 180 days or 6 months</strong> - Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of one year.</p>\n\n<p><strong>Both Standard and Express Workflows support all service integrations, activities, and design patterns</strong> - Standard Workflows support all service integrations, activities, and design patterns. Express Workflows do not support activities, job-run (.sync), and Callback patterns.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/features/\">https://aws.amazon.com/step-functions/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/implementing-serverless-manual-approval-steps-in-aws-step-functions-and-amazon-api-gateway/\">https://aws.amazon.com/blogs/compute/implementing-serverless-manual-approval-steps-in-aws-step-functions-and-amazon-api-gateway/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that can also support any human approval steps</strong> - Standard Workflows on AWS Step Functions are more suitable for long-running, durable, and auditable workflows where repeating workflow steps is expensive (e.g., restarting a long-running media transcode) or harmful (e.g., charging a credit card twice). Example workloads include training and deploying machine learning models, report generation, billing, credit card processing, and ordering and fulfillment processes. Step functions also support any human approval steps."
      },
      {
        "answer": "",
        "explanation": "<em>You should use Express Workflows for workloads with high event rates and short duration</em>* - You should use Express Workflows for workloads with high event rates and short durations. Express Workflows support event rates of more than 100,000 per second."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Standard Workflows on AWS Step Functions are suitable for long-running, durable, and auditable workflows that do not support any human approval steps</strong> - As Step functions support any human approval steps, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of 180 days or 6 months</strong> - Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of one year."
      },
      {
        "answer": "",
        "explanation": "<strong>Both Standard and Express Workflows support all service integrations, activities, and design patterns</strong> - Standard Workflows support all service integrations, activities, and design patterns. Express Workflows do not support activities, job-run (.sync), and Callback patterns."
      }
    ],
    "references": [
      "https://aws.amazon.com/step-functions/features/",
      "https://aws.amazon.com/blogs/compute/implementing-serverless-manual-approval-steps-in-aws-step-functions-and-amazon-api-gateway/"
    ]
  },
  {
    "id": 58,
    "question": "<p>An e-commerce company has an order processing workflow with several tasks to be done in parallel as well as decision steps to be evaluated for successful processing of the order. All the tasks are implemented via Lambda functions.</p>\n\n<p>Which of the following is the BEST solution to meet these business requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Glue to orchestrate the workflow</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Step Functions state machines to orchestrate the workflow</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Step Functions activities to orchestrate the workflow</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Batch to orchestrate the workflow</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Step Functions state machines to orchestrate the workflow</strong></p>\n\n<p>AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly.</p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a><p></p>\n\n<p>The following are key features of AWS Step Functions:</p>\n\n<p>Step Functions are based on the concepts of tasks and state machines. You define state machines using the JSON-based Amazon States Language. A state machine is defined by the states it contains and the relationships between them. States are elements in your state machine. Individual states can make decisions based on their input, perform actions, and pass output to other states. In this way, a state machine can orchestrate workflows.</p>\n\n<p>Please see this note for a simple example of a State Machine:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q8-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q8-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Step Functions activities to orchestrate the workflow</strong> - In AWS Step Functions, activities are a way to associate code running somewhere (known as an activity worker) with a specific task in a state machine. When a Step Function reaches an activity task state, the workflow waits for an activity worker to poll for a task. For example, an activity worker can be an application running on an Amazon EC2 instance or an AWS Lambda function. AWS Step Functions activities cannot orchestrate a workflow.</p>\n\n<p><strong>Use AWS Glue to orchestrate the workflow</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue cannot orchestrate a workflow.</p>\n\n<p><strong>Use AWS Batch to orchestrate the workflow</strong> - AWS Batch runs batch computing jobs on the AWS Cloud. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch cannot orchestrate a workflow.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use AWS Step Functions state machines to orchestrate the workflow</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly."
      },
      {
        "image": "https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png",
        "answer": "",
        "explanation": "How Step Functions Work:"
      },
      {
        "link": "https://aws.amazon.com/step-functions/"
      },
      {
        "answer": "",
        "explanation": "The following are key features of AWS Step Functions:"
      },
      {
        "answer": "",
        "explanation": "Step Functions are based on the concepts of tasks and state machines. You define state machines using the JSON-based Amazon States Language. A state machine is defined by the states it contains and the relationships between them. States are elements in your state machine. Individual states can make decisions based on their input, perform actions, and pass output to other states. In this way, a state machine can orchestrate workflows."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q8-i1.jpg",
        "answer": "",
        "explanation": "Please see this note for a simple example of a State Machine:"
      },
      {
        "link": "https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Step Functions activities to orchestrate the workflow</strong> - In AWS Step Functions, activities are a way to associate code running somewhere (known as an activity worker) with a specific task in a state machine. When a Step Function reaches an activity task state, the workflow waits for an activity worker to poll for a task. For example, an activity worker can be an application running on an Amazon EC2 instance or an AWS Lambda function. AWS Step Functions activities cannot orchestrate a workflow."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue to orchestrate the workflow</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue cannot orchestrate a workflow."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Batch to orchestrate the workflow</strong> - AWS Batch runs batch computing jobs on the AWS Cloud. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch cannot orchestrate a workflow."
      }
    ],
    "references": [
      "https://aws.amazon.com/step-functions/",
      "https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html"
    ]
  },
  {
    "id": 59,
    "question": "<p>A developer wants to enable X-Ray tracing on an on-premises Linux server running a custom application that is accessed through Amazon API Gateway.</p>\n\n<p>What is the most efficient solution that requires minimal configuration?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Install and run the CloudWatch Unified Agent on the on-premises servers to capture and relay the X-Ray data to the X-Ray service using the PutTraceSegments API call</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure a Lambda function to analyze the incoming traffic data on the on-premises servers and then relay the X-Ray data to the X-Ray service using the PutTelemetryRecords API call</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Install and run the X-Ray SDK on the on-premises servers to capture and relay the data to the X-Ray service</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service</strong></p>\n\n<p>The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service.</p>\n\n<p>To run the X-Ray daemon locally, on-premises, or on other AWS services, download it, run it, and then give it permission to upload segment documents to X-Ray.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Install and run the X-Ray SDK on the on-premises servers to capture and relay the data to the X-Ray service</strong> - As mentioned above, you need to run the X-Ray daemon on the on-premises servers and give it the required permission to upload X-Ray data to the X-Ray service. So this option is incorrect.</p>\n\n<p><strong>Install and run the CloudWatch Unified Agent on the on-premises servers to capture and relay the X-Ray data to the X-Ray service using the PutTraceSegments API call</strong> - This option has been added as a distractor. CloudWatch Agent cannot relay X-Ray data to the X-Ray service using the PutTraceSegments API call.</p>\n\n<p><strong>Configure a Lambda function to analyze the incoming traffic data on the on-premises servers and then relay the X-Ray data to the X-Ray service using the PutTelemetryRecords API call</strong> - This option is incorrect as the Lambda function cannot process the X-Ray data for an on-premises instance and then relay it to the X-Ray service.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service."
      },
      {
        "answer": "",
        "explanation": "To run the X-Ray daemon locally, on-premises, or on other AWS services, download it, run it, and then give it permission to upload segment documents to X-Ray."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Install and run the X-Ray SDK on the on-premises servers to capture and relay the data to the X-Ray service</strong> - As mentioned above, you need to run the X-Ray daemon on the on-premises servers and give it the required permission to upload X-Ray data to the X-Ray service. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Install and run the CloudWatch Unified Agent on the on-premises servers to capture and relay the X-Ray data to the X-Ray service using the PutTraceSegments API call</strong> - This option has been added as a distractor. CloudWatch Agent cannot relay X-Ray data to the X-Ray service using the PutTraceSegments API call."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a Lambda function to analyze the incoming traffic data on the on-premises servers and then relay the X-Ray data to the X-Ray service using the PutTelemetryRecords API call</strong> - This option is incorrect as the Lambda function cannot process the X-Ray data for an on-premises instance and then relay it to the X-Ray service."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html"
    ]
  },
  {
    "id": 60,
    "question": "<p>A company has more than 100 million members worldwide enjoying 125 million hours of TV shows and movies each day. The company uses AWS for nearly all its computing and storage needs, which use more than 10,000 server instances on AWS. This results in an extremely complex and dynamic networking environment where applications are constantly communicating inside AWS and across the Internet. Monitoring and optimizing its network is critical for the company.</p>\n\n<p>The company needs a solution for ingesting and analyzing the multiple terabytes of real-time data its network generates daily in the form of flow logs. Which technology/service should the company use to ingest this data economically and has the flexibility to direct this data to other downstream systems?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Kinesis Data Streams</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon Kinesis Firehose</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Glue</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Simple Queue Service (SQS)</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Queue Service (SQS)</strong> - Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. AWS recommends using Amazon SQS for cases where individual message fail/success are important, message delays are needed and there is only one consumer for the messages received (if more than one consumers need to consume the message, then AWS suggests configuring more queues).</p>\n\n<p><strong>Amazon Kinesis Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards youre already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration.</p>\n\n<p>Kinesis data streams is highly customizable and best suited for developers building custom applications or streaming data for specialized needs. Data Streams also provide greater flexibility in integrating downstream applications than Firehose. Data Streams is also a cost-effective option compared to Firehose. Therefore, KDS is the right solution.</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue provides all of the capabilities needed for data integration so that you can start analyzing your data and putting it to use in minutes instead of months. Glue is not best suited to handle real-time data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Amazon Kinesis Data Streams</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more."
      },
      {
        "answer": "",
        "explanation": "Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering)."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Queue Service (SQS)</strong> - Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. AWS recommends using Amazon SQS for cases where individual message fail/success are important, message delays are needed and there is only one consumer for the messages received (if more than one consumers need to consume the message, then AWS suggests configuring more queues)."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards youre already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration."
      },
      {
        "answer": "",
        "explanation": "Kinesis data streams is highly customizable and best suited for developers building custom applications or streaming data for specialized needs. Data Streams also provide greater flexibility in integrating downstream applications than Firehose. Data Streams is also a cost-effective option compared to Firehose. Therefore, KDS is the right solution."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Glue</strong> - AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue provides all of the capabilities needed for data integration so that you can start analyzing your data and putting it to use in minutes instead of months. Glue is not best suited to handle real-time data."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/sqs/",
      "https://aws.amazon.com/kinesis/data-firehose/"
    ]
  },
  {
    "id": 61,
    "question": "<p>A company wants to automate and orchestrate a multi-source high-volume flow of data in a scalable data management solution built using AWS services. The solution must ensure that the business rules and transformations run in sequence, handle reprocessing of data in case of errors, and require minimal maintenance.</p>\n\n<p>Which AWS service should the company use to manage and automate the orchestration of the data flows?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Step Functions</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>AWS Glue</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Kinesis Data Streams</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Batch</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Step Functions</strong></p>\n\n<p>AWS Step Functions is a visual workflow service that helps developers use AWS services to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines.</p>\n\n<p><img src=\"https://d1.awsstatic.com/video-thumbs/Step-Functions/AWS_Step_Functions_HIW.bc3d2930f00dd0401269367b8e8617a7dba5915c.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/video-thumbs/Step-Functions/AWS_Step_Functions_HIW.bc3d2930f00dd0401269367b8e8617a7dba5915c.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams is a serverless streaming data service that makes it easy to capture, process, and store data streams at any scale.</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue is a serverless data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning (ML), and application development.</p>\n\n<p><strong>AWS Batch</strong> - AWS Batch is a set of batch management capabilities that enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p>\n\n<p><a href=\"https://aws.amazon.com/batch/faqs/\">https://aws.amazon.com/batch/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>AWS Step Functions</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Step Functions is a visual workflow service that helps developers use AWS services to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines."
      },
      {
        "link": "https://aws.amazon.com/step-functions/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams is a serverless streaming data service that makes it easy to capture, process, and store data streams at any scale."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Glue</strong> - AWS Glue is a serverless data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning (ML), and application development."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Batch</strong> - AWS Batch is a set of batch management capabilities that enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted."
      }
    ],
    "references": [
      "https://aws.amazon.com/step-functions/",
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/glue/",
      "https://aws.amazon.com/batch/faqs/"
    ]
  },
  {
    "id": 62,
    "question": "<p>A developer wants to integrate user-specific file upload and download features in an application that uses both Amazon Cognito user pools and Cognito identity pools for secure access with Amazon S3. The developer also wants to ensure that only authorized users can access their own files and that the files are securely saved and retrieved. The files are 5 KB to 500 MB in size.</p>\n\n<p>What do you recommend as the most efficient solution?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use S3 Event Notifications to trigger a Lambda function that validates that the given file is uploaded and downloaded only by the authorized user</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Leverage an IAM policy with the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use CloudFront Lambda@Edge to validate that the given file is uploaded to S3 and downloaded from S3 only by the authorized user</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Integrate Amazon API Gateway with a Lambda function that validates that the given file is uploaded to S3 and downloaded from S3 only by the authorized user</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage an IAM policy with the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3</strong></p>\n\n<p>Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. Amazon Cognito identity pools support the following identity providers:</p>\n\n<p>Public providers: Login with Amazon (identity pools), Facebook (identity pools), Google (identity pools), Sign in with Apple (identity pools).</p>\n\n<p>Amazon Cognito user pools</p>\n\n<p>OpenID Connect providers (identity pools)</p>\n\n<p>SAML identity providers (identity pools)</p>\n\n<p>Developer authenticated identities (identity pools)</p>\n\n<p>You can create an identity-based policy that allows Amazon Cognito users to access objects in a specific S3 bucket. This policy allows access only to objects with a name that includes Cognito, the name of the application, and the federated user's ID, represented by the ${cognito-identity.amazonaws.com:sub} variable.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q26-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q26-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use S3 Event Notifications to trigger a Lambda function that validates that the given file is uploaded and downloaded only by the authorized user</strong> - While it is certainly possible to build this solution, however, it is not the most optimal solution as it does not prevent an invalid upload of a file into another user's designated folder. So this option is incorrect.</p>\n\n<p><strong>Integrate Amazon API Gateway with a Lambda function that validates that the given file is uploaded to S3 and downloaded from S3 only by the authorized user</strong> - Again, it is certainly possible to build this solution, however, it is not the most optimal solution as it does not prevent an invalid upload of a file into another user's designated folder. So this option is incorrect.</p>\n\n<p><strong>Use CloudFront Lambda@Edge to validate that the given file is uploaded to S3 and downloaded from S3 only by the authorized user</strong> - This option assumes that the solution comprises a CloudFront distribution. This introduces inefficiency in the solution, as one needs to pay for CloudFront/Lambda@Edge and adds unnecessary hops in the data flow for both uploads and downloads.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-integrating-user-pools-with-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-integrating-user-pools-with-identity-pools.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Leverage an IAM policy with the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. Amazon Cognito identity pools support the following identity providers:"
      },
      {
        "answer": "",
        "explanation": "Public providers: Login with Amazon (identity pools), Facebook (identity pools), Google (identity pools), Sign in with Apple (identity pools)."
      },
      {
        "answer": "",
        "explanation": "Amazon Cognito user pools"
      },
      {
        "answer": "",
        "explanation": "OpenID Connect providers (identity pools)"
      },
      {
        "answer": "",
        "explanation": "SAML identity providers (identity pools)"
      },
      {
        "answer": "",
        "explanation": "Developer authenticated identities (identity pools)"
      },
      {
        "answer": "",
        "explanation": "You can create an identity-based policy that allows Amazon Cognito users to access objects in a specific S3 bucket. This policy allows access only to objects with a name that includes Cognito, the name of the application, and the federated user's ID, represented by the ${cognito-identity.amazonaws.com:sub} variable."
      },
      {
        "link": "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use S3 Event Notifications to trigger a Lambda function that validates that the given file is uploaded and downloaded only by the authorized user</strong> - While it is certainly possible to build this solution, however, it is not the most optimal solution as it does not prevent an invalid upload of a file into another user's designated folder. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Integrate Amazon API Gateway with a Lambda function that validates that the given file is uploaded to S3 and downloaded from S3 only by the authorized user</strong> - Again, it is certainly possible to build this solution, however, it is not the most optimal solution as it does not prevent an invalid upload of a file into another user's designated folder. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use CloudFront Lambda@Edge to validate that the given file is uploaded to S3 and downloaded from S3 only by the authorized user</strong> - This option assumes that the solution comprises a CloudFront distribution. This introduces inefficiency in the solution, as one needs to pay for CloudFront/Lambda@Edge and adds unnecessary hops in the data flow for both uploads and downloads."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-integrating-user-pools-with-identity-pools.html"
    ]
  },
  {
    "id": 63,
    "question": "<p>Your web application reads and writes data to your DynamoDB table. The table is provisioned with 400 Write Capacity Units (WCUs) shared across 4 partitions. One of the partitions receives 250 WCU/second while others receive much less. You receive the error 'ProvisionedThroughputExceededException'.</p>\n\n<p>What is the likely cause of this error?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>CloudWatch monitoring is lagging</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>You have a hot partition</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configured IAM policy is wrong</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Write Capacity Units (WCUs) are applied across to all your DynamoDB tables and this needs reconfiguration</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>You have a hot partition</strong></p>\n\n<p>It's not always possible to distribute read and write activity evenly. When data access is imbalanced, a \"hot\" partition can receive a higher volume of read and write traffic compared to other partitions.\nTo better accommodate uneven access patterns, DynamoDB adaptive capacity enables your application to continue reading and writing to hot partitions without being throttled, provided that traffic does not exceed your tables total provisioned capacity or the partition maximum capacity.</p>\n\n<p>ProvisionedThroughputExceededException explained:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q41-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q41-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a><p></p>\n\n<p>Hot partition explained:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q41-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q41-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudWatch monitoring is lagging</strong> - The error is specific to DynamoDB itself and not to any connected service. CloudWatch is a fully managed service from AWS and does not result in throttling.</p>\n\n<p><strong>Configured IAM policy is wrong</strong> - The error is not associated with authorization but to exceeding something pre-configured value. So, it's clearly not a permissions issue.</p>\n\n<p><strong>Write-capacity units (WCUs) are applied across to all your DynamoDB tables and this needs reconfiguration</strong> - This statement is incorrect. Read Capacity Units and Write Capacity Units are specific to one table.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>You have a hot partition</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "It's not always possible to distribute read and write activity evenly. When data access is imbalanced, a \"hot\" partition can receive a higher volume of read and write traffic compared to other partitions.\nTo better accommodate uneven access patterns, DynamoDB adaptive capacity enables your application to continue reading and writing to hot partitions without being throttled, provided that traffic does not exceed your tables total provisioned capacity or the partition maximum capacity."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q41-i1.jpg",
        "answer": "",
        "explanation": "ProvisionedThroughputExceededException explained:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q41-i2.jpg",
        "answer": "",
        "explanation": "Hot partition explained:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>CloudWatch monitoring is lagging</strong> - The error is specific to DynamoDB itself and not to any connected service. CloudWatch is a fully managed service from AWS and does not result in throttling."
      },
      {
        "answer": "",
        "explanation": "<strong>Configured IAM policy is wrong</strong> - The error is not associated with authorization but to exceeding something pre-configured value. So, it's clearly not a permissions issue."
      },
      {
        "answer": "",
        "explanation": "<strong>Write-capacity units (WCUs) are applied across to all your DynamoDB tables and this needs reconfiguration</strong> - This statement is incorrect. Read Capacity Units and Write Capacity Units are specific to one table."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html"
    ]
  },
  {
    "id": 64,
    "question": "<p>A telecom service provider stores its critical customer data on Amazon Simple Storage Service (Amazon S3).</p>\n\n<p>Which of the following options can be used to control access to data stored on Amazon S3? (Select two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Permissions boundaries, Identity and Access Management (IAM) policies</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Query String Authentication, Access Control Lists (ACLs)</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Bucket policies, Identity and Access Management (IAM) policies</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>IAM database authentication, Bucket policies</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Query String Authentication, Permissions boundaries</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Security",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Bucket policies, Identity and Access Management (IAM) policies</strong></p>\n\n<p><strong>Query String Authentication, Access Control Lists (ACLs)</strong></p>\n\n<p>Customers may use four mechanisms for controlling access to Amazon S3 resources: Identity and Access Management (IAM) policies, bucket policies, Access Control Lists (ACLs), and Query String Authentication.</p>\n\n<p>IAM enables organizations with multiple employees to create and manage multiple users under a single AWS account. With IAM policies, customers can grant IAM users fine-grained control to their Amazon S3 bucket or objects while also retaining full control over everything the users do.</p>\n\n<p>With bucket policies, customers can define rules which apply broadly across all requests to their Amazon S3 resources, such as granting write privileges to a subset of Amazon S3 resources. Customers can also restrict access based on an aspect of the request, such as HTTP referrer and IP address.</p>\n\n<p>With ACLs, customers can grant specific permissions (i.e. READ, WRITE, FULL_CONTROL) to specific users for an individual bucket or object.</p>\n\n<p>With Query String Authentication, customers can create a URL to an Amazon S3 object which is only valid for a limited time. Using query parameters to authenticate requests is useful when you want to express a request entirely in a URL. This method is also referred as presigning a URL.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Permissions boundaries, Identity and Access Management (IAM) policies</strong></p>\n\n<p><strong>Query String Authentication, Permissions boundaries</strong></p>\n\n<p><strong>IAM database authentication, Bucket policies</strong></p>\n\n<p>Permissions boundary - A Permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. When you use a policy to set the permissions boundary for a user, it limits the user's permissions but does not provide permissions on its own.</p>\n\n<p>IAM database authentication - IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. It is a database authentication technique and cannot be used to authenticate for S3.</p>\n\n<p>Therefore, all three options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-overview.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Bucket policies, Identity and Access Management (IAM) policies</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Query String Authentication, Access Control Lists (ACLs)</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Customers may use four mechanisms for controlling access to Amazon S3 resources: Identity and Access Management (IAM) policies, bucket policies, Access Control Lists (ACLs), and Query String Authentication."
      },
      {
        "answer": "",
        "explanation": "IAM enables organizations with multiple employees to create and manage multiple users under a single AWS account. With IAM policies, customers can grant IAM users fine-grained control to their Amazon S3 bucket or objects while also retaining full control over everything the users do."
      },
      {
        "answer": "",
        "explanation": "With bucket policies, customers can define rules which apply broadly across all requests to their Amazon S3 resources, such as granting write privileges to a subset of Amazon S3 resources. Customers can also restrict access based on an aspect of the request, such as HTTP referrer and IP address."
      },
      {
        "answer": "",
        "explanation": "With ACLs, customers can grant specific permissions (i.e. READ, WRITE, FULL_CONTROL) to specific users for an individual bucket or object."
      },
      {
        "answer": "",
        "explanation": "With Query String Authentication, customers can create a URL to an Amazon S3 object which is only valid for a limited time. Using query parameters to authenticate requests is useful when you want to express a request entirely in a URL. This method is also referred as presigning a URL."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Permissions boundaries, Identity and Access Management (IAM) policies</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Query String Authentication, Permissions boundaries</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>IAM database authentication, Bucket policies</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Permissions boundary - A Permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. When you use a policy to set the permissions boundary for a user, it limits the user's permissions but does not provide permissions on its own."
      },
      {
        "answer": "",
        "explanation": "IAM database authentication - IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. It is a database authentication technique and cannot be used to authenticate for S3."
      },
      {
        "answer": "",
        "explanation": "Therefore, all three options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-overview.html",
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html"
    ]
  },
  {
    "id": 65,
    "question": "<p>A financial services company wants to ensure that the customer data is always kept encrypted on Amazon S3 but wants an AWS managed solution that allows full control to create, rotate and remove the encryption keys.</p>\n\n<p>As a Developer Associate, which of the following would you recommend to address the given use-case?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Server-Side Encryption with Secrets Manager</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Server-Side Encryption with Customer-Provided Keys (SSE-C)</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</strong></p>\n\n<p>You have the following options for protecting data at rest in Amazon S3:</p>\n\n<p>Server-Side Encryption  Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p>\n\n<p>Client-Side Encryption  Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>When you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed CMK, or you can specify a customer-managed CMK that you have already created.</p>\n\n<p>Creating your own customer-managed CMK gives you more flexibility and control over the CMK. For example, you can create, rotate, and disable customer-managed CMKs. You can also define access controls and audit the customer-managed CMKs that you use to protect your data.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q7-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q7-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, AWS encrypts the key itself with a master key that it regularly rotates. So this option is incorrect for the given use-case.</p>\n\n<p><strong>Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you will need to create the encryption keys as well as manage the corresponding process to rotate and remove the encryption keys. Amazon S3 manages the data encryption, as it writes to disks, as well as the data decryption when you access your objects. So this option is incorrect for the given use-case.</p>\n\n<p><strong>Server-Side Encryption with Secrets Manager</strong> - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. You cannot combine Server-Side Encryption with Secrets Manager to create, rotate, or disable the encryption keys.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You have the following options for protecting data at rest in Amazon S3:"
      },
      {
        "answer": "",
        "explanation": "Server-Side Encryption  Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects."
      },
      {
        "answer": "",
        "explanation": "Client-Side Encryption  Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools."
      },
      {
        "answer": "",
        "explanation": "When you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed CMK, or you can specify a customer-managed CMK that you have already created."
      },
      {
        "answer": "",
        "explanation": "Creating your own customer-managed CMK gives you more flexibility and control over the CMK. For example, you can create, rotate, and disable customer-managed CMKs. You can also define access controls and audit the customer-managed CMKs that you use to protect your data."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q7-i1.jpg",
        "answer": "",
        "explanation": "Please see this note for more details:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, AWS encrypts the key itself with a master key that it regularly rotates. So this option is incorrect for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you will need to create the encryption keys as well as manage the corresponding process to rotate and remove the encryption keys. Amazon S3 manages the data encryption, as it writes to disks, as well as the data decryption when you access your objects. So this option is incorrect for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Server-Side Encryption with Secrets Manager</strong> - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. You cannot combine Server-Side Encryption with Secrets Manager to create, rotate, or disable the encryption keys."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html"
    ]
  }
]